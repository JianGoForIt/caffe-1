Dec 26 08:55:27 2016 94294 3 10.1 NIOS_DEBUG: stdin_fd set to -1
Dec 26 08:55:27 2016 94294 3 10.1 NIOS_DEBUG: fds[0] has a value of -1
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 08:55:30.554322 91127 mpiutil.cpp:166] Process rank 6 from number of 9 processes running on knl-node051
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 08:55:27.292984 90165 mpiutil.cpp:166] Process rank 1 from number of 9 processes running on knl-node048
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 08:55:30.563724 94304 mpiutil.cpp:166] Process rank 0 from number of 9 processes running on knl-node079
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 08:55:30.562135 95005 mpiutil.cpp:166] Process rank 5 from number of 9 processes running on knl-node078
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 08:55:30.560186 92150 mpiutil.cpp:166] Process rank 2 from number of 9 processes running on knl-node015
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 08:55:30.558873 86102 mpiutil.cpp:166] Process rank 3 from number of 9 processes running on knl-node070
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 08:55:30.550992 85894 mpiutil.cpp:166] Process rank 8 from number of 9 processes running on knl-node081
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 08:55:30.551206 119888 mpiutil.cpp:166] Process rank 4 from number of 9 processes running on knl-node035
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 08:55:30.556356 89709 mpiutil.cpp:166] Process rank 7 from number of 9 processes running on knl-node006
I1226 08:55:30.575487 95005 caffe.cpp:316] Use CPU.
I1226 08:55:30.568506 91127 caffe.cpp:316] Use CPU.
I1226 08:55:27.307603 90165 caffe.cpp:316] Use CPU.
I1226 08:55:30.573665 86102 caffe.cpp:316] Use CPU.
I1226 08:55:30.565835 85894 caffe.cpp:316] Use CPU.
I1226 08:55:30.576783 95005 solver.cpp:105] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_6.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 08:55:30.569612 91127 solver.cpp:105] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_7.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 08:55:30.570572 91127 solver.cpp:140] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_7.prototxt
I1226 08:55:30.577754 95005 solver.cpp:140] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_6.prototxt
I1226 08:55:27.308703 90165 solver.cpp:105] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_2.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 08:55:27.309540 90165 solver.cpp:140] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_2.prototxt
I1226 08:55:30.574848 86102 solver.cpp:105] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_4.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 08:55:30.567028 85894 solver.cpp:105] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_dummy.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 08:55:30.567826 85894 solver.cpp:140] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_dummy.prototxt
I1226 08:55:30.579151 94304 caffe.cpp:316] Use CPU.
I1226 08:55:30.575712 86102 solver.cpp:140] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_4.prototxt
I1226 08:55:30.580546 94304 solver.cpp:105] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_1.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 08:55:30.577812 92150 caffe.cpp:316] Use CPU.
I1226 08:55:30.581830 94304 solver.cpp:140] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_1.prototxt
I1226 08:55:30.579077 92150 solver.cpp:105] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_3.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 08:55:30.580027 92150 solver.cpp:140] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_3.prototxt
I1226 08:55:30.575752 89709 caffe.cpp:316] Use CPU.
I1226 08:55:30.577100 89709 solver.cpp:105] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_8.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 08:55:30.578184 89709 solver.cpp:140] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_8.prototxt
I1226 08:55:30.574308 119888 caffe.cpp:316] Use CPU.
I1226 08:55:30.575685 119888 solver.cpp:105] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_5.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 08:55:30.576884 119888 solver.cpp:140] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_5.prototxt
I1226 08:55:30.601781 91127 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 08:55:30.601861 91127 cpu_info.cpp:455] Total number of sockets: 1
I1226 08:55:30.601883 91127 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 08:55:30.601904 91127 cpu_info.cpp:461] Total number of processors: 272
I1226 08:55:30.601923 91127 cpu_info.cpp:464] GPU is used: no
I1226 08:55:30.601944 91127 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 08:55:30.601976 91127 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 08:55:30.606744 86102 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 08:55:30.606822 86102 cpu_info.cpp:455] Total number of sockets: 1
I1226 08:55:30.606845 86102 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 08:55:30.606865 86102 cpu_info.cpp:461] Total number of processors: 272
I1226 08:55:30.606885 86102 cpu_info.cpp:464] GPU is used: no
I1226 08:55:30.606906 86102 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 08:55:30.606993 86102 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 08:55:30.599680 85894 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 08:55:30.599759 85894 cpu_info.cpp:455] Total number of sockets: 1
I1226 08:55:30.599783 85894 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 08:55:30.599803 85894 cpu_info.cpp:461] Total number of processors: 272
I1226 08:55:30.599823 85894 cpu_info.cpp:464] GPU is used: no
I1226 08:55:30.599843 85894 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 08:55:30.599865 85894 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 08:55:27.341974 90165 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 08:55:27.342044 90165 cpu_info.cpp:455] Total number of sockets: 1
I1226 08:55:27.342067 90165 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 08:55:27.342088 90165 cpu_info.cpp:461] Total number of processors: 272
I1226 08:55:27.342109 90165 cpu_info.cpp:464] GPU is used: no
I1226 08:55:27.342129 90165 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 08:55:27.342149 90165 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 08:55:30.612177 95005 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 08:55:30.612267 95005 cpu_info.cpp:455] Total number of sockets: 1
I1226 08:55:30.612293 95005 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 08:55:30.612318 95005 cpu_info.cpp:461] Total number of processors: 272
I1226 08:55:30.612339 95005 cpu_info.cpp:464] GPU is used: no
I1226 08:55:30.612361 95005 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 08:55:30.612383 95005 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 08:55:30.612653 92150 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 08:55:30.612730 92150 cpu_info.cpp:455] Total number of sockets: 1
I1226 08:55:30.612756 92150 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 08:55:30.612777 92150 cpu_info.cpp:461] Total number of processors: 272
I1226 08:55:30.612798 92150 cpu_info.cpp:464] GPU is used: no
I1226 08:55:30.612819 92150 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 08:55:30.612841 92150 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 08:55:30.616403 94304 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 08:55:30.616495 94304 cpu_info.cpp:455] Total number of sockets: 1
I1226 08:55:30.616524 94304 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 08:55:30.616547 94304 cpu_info.cpp:461] Total number of processors: 272
I1226 08:55:30.616571 94304 cpu_info.cpp:464] GPU is used: no
I1226 08:55:30.616595 94304 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 08:55:30.616618 94304 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 08:55:30.616626 89709 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 08:55:30.616704 89709 cpu_info.cpp:455] Total number of sockets: 1
I1226 08:55:30.616727 89709 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 08:55:30.616747 89709 cpu_info.cpp:461] Total number of processors: 272
I1226 08:55:30.616767 89709 cpu_info.cpp:464] GPU is used: no
I1226 08:55:30.616788 89709 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 08:55:30.616807 89709 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 08:55:30.622272 119888 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 08:55:30.622349 119888 cpu_info.cpp:455] Total number of sockets: 1
I1226 08:55:30.622373 119888 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 08:55:30.622395 119888 cpu_info.cpp:461] Total number of processors: 272
I1226 08:55:30.622416 119888 cpu_info.cpp:464] GPU is used: no
I1226 08:55:30.622437 119888 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 08:55:30.622457 119888 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 08:55:30.641490 95005 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 08:55:30.641907 95005 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 08:55:30.643985 95005 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_5"
    batch_size: 256
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 08:55:30.648136 86102 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 08:55:30.648432 86102 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 08:55:27.384656 90165 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 08:55:30.650403 86102 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_3"
    batch_size: 256
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 08:55:27.385038 90165 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 08:55:27.387184 90165 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_1"
    batch_size: 256
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 08:55:30.662101 94304 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 08:55:30.662448 94304 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 08:55:30.662389 95005 layer_factory.hpp:114] Creating layer data
I1226 08:55:30.664463 94304 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_0"
    batch_size: 256
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 08:55:30.664342 86102 layer_factory.hpp:114] Creating layer data
I1226 08:55:30.662564 89709 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 08:55:30.662919 89709 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 08:55:30.665267 89709 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_7"
    batch_size: 256
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 08:55:30.665293 91127 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 08:55:30.665709 91127 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 08:55:30.667501 91127 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_6"
    batch_size: 256
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 08:55:30.672966 86102 net.cpp:178] Creating Layer data
I1226 08:55:30.673046 86102 net.cpp:586] data -> data
I1226 08:55:30.673147 86102 net.cpp:586] data -> label
I1226 08:55:30.676522 95005 net.cpp:178] Creating Layer data
I1226 08:55:30.676656 95005 net.cpp:586] data -> data
I1226 08:55:30.676826 95005 net.cpp:586] data -> label
I1226 08:55:30.666712 119888 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 08:55:30.667132 119888 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 08:55:30.669438 119888 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_4"
    batch_size: 256
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 08:55:30.679702 89709 layer_factory.hpp:114] Creating layer data
I1226 08:55:30.675465 85894 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 08:55:30.675788 85894 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 08:55:30.677516 85894 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "DummyData"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  dummy_data_param {
    data_filler {
      type: "gaussian"
      std: 1
    }
    data_filler {
      type: "constant"
      value: 0
    }
    shape {
      dim: 256
      dim: 3
      dim: 227
      dim: 227
    }
    shape {
      dim: 256
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 08:55:30.688951 92150 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 08:55:30.689316 92150 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 08:55:30.691468 92150 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_2"
    batch_size: 256
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 08:55:27.425257 90165 layer_factory.hpp:114] Creating layer data
I1226 08:55:30.688355 89709 net.cpp:178] Creating Layer data
I1226 08:55:30.688483 89709 net.cpp:586] data -> data
I1226 08:55:30.688576 89709 net.cpp:586] data -> label
I1226 08:55:30.697074 85894 layer_factory.hpp:114] Creating layer data
I1226 08:55:30.697278 85894 net.cpp:178] Creating Layer data
I1226 08:55:30.697322 85894 net.cpp:586] data -> data
I1226 08:55:30.697440 85894 net.cpp:586] data -> label
I1226 08:55:27.446521 90165 net.cpp:178] Creating Layer data
I1226 08:55:27.446636 90165 net.cpp:586] data -> data
I1226 08:55:27.446719 90165 net.cpp:586] data -> label
I1226 08:55:30.717552 95007 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_5
I1226 08:55:30.711843 91127 layer_factory.hpp:114] Creating layer data
I1226 08:55:30.712658 119888 layer_factory.hpp:114] Creating layer data
I1226 08:55:30.728293 86105 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_3
I1226 08:55:30.725160 91127 net.cpp:178] Creating Layer data
I1226 08:55:30.725273 91127 net.cpp:586] data -> data
I1226 08:55:30.725410 91127 net.cpp:586] data -> label
I1226 08:55:30.723803 119888 net.cpp:178] Creating Layer data
I1226 08:55:30.723893 119888 net.cpp:586] data -> data
I1226 08:55:30.723974 119888 net.cpp:586] data -> label
I1226 08:55:30.732576 92150 layer_factory.hpp:114] Creating layer data
I1226 08:55:30.741242 92150 net.cpp:178] Creating Layer data
I1226 08:55:30.741333 92150 net.cpp:586] data -> data
I1226 08:55:30.741477 92150 net.cpp:586] data -> label
I1226 08:55:30.736163 85894 net.cpp:228] Setting up data
I1226 08:55:30.736279 85894 net.cpp:235] Top shape: 256 3 227 227 (39574272)
I1226 08:55:30.736315 85894 net.cpp:235] Top shape: 256 1 1 1 (256)
I1226 08:55:30.736340 85894 net.cpp:243] Memory required for data: 158298112
I1226 08:55:30.736420 85894 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 08:55:30.736502 85894 net.cpp:178] Creating Layer label_data_1_split
I1226 08:55:30.736618 85894 net.cpp:612] label_data_1_split <- label
I1226 08:55:30.736656 85894 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 08:55:30.736706 85894 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 08:55:30.744710 86102 data_layer.cpp:80] output data size: 256,3,227,227
I1226 08:55:30.745218 89711 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_7
I1226 08:55:30.753170 95005 data_layer.cpp:80] output data size: 256,3,227,227
I1226 08:55:30.756562 94304 layer_factory.hpp:114] Creating layer data
I1226 08:55:30.753227 89709 data_layer.cpp:80] output data size: 256,3,227,227
I1226 08:55:30.748505 85894 net.cpp:228] Setting up label_data_1_split
I1226 08:55:30.748625 85894 net.cpp:235] Top shape: 256 1 1 1 (256)
I1226 08:55:30.748656 85894 net.cpp:235] Top shape: 256 1 1 1 (256)
I1226 08:55:30.748680 85894 net.cpp:243] Memory required for data: 158300160
I1226 08:55:30.748713 85894 layer_factory.hpp:114] Creating layer conv1
I1226 08:55:30.748811 85894 net.cpp:178] Creating Layer conv1
I1226 08:55:30.748842 85894 net.cpp:612] conv1 <- data
I1226 08:55:30.748888 85894 net.cpp:586] conv1 -> conv1
I1226 08:55:30.766620 94304 net.cpp:178] Creating Layer data
I1226 08:55:30.766734 94304 net.cpp:586] data -> data
I1226 08:55:30.766882 94304 net.cpp:586] data -> label
I1226 08:55:30.761483 91129 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_6
I1226 08:55:27.501034 90167 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_1
I1226 08:55:30.762831 119890 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_4
I1226 08:55:27.514291 90165 data_layer.cpp:80] output data size: 256,3,227,227
I1226 08:55:30.784286 92152 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_2
I1226 08:55:30.780725 91127 data_layer.cpp:80] output data size: 256,3,227,227
I1226 08:55:30.780252 119888 data_layer.cpp:80] output data size: 256,3,227,227
I1226 08:55:30.804365 94307 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_0
I1226 08:55:30.819265 92150 data_layer.cpp:80] output data size: 256,3,227,227
I1226 08:55:30.827575 94304 data_layer.cpp:80] output data size: 256,3,227,227
I1226 08:55:30.909132 85894 net.cpp:228] Setting up conv1
I1226 08:55:30.909243 85894 net.cpp:235] Top shape: 256 96 55 55 (74342400)
I1226 08:55:30.909270 85894 net.cpp:243] Memory required for data: 455669760
I1226 08:55:30.909567 85894 layer_factory.hpp:114] Creating layer relu1
I1226 08:55:30.909654 85894 net.cpp:178] Creating Layer relu1
I1226 08:55:30.909771 85894 net.cpp:612] relu1 <- conv1
I1226 08:55:30.909808 85894 net.cpp:573] relu1 -> conv1 (in-place)
I1226 08:55:30.909899 85894 net.cpp:228] Setting up relu1
I1226 08:55:30.909948 85894 net.cpp:235] Top shape: 256 96 55 55 (74342400)
I1226 08:55:30.909971 85894 net.cpp:243] Memory required for data: 753039360
I1226 08:55:30.910004 85894 layer_factory.hpp:114] Creating layer norm1
I1226 08:55:30.910066 85894 net.cpp:178] Creating Layer norm1
I1226 08:55:30.910091 85894 net.cpp:612] norm1 <- conv1
I1226 08:55:30.910132 85894 net.cpp:586] norm1 -> norm1
I1226 08:55:30.910315 85894 net.cpp:228] Setting up norm1
I1226 08:55:30.910372 85894 net.cpp:235] Top shape: 256 96 55 55 (74342400)
I1226 08:55:30.910395 85894 net.cpp:243] Memory required for data: 1050408960
I1226 08:55:30.910423 85894 layer_factory.hpp:114] Creating layer pool1
I1226 08:55:30.910473 85894 net.cpp:178] Creating Layer pool1
I1226 08:55:30.910504 85894 net.cpp:612] pool1 <- norm1
I1226 08:55:30.910543 85894 net.cpp:586] pool1 -> pool1
I1226 08:55:30.910627 85894 net.cpp:228] Setting up pool1
I1226 08:55:30.910667 85894 net.cpp:235] Top shape: 256 96 27 27 (17915904)
I1226 08:55:30.910689 85894 net.cpp:243] Memory required for data: 1122072576
I1226 08:55:30.910714 85894 layer_factory.hpp:114] Creating layer conv2
I1226 08:55:30.910782 85894 net.cpp:178] Creating Layer conv2
I1226 08:55:30.910814 85894 net.cpp:612] conv2 <- pool1
I1226 08:55:30.910856 85894 net.cpp:586] conv2 -> conv2
I1226 08:55:31.015528 86102 net.cpp:228] Setting up data
I1226 08:55:31.015666 86102 net.cpp:235] Top shape: 256 3 227 227 (39574272)
I1226 08:55:31.015704 86102 net.cpp:235] Top shape: 256 (256)
I1226 08:55:31.015729 86102 net.cpp:243] Memory required for data: 158298112
I1226 08:55:31.015782 86102 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 08:55:31.015974 86102 net.cpp:178] Creating Layer label_data_1_split
I1226 08:55:31.016023 86102 net.cpp:612] label_data_1_split <- label
I1226 08:55:31.016072 86102 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 08:55:31.016124 86102 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 08:55:31.016062 89709 net.cpp:228] Setting up data
I1226 08:55:31.016175 89709 net.cpp:235] Top shape: 256 3 227 227 (39574272)
I1226 08:55:31.016212 89709 net.cpp:235] Top shape: 256 (256)
I1226 08:55:31.016263 89709 net.cpp:243] Memory required for data: 158298112
I1226 08:55:31.016301 89709 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 08:55:31.016576 89709 net.cpp:178] Creating Layer label_data_1_split
I1226 08:55:31.016621 89709 net.cpp:612] label_data_1_split <- label
I1226 08:55:31.016660 89709 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 08:55:31.016710 89709 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 08:55:31.031294 86102 net.cpp:228] Setting up label_data_1_split
I1226 08:55:31.031409 86102 net.cpp:235] Top shape: 256 (256)
I1226 08:55:31.031443 86102 net.cpp:235] Top shape: 256 (256)
I1226 08:55:31.031469 86102 net.cpp:243] Memory required for data: 158300160
I1226 08:55:31.031507 86102 layer_factory.hpp:114] Creating layer conv1
I1226 08:55:31.031644 86102 net.cpp:178] Creating Layer conv1
I1226 08:55:31.031690 86102 net.cpp:612] conv1 <- data
I1226 08:55:31.031733 86102 net.cpp:586] conv1 -> conv1
I1226 08:55:31.035459 89709 net.cpp:228] Setting up label_data_1_split
I1226 08:55:31.035560 89709 net.cpp:235] Top shape: 256 (256)
I1226 08:55:31.035593 89709 net.cpp:235] Top shape: 256 (256)
I1226 08:55:31.035614 89709 net.cpp:243] Memory required for data: 158300160
I1226 08:55:31.035677 89709 layer_factory.hpp:114] Creating layer conv1
I1226 08:55:31.035778 89709 net.cpp:178] Creating Layer conv1
I1226 08:55:31.035820 89709 net.cpp:612] conv1 <- data
I1226 08:55:31.035869 89709 net.cpp:586] conv1 -> conv1
I1226 08:55:27.775869 90165 net.cpp:228] Setting up data
I1226 08:55:27.775988 90165 net.cpp:235] Top shape: 256 3 227 227 (39574272)
I1226 08:55:27.776036 90165 net.cpp:235] Top shape: 256 (256)
I1226 08:55:27.776062 90165 net.cpp:243] Memory required for data: 158298112
I1226 08:55:27.776100 90165 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 08:55:27.776834 90165 net.cpp:178] Creating Layer label_data_1_split
I1226 08:55:27.776932 90165 net.cpp:612] label_data_1_split <- label
I1226 08:55:27.776983 90165 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 08:55:27.777045 90165 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 08:55:31.040024 119888 net.cpp:228] Setting up data
I1226 08:55:31.040138 119888 net.cpp:235] Top shape: 256 3 227 227 (39574272)
I1226 08:55:31.040174 119888 net.cpp:235] Top shape: 256 (256)
I1226 08:55:31.040227 119888 net.cpp:243] Memory required for data: 158298112
I1226 08:55:31.040266 119888 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 08:55:31.040451 119888 net.cpp:178] Creating Layer label_data_1_split
I1226 08:55:31.040495 119888 net.cpp:612] label_data_1_split <- label
I1226 08:55:31.040536 119888 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 08:55:31.040599 119888 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 08:55:31.059167 119888 net.cpp:228] Setting up label_data_1_split
I1226 08:55:31.059273 119888 net.cpp:235] Top shape: 256 (256)
I1226 08:55:31.059335 119888 net.cpp:235] Top shape: 256 (256)
I1226 08:55:31.059489 119888 net.cpp:243] Memory required for data: 158300160
I1226 08:55:31.059530 119888 layer_factory.hpp:114] Creating layer conv1
I1226 08:55:31.059682 119888 net.cpp:178] Creating Layer conv1
I1226 08:55:31.059723 119888 net.cpp:612] conv1 <- data
I1226 08:55:31.059788 119888 net.cpp:586] conv1 -> conv1
I1226 08:55:27.802990 90165 net.cpp:228] Setting up label_data_1_split
I1226 08:55:27.803098 90165 net.cpp:235] Top shape: 256 (256)
I1226 08:55:27.803134 90165 net.cpp:235] Top shape: 256 (256)
I1226 08:55:27.803189 90165 net.cpp:243] Memory required for data: 158300160
I1226 08:55:27.803346 90165 layer_factory.hpp:114] Creating layer conv1
I1226 08:55:27.803442 90165 net.cpp:178] Creating Layer conv1
I1226 08:55:27.803481 90165 net.cpp:612] conv1 <- data
I1226 08:55:27.803524 90165 net.cpp:586] conv1 -> conv1
I1226 08:55:31.071586 92150 net.cpp:228] Setting up data
I1226 08:55:31.071710 92150 net.cpp:235] Top shape: 256 3 227 227 (39574272)
I1226 08:55:31.071753 92150 net.cpp:235] Top shape: 256 (256)
I1226 08:55:31.071781 92150 net.cpp:243] Memory required for data: 158298112
I1226 08:55:31.071840 92150 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 08:55:31.072057 92150 net.cpp:178] Creating Layer label_data_1_split
I1226 08:55:31.072109 92150 net.cpp:612] label_data_1_split <- label
I1226 08:55:31.072154 92150 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 08:55:31.072212 92150 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 08:55:31.102270 92150 net.cpp:228] Setting up label_data_1_split
I1226 08:55:31.102432 92150 net.cpp:235] Top shape: 256 (256)
I1226 08:55:31.102470 92150 net.cpp:235] Top shape: 256 (256)
I1226 08:55:31.102495 92150 net.cpp:243] Memory required for data: 158300160
I1226 08:55:31.102533 92150 layer_factory.hpp:114] Creating layer conv1
I1226 08:55:31.102721 92150 net.cpp:178] Creating Layer conv1
I1226 08:55:31.102764 92150 net.cpp:612] conv1 <- data
I1226 08:55:31.102807 92150 net.cpp:586] conv1 -> conv1
I1226 08:55:31.116467 91127 net.cpp:228] Setting up data
I1226 08:55:31.116585 91127 net.cpp:235] Top shape: 256 3 227 227 (39574272)
I1226 08:55:31.116621 91127 net.cpp:235] Top shape: 256 (256)
I1226 08:55:31.116642 91127 net.cpp:243] Memory required for data: 158298112
I1226 08:55:31.116677 91127 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 08:55:31.116885 91127 net.cpp:178] Creating Layer label_data_1_split
I1226 08:55:31.116925 91127 net.cpp:612] label_data_1_split <- label
I1226 08:55:31.116963 91127 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 08:55:31.117017 91127 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 08:55:31.131541 85894 net.cpp:228] Setting up conv2
I1226 08:55:31.131652 85894 net.cpp:235] Top shape: 256 256 27 27 (47775744)
I1226 08:55:31.131680 85894 net.cpp:243] Memory required for data: 1313175552
I1226 08:55:31.131755 85894 layer_factory.hpp:114] Creating layer relu2
I1226 08:55:31.131840 85894 net.cpp:178] Creating Layer relu2
I1226 08:55:31.131880 85894 net.cpp:612] relu2 <- conv2
I1226 08:55:31.131928 85894 net.cpp:573] relu2 -> conv2 (in-place)
I1226 08:55:31.132021 85894 net.cpp:228] Setting up relu2
I1226 08:55:31.132061 85894 net.cpp:235] Top shape: 256 256 27 27 (47775744)
I1226 08:55:31.132091 85894 net.cpp:243] Memory required for data: 1504278528
I1226 08:55:31.132128 85894 layer_factory.hpp:114] Creating layer norm2
I1226 08:55:31.132186 85894 net.cpp:178] Creating Layer norm2
I1226 08:55:31.132215 85894 net.cpp:612] norm2 <- conv2
I1226 08:55:31.132252 85894 net.cpp:586] norm2 -> norm2
I1226 08:55:31.132372 85894 net.cpp:228] Setting up norm2
I1226 08:55:31.132426 85894 net.cpp:235] Top shape: 256 256 27 27 (47775744)
I1226 08:55:31.132450 85894 net.cpp:243] Memory required for data: 1695381504
I1226 08:55:31.132478 85894 layer_factory.hpp:114] Creating layer pool2
I1226 08:55:31.132544 85894 net.cpp:178] Creating Layer pool2
I1226 08:55:31.132571 85894 net.cpp:612] pool2 <- norm2
I1226 08:55:31.132606 85894 net.cpp:586] pool2 -> pool2
I1226 08:55:31.132690 85894 net.cpp:228] Setting up pool2
I1226 08:55:31.132735 85894 net.cpp:235] Top shape: 256 256 13 13 (11075584)
I1226 08:55:31.132853 85894 net.cpp:243] Memory required for data: 1739683840
I1226 08:55:31.132887 85894 layer_factory.hpp:114] Creating layer conv3
I1226 08:55:31.132964 85894 net.cpp:178] Creating Layer conv3
I1226 08:55:31.133000 85894 net.cpp:612] conv3 <- pool2
I1226 08:55:31.133049 85894 net.cpp:586] conv3 -> conv3
I1226 08:55:31.142760 91127 net.cpp:228] Setting up label_data_1_split
I1226 08:55:31.142860 91127 net.cpp:235] Top shape: 256 (256)
I1226 08:55:31.142892 91127 net.cpp:235] Top shape: 256 (256)
I1226 08:55:31.142915 91127 net.cpp:243] Memory required for data: 158300160
I1226 08:55:31.142979 91127 layer_factory.hpp:114] Creating layer conv1
I1226 08:55:31.143218 91127 net.cpp:178] Creating Layer conv1
I1226 08:55:31.143288 91127 net.cpp:612] conv1 <- data
I1226 08:55:31.143331 91127 net.cpp:586] conv1 -> conv1
I1226 08:55:31.241016 86102 net.cpp:228] Setting up conv1
I1226 08:55:31.241134 86102 net.cpp:235] Top shape: 256 96 55 55 (74342400)
I1226 08:55:31.241163 86102 net.cpp:243] Memory required for data: 455669760
I1226 08:55:31.241271 86102 layer_factory.hpp:114] Creating layer relu1
I1226 08:55:31.241363 86102 net.cpp:178] Creating Layer relu1
I1226 08:55:31.241401 86102 net.cpp:612] relu1 <- conv1
I1226 08:55:31.241441 86102 net.cpp:573] relu1 -> conv1 (in-place)
I1226 08:55:31.241576 86102 net.cpp:228] Setting up relu1
I1226 08:55:31.241626 86102 net.cpp:235] Top shape: 256 96 55 55 (74342400)
I1226 08:55:31.241649 86102 net.cpp:243] Memory required for data: 753039360
I1226 08:55:31.241678 86102 layer_factory.hpp:114] Creating layer norm1
I1226 08:55:31.241762 86102 net.cpp:178] Creating Layer norm1
I1226 08:55:31.241796 86102 net.cpp:612] norm1 <- conv1
I1226 08:55:31.241833 86102 net.cpp:586] norm1 -> norm1
I1226 08:55:31.241932 86102 net.cpp:228] Setting up norm1
I1226 08:55:31.241978 86102 net.cpp:235] Top shape: 256 96 55 55 (74342400)
I1226 08:55:31.242002 86102 net.cpp:243] Memory required for data: 1050408960
I1226 08:55:31.242028 86102 layer_factory.hpp:114] Creating layer pool1
I1226 08:55:31.242086 86102 net.cpp:178] Creating Layer pool1
I1226 08:55:31.242120 86102 net.cpp:612] pool1 <- norm1
I1226 08:55:31.242166 86102 net.cpp:586] pool1 -> pool1
I1226 08:55:31.242264 86102 net.cpp:228] Setting up pool1
I1226 08:55:31.242311 86102 net.cpp:235] Top shape: 256 96 27 27 (17915904)
I1226 08:55:31.242334 86102 net.cpp:243] Memory required for data: 1122072576
I1226 08:55:31.242362 86102 layer_factory.hpp:114] Creating layer conv2
I1226 08:55:31.242434 86102 net.cpp:178] Creating Layer conv2
I1226 08:55:31.242472 86102 net.cpp:612] conv2 <- pool1
I1226 08:55:31.242512 86102 net.cpp:586] conv2 -> conv2
I1226 08:55:31.245493 89709 net.cpp:228] Setting up conv1
I1226 08:55:31.245630 89709 net.cpp:235] Top shape: 256 96 55 55 (74342400)
I1226 08:55:31.245657 89709 net.cpp:243] Memory required for data: 455669760
I1226 08:55:31.245795 89709 layer_factory.hpp:114] Creating layer relu1
I1226 08:55:31.245909 89709 net.cpp:178] Creating Layer relu1
I1226 08:55:31.246000 89709 net.cpp:612] relu1 <- conv1
I1226 08:55:31.246057 89709 net.cpp:573] relu1 -> conv1 (in-place)
I1226 08:55:31.246181 89709 net.cpp:228] Setting up relu1
I1226 08:55:31.246572 89709 net.cpp:235] Top shape: 256 96 55 55 (74342400)
I1226 08:55:31.246634 89709 net.cpp:243] Memory required for data: 753039360
I1226 08:55:31.246670 89709 layer_factory.hpp:114] Creating layer norm1
I1226 08:55:31.246754 89709 net.cpp:178] Creating Layer norm1
I1226 08:55:31.246829 89709 net.cpp:612] norm1 <- conv1
I1226 08:55:31.246878 89709 net.cpp:586] norm1 -> norm1
I1226 08:55:31.247018 89709 net.cpp:228] Setting up norm1
I1226 08:55:31.247068 89709 net.cpp:235] Top shape: 256 96 55 55 (74342400)
I1226 08:55:31.247092 89709 net.cpp:243] Memory required for data: 1050408960
I1226 08:55:31.247122 89709 layer_factory.hpp:114] Creating layer pool1
I1226 08:55:31.247184 89709 net.cpp:178] Creating Layer pool1
I1226 08:55:31.247223 89709 net.cpp:612] pool1 <- norm1
I1226 08:55:31.247268 89709 net.cpp:586] pool1 -> pool1
I1226 08:55:31.247365 89709 net.cpp:228] Setting up pool1
I1226 08:55:31.247455 89709 net.cpp:235] Top shape: 256 96 27 27 (17915904)
I1226 08:55:31.247479 89709 net.cpp:243] Memory required for data: 1122072576
I1226 08:55:31.247506 89709 layer_factory.hpp:114] Creating layer conv2
I1226 08:55:31.247620 89709 net.cpp:178] Creating Layer conv2
I1226 08:55:31.247659 89709 net.cpp:612] conv2 <- pool1
I1226 08:55:31.247704 89709 net.cpp:586] conv2 -> conv2
I1226 08:55:28.016211 90165 net.cpp:228] Setting up conv1
I1226 08:55:28.016340 90165 net.cpp:235] Top shape: 256 96 55 55 (74342400)
I1226 08:55:28.016391 90165 net.cpp:243] Memory required for data: 455669760
I1226 08:55:28.016541 90165 layer_factory.hpp:114] Creating layer relu1
I1226 08:55:28.016691 90165 net.cpp:178] Creating Layer relu1
I1226 08:55:28.016790 90165 net.cpp:612] relu1 <- conv1
I1226 08:55:28.016834 90165 net.cpp:573] relu1 -> conv1 (in-place)
I1226 08:55:28.017266 90165 net.cpp:228] Setting up relu1
I1226 08:55:28.017400 90165 net.cpp:235] Top shape: 256 96 55 55 (74342400)
I1226 08:55:28.017449 90165 net.cpp:243] Memory required for data: 753039360
I1226 08:55:28.017524 90165 layer_factory.hpp:114] Creating layer norm1
I1226 08:55:28.017660 90165 net.cpp:178] Creating Layer norm1
I1226 08:55:28.017750 90165 net.cpp:612] norm1 <- conv1
I1226 08:55:28.017792 90165 net.cpp:586] norm1 -> norm1
I1226 08:55:28.017907 90165 net.cpp:228] Setting up norm1
I1226 08:55:28.017961 90165 net.cpp:235] Top shape: 256 96 55 55 (74342400)
I1226 08:55:28.017984 90165 net.cpp:243] Memory required for data: 1050408960
I1226 08:55:28.018026 90165 layer_factory.hpp:114] Creating layer pool1
I1226 08:55:28.018100 90165 net.cpp:178] Creating Layer pool1
I1226 08:55:28.018131 90165 net.cpp:612] pool1 <- norm1
I1226 08:55:28.018182 90165 net.cpp:586] pool1 -> pool1
I1226 08:55:28.018306 90165 net.cpp:228] Setting up pool1
I1226 08:55:28.018357 90165 net.cpp:235] Top shape: 256 96 27 27 (17915904)
I1226 08:55:28.018398 90165 net.cpp:243] Memory required for data: 1122072576
I1226 08:55:28.018429 90165 layer_factory.hpp:114] Creating layer conv2
I1226 08:55:28.018527 90165 net.cpp:178] Creating Layer conv2
I1226 08:55:28.018597 90165 net.cpp:612] conv2 <- pool1
I1226 08:55:28.018647 90165 net.cpp:586] conv2 -> conv2
I1226 08:55:31.276790 119888 net.cpp:228] Setting up conv1
I1226 08:55:31.276898 119888 net.cpp:235] Top shape: 256 96 55 55 (74342400)
I1226 08:55:31.276929 119888 net.cpp:243] Memory required for data: 455669760
I1226 08:55:31.277070 119888 layer_factory.hpp:114] Creating layer relu1
I1226 08:55:31.277182 119888 net.cpp:178] Creating Layer relu1
I1226 08:55:31.277225 119888 net.cpp:612] relu1 <- conv1
I1226 08:55:31.277266 119888 net.cpp:573] relu1 -> conv1 (in-place)
I1226 08:55:31.277396 119888 net.cpp:228] Setting up relu1
I1226 08:55:31.277446 119888 net.cpp:235] Top shape: 256 96 55 55 (74342400)
I1226 08:55:31.277469 119888 net.cpp:243] Memory required for data: 753039360
I1226 08:55:31.277519 119888 layer_factory.hpp:114] Creating layer norm1
I1226 08:55:31.277705 119888 net.cpp:178] Creating Layer norm1
I1226 08:55:31.277747 119888 net.cpp:612] norm1 <- conv1
I1226 08:55:31.277786 119888 net.cpp:586] norm1 -> norm1
I1226 08:55:31.277912 119888 net.cpp:228] Setting up norm1
I1226 08:55:31.277992 119888 net.cpp:235] Top shape: 256 96 55 55 (74342400)
I1226 08:55:31.278015 119888 net.cpp:243] Memory required for data: 1050408960
I1226 08:55:31.278045 119888 layer_factory.hpp:114] Creating layer pool1
I1226 08:55:31.278141 119888 net.cpp:178] Creating Layer pool1
I1226 08:55:31.278174 119888 net.cpp:612] pool1 <- norm1
I1226 08:55:31.278218 119888 net.cpp:586] pool1 -> pool1
I1226 08:55:31.278347 119888 net.cpp:228] Setting up pool1
I1226 08:55:31.278390 119888 net.cpp:235] Top shape: 256 96 27 27 (17915904)
I1226 08:55:31.278422 119888 net.cpp:243] Memory required for data: 1122072576
I1226 08:55:31.278475 119888 layer_factory.hpp:114] Creating layer conv2
I1226 08:55:31.278560 119888 net.cpp:178] Creating Layer conv2
I1226 08:55:31.278594 119888 net.cpp:612] conv2 <- pool1
I1226 08:55:31.278690 119888 net.cpp:586] conv2 -> conv2
I1226 08:55:31.317538 92150 net.cpp:228] Setting up conv1
I1226 08:55:31.317682 92150 net.cpp:235] Top shape: 256 96 55 55 (74342400)
I1226 08:55:31.317708 92150 net.cpp:243] Memory required for data: 455669760
I1226 08:55:31.317842 92150 layer_factory.hpp:114] Creating layer relu1
I1226 08:55:31.317950 92150 net.cpp:178] Creating Layer relu1
I1226 08:55:31.317994 92150 net.cpp:612] relu1 <- conv1
I1226 08:55:31.318032 92150 net.cpp:573] relu1 -> conv1 (in-place)
I1226 08:55:31.318131 92150 net.cpp:228] Setting up relu1
I1226 08:55:31.318178 92150 net.cpp:235] Top shape: 256 96 55 55 (74342400)
I1226 08:55:31.318202 92150 net.cpp:243] Memory required for data: 753039360
I1226 08:55:31.318230 92150 layer_factory.hpp:114] Creating layer norm1
I1226 08:55:31.318295 92150 net.cpp:178] Creating Layer norm1
I1226 08:55:31.318331 92150 net.cpp:612] norm1 <- conv1
I1226 08:55:31.318364 92150 net.cpp:586] norm1 -> norm1
I1226 08:55:31.318894 92150 net.cpp:228] Setting up norm1
I1226 08:55:31.318979 92150 net.cpp:235] Top shape: 256 96 55 55 (74342400)
I1226 08:55:31.319005 92150 net.cpp:243] Memory required for data: 1050408960
I1226 08:55:31.319036 92150 layer_factory.hpp:114] Creating layer pool1
I1226 08:55:31.319131 92150 net.cpp:178] Creating Layer pool1
I1226 08:55:31.319174 92150 net.cpp:612] pool1 <- norm1
I1226 08:55:31.319221 92150 net.cpp:586] pool1 -> pool1
I1226 08:55:31.319331 92150 net.cpp:228] Setting up pool1
I1226 08:55:31.319396 92150 net.cpp:235] Top shape: 256 96 27 27 (17915904)
I1226 08:55:31.319423 92150 net.cpp:243] Memory required for data: 1122072576
I1226 08:55:31.319453 92150 layer_factory.hpp:114] Creating layer conv2
I1226 08:55:31.319530 92150 net.cpp:178] Creating Layer conv2
I1226 08:55:31.319566 92150 net.cpp:612] conv2 <- pool1
I1226 08:55:31.319609 92150 net.cpp:586] conv2 -> conv2
I1226 08:55:31.366888 85894 net.cpp:228] Setting up conv3
I1226 08:55:31.366997 85894 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 08:55:31.367024 85894 net.cpp:243] Memory required for data: 1806137344
I1226 08:55:31.367096 85894 layer_factory.hpp:114] Creating layer relu3
I1226 08:55:31.367177 85894 net.cpp:178] Creating Layer relu3
I1226 08:55:31.367209 85894 net.cpp:612] relu3 <- conv3
I1226 08:55:31.367260 85894 net.cpp:573] relu3 -> conv3 (in-place)
I1226 08:55:31.367372 85894 net.cpp:228] Setting up relu3
I1226 08:55:31.367421 85894 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 08:55:31.367451 85894 net.cpp:243] Memory required for data: 1872590848
I1226 08:55:31.367480 85894 layer_factory.hpp:114] Creating layer conv4
I1226 08:55:31.367547 85894 net.cpp:178] Creating Layer conv4
I1226 08:55:31.367581 85894 net.cpp:612] conv4 <- conv3
I1226 08:55:31.367620 85894 net.cpp:586] conv4 -> conv4
I1226 08:55:31.405191 91127 net.cpp:228] Setting up conv1
I1226 08:55:31.405300 91127 net.cpp:235] Top shape: 256 96 55 55 (74342400)
I1226 08:55:31.405325 91127 net.cpp:243] Memory required for data: 455669760
I1226 08:55:31.405483 91127 layer_factory.hpp:114] Creating layer relu1
I1226 08:55:31.405573 91127 net.cpp:178] Creating Layer relu1
I1226 08:55:31.405613 91127 net.cpp:612] relu1 <- conv1
I1226 08:55:31.405649 91127 net.cpp:573] relu1 -> conv1 (in-place)
I1226 08:55:31.405750 91127 net.cpp:228] Setting up relu1
I1226 08:55:31.405791 91127 net.cpp:235] Top shape: 256 96 55 55 (74342400)
I1226 08:55:31.405820 91127 net.cpp:243] Memory required for data: 753039360
I1226 08:55:31.405856 91127 layer_factory.hpp:114] Creating layer norm1
I1226 08:55:31.405925 91127 net.cpp:178] Creating Layer norm1
I1226 08:55:31.405951 91127 net.cpp:612] norm1 <- conv1
I1226 08:55:31.406003 91127 net.cpp:586] norm1 -> norm1
I1226 08:55:31.406096 91127 net.cpp:228] Setting up norm1
I1226 08:55:31.406136 91127 net.cpp:235] Top shape: 256 96 55 55 (74342400)
I1226 08:55:31.406158 91127 net.cpp:243] Memory required for data: 1050408960
I1226 08:55:31.406185 91127 layer_factory.hpp:114] Creating layer pool1
I1226 08:55:31.406245 91127 net.cpp:178] Creating Layer pool1
I1226 08:55:31.406276 91127 net.cpp:612] pool1 <- norm1
I1226 08:55:31.406319 91127 net.cpp:586] pool1 -> pool1
I1226 08:55:31.406430 91127 net.cpp:228] Setting up pool1
I1226 08:55:31.406474 91127 net.cpp:235] Top shape: 256 96 27 27 (17915904)
I1226 08:55:31.406507 91127 net.cpp:243] Memory required for data: 1122072576
I1226 08:55:31.406533 91127 layer_factory.hpp:114] Creating layer conv2
I1226 08:55:31.406612 91127 net.cpp:178] Creating Layer conv2
I1226 08:55:31.406646 91127 net.cpp:612] conv2 <- pool1
I1226 08:55:31.406689 91127 net.cpp:586] conv2 -> conv2
I1226 08:55:31.569686 85894 net.cpp:228] Setting up conv4
I1226 08:55:31.569794 85894 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 08:55:31.569818 85894 net.cpp:243] Memory required for data: 1939044352
I1226 08:55:31.569901 85894 layer_factory.hpp:114] Creating layer relu4
I1226 08:55:31.569986 85894 net.cpp:178] Creating Layer relu4
I1226 08:55:31.570026 85894 net.cpp:612] relu4 <- conv4
I1226 08:55:31.570073 85894 net.cpp:573] relu4 -> conv4 (in-place)
I1226 08:55:31.570160 85894 net.cpp:228] Setting up relu4
I1226 08:55:31.570205 85894 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 08:55:31.570228 85894 net.cpp:243] Memory required for data: 2005497856
I1226 08:55:31.570255 85894 layer_factory.hpp:114] Creating layer conv5
I1226 08:55:31.570338 85894 net.cpp:178] Creating Layer conv5
I1226 08:55:31.570397 85894 net.cpp:612] conv5 <- conv4
I1226 08:55:31.570444 85894 net.cpp:586] conv5 -> conv5
I1226 08:55:31.590797 95005 net.cpp:228] Setting up data
I1226 08:55:31.590924 95005 net.cpp:235] Top shape: 256 3 227 227 (39574272)
I1226 08:55:31.590968 95005 net.cpp:235] Top shape: 256 (256)
I1226 08:55:31.590996 95005 net.cpp:243] Memory required for data: 158298112
I1226 08:55:31.591037 95005 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 08:55:31.591249 95005 net.cpp:178] Creating Layer label_data_1_split
I1226 08:55:31.591305 95005 net.cpp:612] label_data_1_split <- label
I1226 08:55:31.591351 95005 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 08:55:31.591413 95005 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 08:55:31.607616 95005 net.cpp:228] Setting up label_data_1_split
I1226 08:55:31.607761 95005 net.cpp:235] Top shape: 256 (256)
I1226 08:55:31.607805 95005 net.cpp:235] Top shape: 256 (256)
I1226 08:55:31.607833 95005 net.cpp:243] Memory required for data: 158300160
I1226 08:55:31.607872 95005 layer_factory.hpp:114] Creating layer conv1
I1226 08:55:31.607991 95005 net.cpp:178] Creating Layer conv1
I1226 08:55:31.608037 95005 net.cpp:612] conv1 <- data
I1226 08:55:31.608085 95005 net.cpp:586] conv1 -> conv1
I1226 08:55:31.605254 86102 net.cpp:228] Setting up conv2
I1226 08:55:31.605396 86102 net.cpp:235] Top shape: 256 256 27 27 (47775744)
I1226 08:55:31.605433 86102 net.cpp:243] Memory required for data: 1313175552
I1226 08:55:31.605536 86102 layer_factory.hpp:114] Creating layer relu2
I1226 08:55:31.605669 86102 net.cpp:178] Creating Layer relu2
I1226 08:55:31.605712 86102 net.cpp:612] relu2 <- conv2
I1226 08:55:31.605756 86102 net.cpp:573] relu2 -> conv2 (in-place)
I1226 08:55:31.606422 86102 net.cpp:228] Setting up relu2
I1226 08:55:31.606525 86102 net.cpp:235] Top shape: 256 256 27 27 (47775744)
I1226 08:55:31.606590 86102 net.cpp:243] Memory required for data: 1504278528
I1226 08:55:31.606626 86102 layer_factory.hpp:114] Creating layer norm2
I1226 08:55:31.606711 86102 net.cpp:178] Creating Layer norm2
I1226 08:55:31.606832 86102 net.cpp:612] norm2 <- conv2
I1226 08:55:31.606895 86102 net.cpp:586] norm2 -> norm2
I1226 08:55:31.607024 86102 net.cpp:228] Setting up norm2
I1226 08:55:31.607095 86102 net.cpp:235] Top shape: 256 256 27 27 (47775744)
I1226 08:55:31.607218 86102 net.cpp:243] Memory required for data: 1695381504
I1226 08:55:31.607251 86102 layer_factory.hpp:114] Creating layer pool2
I1226 08:55:31.607328 86102 net.cpp:178] Creating Layer pool2
I1226 08:55:31.607430 86102 net.cpp:612] pool2 <- norm2
I1226 08:55:31.607475 86102 net.cpp:586] pool2 -> pool2
I1226 08:55:31.607595 86102 net.cpp:228] Setting up pool2
I1226 08:55:31.608074 86102 net.cpp:235] Top shape: 256 256 13 13 (11075584)
I1226 08:55:31.608158 86102 net.cpp:243] Memory required for data: 1739683840
I1226 08:55:31.608197 86102 layer_factory.hpp:114] Creating layer conv3
I1226 08:55:31.608314 86102 net.cpp:178] Creating Layer conv3
I1226 08:55:31.608361 86102 net.cpp:612] conv3 <- pool2
I1226 08:55:31.608424 86102 net.cpp:586] conv3 -> conv3
I1226 08:55:31.615399 89709 net.cpp:228] Setting up conv2
I1226 08:55:31.615540 89709 net.cpp:235] Top shape: 256 256 27 27 (47775744)
I1226 08:55:31.615578 89709 net.cpp:243] Memory required for data: 1313175552
I1226 08:55:31.615658 89709 layer_factory.hpp:114] Creating layer relu2
I1226 08:55:31.615736 89709 net.cpp:178] Creating Layer relu2
I1226 08:55:31.616351 89709 net.cpp:612] relu2 <- conv2
I1226 08:55:31.616525 89709 net.cpp:573] relu2 -> conv2 (in-place)
I1226 08:55:31.616782 89709 net.cpp:228] Setting up relu2
I1226 08:55:31.616858 89709 net.cpp:235] Top shape: 256 256 27 27 (47775744)
I1226 08:55:31.616909 89709 net.cpp:243] Memory required for data: 1504278528
I1226 08:55:31.616967 89709 layer_factory.hpp:114] Creating layer norm2
I1226 08:55:31.617039 89709 net.cpp:178] Creating Layer norm2
I1226 08:55:31.617086 89709 net.cpp:612] norm2 <- conv2
I1226 08:55:31.617151 89709 net.cpp:586] norm2 -> norm2
I1226 08:55:31.617455 89709 net.cpp:228] Setting up norm2
I1226 08:55:31.617903 89709 net.cpp:235] Top shape: 256 256 27 27 (47775744)
I1226 08:55:31.618002 89709 net.cpp:243] Memory required for data: 1695381504
I1226 08:55:31.618108 89709 layer_factory.hpp:114] Creating layer pool2
I1226 08:55:31.618238 89709 net.cpp:178] Creating Layer pool2
I1226 08:55:31.618314 89709 net.cpp:612] pool2 <- norm2
I1226 08:55:31.618362 89709 net.cpp:586] pool2 -> pool2
I1226 08:55:31.618536 89709 net.cpp:228] Setting up pool2
I1226 08:55:31.618899 89709 net.cpp:235] Top shape: 256 256 13 13 (11075584)
I1226 08:55:31.618964 89709 net.cpp:243] Memory required for data: 1739683840
I1226 08:55:31.619002 89709 layer_factory.hpp:114] Creating layer conv3
I1226 08:55:31.619124 89709 net.cpp:178] Creating Layer conv3
I1226 08:55:31.619180 89709 net.cpp:612] conv3 <- pool2
I1226 08:55:31.619658 89709 net.cpp:586] conv3 -> conv3
I1226 08:55:31.660850 94304 net.cpp:228] Setting up data
I1226 08:55:31.660971 94304 net.cpp:235] Top shape: 256 3 227 227 (39574272)
I1226 08:55:31.661020 94304 net.cpp:235] Top shape: 256 (256)
I1226 08:55:31.661051 94304 net.cpp:243] Memory required for data: 158298112
I1226 08:55:31.661097 94304 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 08:55:31.661322 94304 net.cpp:178] Creating Layer label_data_1_split
I1226 08:55:31.661392 94304 net.cpp:612] label_data_1_split <- label
I1226 08:55:31.661445 94304 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 08:55:31.661507 94304 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 08:55:28.393296 90165 net.cpp:228] Setting up conv2
I1226 08:55:28.393463 90165 net.cpp:235] Top shape: 256 256 27 27 (47775744)
I1226 08:55:28.393501 90165 net.cpp:243] Memory required for data: 1313175552
I1226 08:55:28.393712 90165 layer_factory.hpp:114] Creating layer relu2
I1226 08:55:28.393844 90165 net.cpp:178] Creating Layer relu2
I1226 08:55:28.394016 90165 net.cpp:612] relu2 <- conv2
I1226 08:55:28.394063 90165 net.cpp:573] relu2 -> conv2 (in-place)
I1226 08:55:28.394549 90165 net.cpp:228] Setting up relu2
I1226 08:55:28.394681 90165 net.cpp:235] Top shape: 256 256 27 27 (47775744)
I1226 08:55:28.394708 90165 net.cpp:243] Memory required for data: 1504278528
I1226 08:55:28.394744 90165 layer_factory.hpp:114] Creating layer norm2
I1226 08:55:28.394860 90165 net.cpp:178] Creating Layer norm2
I1226 08:55:28.394908 90165 net.cpp:612] norm2 <- conv2
I1226 08:55:28.394953 90165 net.cpp:586] norm2 -> norm2
I1226 08:55:28.395092 90165 net.cpp:228] Setting up norm2
I1226 08:55:28.395150 90165 net.cpp:235] Top shape: 256 256 27 27 (47775744)
I1226 08:55:28.395174 90165 net.cpp:243] Memory required for data: 1695381504
I1226 08:55:28.395216 90165 layer_factory.hpp:114] Creating layer pool2
I1226 08:55:28.395300 90165 net.cpp:178] Creating Layer pool2
I1226 08:55:28.395328 90165 net.cpp:612] pool2 <- norm2
I1226 08:55:28.395364 90165 net.cpp:586] pool2 -> pool2
I1226 08:55:28.395476 90165 net.cpp:228] Setting up pool2
I1226 08:55:28.395678 90165 net.cpp:235] Top shape: 256 256 13 13 (11075584)
I1226 08:55:28.395709 90165 net.cpp:243] Memory required for data: 1739683840
I1226 08:55:28.395751 90165 layer_factory.hpp:114] Creating layer conv3
I1226 08:55:28.395830 90165 net.cpp:178] Creating Layer conv3
I1226 08:55:28.395864 90165 net.cpp:612] conv3 <- pool2
I1226 08:55:28.395921 90165 net.cpp:586] conv3 -> conv3
I1226 08:55:31.658844 119888 net.cpp:228] Setting up conv2
I1226 08:55:31.658983 119888 net.cpp:235] Top shape: 256 256 27 27 (47775744)
I1226 08:55:31.659014 119888 net.cpp:243] Memory required for data: 1313175552
I1226 08:55:31.659090 119888 layer_factory.hpp:114] Creating layer relu2
I1226 08:55:31.659241 119888 net.cpp:178] Creating Layer relu2
I1226 08:55:31.659286 119888 net.cpp:612] relu2 <- conv2
I1226 08:55:31.659327 119888 net.cpp:573] relu2 -> conv2 (in-place)
I1226 08:55:31.659422 119888 net.cpp:228] Setting up relu2
I1226 08:55:31.659466 119888 net.cpp:235] Top shape: 256 256 27 27 (47775744)
I1226 08:55:31.659492 119888 net.cpp:243] Memory required for data: 1504278528
I1226 08:55:31.659519 119888 layer_factory.hpp:114] Creating layer norm2
I1226 08:55:31.659621 119888 net.cpp:178] Creating Layer norm2
I1226 08:55:31.659654 119888 net.cpp:612] norm2 <- conv2
I1226 08:55:31.659724 119888 net.cpp:586] norm2 -> norm2
I1226 08:55:31.659880 119888 net.cpp:228] Setting up norm2
I1226 08:55:31.659932 119888 net.cpp:235] Top shape: 256 256 27 27 (47775744)
I1226 08:55:31.659957 119888 net.cpp:243] Memory required for data: 1695381504
I1226 08:55:31.659986 119888 layer_factory.hpp:114] Creating layer pool2
I1226 08:55:31.660064 119888 net.cpp:178] Creating Layer pool2
I1226 08:55:31.660110 119888 net.cpp:612] pool2 <- norm2
I1226 08:55:31.660146 119888 net.cpp:586] pool2 -> pool2
I1226 08:55:31.660699 119888 net.cpp:228] Setting up pool2
I1226 08:55:31.660820 119888 net.cpp:235] Top shape: 256 256 13 13 (11075584)
I1226 08:55:31.660856 119888 net.cpp:243] Memory required for data: 1739683840
I1226 08:55:31.660888 119888 layer_factory.hpp:114] Creating layer conv3
I1226 08:55:31.660980 119888 net.cpp:178] Creating Layer conv3
I1226 08:55:31.661039 119888 net.cpp:612] conv3 <- pool2
I1226 08:55:31.661098 119888 net.cpp:586] conv3 -> conv3
I1226 08:55:31.678990 94304 net.cpp:228] Setting up label_data_1_split
I1226 08:55:31.679108 94304 net.cpp:235] Top shape: 256 (256)
I1226 08:55:31.679147 94304 net.cpp:235] Top shape: 256 (256)
I1226 08:55:31.679174 94304 net.cpp:243] Memory required for data: 158300160
I1226 08:55:31.679215 94304 layer_factory.hpp:114] Creating layer conv1
I1226 08:55:31.679400 94304 net.cpp:178] Creating Layer conv1
I1226 08:55:31.679509 94304 net.cpp:612] conv1 <- data
I1226 08:55:31.679652 94304 net.cpp:586] conv1 -> conv1
I1226 08:55:31.683442 92150 net.cpp:228] Setting up conv2
I1226 08:55:31.683585 92150 net.cpp:235] Top shape: 256 256 27 27 (47775744)
I1226 08:55:31.683615 92150 net.cpp:243] Memory required for data: 1313175552
I1226 08:55:31.683745 92150 layer_factory.hpp:114] Creating layer relu2
I1226 08:55:31.683924 92150 net.cpp:178] Creating Layer relu2
I1226 08:55:31.683985 92150 net.cpp:612] relu2 <- conv2
I1226 08:55:31.684031 92150 net.cpp:573] relu2 -> conv2 (in-place)
I1226 08:55:31.684417 92150 net.cpp:228] Setting up relu2
I1226 08:55:31.684480 92150 net.cpp:235] Top shape: 256 256 27 27 (47775744)
I1226 08:55:31.684507 92150 net.cpp:243] Memory required for data: 1504278528
I1226 08:55:31.685106 92150 layer_factory.hpp:114] Creating layer norm2
I1226 08:55:31.685263 92150 net.cpp:178] Creating Layer norm2
I1226 08:55:31.685317 92150 net.cpp:612] norm2 <- conv2
I1226 08:55:31.685415 92150 net.cpp:586] norm2 -> norm2
I1226 08:55:31.685555 92150 net.cpp:228] Setting up norm2
I1226 08:55:31.685621 92150 net.cpp:235] Top shape: 256 256 27 27 (47775744)
I1226 08:55:31.685647 92150 net.cpp:243] Memory required for data: 1695381504
I1226 08:55:31.685679 92150 layer_factory.hpp:114] Creating layer pool2
I1226 08:55:31.685750 92150 net.cpp:178] Creating Layer pool2
I1226 08:55:31.685781 92150 net.cpp:612] pool2 <- norm2
I1226 08:55:31.685819 92150 net.cpp:586] pool2 -> pool2
I1226 08:55:31.685942 92150 net.cpp:228] Setting up pool2
I1226 08:55:31.686091 92150 net.cpp:235] Top shape: 256 256 13 13 (11075584)
I1226 08:55:31.686138 92150 net.cpp:243] Memory required for data: 1739683840
I1226 08:55:31.686170 92150 layer_factory.hpp:114] Creating layer conv3
I1226 08:55:31.686290 92150 net.cpp:178] Creating Layer conv3
I1226 08:55:31.686349 92150 net.cpp:612] conv3 <- pool2
I1226 08:55:31.686442 92150 net.cpp:586] conv3 -> conv3
I1226 08:55:31.716874 85894 net.cpp:228] Setting up conv5
I1226 08:55:31.716984 85894 net.cpp:235] Top shape: 256 256 13 13 (11075584)
I1226 08:55:31.717010 85894 net.cpp:243] Memory required for data: 2049800192
I1226 08:55:31.717083 85894 layer_factory.hpp:114] Creating layer relu5
I1226 08:55:31.717161 85894 net.cpp:178] Creating Layer relu5
I1226 08:55:31.717205 85894 net.cpp:612] relu5 <- conv5
I1226 08:55:31.717253 85894 net.cpp:573] relu5 -> conv5 (in-place)
I1226 08:55:31.717367 85894 net.cpp:228] Setting up relu5
I1226 08:55:31.717417 85894 net.cpp:235] Top shape: 256 256 13 13 (11075584)
I1226 08:55:31.717447 85894 net.cpp:243] Memory required for data: 2094102528
I1226 08:55:31.717481 85894 layer_factory.hpp:114] Creating layer pool5
I1226 08:55:31.717542 85894 net.cpp:178] Creating Layer pool5
I1226 08:55:31.717571 85894 net.cpp:612] pool5 <- conv5
I1226 08:55:31.717614 85894 net.cpp:586] pool5 -> pool5
I1226 08:55:31.717793 85894 net.cpp:228] Setting up pool5
I1226 08:55:31.717839 85894 net.cpp:235] Top shape: 256 256 6 6 (2359296)
I1226 08:55:31.717862 85894 net.cpp:243] Memory required for data: 2103539712
I1226 08:55:31.717890 85894 layer_factory.hpp:114] Creating layer fc6
I1226 08:55:31.717963 85894 net.cpp:178] Creating Layer fc6
I1226 08:55:31.717996 85894 net.cpp:612] fc6 <- pool5
I1226 08:55:31.718034 85894 net.cpp:586] fc6 -> fc6
I1226 08:55:31.836488 91127 net.cpp:228] Setting up conv2
I1226 08:55:31.836599 91127 net.cpp:235] Top shape: 256 256 27 27 (47775744)
I1226 08:55:31.836625 91127 net.cpp:243] Memory required for data: 1313175552
I1226 08:55:31.836696 91127 layer_factory.hpp:114] Creating layer relu2
I1226 08:55:31.836781 91127 net.cpp:178] Creating Layer relu2
I1226 08:55:31.836935 91127 net.cpp:612] relu2 <- conv2
I1226 08:55:31.836977 91127 net.cpp:573] relu2 -> conv2 (in-place)
I1226 08:55:31.837075 91127 net.cpp:228] Setting up relu2
I1226 08:55:31.837117 91127 net.cpp:235] Top shape: 256 256 27 27 (47775744)
I1226 08:55:31.837141 91127 net.cpp:243] Memory required for data: 1504278528
I1226 08:55:31.837170 91127 layer_factory.hpp:114] Creating layer norm2
I1226 08:55:31.837220 91127 net.cpp:178] Creating Layer norm2
I1226 08:55:31.837332 91127 net.cpp:612] norm2 <- conv2
I1226 08:55:31.837368 91127 net.cpp:586] norm2 -> norm2
I1226 08:55:31.837494 91127 net.cpp:228] Setting up norm2
I1226 08:55:31.837541 91127 net.cpp:235] Top shape: 256 256 27 27 (47775744)
I1226 08:55:31.837564 91127 net.cpp:243] Memory required for data: 1695381504
I1226 08:55:31.837592 91127 layer_factory.hpp:114] Creating layer pool2
I1226 08:55:31.837647 91127 net.cpp:178] Creating Layer pool2
I1226 08:55:31.837680 91127 net.cpp:612] pool2 <- norm2
I1226 08:55:31.837714 91127 net.cpp:586] pool2 -> pool2
I1226 08:55:31.837793 91127 net.cpp:228] Setting up pool2
I1226 08:55:31.837921 91127 net.cpp:235] Top shape: 256 256 13 13 (11075584)
I1226 08:55:31.837947 91127 net.cpp:243] Memory required for data: 1739683840
I1226 08:55:31.837977 91127 layer_factory.hpp:114] Creating layer conv3
I1226 08:55:31.838055 91127 net.cpp:178] Creating Layer conv3
I1226 08:55:31.838088 91127 net.cpp:612] conv3 <- pool2
I1226 08:55:31.838135 91127 net.cpp:586] conv3 -> conv3
I1226 08:55:31.983474 86102 net.cpp:228] Setting up conv3
I1226 08:55:31.984119 86102 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 08:55:31.984177 86102 net.cpp:243] Memory required for data: 1806137344
I1226 08:55:31.984316 86102 layer_factory.hpp:114] Creating layer relu3
I1226 08:55:31.984400 86102 net.cpp:178] Creating Layer relu3
I1226 08:55:31.984467 86102 net.cpp:612] relu3 <- conv3
I1226 08:55:31.984635 86102 net.cpp:573] relu3 -> conv3 (in-place)
I1226 08:55:31.984772 86102 net.cpp:228] Setting up relu3
I1226 08:55:31.984828 86102 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 08:55:31.984854 86102 net.cpp:243] Memory required for data: 1872590848
I1226 08:55:31.984884 86102 layer_factory.hpp:114] Creating layer conv4
I1226 08:55:31.985081 86102 net.cpp:178] Creating Layer conv4
I1226 08:55:31.985121 86102 net.cpp:612] conv4 <- conv3
I1226 08:55:31.985196 86102 net.cpp:586] conv4 -> conv4
I1226 08:55:32.001039 89709 net.cpp:228] Setting up conv3
I1226 08:55:32.001155 89709 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 08:55:32.001185 89709 net.cpp:243] Memory required for data: 1806137344
I1226 08:55:32.001263 89709 layer_factory.hpp:114] Creating layer relu3
I1226 08:55:32.001333 89709 net.cpp:178] Creating Layer relu3
I1226 08:55:32.001368 89709 net.cpp:612] relu3 <- conv3
I1226 08:55:32.001452 89709 net.cpp:573] relu3 -> conv3 (in-place)
I1226 08:55:32.001554 89709 net.cpp:228] Setting up relu3
I1226 08:55:32.001721 89709 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 08:55:32.001749 89709 net.cpp:243] Memory required for data: 1872590848
I1226 08:55:32.001780 89709 layer_factory.hpp:114] Creating layer conv4
I1226 08:55:32.001948 89709 net.cpp:178] Creating Layer conv4
I1226 08:55:32.002020 89709 net.cpp:612] conv4 <- conv3
I1226 08:55:32.002074 89709 net.cpp:586] conv4 -> conv4
I1226 08:55:28.783241 90165 net.cpp:228] Setting up conv3
I1226 08:55:28.783375 90165 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 08:55:28.783411 90165 net.cpp:243] Memory required for data: 1806137344
I1226 08:55:28.783488 90165 layer_factory.hpp:114] Creating layer relu3
I1226 08:55:28.783654 90165 net.cpp:178] Creating Layer relu3
I1226 08:55:28.783704 90165 net.cpp:612] relu3 <- conv3
I1226 08:55:28.783812 90165 net.cpp:573] relu3 -> conv3 (in-place)
I1226 08:55:28.783931 90165 net.cpp:228] Setting up relu3
I1226 08:55:28.784337 90165 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 08:55:28.784392 90165 net.cpp:243] Memory required for data: 1872590848
I1226 08:55:28.784477 90165 layer_factory.hpp:114] Creating layer conv4
I1226 08:55:28.784647 90165 net.cpp:178] Creating Layer conv4
I1226 08:55:28.784696 90165 net.cpp:612] conv4 <- conv3
I1226 08:55:28.784756 90165 net.cpp:586] conv4 -> conv4
I1226 08:55:32.055302 92150 net.cpp:228] Setting up conv3
I1226 08:55:32.055486 92150 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 08:55:32.055546 92150 net.cpp:243] Memory required for data: 1806137344
I1226 08:55:32.055621 92150 layer_factory.hpp:114] Creating layer relu3
I1226 08:55:32.055740 92150 net.cpp:178] Creating Layer relu3
I1226 08:55:32.055784 92150 net.cpp:612] relu3 <- conv3
I1226 08:55:32.055827 92150 net.cpp:573] relu3 -> conv3 (in-place)
I1226 08:55:32.055922 92150 net.cpp:228] Setting up relu3
I1226 08:55:32.055968 92150 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 08:55:32.055992 92150 net.cpp:243] Memory required for data: 1872590848
I1226 08:55:32.056020 92150 layer_factory.hpp:114] Creating layer conv4
I1226 08:55:32.056126 92150 net.cpp:178] Creating Layer conv4
I1226 08:55:32.056167 92150 net.cpp:612] conv4 <- conv3
I1226 08:55:32.056223 92150 net.cpp:586] conv4 -> conv4
I1226 08:55:32.096360 119888 net.cpp:228] Setting up conv3
I1226 08:55:32.096472 119888 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 08:55:32.096503 119888 net.cpp:243] Memory required for data: 1806137344
I1226 08:55:32.096580 119888 layer_factory.hpp:114] Creating layer relu3
I1226 08:55:32.096681 119888 net.cpp:178] Creating Layer relu3
I1226 08:55:32.096725 119888 net.cpp:612] relu3 <- conv3
I1226 08:55:32.096771 119888 net.cpp:573] relu3 -> conv3 (in-place)
I1226 08:55:32.096873 119888 net.cpp:228] Setting up relu3
I1226 08:55:32.096930 119888 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 08:55:32.096954 119888 net.cpp:243] Memory required for data: 1872590848
I1226 08:55:32.096984 119888 layer_factory.hpp:114] Creating layer conv4
I1226 08:55:32.097069 119888 net.cpp:178] Creating Layer conv4
I1226 08:55:32.097105 119888 net.cpp:612] conv4 <- conv3
I1226 08:55:32.097153 119888 net.cpp:586] conv4 -> conv4
I1226 08:55:32.249352 95005 net.cpp:228] Setting up conv1
I1226 08:55:32.249518 95005 net.cpp:235] Top shape: 256 96 55 55 (74342400)
I1226 08:55:32.249554 95005 net.cpp:243] Memory required for data: 455669760
I1226 08:55:32.249840 95005 layer_factory.hpp:114] Creating layer relu1
I1226 08:55:32.250450 95005 net.cpp:178] Creating Layer relu1
I1226 08:55:32.250635 95005 net.cpp:612] relu1 <- conv1
I1226 08:55:32.250705 95005 net.cpp:573] relu1 -> conv1 (in-place)
I1226 08:55:32.250978 95005 net.cpp:228] Setting up relu1
I1226 08:55:32.251060 95005 net.cpp:235] Top shape: 256 96 55 55 (74342400)
I1226 08:55:32.251197 95005 net.cpp:243] Memory required for data: 753039360
I1226 08:55:32.251247 95005 layer_factory.hpp:114] Creating layer norm1
I1226 08:55:32.251693 95005 net.cpp:178] Creating Layer norm1
I1226 08:55:32.251817 95005 net.cpp:612] norm1 <- conv1
I1226 08:55:32.251940 95005 net.cpp:586] norm1 -> norm1
I1226 08:55:32.252117 95005 net.cpp:228] Setting up norm1
I1226 08:55:32.252207 95005 net.cpp:235] Top shape: 256 96 55 55 (74342400)
I1226 08:55:32.252233 95005 net.cpp:243] Memory required for data: 1050408960
I1226 08:55:32.252296 95005 layer_factory.hpp:114] Creating layer pool1
I1226 08:55:32.252393 95005 net.cpp:178] Creating Layer pool1
I1226 08:55:32.252473 95005 net.cpp:612] pool1 <- norm1
I1226 08:55:32.252526 95005 net.cpp:586] pool1 -> pool1
I1226 08:55:32.253015 95005 net.cpp:228] Setting up pool1
I1226 08:55:32.253178 95005 net.cpp:235] Top shape: 256 96 27 27 (17915904)
I1226 08:55:32.253211 95005 net.cpp:243] Memory required for data: 1122072576
I1226 08:55:32.253247 95005 layer_factory.hpp:114] Creating layer conv2
I1226 08:55:32.253357 95005 net.cpp:178] Creating Layer conv2
I1226 08:55:32.253423 95005 net.cpp:612] conv2 <- pool1
I1226 08:55:32.253499 95005 net.cpp:586] conv2 -> conv2
I1226 08:55:32.307432 86102 net.cpp:228] Setting up conv4
I1226 08:55:32.307569 86102 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 08:55:32.307605 86102 net.cpp:243] Memory required for data: 1939044352
I1226 08:55:32.307669 86102 layer_factory.hpp:114] Creating layer relu4
I1226 08:55:32.307737 86102 net.cpp:178] Creating Layer relu4
I1226 08:55:32.307771 86102 net.cpp:612] relu4 <- conv4
I1226 08:55:32.307837 86102 net.cpp:573] relu4 -> conv4 (in-place)
I1226 08:55:32.307942 86102 net.cpp:228] Setting up relu4
I1226 08:55:32.308018 86102 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 08:55:32.308164 86102 net.cpp:243] Memory required for data: 2005497856
I1226 08:55:32.308218 86102 layer_factory.hpp:114] Creating layer conv5
I1226 08:55:32.308331 86102 net.cpp:178] Creating Layer conv5
I1226 08:55:32.308373 86102 net.cpp:612] conv5 <- conv4
I1226 08:55:32.308429 86102 net.cpp:586] conv5 -> conv5
I1226 08:55:32.326020 94304 net.cpp:228] Setting up conv1
I1226 08:55:32.326179 94304 net.cpp:235] Top shape: 256 96 55 55 (74342400)
I1226 08:55:32.326221 94304 net.cpp:243] Memory required for data: 455669760
I1226 08:55:32.326778 94304 layer_factory.hpp:114] Creating layer relu1
I1226 08:55:32.326987 94304 net.cpp:178] Creating Layer relu1
I1226 08:55:32.327035 94304 net.cpp:612] relu1 <- conv1
I1226 08:55:32.327087 94304 net.cpp:573] relu1 -> conv1 (in-place)
I1226 08:55:32.327252 94304 net.cpp:228] Setting up relu1
I1226 08:55:32.327314 94304 net.cpp:235] Top shape: 256 96 55 55 (74342400)
I1226 08:55:32.327342 94304 net.cpp:243] Memory required for data: 753039360
I1226 08:55:32.327376 94304 layer_factory.hpp:114] Creating layer norm1
I1226 08:55:32.327492 94304 net.cpp:178] Creating Layer norm1
I1226 08:55:32.327567 94304 net.cpp:612] norm1 <- conv1
I1226 08:55:32.327638 94304 net.cpp:586] norm1 -> norm1
I1226 08:55:32.328166 94304 net.cpp:228] Setting up norm1
I1226 08:55:32.328341 94304 net.cpp:235] Top shape: 256 96 55 55 (74342400)
I1226 08:55:32.328374 94304 net.cpp:243] Memory required for data: 1050408960
I1226 08:55:32.328413 94304 layer_factory.hpp:114] Creating layer pool1
I1226 08:55:32.328505 94304 net.cpp:178] Creating Layer pool1
I1226 08:55:32.319674 91127 net.cpp:228] Setting up conv3
I1226 08:55:32.328562 94304 net.cpp:612] pool1 <- norm1
I1226 08:55:32.328619 94304 net.cpp:586] pool1 -> pool1
I1226 08:55:32.319777 91127 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 08:55:32.319804 91127 net.cpp:243] Memory required for data: 1806137344
I1226 08:55:32.328835 94304 net.cpp:228] Setting up pool1
I1226 08:55:32.319880 91127 layer_factory.hpp:114] Creating layer relu3
I1226 08:55:32.329121 94304 net.cpp:235] Top shape: 256 96 27 27 (17915904)
I1226 08:55:32.320091 91127 net.cpp:178] Creating Layer relu3
I1226 08:55:32.329309 94304 net.cpp:243] Memory required for data: 1122072576
I1226 08:55:32.329356 94304 layer_factory.hpp:114] Creating layer conv2
I1226 08:55:32.329468 94304 net.cpp:178] Creating Layer conv2
I1226 08:55:32.329505 94304 net.cpp:612] conv2 <- pool1
I1226 08:55:32.329553 94304 net.cpp:586] conv2 -> conv2
I1226 08:55:32.320205 91127 net.cpp:612] relu3 <- conv3
I1226 08:55:32.320245 91127 net.cpp:573] relu3 -> conv3 (in-place)
I1226 08:55:32.320451 91127 net.cpp:228] Setting up relu3
I1226 08:55:32.320492 91127 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 08:55:32.320516 91127 net.cpp:243] Memory required for data: 1872590848
I1226 08:55:32.320544 91127 layer_factory.hpp:114] Creating layer conv4
I1226 08:55:32.320736 91127 net.cpp:178] Creating Layer conv4
I1226 08:55:32.320775 91127 net.cpp:612] conv4 <- conv3
I1226 08:55:32.320819 91127 net.cpp:586] conv4 -> conv4
I1226 08:55:32.354421 89709 net.cpp:228] Setting up conv4
I1226 08:55:32.354544 89709 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 08:55:32.354573 89709 net.cpp:243] Memory required for data: 1939044352
I1226 08:55:32.354635 89709 layer_factory.hpp:114] Creating layer relu4
I1226 08:55:32.354704 89709 net.cpp:178] Creating Layer relu4
I1226 08:55:32.354739 89709 net.cpp:612] relu4 <- conv4
I1226 08:55:32.354790 89709 net.cpp:573] relu4 -> conv4 (in-place)
I1226 08:55:32.354938 89709 net.cpp:228] Setting up relu4
I1226 08:55:32.354979 89709 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 08:55:32.355003 89709 net.cpp:243] Memory required for data: 2005497856
I1226 08:55:32.355039 89709 layer_factory.hpp:114] Creating layer conv5
I1226 08:55:32.355131 89709 net.cpp:178] Creating Layer conv5
I1226 08:55:32.355167 89709 net.cpp:612] conv5 <- conv4
I1226 08:55:32.355208 89709 net.cpp:586] conv5 -> conv5
I1226 08:55:32.359710 92150 net.cpp:228] Setting up conv4
I1226 08:55:32.360188 92150 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 08:55:32.360307 92150 net.cpp:243] Memory required for data: 1939044352
I1226 08:55:32.360479 92150 layer_factory.hpp:114] Creating layer relu4
I1226 08:55:32.360604 92150 net.cpp:178] Creating Layer relu4
I1226 08:55:32.360656 92150 net.cpp:612] relu4 <- conv4
I1226 08:55:32.360713 92150 net.cpp:573] relu4 -> conv4 (in-place)
I1226 08:55:32.360832 92150 net.cpp:228] Setting up relu4
I1226 08:55:32.360893 92150 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 08:55:32.360954 92150 net.cpp:243] Memory required for data: 2005497856
I1226 08:55:32.360998 92150 layer_factory.hpp:114] Creating layer conv5
I1226 08:55:32.361130 92150 net.cpp:178] Creating Layer conv5
I1226 08:55:32.361244 92150 net.cpp:612] conv5 <- conv4
I1226 08:55:32.361311 92150 net.cpp:586] conv5 -> conv5
I1226 08:55:29.131894 90165 net.cpp:228] Setting up conv4
I1226 08:55:29.132011 90165 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 08:55:29.132041 90165 net.cpp:243] Memory required for data: 1939044352
I1226 08:55:29.132103 90165 layer_factory.hpp:114] Creating layer relu4
I1226 08:55:29.132171 90165 net.cpp:178] Creating Layer relu4
I1226 08:55:29.132325 90165 net.cpp:612] relu4 <- conv4
I1226 08:55:29.132454 90165 net.cpp:573] relu4 -> conv4 (in-place)
I1226 08:55:29.132534 90165 net.cpp:228] Setting up relu4
I1226 08:55:29.132668 90165 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 08:55:29.132694 90165 net.cpp:243] Memory required for data: 2005497856
I1226 08:55:29.132722 90165 layer_factory.hpp:114] Creating layer conv5
I1226 08:55:29.132787 90165 net.cpp:178] Creating Layer conv5
I1226 08:55:29.132815 90165 net.cpp:612] conv5 <- conv4
I1226 08:55:29.132855 90165 net.cpp:586] conv5 -> conv5
I1226 08:55:32.477589 119888 net.cpp:228] Setting up conv4
I1226 08:55:32.477721 119888 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 08:55:32.477751 119888 net.cpp:243] Memory required for data: 1939044352
I1226 08:55:32.477805 119888 layer_factory.hpp:114] Creating layer relu4
I1226 08:55:32.477862 119888 net.cpp:178] Creating Layer relu4
I1226 08:55:32.477922 119888 net.cpp:612] relu4 <- conv4
I1226 08:55:32.477970 119888 net.cpp:573] relu4 -> conv4 (in-place)
I1226 08:55:32.478055 119888 net.cpp:228] Setting up relu4
I1226 08:55:32.478147 119888 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 08:55:32.478257 119888 net.cpp:243] Memory required for data: 2005497856
I1226 08:55:32.478286 119888 layer_factory.hpp:114] Creating layer conv5
I1226 08:55:32.478353 119888 net.cpp:178] Creating Layer conv5
I1226 08:55:32.478471 119888 net.cpp:612] conv5 <- conv4
I1226 08:55:32.478513 119888 net.cpp:586] conv5 -> conv5
I1226 08:55:32.547327 86102 net.cpp:228] Setting up conv5
I1226 08:55:32.547981 86102 net.cpp:235] Top shape: 256 256 13 13 (11075584)
I1226 08:55:32.548032 86102 net.cpp:243] Memory required for data: 2049800192
I1226 08:55:32.548252 86102 layer_factory.hpp:114] Creating layer relu5
I1226 08:55:32.548321 86102 net.cpp:178] Creating Layer relu5
I1226 08:55:32.548362 86102 net.cpp:612] relu5 <- conv5
I1226 08:55:32.548450 86102 net.cpp:573] relu5 -> conv5 (in-place)
I1226 08:55:32.548666 86102 net.cpp:228] Setting up relu5
I1226 08:55:32.548717 86102 net.cpp:235] Top shape: 256 256 13 13 (11075584)
I1226 08:55:32.548760 86102 net.cpp:243] Memory required for data: 2094102528
I1226 08:55:32.548792 86102 layer_factory.hpp:114] Creating layer pool5
I1226 08:55:32.548878 86102 net.cpp:178] Creating Layer pool5
I1226 08:55:32.548910 86102 net.cpp:612] pool5 <- conv5
I1226 08:55:32.549319 86102 net.cpp:586] pool5 -> pool5
I1226 08:55:32.549479 86102 net.cpp:228] Setting up pool5
I1226 08:55:32.549532 86102 net.cpp:235] Top shape: 256 256 6 6 (2359296)
I1226 08:55:32.549593 86102 net.cpp:243] Memory required for data: 2103539712
I1226 08:55:32.549623 86102 layer_factory.hpp:114] Creating layer fc6
I1226 08:55:32.549706 86102 net.cpp:178] Creating Layer fc6
I1226 08:55:32.549739 86102 net.cpp:612] fc6 <- pool5
I1226 08:55:32.549779 86102 net.cpp:586] fc6 -> fc6
I1226 08:55:32.603930 89709 net.cpp:228] Setting up conv5
I1226 08:55:32.604341 89709 net.cpp:235] Top shape: 256 256 13 13 (11075584)
I1226 08:55:32.604440 89709 net.cpp:243] Memory required for data: 2049800192
I1226 08:55:32.604564 89709 layer_factory.hpp:114] Creating layer relu5
I1226 08:55:32.604660 89709 net.cpp:178] Creating Layer relu5
I1226 08:55:32.604817 89709 net.cpp:612] relu5 <- conv5
I1226 08:55:32.604934 89709 net.cpp:573] relu5 -> conv5 (in-place)
I1226 08:55:32.605034 89709 net.cpp:228] Setting up relu5
I1226 08:55:32.605080 89709 net.cpp:235] Top shape: 256 256 13 13 (11075584)
I1226 08:55:32.605103 89709 net.cpp:243] Memory required for data: 2094102528
I1226 08:55:32.605132 89709 layer_factory.hpp:114] Creating layer pool5
I1226 08:55:32.605181 89709 net.cpp:178] Creating Layer pool5
I1226 08:55:32.605231 89709 net.cpp:612] pool5 <- conv5
I1226 08:55:32.605289 89709 net.cpp:586] pool5 -> pool5
I1226 08:55:32.605520 89709 net.cpp:228] Setting up pool5
I1226 08:55:32.605574 89709 net.cpp:235] Top shape: 256 256 6 6 (2359296)
I1226 08:55:32.605614 89709 net.cpp:243] Memory required for data: 2103539712
I1226 08:55:32.605645 89709 layer_factory.hpp:114] Creating layer fc6
I1226 08:55:32.605734 89709 net.cpp:178] Creating Layer fc6
I1226 08:55:32.605774 89709 net.cpp:612] fc6 <- pool5
I1226 08:55:32.605815 89709 net.cpp:586] fc6 -> fc6
I1226 08:55:32.617424 92150 net.cpp:228] Setting up conv5
I1226 08:55:32.617547 92150 net.cpp:235] Top shape: 256 256 13 13 (11075584)
I1226 08:55:32.617612 92150 net.cpp:243] Memory required for data: 2049800192
I1226 08:55:32.617740 92150 layer_factory.hpp:114] Creating layer relu5
I1226 08:55:32.617826 92150 net.cpp:178] Creating Layer relu5
I1226 08:55:32.617889 92150 net.cpp:612] relu5 <- conv5
I1226 08:55:32.617971 92150 net.cpp:573] relu5 -> conv5 (in-place)
I1226 08:55:32.618229 92150 net.cpp:228] Setting up relu5
I1226 08:55:32.619779 92150 net.cpp:235] Top shape: 256 256 13 13 (11075584)
I1226 08:55:32.619853 92150 net.cpp:243] Memory required for data: 2094102528
I1226 08:55:32.620008 92150 layer_factory.hpp:114] Creating layer pool5
I1226 08:55:32.620111 92150 net.cpp:178] Creating Layer pool5
I1226 08:55:32.620344 92150 net.cpp:612] pool5 <- conv5
I1226 08:55:32.620426 92150 net.cpp:586] pool5 -> pool5
I1226 08:55:32.620586 92150 net.cpp:228] Setting up pool5
I1226 08:55:32.620651 92150 net.cpp:235] Top shape: 256 256 6 6 (2359296)
I1226 08:55:32.620678 92150 net.cpp:243] Memory required for data: 2103539712
I1226 08:55:32.620717 92150 layer_factory.hpp:114] Creating layer fc6
I1226 08:55:32.620801 92150 net.cpp:178] Creating Layer fc6
I1226 08:55:32.620841 92150 net.cpp:612] fc6 <- pool5
I1226 08:55:32.620898 92150 net.cpp:586] fc6 -> fc6
I1226 08:55:29.373510 90165 net.cpp:228] Setting up conv5
I1226 08:55:29.373708 90165 net.cpp:235] Top shape: 256 256 13 13 (11075584)
I1226 08:55:29.373739 90165 net.cpp:243] Memory required for data: 2049800192
I1226 08:55:29.373816 90165 layer_factory.hpp:114] Creating layer relu5
I1226 08:55:29.373909 90165 net.cpp:178] Creating Layer relu5
I1226 08:55:29.373952 90165 net.cpp:612] relu5 <- conv5
I1226 08:55:29.374011 90165 net.cpp:573] relu5 -> conv5 (in-place)
I1226 08:55:29.374104 90165 net.cpp:228] Setting up relu5
I1226 08:55:29.374157 90165 net.cpp:235] Top shape: 256 256 13 13 (11075584)
I1226 08:55:29.374181 90165 net.cpp:243] Memory required for data: 2094102528
I1226 08:55:29.374212 90165 layer_factory.hpp:114] Creating layer pool5
I1226 08:55:29.374263 90165 net.cpp:178] Creating Layer pool5
I1226 08:55:29.374289 90165 net.cpp:612] pool5 <- conv5
I1226 08:55:29.374341 90165 net.cpp:586] pool5 -> pool5
I1226 08:55:29.374440 90165 net.cpp:228] Setting up pool5
I1226 08:55:29.374485 90165 net.cpp:235] Top shape: 256 256 6 6 (2359296)
I1226 08:55:29.374507 90165 net.cpp:243] Memory required for data: 2103539712
I1226 08:55:29.374536 90165 layer_factory.hpp:114] Creating layer fc6
I1226 08:55:29.374634 90165 net.cpp:178] Creating Layer fc6
I1226 08:55:29.374673 90165 net.cpp:612] fc6 <- pool5
I1226 08:55:29.374714 90165 net.cpp:586] fc6 -> fc6
I1226 08:55:32.739527 91127 net.cpp:228] Setting up conv4
I1226 08:55:32.739665 91127 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 08:55:32.739689 91127 net.cpp:243] Memory required for data: 1939044352
I1226 08:55:32.739771 91127 layer_factory.hpp:114] Creating layer relu4
I1226 08:55:32.739826 91127 net.cpp:178] Creating Layer relu4
I1226 08:55:32.739861 91127 net.cpp:612] relu4 <- conv4
I1226 08:55:32.739898 91127 net.cpp:573] relu4 -> conv4 (in-place)
I1226 08:55:32.739970 91127 net.cpp:228] Setting up relu4
I1226 08:55:32.740011 91127 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 08:55:32.740036 91127 net.cpp:243] Memory required for data: 2005497856
I1226 08:55:32.740061 91127 layer_factory.hpp:114] Creating layer conv5
I1226 08:55:32.740126 91127 net.cpp:178] Creating Layer conv5
I1226 08:55:32.740159 91127 net.cpp:612] conv5 <- conv4
I1226 08:55:32.740195 91127 net.cpp:586] conv5 -> conv5
I1226 08:55:32.761090 119888 net.cpp:228] Setting up conv5
I1226 08:55:32.761466 119888 net.cpp:235] Top shape: 256 256 13 13 (11075584)
I1226 08:55:32.761502 119888 net.cpp:243] Memory required for data: 2049800192
I1226 08:55:32.761633 119888 layer_factory.hpp:114] Creating layer relu5
I1226 08:55:32.761807 119888 net.cpp:178] Creating Layer relu5
I1226 08:55:32.761853 119888 net.cpp:612] relu5 <- conv5
I1226 08:55:32.761909 119888 net.cpp:573] relu5 -> conv5 (in-place)
I1226 08:55:32.762007 119888 net.cpp:228] Setting up relu5
I1226 08:55:32.762054 119888 net.cpp:235] Top shape: 256 256 13 13 (11075584)
I1226 08:55:32.762079 119888 net.cpp:243] Memory required for data: 2094102528
I1226 08:55:32.762130 119888 layer_factory.hpp:114] Creating layer pool5
I1226 08:55:32.762257 119888 net.cpp:178] Creating Layer pool5
I1226 08:55:32.762305 119888 net.cpp:612] pool5 <- conv5
I1226 08:55:32.762374 119888 net.cpp:586] pool5 -> pool5
I1226 08:55:32.762846 119888 net.cpp:228] Setting up pool5
I1226 08:55:32.762940 119888 net.cpp:235] Top shape: 256 256 6 6 (2359296)
I1226 08:55:32.762969 119888 net.cpp:243] Memory required for data: 2103539712
I1226 08:55:32.763036 119888 layer_factory.hpp:114] Creating layer fc6
I1226 08:55:32.763123 119888 net.cpp:178] Creating Layer fc6
I1226 08:55:32.763160 119888 net.cpp:612] fc6 <- pool5
I1226 08:55:32.763205 119888 net.cpp:586] fc6 -> fc6
I1226 08:55:33.033567 91127 net.cpp:228] Setting up conv5
I1226 08:55:33.033680 91127 net.cpp:235] Top shape: 256 256 13 13 (11075584)
I1226 08:55:33.033707 91127 net.cpp:243] Memory required for data: 2049800192
I1226 08:55:33.033797 91127 layer_factory.hpp:114] Creating layer relu5
I1226 08:55:33.033864 91127 net.cpp:178] Creating Layer relu5
I1226 08:55:33.034032 91127 net.cpp:612] relu5 <- conv5
I1226 08:55:33.034085 91127 net.cpp:573] relu5 -> conv5 (in-place)
I1226 08:55:33.034236 91127 net.cpp:228] Setting up relu5
I1226 08:55:33.034329 91127 net.cpp:235] Top shape: 256 256 13 13 (11075584)
I1226 08:55:33.034353 91127 net.cpp:243] Memory required for data: 2094102528
I1226 08:55:33.044474 91127 layer_factory.hpp:114] Creating layer pool5
I1226 08:55:33.044625 91127 net.cpp:178] Creating Layer pool5
I1226 08:55:33.044666 91127 net.cpp:612] pool5 <- conv5
I1226 08:55:33.044747 91127 net.cpp:586] pool5 -> pool5
I1226 08:55:33.044894 91127 net.cpp:228] Setting up pool5
I1226 08:55:33.045058 91127 net.cpp:235] Top shape: 256 256 6 6 (2359296)
I1226 08:55:33.045188 91127 net.cpp:243] Memory required for data: 2103539712
I1226 08:55:33.045220 91127 layer_factory.hpp:114] Creating layer fc6
I1226 08:55:33.045312 91127 net.cpp:178] Creating Layer fc6
I1226 08:55:33.045351 91127 net.cpp:612] fc6 <- pool5
I1226 08:55:33.045451 91127 net.cpp:586] fc6 -> fc6
I1226 08:55:33.652916 95005 net.cpp:228] Setting up conv2
I1226 08:55:33.653053 95005 net.cpp:235] Top shape: 256 256 27 27 (47775744)
I1226 08:55:33.653136 95005 net.cpp:243] Memory required for data: 1313175552
I1226 08:55:33.653265 95005 layer_factory.hpp:114] Creating layer relu2
I1226 08:55:33.653758 95005 net.cpp:178] Creating Layer relu2
I1226 08:55:33.653903 95005 net.cpp:612] relu2 <- conv2
I1226 08:55:33.654103 95005 net.cpp:573] relu2 -> conv2 (in-place)
I1226 08:55:33.654275 95005 net.cpp:228] Setting up relu2
I1226 08:55:33.654366 95005 net.cpp:235] Top shape: 256 256 27 27 (47775744)
I1226 08:55:33.654516 95005 net.cpp:243] Memory required for data: 1504278528
I1226 08:55:33.654569 95005 layer_factory.hpp:114] Creating layer norm2
I1226 08:55:33.655226 95005 net.cpp:178] Creating Layer norm2
I1226 08:55:33.655365 95005 net.cpp:612] norm2 <- conv2
I1226 08:55:33.655467 95005 net.cpp:586] norm2 -> norm2
I1226 08:55:33.655778 95005 net.cpp:228] Setting up norm2
I1226 08:55:33.655874 95005 net.cpp:235] Top shape: 256 256 27 27 (47775744)
I1226 08:55:33.655942 95005 net.cpp:243] Memory required for data: 1695381504
I1226 08:55:33.656138 95005 layer_factory.hpp:114] Creating layer pool2
I1226 08:55:33.656285 95005 net.cpp:178] Creating Layer pool2
I1226 08:55:33.656816 95005 net.cpp:612] pool2 <- norm2
I1226 08:55:33.656970 95005 net.cpp:586] pool2 -> pool2
I1226 08:55:33.657202 95005 net.cpp:228] Setting up pool2
I1226 08:55:33.657444 95005 net.cpp:235] Top shape: 256 256 13 13 (11075584)
I1226 08:55:33.657531 95005 net.cpp:243] Memory required for data: 1739683840
I1226 08:55:33.657580 95005 layer_factory.hpp:114] Creating layer conv3
I1226 08:55:33.657780 95005 net.cpp:178] Creating Layer conv3
I1226 08:55:33.657855 95005 net.cpp:612] conv3 <- pool2
I1226 08:55:33.658383 95005 net.cpp:586] conv3 -> conv3
I1226 08:55:33.756367 94304 net.cpp:228] Setting up conv2
I1226 08:55:33.756505 94304 net.cpp:235] Top shape: 256 256 27 27 (47775744)
I1226 08:55:33.756561 94304 net.cpp:243] Memory required for data: 1313175552
I1226 08:55:33.756649 94304 layer_factory.hpp:114] Creating layer relu2
I1226 08:55:33.756752 94304 net.cpp:178] Creating Layer relu2
I1226 08:55:33.756849 94304 net.cpp:612] relu2 <- conv2
I1226 08:55:33.757331 94304 net.cpp:573] relu2 -> conv2 (in-place)
I1226 08:55:33.757609 94304 net.cpp:228] Setting up relu2
I1226 08:55:33.757684 94304 net.cpp:235] Top shape: 256 256 27 27 (47775744)
I1226 08:55:33.757714 94304 net.cpp:243] Memory required for data: 1504278528
I1226 08:55:33.757750 94304 layer_factory.hpp:114] Creating layer norm2
I1226 08:55:33.757844 94304 net.cpp:178] Creating Layer norm2
I1226 08:55:33.757890 94304 net.cpp:612] norm2 <- conv2
I1226 08:55:33.757943 94304 net.cpp:586] norm2 -> norm2
I1226 08:55:33.758105 94304 net.cpp:228] Setting up norm2
I1226 08:55:33.758159 94304 net.cpp:235] Top shape: 256 256 27 27 (47775744)
I1226 08:55:33.758210 94304 net.cpp:243] Memory required for data: 1695381504
I1226 08:55:33.758257 94304 layer_factory.hpp:114] Creating layer pool2
I1226 08:55:33.758376 94304 net.cpp:178] Creating Layer pool2
I1226 08:55:33.758419 94304 net.cpp:612] pool2 <- norm2
I1226 08:55:33.758492 94304 net.cpp:586] pool2 -> pool2
I1226 08:55:33.759253 94304 net.cpp:228] Setting up pool2
I1226 08:55:33.759543 94304 net.cpp:235] Top shape: 256 256 13 13 (11075584)
I1226 08:55:33.759584 94304 net.cpp:243] Memory required for data: 1739683840
I1226 08:55:33.759625 94304 layer_factory.hpp:114] Creating layer conv3
I1226 08:55:33.759740 94304 net.cpp:178] Creating Layer conv3
I1226 08:55:33.759779 94304 net.cpp:612] conv3 <- pool2
I1226 08:55:33.759910 94304 net.cpp:586] conv3 -> conv3
I1226 08:55:35.545173 95005 net.cpp:228] Setting up conv3
I1226 08:55:35.545315 95005 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 08:55:35.545368 95005 net.cpp:243] Memory required for data: 1806137344
I1226 08:55:35.545570 95005 layer_factory.hpp:114] Creating layer relu3
I1226 08:55:35.545799 95005 net.cpp:178] Creating Layer relu3
I1226 08:55:35.545912 95005 net.cpp:612] relu3 <- conv3
I1226 08:55:35.545977 95005 net.cpp:573] relu3 -> conv3 (in-place)
I1226 08:55:35.546164 95005 net.cpp:228] Setting up relu3
I1226 08:55:35.546324 95005 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 08:55:35.546371 95005 net.cpp:243] Memory required for data: 1872590848
I1226 08:55:35.546422 95005 layer_factory.hpp:114] Creating layer conv4
I1226 08:55:35.546566 95005 net.cpp:178] Creating Layer conv4
I1226 08:55:35.546628 95005 net.cpp:612] conv4 <- conv3
I1226 08:55:35.546699 95005 net.cpp:586] conv4 -> conv4
I1226 08:55:35.640602 94304 net.cpp:228] Setting up conv3
I1226 08:55:35.640719 94304 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 08:55:35.640753 94304 net.cpp:243] Memory required for data: 1806137344
I1226 08:55:35.640938 94304 layer_factory.hpp:114] Creating layer relu3
I1226 08:55:35.641222 94304 net.cpp:178] Creating Layer relu3
I1226 08:55:35.641315 94304 net.cpp:612] relu3 <- conv3
I1226 08:55:35.641367 94304 net.cpp:573] relu3 -> conv3 (in-place)
I1226 08:55:35.641543 94304 net.cpp:228] Setting up relu3
I1226 08:55:35.642057 94304 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 08:55:35.642135 94304 net.cpp:243] Memory required for data: 1872590848
I1226 08:55:35.642244 94304 layer_factory.hpp:114] Creating layer conv4
I1226 08:55:35.642416 94304 net.cpp:178] Creating Layer conv4
I1226 08:55:35.642457 94304 net.cpp:612] conv4 <- conv3
I1226 08:55:35.642520 94304 net.cpp:586] conv4 -> conv4
I1226 08:55:36.842593 85894 net.cpp:228] Setting up fc6
I1226 08:55:36.842706 85894 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 08:55:36.842732 85894 net.cpp:243] Memory required for data: 2107734016
I1226 08:55:36.842813 85894 layer_factory.hpp:114] Creating layer relu6
I1226 08:55:36.842895 85894 net.cpp:178] Creating Layer relu6
I1226 08:55:36.842938 85894 net.cpp:612] relu6 <- fc6
I1226 08:55:36.842993 85894 net.cpp:573] relu6 -> fc6 (in-place)
I1226 08:55:36.843093 85894 net.cpp:228] Setting up relu6
I1226 08:55:36.843240 85894 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 08:55:36.843268 85894 net.cpp:243] Memory required for data: 2111928320
I1226 08:55:36.843302 85894 layer_factory.hpp:114] Creating layer drop6
I1226 08:55:36.843385 85894 net.cpp:178] Creating Layer drop6
I1226 08:55:36.843420 85894 net.cpp:612] drop6 <- fc6
I1226 08:55:36.843459 85894 net.cpp:573] drop6 -> fc6 (in-place)
I1226 08:55:36.843521 85894 net.cpp:228] Setting up drop6
I1226 08:55:36.843565 85894 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 08:55:36.843588 85894 net.cpp:243] Memory required for data: 2116122624
I1226 08:55:36.843616 85894 layer_factory.hpp:114] Creating layer fc7
I1226 08:55:36.843693 85894 net.cpp:178] Creating Layer fc7
I1226 08:55:36.843729 85894 net.cpp:612] fc7 <- fc6
I1226 08:55:36.843780 85894 net.cpp:586] fc7 -> fc7
I1226 08:55:37.155555 95005 net.cpp:228] Setting up conv4
I1226 08:55:37.155702 95005 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 08:55:37.155827 95005 net.cpp:243] Memory required for data: 1939044352
I1226 08:55:37.155957 95005 layer_factory.hpp:114] Creating layer relu4
I1226 08:55:37.156600 95005 net.cpp:178] Creating Layer relu4
I1226 08:55:37.156709 95005 net.cpp:612] relu4 <- conv4
I1226 08:55:37.157119 95005 net.cpp:573] relu4 -> conv4 (in-place)
I1226 08:55:37.157387 95005 net.cpp:228] Setting up relu4
I1226 08:55:37.158048 95005 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 08:55:37.158278 95005 net.cpp:243] Memory required for data: 2005497856
I1226 08:55:37.158370 95005 layer_factory.hpp:114] Creating layer conv5
I1226 08:55:37.158576 95005 net.cpp:178] Creating Layer conv5
I1226 08:55:37.158649 95005 net.cpp:612] conv5 <- conv4
I1226 08:55:37.159112 95005 net.cpp:586] conv5 -> conv5
I1226 08:55:37.245134 94304 net.cpp:228] Setting up conv4
I1226 08:55:37.245250 94304 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 08:55:37.245283 94304 net.cpp:243] Memory required for data: 1939044352
I1226 08:55:37.245352 94304 layer_factory.hpp:114] Creating layer relu4
I1226 08:55:37.245447 94304 net.cpp:178] Creating Layer relu4
I1226 08:55:37.245496 94304 net.cpp:612] relu4 <- conv4
I1226 08:55:37.245546 94304 net.cpp:573] relu4 -> conv4 (in-place)
I1226 08:55:37.245661 94304 net.cpp:228] Setting up relu4
I1226 08:55:37.245713 94304 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 08:55:37.245740 94304 net.cpp:243] Memory required for data: 2005497856
I1226 08:55:37.245772 94304 layer_factory.hpp:114] Creating layer conv5
I1226 08:55:37.245906 94304 net.cpp:178] Creating Layer conv5
I1226 08:55:37.245952 94304 net.cpp:612] conv5 <- conv4
I1226 08:55:37.246007 94304 net.cpp:586] conv5 -> conv5
I1226 08:55:38.320298 95005 net.cpp:228] Setting up conv5
I1226 08:55:38.320415 95005 net.cpp:235] Top shape: 256 256 13 13 (11075584)
I1226 08:55:38.320452 95005 net.cpp:243] Memory required for data: 2049800192
I1226 08:55:38.320623 95005 layer_factory.hpp:114] Creating layer relu5
I1226 08:55:38.320904 95005 net.cpp:178] Creating Layer relu5
I1226 08:55:38.320969 95005 net.cpp:612] relu5 <- conv5
I1226 08:55:38.321024 95005 net.cpp:573] relu5 -> conv5 (in-place)
I1226 08:55:38.321161 95005 net.cpp:228] Setting up relu5
I1226 08:55:38.321259 95005 net.cpp:235] Top shape: 256 256 13 13 (11075584)
I1226 08:55:38.321295 95005 net.cpp:243] Memory required for data: 2094102528
I1226 08:55:38.321337 95005 layer_factory.hpp:114] Creating layer pool5
I1226 08:55:38.321415 95005 net.cpp:178] Creating Layer pool5
I1226 08:55:38.338841 95005 net.cpp:612] pool5 <- conv5
I1226 08:55:38.338949 95005 net.cpp:586] pool5 -> pool5
I1226 08:55:38.339128 95005 net.cpp:228] Setting up pool5
I1226 08:55:38.339201 95005 net.cpp:235] Top shape: 256 256 6 6 (2359296)
I1226 08:55:38.339234 95005 net.cpp:243] Memory required for data: 2103539712
I1226 08:55:38.339278 95005 layer_factory.hpp:114] Creating layer fc6
I1226 08:55:38.339357 95005 net.cpp:178] Creating Layer fc6
I1226 08:55:38.339402 95005 net.cpp:612] fc6 <- pool5
I1226 08:55:38.339455 95005 net.cpp:586] fc6 -> fc6
I1226 08:55:38.406343 94304 net.cpp:228] Setting up conv5
I1226 08:55:38.406460 94304 net.cpp:235] Top shape: 256 256 13 13 (11075584)
I1226 08:55:38.406491 94304 net.cpp:243] Memory required for data: 2049800192
I1226 08:55:38.406574 94304 layer_factory.hpp:114] Creating layer relu5
I1226 08:55:38.406646 94304 net.cpp:178] Creating Layer relu5
I1226 08:55:38.406682 94304 net.cpp:612] relu5 <- conv5
I1226 08:55:38.406728 94304 net.cpp:573] relu5 -> conv5 (in-place)
I1226 08:55:38.406867 94304 net.cpp:228] Setting up relu5
I1226 08:55:38.406925 94304 net.cpp:235] Top shape: 256 256 13 13 (11075584)
I1226 08:55:38.406952 94304 net.cpp:243] Memory required for data: 2094102528
I1226 08:55:38.406987 94304 layer_factory.hpp:114] Creating layer pool5
I1226 08:55:38.407063 94304 net.cpp:178] Creating Layer pool5
I1226 08:55:38.407115 94304 net.cpp:612] pool5 <- conv5
I1226 08:55:38.407163 94304 net.cpp:586] pool5 -> pool5
I1226 08:55:38.407279 94304 net.cpp:228] Setting up pool5
I1226 08:55:38.407342 94304 net.cpp:235] Top shape: 256 256 6 6 (2359296)
I1226 08:55:38.407368 94304 net.cpp:243] Memory required for data: 2103539712
I1226 08:55:38.407398 94304 layer_factory.hpp:114] Creating layer fc6
I1226 08:55:38.407462 94304 net.cpp:178] Creating Layer fc6
I1226 08:55:38.407492 94304 net.cpp:612] fc6 <- pool5
I1226 08:55:38.407536 94304 net.cpp:586] fc6 -> fc6
I1226 08:55:39.112212 85894 net.cpp:228] Setting up fc7
I1226 08:55:39.112325 85894 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 08:55:39.112402 85894 net.cpp:243] Memory required for data: 2120316928
I1226 08:55:39.112463 85894 layer_factory.hpp:114] Creating layer relu7
I1226 08:55:39.112526 85894 net.cpp:178] Creating Layer relu7
I1226 08:55:39.112571 85894 net.cpp:612] relu7 <- fc7
I1226 08:55:39.112610 85894 net.cpp:573] relu7 -> fc7 (in-place)
I1226 08:55:39.112697 85894 net.cpp:228] Setting up relu7
I1226 08:55:39.112743 85894 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 08:55:39.112767 85894 net.cpp:243] Memory required for data: 2124511232
I1226 08:55:39.112795 85894 layer_factory.hpp:114] Creating layer drop7
I1226 08:55:39.112840 85894 net.cpp:178] Creating Layer drop7
I1226 08:55:39.112866 85894 net.cpp:612] drop7 <- fc7
I1226 08:55:39.112911 85894 net.cpp:573] drop7 -> fc7 (in-place)
I1226 08:55:39.112953 85894 net.cpp:228] Setting up drop7
I1226 08:55:39.112987 85894 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 08:55:39.113008 85894 net.cpp:243] Memory required for data: 2128705536
I1226 08:55:39.113042 85894 layer_factory.hpp:114] Creating layer fc8
I1226 08:55:39.113100 85894 net.cpp:178] Creating Layer fc8
I1226 08:55:39.113126 85894 net.cpp:612] fc8 <- fc7
I1226 08:55:39.113186 85894 net.cpp:586] fc8 -> fc8
I1226 08:55:39.164944 91127 net.cpp:228] Setting up fc6
I1226 08:55:39.165058 91127 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 08:55:39.165088 91127 net.cpp:243] Memory required for data: 2107734016
I1226 08:55:39.165146 91127 layer_factory.hpp:114] Creating layer relu6
I1226 08:55:39.165233 91127 net.cpp:178] Creating Layer relu6
I1226 08:55:39.165362 91127 net.cpp:612] relu6 <- fc6
I1226 08:55:39.165449 91127 net.cpp:573] relu6 -> fc6 (in-place)
I1226 08:55:39.165650 91127 net.cpp:228] Setting up relu6
I1226 08:55:39.165707 91127 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 08:55:39.165731 91127 net.cpp:243] Memory required for data: 2111928320
I1226 08:55:39.165762 91127 layer_factory.hpp:114] Creating layer drop6
I1226 08:55:39.165815 91127 net.cpp:178] Creating Layer drop6
I1226 08:55:39.165850 91127 net.cpp:612] drop6 <- fc6
I1226 08:55:39.165886 91127 net.cpp:573] drop6 -> fc6 (in-place)
I1226 08:55:39.165940 91127 net.cpp:228] Setting up drop6
I1226 08:55:39.165978 91127 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 08:55:39.166007 91127 net.cpp:243] Memory required for data: 2116122624
I1226 08:55:39.166035 91127 layer_factory.hpp:114] Creating layer fc7
I1226 08:55:39.166095 91127 net.cpp:178] Creating Layer fc7
I1226 08:55:39.166128 91127 net.cpp:612] fc7 <- fc6
I1226 08:55:39.166167 91127 net.cpp:586] fc7 -> fc7
I1226 08:55:39.677559 85894 net.cpp:228] Setting up fc8
I1226 08:55:39.677675 85894 net.cpp:235] Top shape: 256 1000 (256000)
I1226 08:55:39.677703 85894 net.cpp:243] Memory required for data: 2129729536
I1226 08:55:39.677758 85894 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 08:55:39.677848 85894 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 08:55:39.677893 85894 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 08:55:39.677933 85894 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 08:55:39.677992 85894 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 08:55:39.678073 85894 net.cpp:228] Setting up fc8_fc8_0_split
I1226 08:55:39.678112 85894 net.cpp:235] Top shape: 256 1000 (256000)
I1226 08:55:39.678141 85894 net.cpp:235] Top shape: 256 1000 (256000)
I1226 08:55:39.678174 85894 net.cpp:243] Memory required for data: 2131777536
I1226 08:55:39.678203 85894 layer_factory.hpp:114] Creating layer accuracy
I1226 08:55:39.678252 85894 net.cpp:178] Creating Layer accuracy
I1226 08:55:39.678279 85894 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 08:55:39.678315 85894 net.cpp:612] accuracy <- label_data_1_split_0
I1226 08:55:39.678383 85894 net.cpp:586] accuracy -> accuracy
I1226 08:55:39.678436 85894 net.cpp:228] Setting up accuracy
I1226 08:55:39.678472 85894 net.cpp:235] Top shape: (1)
I1226 08:55:39.678501 85894 net.cpp:243] Memory required for data: 2131777540
I1226 08:55:39.678529 85894 layer_factory.hpp:114] Creating layer loss
I1226 08:55:39.678575 85894 net.cpp:178] Creating Layer loss
I1226 08:55:39.678603 85894 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 08:55:39.678642 85894 net.cpp:612] loss <- label_data_1_split_1
I1226 08:55:39.678675 85894 net.cpp:586] loss -> loss
I1226 08:55:39.678730 85894 layer_factory.hpp:114] Creating layer loss
I1226 08:55:39.713567 85894 net.cpp:228] Setting up loss
I1226 08:55:39.713677 85894 net.cpp:235] Top shape: (1)
I1226 08:55:39.713982 85894 net.cpp:238]     with loss weight 1
I1226 08:55:39.714138 85894 net.cpp:243] Memory required for data: 2131777544
I1226 08:55:39.714187 85894 net.cpp:305] loss needs backward computation.
I1226 08:55:39.714231 85894 net.cpp:307] accuracy does not need backward computation.
I1226 08:55:39.714268 85894 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 08:55:39.714308 85894 net.cpp:305] fc8 needs backward computation.
I1226 08:55:39.714386 85894 net.cpp:305] drop7 needs backward computation.
I1226 08:55:39.714426 85894 net.cpp:305] relu7 needs backward computation.
I1226 08:55:39.714453 85894 net.cpp:305] fc7 needs backward computation.
I1226 08:55:39.714483 85894 net.cpp:305] drop6 needs backward computation.
I1226 08:55:39.714510 85894 net.cpp:305] relu6 needs backward computation.
I1226 08:55:39.714539 85894 net.cpp:305] fc6 needs backward computation.
I1226 08:55:39.714566 85894 net.cpp:305] pool5 needs backward computation.
I1226 08:55:39.714596 85894 net.cpp:305] relu5 needs backward computation.
I1226 08:55:39.714622 85894 net.cpp:305] conv5 needs backward computation.
I1226 08:55:39.714650 85894 net.cpp:305] relu4 needs backward computation.
I1226 08:55:39.714679 85894 net.cpp:305] conv4 needs backward computation.
I1226 08:55:39.714709 85894 net.cpp:305] relu3 needs backward computation.
I1226 08:55:39.714735 85894 net.cpp:305] conv3 needs backward computation.
I1226 08:55:39.714764 85894 net.cpp:305] pool2 needs backward computation.
I1226 08:55:39.714793 85894 net.cpp:305] norm2 needs backward computation.
I1226 08:55:39.714821 85894 net.cpp:305] relu2 needs backward computation.
I1226 08:55:39.714848 85894 net.cpp:305] conv2 needs backward computation.
I1226 08:55:39.714876 85894 net.cpp:305] pool1 needs backward computation.
I1226 08:55:39.714905 85894 net.cpp:305] norm1 needs backward computation.
I1226 08:55:39.714933 85894 net.cpp:305] relu1 needs backward computation.
I1226 08:55:39.714959 85894 net.cpp:305] conv1 needs backward computation.
I1226 08:55:39.714989 85894 net.cpp:307] label_data_1_split does not need backward computation.
I1226 08:55:39.715019 85894 net.cpp:307] data does not need backward computation.
I1226 08:55:39.715045 85894 net.cpp:349] This network produces output accuracy
I1226 08:55:39.715076 85894 net.cpp:349] This network produces output loss
I1226 08:55:39.715163 85894 net.cpp:363] Network initialization done.
I1226 08:55:39.715734 85894 solver.cpp:119] Solver scaffolding done.
I1226 08:55:39.715948 85894 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 08:55:41.388530 89709 net.cpp:228] Setting up fc6
I1226 08:55:41.388635 89709 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 08:55:41.388661 89709 net.cpp:243] Memory required for data: 2107734016
I1226 08:55:41.388715 89709 layer_factory.hpp:114] Creating layer relu6
I1226 08:55:41.388789 89709 net.cpp:178] Creating Layer relu6
I1226 08:55:41.388821 89709 net.cpp:612] relu6 <- fc6
I1226 08:55:41.388866 89709 net.cpp:573] relu6 -> fc6 (in-place)
I1226 08:55:41.389047 89709 net.cpp:228] Setting up relu6
I1226 08:55:41.389099 89709 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 08:55:41.389132 89709 net.cpp:243] Memory required for data: 2111928320
I1226 08:55:41.389160 89709 layer_factory.hpp:114] Creating layer drop6
I1226 08:55:41.389209 89709 net.cpp:178] Creating Layer drop6
I1226 08:55:41.389235 89709 net.cpp:612] drop6 <- fc6
I1226 08:55:41.389284 89709 net.cpp:573] drop6 -> fc6 (in-place)
I1226 08:55:41.389339 89709 net.cpp:228] Setting up drop6
I1226 08:55:41.389400 89709 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 08:55:41.389425 89709 net.cpp:243] Memory required for data: 2116122624
I1226 08:55:41.389451 89709 layer_factory.hpp:114] Creating layer fc7
I1226 08:55:41.389518 89709 net.cpp:178] Creating Layer fc7
I1226 08:55:41.389549 89709 net.cpp:612] fc7 <- fc6
I1226 08:55:41.389586 89709 net.cpp:586] fc7 -> fc7
I1226 08:55:41.414541 91127 net.cpp:228] Setting up fc7
I1226 08:55:41.414654 91127 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 08:55:41.414682 91127 net.cpp:243] Memory required for data: 2120316928
I1226 08:55:41.414738 91127 layer_factory.hpp:114] Creating layer relu7
I1226 08:55:41.414825 91127 net.cpp:178] Creating Layer relu7
I1226 08:55:41.414952 91127 net.cpp:612] relu7 <- fc7
I1226 08:55:41.414990 91127 net.cpp:573] relu7 -> fc7 (in-place)
I1226 08:55:41.415082 91127 net.cpp:228] Setting up relu7
I1226 08:55:41.415138 91127 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 08:55:41.415160 91127 net.cpp:243] Memory required for data: 2124511232
I1226 08:55:41.415189 91127 layer_factory.hpp:114] Creating layer drop7
I1226 08:55:41.415237 91127 net.cpp:178] Creating Layer drop7
I1226 08:55:41.415266 91127 net.cpp:612] drop7 <- fc7
I1226 08:55:41.415302 91127 net.cpp:573] drop7 -> fc7 (in-place)
I1226 08:55:41.415354 91127 net.cpp:228] Setting up drop7
I1226 08:55:41.415416 91127 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 08:55:41.415464 91127 net.cpp:243] Memory required for data: 2128705536
I1226 08:55:41.415503 91127 layer_factory.hpp:114] Creating layer fc8
I1226 08:55:41.415563 91127 net.cpp:178] Creating Layer fc8
I1226 08:55:41.415593 91127 net.cpp:612] fc8 <- fc7
I1226 08:55:41.415633 91127 net.cpp:586] fc8 -> fc8
I1226 08:55:41.477100 86102 net.cpp:228] Setting up fc6
I1226 08:55:41.477211 86102 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 08:55:41.477239 86102 net.cpp:243] Memory required for data: 2107734016
I1226 08:55:41.477293 86102 layer_factory.hpp:114] Creating layer relu6
I1226 08:55:41.477362 86102 net.cpp:178] Creating Layer relu6
I1226 08:55:41.477394 86102 net.cpp:612] relu6 <- fc6
I1226 08:55:41.477440 86102 net.cpp:573] relu6 -> fc6 (in-place)
I1226 08:55:41.477656 86102 net.cpp:228] Setting up relu6
I1226 08:55:41.477710 86102 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 08:55:41.477735 86102 net.cpp:243] Memory required for data: 2111928320
I1226 08:55:41.477762 86102 layer_factory.hpp:114] Creating layer drop6
I1226 08:55:41.477825 86102 net.cpp:178] Creating Layer drop6
I1226 08:55:41.477859 86102 net.cpp:612] drop6 <- fc6
I1226 08:55:41.477895 86102 net.cpp:573] drop6 -> fc6 (in-place)
I1226 08:55:41.477946 86102 net.cpp:228] Setting up drop6
I1226 08:55:41.477987 86102 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 08:55:41.478009 86102 net.cpp:243] Memory required for data: 2116122624
I1226 08:55:41.478035 86102 layer_factory.hpp:114] Creating layer fc7
I1226 08:55:41.478088 86102 net.cpp:178] Creating Layer fc7
I1226 08:55:41.478123 86102 net.cpp:612] fc7 <- fc6
I1226 08:55:41.478160 86102 net.cpp:586] fc7 -> fc7
I1226 08:55:41.595641 92150 net.cpp:228] Setting up fc6
I1226 08:55:41.595762 92150 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 08:55:41.595795 92150 net.cpp:243] Memory required for data: 2107734016
I1226 08:55:41.595880 92150 layer_factory.hpp:114] Creating layer relu6
I1226 08:55:41.596192 92150 net.cpp:178] Creating Layer relu6
I1226 08:55:41.596238 92150 net.cpp:612] relu6 <- fc6
I1226 08:55:41.596307 92150 net.cpp:573] relu6 -> fc6 (in-place)
I1226 08:55:41.596561 92150 net.cpp:228] Setting up relu6
I1226 08:55:41.596797 92150 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 08:55:41.596830 92150 net.cpp:243] Memory required for data: 2111928320
I1226 08:55:41.596870 92150 layer_factory.hpp:114] Creating layer drop6
I1226 08:55:41.596937 92150 net.cpp:178] Creating Layer drop6
I1226 08:55:41.597121 92150 net.cpp:612] drop6 <- fc6
I1226 08:55:41.597169 92150 net.cpp:573] drop6 -> fc6 (in-place)
I1226 08:55:41.597235 92150 net.cpp:228] Setting up drop6
I1226 08:55:41.597427 92150 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 08:55:41.597461 92150 net.cpp:243] Memory required for data: 2116122624
I1226 08:55:41.597499 92150 layer_factory.hpp:114] Creating layer fc7
I1226 08:55:41.597578 92150 net.cpp:178] Creating Layer fc7
I1226 08:55:41.597776 92150 net.cpp:612] fc7 <- fc6
I1226 08:55:41.597842 92150 net.cpp:586] fc7 -> fc7
I1226 08:55:38.432232 90165 net.cpp:228] Setting up fc6
I1226 08:55:38.432343 90165 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 08:55:38.432369 90165 net.cpp:243] Memory required for data: 2107734016
I1226 08:55:38.432425 90165 layer_factory.hpp:114] Creating layer relu6
I1226 08:55:38.432492 90165 net.cpp:178] Creating Layer relu6
I1226 08:55:38.432530 90165 net.cpp:612] relu6 <- fc6
I1226 08:55:38.432569 90165 net.cpp:573] relu6 -> fc6 (in-place)
I1226 08:55:38.437495 90165 net.cpp:228] Setting up relu6
I1226 08:55:38.437598 90165 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 08:55:38.437669 90165 net.cpp:243] Memory required for data: 2111928320
I1226 08:55:38.437712 90165 layer_factory.hpp:114] Creating layer drop6
I1226 08:55:38.437783 90165 net.cpp:178] Creating Layer drop6
I1226 08:55:38.437947 90165 net.cpp:612] drop6 <- fc6
I1226 08:55:38.438011 90165 net.cpp:573] drop6 -> fc6 (in-place)
I1226 08:55:38.438091 90165 net.cpp:228] Setting up drop6
I1226 08:55:38.438136 90165 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 08:55:38.438169 90165 net.cpp:243] Memory required for data: 2116122624
I1226 08:55:38.438205 90165 layer_factory.hpp:114] Creating layer fc7
I1226 08:55:38.438287 90165 net.cpp:178] Creating Layer fc7
I1226 08:55:38.438328 90165 net.cpp:612] fc7 <- fc6
I1226 08:55:38.438375 90165 net.cpp:586] fc7 -> fc7
I1226 08:55:41.963279 91127 net.cpp:228] Setting up fc8
I1226 08:55:41.963423 91127 net.cpp:235] Top shape: 256 1000 (256000)
I1226 08:55:41.963454 91127 net.cpp:243] Memory required for data: 2129729536
I1226 08:55:41.963536 91127 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 08:55:41.963609 91127 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 08:55:41.963640 91127 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 08:55:41.963678 91127 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 08:55:41.963735 91127 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 08:55:41.963821 91127 net.cpp:228] Setting up fc8_fc8_0_split
I1226 08:55:41.963863 91127 net.cpp:235] Top shape: 256 1000 (256000)
I1226 08:55:41.963902 91127 net.cpp:235] Top shape: 256 1000 (256000)
I1226 08:55:41.963924 91127 net.cpp:243] Memory required for data: 2131777536
I1226 08:55:41.963953 91127 layer_factory.hpp:114] Creating layer accuracy
I1226 08:55:41.964126 91127 net.cpp:178] Creating Layer accuracy
I1226 08:55:41.964154 91127 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 08:55:41.964195 91127 net.cpp:612] accuracy <- label_data_1_split_0
I1226 08:55:41.964241 91127 net.cpp:586] accuracy -> accuracy
I1226 08:55:41.964285 91127 net.cpp:228] Setting up accuracy
I1226 08:55:41.964319 91127 net.cpp:235] Top shape: (1)
I1226 08:55:41.964342 91127 net.cpp:243] Memory required for data: 2131777540
I1226 08:55:41.964395 91127 layer_factory.hpp:114] Creating layer loss
I1226 08:55:41.964460 91127 net.cpp:178] Creating Layer loss
I1226 08:55:41.964489 91127 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 08:55:41.964519 91127 net.cpp:612] loss <- label_data_1_split_1
I1226 08:55:41.964551 91127 net.cpp:586] loss -> loss
I1226 08:55:41.964612 91127 layer_factory.hpp:114] Creating layer loss
I1226 08:55:41.996834 91127 net.cpp:228] Setting up loss
I1226 08:55:41.997033 91127 net.cpp:235] Top shape: (1)
I1226 08:55:41.997079 91127 net.cpp:238]     with loss weight 1
I1226 08:55:41.997225 91127 net.cpp:243] Memory required for data: 2131777544
I1226 08:55:41.997268 91127 net.cpp:305] loss needs backward computation.
I1226 08:55:41.997308 91127 net.cpp:307] accuracy does not need backward computation.
I1226 08:55:41.997344 91127 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 08:55:41.997406 91127 net.cpp:305] fc8 needs backward computation.
I1226 08:55:41.997440 91127 net.cpp:305] drop7 needs backward computation.
I1226 08:55:41.997486 91127 net.cpp:305] relu7 needs backward computation.
I1226 08:55:41.997519 91127 net.cpp:305] fc7 needs backward computation.
I1226 08:55:41.997557 91127 net.cpp:305] drop6 needs backward computation.
I1226 08:55:41.997596 91127 net.cpp:305] relu6 needs backward computation.
I1226 08:55:41.997632 91127 net.cpp:305] fc6 needs backward computation.
I1226 08:55:41.997670 91127 net.cpp:305] pool5 needs backward computation.
I1226 08:55:41.997701 91127 net.cpp:305] relu5 needs backward computation.
I1226 08:55:41.997732 91127 net.cpp:305] conv5 needs backward computation.
I1226 08:55:41.997762 91127 net.cpp:305] relu4 needs backward computation.
I1226 08:55:41.997792 91127 net.cpp:305] conv4 needs backward computation.
I1226 08:55:41.997822 91127 net.cpp:305] relu3 needs backward computation.
I1226 08:55:41.997851 91127 net.cpp:305] conv3 needs backward computation.
I1226 08:55:41.997884 91127 net.cpp:305] pool2 needs backward computation.
I1226 08:55:41.997921 91127 net.cpp:305] norm2 needs backward computation.
I1226 08:55:41.997958 91127 net.cpp:305] relu2 needs backward computation.
I1226 08:55:41.997990 91127 net.cpp:305] conv2 needs backward computation.
I1226 08:55:41.998021 91127 net.cpp:305] pool1 needs backward computation.
I1226 08:55:41.998052 91127 net.cpp:305] norm1 needs backward computation.
I1226 08:55:41.998091 91127 net.cpp:305] relu1 needs backward computation.
I1226 08:55:41.998121 91127 net.cpp:305] conv1 needs backward computation.
I1226 08:55:41.998153 91127 net.cpp:307] label_data_1_split does not need backward computation.
I1226 08:55:41.998191 91127 net.cpp:307] data does not need backward computation.
I1226 08:55:41.998224 91127 net.cpp:349] This network produces output accuracy
I1226 08:55:41.998260 91127 net.cpp:349] This network produces output loss
I1226 08:55:41.998360 91127 net.cpp:363] Network initialization done.
I1226 08:55:41.999023 91127 solver.cpp:119] Solver scaffolding done.
I1226 08:55:41.999244 91127 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 08:55:43.870010 85894 caffe.cpp:376] Configuring multinode setup
I1226 08:55:43.886426 85894 caffe.cpp:386] Starting parameter server in mpi environment
I1226 08:55:43.941548 119888 net.cpp:228] Setting up fc6
I1226 08:55:43.941694 119888 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 08:55:43.941726 119888 net.cpp:243] Memory required for data: 2107734016
I1226 08:55:43.941813 119888 layer_factory.hpp:114] Creating layer relu6
I1226 08:55:43.941895 119888 net.cpp:178] Creating Layer relu6
I1226 08:55:43.942055 119888 net.cpp:612] relu6 <- fc6
I1226 08:55:43.942108 119888 net.cpp:573] relu6 -> fc6 (in-place)
I1226 08:55:43.942220 119888 net.cpp:228] Setting up relu6
I1226 08:55:43.942276 119888 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 08:55:43.942302 119888 net.cpp:243] Memory required for data: 2111928320
I1226 08:55:43.942333 119888 layer_factory.hpp:114] Creating layer drop6
I1226 08:55:43.942394 119888 net.cpp:178] Creating Layer drop6
I1226 08:55:43.942435 119888 net.cpp:612] drop6 <- fc6
I1226 08:55:43.942483 119888 net.cpp:573] drop6 -> fc6 (in-place)
I1226 08:55:43.942558 119888 net.cpp:228] Setting up drop6
I1226 08:55:43.942598 119888 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 08:55:43.942648 119888 net.cpp:243] Memory required for data: 2116122624
I1226 08:55:43.942678 119888 layer_factory.hpp:114] Creating layer fc7
I1226 08:55:43.942735 119888 net.cpp:178] Creating Layer fc7
I1226 08:55:43.942776 119888 net.cpp:612] fc7 <- fc6
I1226 08:55:43.942816 119888 net.cpp:586] fc7 -> fc7
I1226 08:55:44.038425 89709 net.cpp:228] Setting up fc7
I1226 08:55:44.038534 89709 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 08:55:44.038563 89709 net.cpp:243] Memory required for data: 2120316928
I1226 08:55:44.038619 89709 layer_factory.hpp:114] Creating layer relu7
I1226 08:55:44.038698 89709 net.cpp:178] Creating Layer relu7
I1226 08:55:44.038743 89709 net.cpp:612] relu7 <- fc7
I1226 08:55:44.038789 89709 net.cpp:573] relu7 -> fc7 (in-place)
I1226 08:55:44.038884 89709 net.cpp:228] Setting up relu7
I1226 08:55:44.038938 89709 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 08:55:44.038969 89709 net.cpp:243] Memory required for data: 2124511232
I1226 08:55:44.039000 89709 layer_factory.hpp:114] Creating layer drop7
I1226 08:55:44.039036 89709 net.cpp:178] Creating Layer drop7
I1226 08:55:44.039064 89709 net.cpp:612] drop7 <- fc7
I1226 08:55:44.039106 89709 net.cpp:573] drop7 -> fc7 (in-place)
I1226 08:55:44.039149 89709 net.cpp:228] Setting up drop7
I1226 08:55:44.039188 89709 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 08:55:44.039221 89709 net.cpp:243] Memory required for data: 2128705536
I1226 08:55:44.039247 89709 layer_factory.hpp:114] Creating layer fc8
I1226 08:55:44.039301 89709 net.cpp:178] Creating Layer fc8
I1226 08:55:44.039329 89709 net.cpp:612] fc8 <- fc7
I1226 08:55:44.039391 89709 net.cpp:586] fc8 -> fc8
I1226 08:55:44.068822 86102 net.cpp:228] Setting up fc7
I1226 08:55:44.068936 86102 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 08:55:44.068967 86102 net.cpp:243] Memory required for data: 2120316928
I1226 08:55:44.069025 86102 layer_factory.hpp:114] Creating layer relu7
I1226 08:55:44.069102 86102 net.cpp:178] Creating Layer relu7
I1226 08:55:44.069226 86102 net.cpp:612] relu7 <- fc7
I1226 08:55:44.069267 86102 net.cpp:573] relu7 -> fc7 (in-place)
I1226 08:55:44.069365 86102 net.cpp:228] Setting up relu7
I1226 08:55:44.069411 86102 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 08:55:44.069442 86102 net.cpp:243] Memory required for data: 2124511232
I1226 08:55:44.069471 86102 layer_factory.hpp:114] Creating layer drop7
I1226 08:55:44.069509 86102 net.cpp:178] Creating Layer drop7
I1226 08:55:44.069567 86102 net.cpp:612] drop7 <- fc7
I1226 08:55:44.069609 86102 net.cpp:573] drop7 -> fc7 (in-place)
I1226 08:55:44.069655 86102 net.cpp:228] Setting up drop7
I1226 08:55:44.069689 86102 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 08:55:44.069711 86102 net.cpp:243] Memory required for data: 2128705536
I1226 08:55:44.069746 86102 layer_factory.hpp:114] Creating layer fc8
I1226 08:55:44.069804 86102 net.cpp:178] Creating Layer fc8
I1226 08:55:44.069835 86102 net.cpp:612] fc8 <- fc7
I1226 08:55:44.069875 86102 net.cpp:586] fc8 -> fc8
I1226 08:55:40.822330 90165 net.cpp:228] Setting up fc7
I1226 08:55:40.822451 90165 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 08:55:40.822489 90165 net.cpp:243] Memory required for data: 2120316928
I1226 08:55:40.822569 90165 layer_factory.hpp:114] Creating layer relu7
I1226 08:55:40.822686 90165 net.cpp:178] Creating Layer relu7
I1226 08:55:40.822733 90165 net.cpp:612] relu7 <- fc7
I1226 08:55:40.822782 90165 net.cpp:573] relu7 -> fc7 (in-place)
I1226 08:55:40.822906 90165 net.cpp:228] Setting up relu7
I1226 08:55:40.822983 90165 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 08:55:40.823012 90165 net.cpp:243] Memory required for data: 2124511232
I1226 08:55:40.823055 90165 layer_factory.hpp:114] Creating layer drop7
I1226 08:55:40.823107 90165 net.cpp:178] Creating Layer drop7
I1226 08:55:40.823142 90165 net.cpp:612] drop7 <- fc7
I1226 08:55:40.823179 90165 net.cpp:573] drop7 -> fc7 (in-place)
I1226 08:55:40.823230 90165 net.cpp:228] Setting up drop7
I1226 08:55:40.823264 90165 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 08:55:40.823297 90165 net.cpp:243] Memory required for data: 2128705536
I1226 08:55:40.823326 90165 layer_factory.hpp:114] Creating layer fc8
I1226 08:55:40.823387 90165 net.cpp:178] Creating Layer fc8
I1226 08:55:40.823415 90165 net.cpp:612] fc8 <- fc7
I1226 08:55:40.823454 90165 net.cpp:586] fc8 -> fc8
I1226 08:55:44.214902 92150 net.cpp:228] Setting up fc7
I1226 08:55:44.215025 92150 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 08:55:44.215065 92150 net.cpp:243] Memory required for data: 2120316928
I1226 08:55:44.215147 92150 layer_factory.hpp:114] Creating layer relu7
I1226 08:55:44.215240 92150 net.cpp:178] Creating Layer relu7
I1226 08:55:44.215296 92150 net.cpp:612] relu7 <- fc7
I1226 08:55:44.215347 92150 net.cpp:573] relu7 -> fc7 (in-place)
I1226 08:55:44.215492 92150 net.cpp:228] Setting up relu7
I1226 08:55:44.215569 92150 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 08:55:44.215603 92150 net.cpp:243] Memory required for data: 2124511232
I1226 08:55:44.215643 92150 layer_factory.hpp:114] Creating layer drop7
I1226 08:55:44.215692 92150 net.cpp:178] Creating Layer drop7
I1226 08:55:44.215728 92150 net.cpp:612] drop7 <- fc7
I1226 08:55:44.215793 92150 net.cpp:573] drop7 -> fc7 (in-place)
I1226 08:55:44.215868 92150 net.cpp:228] Setting up drop7
I1226 08:55:44.215919 92150 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 08:55:44.215950 92150 net.cpp:243] Memory required for data: 2128705536
I1226 08:55:44.215986 92150 layer_factory.hpp:114] Creating layer fc8
I1226 08:55:44.216069 92150 net.cpp:178] Creating Layer fc8
I1226 08:55:44.216109 92150 net.cpp:612] fc8 <- fc7
I1226 08:55:44.216161 92150 net.cpp:586] fc8 -> fc8
I1226 08:55:44.579216 91127 caffe.cpp:376] Configuring multinode setup
I1226 08:55:44.580687 91127 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 08:55:44.602007 89709 net.cpp:228] Setting up fc8
I1226 08:55:44.602121 89709 net.cpp:235] Top shape: 256 1000 (256000)
I1226 08:55:44.602149 89709 net.cpp:243] Memory required for data: 2129729536
I1226 08:55:44.602200 89709 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 08:55:44.602283 89709 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 08:55:44.602319 89709 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 08:55:44.602363 89709 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 08:55:44.602449 89709 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 08:55:44.602535 89709 net.cpp:228] Setting up fc8_fc8_0_split
I1226 08:55:44.602597 89709 net.cpp:235] Top shape: 256 1000 (256000)
I1226 08:55:44.602629 89709 net.cpp:235] Top shape: 256 1000 (256000)
I1226 08:55:44.602653 89709 net.cpp:243] Memory required for data: 2131777536
I1226 08:55:44.602691 89709 layer_factory.hpp:114] Creating layer accuracy
I1226 08:55:44.602742 89709 net.cpp:178] Creating Layer accuracy
I1226 08:55:44.602769 89709 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 08:55:44.602799 89709 net.cpp:612] accuracy <- label_data_1_split_0
I1226 08:55:44.602834 89709 net.cpp:586] accuracy -> accuracy
I1226 08:55:44.602885 89709 net.cpp:228] Setting up accuracy
I1226 08:55:44.602923 89709 net.cpp:235] Top shape: (1)
I1226 08:55:44.602953 89709 net.cpp:243] Memory required for data: 2131777540
I1226 08:55:44.602979 89709 layer_factory.hpp:114] Creating layer loss
I1226 08:55:44.603022 89709 net.cpp:178] Creating Layer loss
I1226 08:55:44.603050 89709 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 08:55:44.603080 89709 net.cpp:612] loss <- label_data_1_split_1
I1226 08:55:44.603132 89709 net.cpp:586] loss -> loss
I1226 08:55:44.603189 89709 layer_factory.hpp:114] Creating layer loss
I1226 08:55:44.626595 86102 net.cpp:228] Setting up fc8
I1226 08:55:44.626706 86102 net.cpp:235] Top shape: 256 1000 (256000)
I1226 08:55:44.626736 86102 net.cpp:243] Memory required for data: 2129729536
I1226 08:55:44.626791 86102 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 08:55:44.626868 86102 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 08:55:44.626901 86102 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 08:55:44.626966 86102 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 08:55:44.627023 86102 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 08:55:44.627104 86102 net.cpp:228] Setting up fc8_fc8_0_split
I1226 08:55:44.627159 86102 net.cpp:235] Top shape: 256 1000 (256000)
I1226 08:55:44.627190 86102 net.cpp:235] Top shape: 256 1000 (256000)
I1226 08:55:44.627213 86102 net.cpp:243] Memory required for data: 2131777536
I1226 08:55:44.627243 86102 layer_factory.hpp:114] Creating layer accuracy
I1226 08:55:44.627300 86102 net.cpp:178] Creating Layer accuracy
I1226 08:55:44.627326 86102 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 08:55:44.627357 86102 net.cpp:612] accuracy <- label_data_1_split_0
I1226 08:55:44.627389 86102 net.cpp:586] accuracy -> accuracy
I1226 08:55:44.627430 86102 net.cpp:228] Setting up accuracy
I1226 08:55:44.627468 86102 net.cpp:235] Top shape: (1)
I1226 08:55:44.627491 86102 net.cpp:243] Memory required for data: 2131777540
I1226 08:55:44.627517 86102 layer_factory.hpp:114] Creating layer loss
I1226 08:55:44.627596 86102 net.cpp:178] Creating Layer loss
I1226 08:55:44.627631 86102 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 08:55:44.627662 86102 net.cpp:612] loss <- label_data_1_split_1
I1226 08:55:44.627709 86102 net.cpp:586] loss -> loss
I1226 08:55:44.627771 86102 layer_factory.hpp:114] Creating layer loss
I1226 08:55:44.636170 89709 net.cpp:228] Setting up loss
I1226 08:55:44.636397 89709 net.cpp:235] Top shape: (1)
I1226 08:55:44.636445 89709 net.cpp:238]     with loss weight 1
I1226 08:55:44.636595 89709 net.cpp:243] Memory required for data: 2131777544
I1226 08:55:44.636658 89709 net.cpp:305] loss needs backward computation.
I1226 08:55:44.636710 89709 net.cpp:307] accuracy does not need backward computation.
I1226 08:55:44.636747 89709 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 08:55:44.636782 89709 net.cpp:305] fc8 needs backward computation.
I1226 08:55:44.636813 89709 net.cpp:305] drop7 needs backward computation.
I1226 08:55:44.636843 89709 net.cpp:305] relu7 needs backward computation.
I1226 08:55:44.636884 89709 net.cpp:305] fc7 needs backward computation.
I1226 08:55:44.636917 89709 net.cpp:305] drop6 needs backward computation.
I1226 08:55:44.636955 89709 net.cpp:305] relu6 needs backward computation.
I1226 08:55:44.636986 89709 net.cpp:305] fc6 needs backward computation.
I1226 08:55:44.637017 89709 net.cpp:305] pool5 needs backward computation.
I1226 08:55:44.637058 89709 net.cpp:305] relu5 needs backward computation.
I1226 08:55:44.637087 89709 net.cpp:305] conv5 needs backward computation.
I1226 08:55:44.637117 89709 net.cpp:305] relu4 needs backward computation.
I1226 08:55:44.637147 89709 net.cpp:305] conv4 needs backward computation.
I1226 08:55:44.637186 89709 net.cpp:305] relu3 needs backward computation.
I1226 08:55:44.637217 89709 net.cpp:305] conv3 needs backward computation.
I1226 08:55:44.637248 89709 net.cpp:305] pool2 needs backward computation.
I1226 08:55:44.637289 89709 net.cpp:305] norm2 needs backward computation.
I1226 08:55:44.637320 89709 net.cpp:305] relu2 needs backward computation.
I1226 08:55:44.637348 89709 net.cpp:305] conv2 needs backward computation.
I1226 08:55:44.637406 89709 net.cpp:305] pool1 needs backward computation.
I1226 08:55:44.637444 89709 net.cpp:305] norm1 needs backward computation.
I1226 08:55:44.637475 89709 net.cpp:305] relu1 needs backward computation.
I1226 08:55:44.637511 89709 net.cpp:305] conv1 needs backward computation.
I1226 08:55:44.637543 89709 net.cpp:307] label_data_1_split does not need backward computation.
I1226 08:55:44.637584 89709 net.cpp:307] data does not need backward computation.
I1226 08:55:44.637612 89709 net.cpp:349] This network produces output accuracy
I1226 08:55:44.637647 89709 net.cpp:349] This network produces output loss
I1226 08:55:44.637750 89709 net.cpp:363] Network initialization done.
I1226 08:55:44.638185 89709 solver.cpp:119] Solver scaffolding done.
I1226 08:55:44.638424 89709 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 08:55:44.654516 86102 net.cpp:228] Setting up loss
I1226 08:55:44.654747 86102 net.cpp:235] Top shape: (1)
I1226 08:55:44.654798 86102 net.cpp:238]     with loss weight 1
I1226 08:55:44.655076 86102 net.cpp:243] Memory required for data: 2131777544
I1226 08:55:44.655122 86102 net.cpp:305] loss needs backward computation.
I1226 08:55:44.655161 86102 net.cpp:307] accuracy does not need backward computation.
I1226 08:55:44.655207 86102 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 08:55:44.655241 86102 net.cpp:305] fc8 needs backward computation.
I1226 08:55:44.655273 86102 net.cpp:305] drop7 needs backward computation.
I1226 08:55:44.655304 86102 net.cpp:305] relu7 needs backward computation.
I1226 08:55:44.655338 86102 net.cpp:305] fc7 needs backward computation.
I1226 08:55:44.655369 86102 net.cpp:305] drop6 needs backward computation.
I1226 08:55:44.655397 86102 net.cpp:305] relu6 needs backward computation.
I1226 08:55:44.655434 86102 net.cpp:305] fc6 needs backward computation.
I1226 08:55:44.655465 86102 net.cpp:305] pool5 needs backward computation.
I1226 08:55:44.655504 86102 net.cpp:305] relu5 needs backward computation.
I1226 08:55:44.655535 86102 net.cpp:305] conv5 needs backward computation.
I1226 08:55:44.655586 86102 net.cpp:305] relu4 needs backward computation.
I1226 08:55:44.655612 86102 net.cpp:305] conv4 needs backward computation.
I1226 08:55:44.655639 86102 net.cpp:305] relu3 needs backward computation.
I1226 08:55:44.655664 86102 net.cpp:305] conv3 needs backward computation.
I1226 08:55:44.655690 86102 net.cpp:305] pool2 needs backward computation.
I1226 08:55:44.655716 86102 net.cpp:305] norm2 needs backward computation.
I1226 08:55:44.655742 86102 net.cpp:305] relu2 needs backward computation.
I1226 08:55:44.655767 86102 net.cpp:305] conv2 needs backward computation.
I1226 08:55:44.655793 86102 net.cpp:305] pool1 needs backward computation.
I1226 08:55:44.655818 86102 net.cpp:305] norm1 needs backward computation.
I1226 08:55:44.655844 86102 net.cpp:305] relu1 needs backward computation.
I1226 08:55:44.655869 86102 net.cpp:305] conv1 needs backward computation.
I1226 08:55:44.655896 86102 net.cpp:307] label_data_1_split does not need backward computation.
I1226 08:55:44.655925 86102 net.cpp:307] data does not need backward computation.
I1226 08:55:44.655947 86102 net.cpp:349] This network produces output accuracy
I1226 08:55:44.655977 86102 net.cpp:349] This network produces output loss
I1226 08:55:44.656077 86102 net.cpp:363] Network initialization done.
I1226 08:55:44.656594 86102 solver.cpp:119] Solver scaffolding done.
I1226 08:55:44.656780 86102 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 08:55:41.392472 90165 net.cpp:228] Setting up fc8
I1226 08:55:41.392604 90165 net.cpp:235] Top shape: 256 1000 (256000)
I1226 08:55:41.392635 90165 net.cpp:243] Memory required for data: 2129729536
I1226 08:55:41.392691 90165 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 08:55:41.392771 90165 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 08:55:41.392813 90165 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 08:55:41.392853 90165 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 08:55:41.392900 90165 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 08:55:41.392998 90165 net.cpp:228] Setting up fc8_fc8_0_split
I1226 08:55:41.393049 90165 net.cpp:235] Top shape: 256 1000 (256000)
I1226 08:55:41.393079 90165 net.cpp:235] Top shape: 256 1000 (256000)
I1226 08:55:41.393112 90165 net.cpp:243] Memory required for data: 2131777536
I1226 08:55:41.393142 90165 layer_factory.hpp:114] Creating layer accuracy
I1226 08:55:41.393195 90165 net.cpp:178] Creating Layer accuracy
I1226 08:55:41.393225 90165 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 08:55:41.393263 90165 net.cpp:612] accuracy <- label_data_1_split_0
I1226 08:55:41.393307 90165 net.cpp:586] accuracy -> accuracy
I1226 08:55:41.393354 90165 net.cpp:228] Setting up accuracy
I1226 08:55:41.393389 90165 net.cpp:235] Top shape: (1)
I1226 08:55:41.393412 90165 net.cpp:243] Memory required for data: 2131777540
I1226 08:55:41.393440 90165 layer_factory.hpp:114] Creating layer loss
I1226 08:55:41.393493 90165 net.cpp:178] Creating Layer loss
I1226 08:55:41.393525 90165 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 08:55:41.393556 90165 net.cpp:612] loss <- label_data_1_split_1
I1226 08:55:41.393627 90165 net.cpp:586] loss -> loss
I1226 08:55:41.393703 90165 layer_factory.hpp:114] Creating layer loss
I1226 08:55:41.426126 90165 net.cpp:228] Setting up loss
I1226 08:55:41.426331 90165 net.cpp:235] Top shape: (1)
I1226 08:55:41.426379 90165 net.cpp:238]     with loss weight 1
I1226 08:55:41.426530 90165 net.cpp:243] Memory required for data: 2131777544
I1226 08:55:41.426609 90165 net.cpp:305] loss needs backward computation.
I1226 08:55:41.426654 90165 net.cpp:307] accuracy does not need backward computation.
I1226 08:55:41.426692 90165 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 08:55:41.426724 90165 net.cpp:305] fc8 needs backward computation.
I1226 08:55:41.426758 90165 net.cpp:305] drop7 needs backward computation.
I1226 08:55:41.426800 90165 net.cpp:305] relu7 needs backward computation.
I1226 08:55:41.426831 90165 net.cpp:305] fc7 needs backward computation.
I1226 08:55:41.426862 90165 net.cpp:305] drop6 needs backward computation.
I1226 08:55:41.426898 90165 net.cpp:305] relu6 needs backward computation.
I1226 08:55:41.426935 90165 net.cpp:305] fc6 needs backward computation.
I1226 08:55:41.426967 90165 net.cpp:305] pool5 needs backward computation.
I1226 08:55:41.426998 90165 net.cpp:305] relu5 needs backward computation.
I1226 08:55:41.427028 90165 net.cpp:305] conv5 needs backward computation.
I1226 08:55:41.427067 90165 net.cpp:305] relu4 needs backward computation.
I1226 08:55:41.427098 90165 net.cpp:305] conv4 needs backward computation.
I1226 08:55:41.427129 90165 net.cpp:305] relu3 needs backward computation.
I1226 08:55:41.427160 90165 net.cpp:305] conv3 needs backward computation.
I1226 08:55:41.427191 90165 net.cpp:305] pool2 needs backward computation.
I1226 08:55:41.427223 90165 net.cpp:305] norm2 needs backward computation.
I1226 08:55:41.427254 90165 net.cpp:305] relu2 needs backward computation.
I1226 08:55:41.427290 90165 net.cpp:305] conv2 needs backward computation.
I1226 08:55:41.427321 90165 net.cpp:305] pool1 needs backward computation.
I1226 08:55:41.427359 90165 net.cpp:305] norm1 needs backward computation.
I1226 08:55:41.427390 90165 net.cpp:305] relu1 needs backward computation.
I1226 08:55:41.427420 90165 net.cpp:305] conv1 needs backward computation.
I1226 08:55:41.427453 90165 net.cpp:307] label_data_1_split does not need backward computation.
I1226 08:55:41.427484 90165 net.cpp:307] data does not need backward computation.
I1226 08:55:41.427512 90165 net.cpp:349] This network produces output accuracy
I1226 08:55:41.427548 90165 net.cpp:349] This network produces output loss
I1226 08:55:41.427678 90165 net.cpp:363] Network initialization done.
I1226 08:55:41.428179 90165 solver.cpp:119] Solver scaffolding done.
I1226 08:55:41.428428 90165 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 08:55:44.781882 92150 net.cpp:228] Setting up fc8
I1226 08:55:44.782007 92150 net.cpp:235] Top shape: 256 1000 (256000)
I1226 08:55:44.782045 92150 net.cpp:243] Memory required for data: 2129729536
I1226 08:55:44.782124 92150 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 08:55:44.782207 92150 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 08:55:44.782249 92150 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 08:55:44.782325 92150 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 08:55:44.782418 92150 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 08:55:44.782544 92150 net.cpp:228] Setting up fc8_fc8_0_split
I1226 08:55:44.782757 92150 net.cpp:235] Top shape: 256 1000 (256000)
I1226 08:55:44.782802 92150 net.cpp:235] Top shape: 256 1000 (256000)
I1226 08:55:44.782831 92150 net.cpp:243] Memory required for data: 2131777536
I1226 08:55:44.782879 92150 layer_factory.hpp:114] Creating layer accuracy
I1226 08:55:44.782951 92150 net.cpp:178] Creating Layer accuracy
I1226 08:55:44.782999 92150 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 08:55:44.783040 92150 net.cpp:612] accuracy <- label_data_1_split_0
I1226 08:55:44.783103 92150 net.cpp:586] accuracy -> accuracy
I1226 08:55:44.783164 92150 net.cpp:228] Setting up accuracy
I1226 08:55:44.783210 92150 net.cpp:235] Top shape: (1)
I1226 08:55:44.783238 92150 net.cpp:243] Memory required for data: 2131777540
I1226 08:55:44.783284 92150 layer_factory.hpp:114] Creating layer loss
I1226 08:55:44.783344 92150 net.cpp:178] Creating Layer loss
I1226 08:55:44.783402 92150 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 08:55:44.783449 92150 net.cpp:612] loss <- label_data_1_split_1
I1226 08:55:44.783507 92150 net.cpp:586] loss -> loss
I1226 08:55:44.783597 92150 layer_factory.hpp:114] Creating layer loss
I1226 08:55:44.818781 92150 net.cpp:228] Setting up loss
I1226 08:55:44.819034 92150 net.cpp:235] Top shape: (1)
I1226 08:55:44.819087 92150 net.cpp:238]     with loss weight 1
I1226 08:55:44.819347 92150 net.cpp:243] Memory required for data: 2131777544
I1226 08:55:44.819447 92150 net.cpp:305] loss needs backward computation.
I1226 08:55:44.819665 92150 net.cpp:307] accuracy does not need backward computation.
I1226 08:55:44.819726 92150 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 08:55:44.819942 92150 net.cpp:305] fc8 needs backward computation.
I1226 08:55:44.819977 92150 net.cpp:305] drop7 needs backward computation.
I1226 08:55:44.820008 92150 net.cpp:305] relu7 needs backward computation.
I1226 08:55:44.820036 92150 net.cpp:305] fc7 needs backward computation.
I1226 08:55:44.820241 92150 net.cpp:305] drop6 needs backward computation.
I1226 08:55:44.820272 92150 net.cpp:305] relu6 needs backward computation.
I1226 08:55:44.820302 92150 net.cpp:305] fc6 needs backward computation.
I1226 08:55:44.820356 92150 net.cpp:305] pool5 needs backward computation.
I1226 08:55:44.820525 92150 net.cpp:305] relu5 needs backward computation.
I1226 08:55:44.820559 92150 net.cpp:305] conv5 needs backward computation.
I1226 08:55:44.820592 92150 net.cpp:305] relu4 needs backward computation.
I1226 08:55:44.820621 92150 net.cpp:305] conv4 needs backward computation.
I1226 08:55:44.820683 92150 net.cpp:305] relu3 needs backward computation.
I1226 08:55:44.820720 92150 net.cpp:305] conv3 needs backward computation.
I1226 08:55:44.820751 92150 net.cpp:305] pool2 needs backward computation.
I1226 08:55:44.820782 92150 net.cpp:305] norm2 needs backward computation.
I1226 08:55:44.820813 92150 net.cpp:305] relu2 needs backward computation.
I1226 08:55:44.820844 92150 net.cpp:305] conv2 needs backward computation.
I1226 08:55:44.820899 92150 net.cpp:305] pool1 needs backward computation.
I1226 08:55:44.820930 92150 net.cpp:305] norm1 needs backward computation.
I1226 08:55:44.820960 92150 net.cpp:305] relu1 needs backward computation.
I1226 08:55:44.820989 92150 net.cpp:305] conv1 needs backward computation.
I1226 08:55:44.821032 92150 net.cpp:307] label_data_1_split does not need backward computation.
I1226 08:55:44.821072 92150 net.cpp:307] data does not need backward computation.
I1226 08:55:44.821100 92150 net.cpp:349] This network produces output accuracy
I1226 08:55:44.821135 92150 net.cpp:349] This network produces output loss
I1226 08:55:44.821290 92150 net.cpp:363] Network initialization done.
I1226 08:55:44.821831 92150 solver.cpp:119] Solver scaffolding done.
I1226 08:55:44.822047 92150 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 08:55:47.152120 119888 net.cpp:228] Setting up fc7
I1226 08:55:47.152233 119888 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 08:55:47.152261 119888 net.cpp:243] Memory required for data: 2120316928
I1226 08:55:47.152318 119888 layer_factory.hpp:114] Creating layer relu7
I1226 08:55:47.152395 119888 net.cpp:178] Creating Layer relu7
I1226 08:55:47.152431 119888 net.cpp:612] relu7 <- fc7
I1226 08:55:47.152479 119888 net.cpp:573] relu7 -> fc7 (in-place)
I1226 08:55:47.152568 119888 net.cpp:228] Setting up relu7
I1226 08:55:47.152642 119888 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 08:55:47.152670 119888 net.cpp:243] Memory required for data: 2124511232
I1226 08:55:47.152700 119888 layer_factory.hpp:114] Creating layer drop7
I1226 08:55:47.152741 119888 net.cpp:178] Creating Layer drop7
I1226 08:55:47.152777 119888 net.cpp:612] drop7 <- fc7
I1226 08:55:47.152820 119888 net.cpp:573] drop7 -> fc7 (in-place)
I1226 08:55:47.152868 119888 net.cpp:228] Setting up drop7
I1226 08:55:47.152909 119888 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 08:55:47.152936 119888 net.cpp:243] Memory required for data: 2128705536
I1226 08:55:47.152964 119888 layer_factory.hpp:114] Creating layer fc8
I1226 08:55:47.153023 119888 net.cpp:178] Creating Layer fc8
I1226 08:55:47.153059 119888 net.cpp:612] fc8 <- fc7
I1226 08:55:47.153098 119888 net.cpp:586] fc8 -> fc8
I1226 08:55:47.950953 119888 net.cpp:228] Setting up fc8
I1226 08:55:47.951067 119888 net.cpp:235] Top shape: 256 1000 (256000)
I1226 08:55:47.951098 119888 net.cpp:243] Memory required for data: 2129729536
I1226 08:55:47.951154 119888 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 08:55:47.951251 119888 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 08:55:47.951299 119888 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 08:55:47.951342 119888 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 08:55:47.951395 119888 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 08:55:47.951491 119888 net.cpp:228] Setting up fc8_fc8_0_split
I1226 08:55:47.951552 119888 net.cpp:235] Top shape: 256 1000 (256000)
I1226 08:55:47.951586 119888 net.cpp:235] Top shape: 256 1000 (256000)
I1226 08:55:47.951637 119888 net.cpp:243] Memory required for data: 2131777536
I1226 08:55:47.951670 119888 layer_factory.hpp:114] Creating layer accuracy
I1226 08:55:47.951735 119888 net.cpp:178] Creating Layer accuracy
I1226 08:55:47.951766 119888 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 08:55:47.951797 119888 net.cpp:612] accuracy <- label_data_1_split_0
I1226 08:55:47.951838 119888 net.cpp:586] accuracy -> accuracy
I1226 08:55:47.951890 119888 net.cpp:228] Setting up accuracy
I1226 08:55:47.951930 119888 net.cpp:235] Top shape: (1)
I1226 08:55:47.951959 119888 net.cpp:243] Memory required for data: 2131777540
I1226 08:55:47.951987 119888 layer_factory.hpp:114] Creating layer loss
I1226 08:55:47.952031 119888 net.cpp:178] Creating Layer loss
I1226 08:55:47.952065 119888 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 08:55:47.952096 119888 net.cpp:612] loss <- label_data_1_split_1
I1226 08:55:47.952265 119888 net.cpp:586] loss -> loss
I1226 08:55:47.952358 119888 layer_factory.hpp:114] Creating layer loss
I1226 08:55:47.981521 119888 net.cpp:228] Setting up loss
I1226 08:55:47.981655 119888 net.cpp:235] Top shape: (1)
I1226 08:55:47.981696 119888 net.cpp:238]     with loss weight 1
I1226 08:55:47.981807 119888 net.cpp:243] Memory required for data: 2131777544
I1226 08:55:47.981849 119888 net.cpp:305] loss needs backward computation.
I1226 08:55:47.981889 119888 net.cpp:307] accuracy does not need backward computation.
I1226 08:55:47.981925 119888 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 08:55:47.981956 119888 net.cpp:305] fc8 needs backward computation.
I1226 08:55:47.981990 119888 net.cpp:305] drop7 needs backward computation.
I1226 08:55:47.982028 119888 net.cpp:305] relu7 needs backward computation.
I1226 08:55:47.982064 119888 net.cpp:305] fc7 needs backward computation.
I1226 08:55:47.982096 119888 net.cpp:305] drop6 needs backward computation.
I1226 08:55:47.982127 119888 net.cpp:305] relu6 needs backward computation.
I1226 08:55:47.982158 119888 net.cpp:305] fc6 needs backward computation.
I1226 08:55:47.982189 119888 net.cpp:305] pool5 needs backward computation.
I1226 08:55:47.982245 119888 net.cpp:305] relu5 needs backward computation.
I1226 08:55:47.982286 119888 net.cpp:305] conv5 needs backward computation.
I1226 08:55:47.982326 119888 net.cpp:305] relu4 needs backward computation.
I1226 08:55:47.982528 119888 net.cpp:305] conv4 needs backward computation.
I1226 08:55:47.982566 119888 net.cpp:305] relu3 needs backward computation.
I1226 08:55:47.982599 119888 net.cpp:305] conv3 needs backward computation.
I1226 08:55:47.982663 119888 net.cpp:305] pool2 needs backward computation.
I1226 08:55:47.982697 119888 net.cpp:305] norm2 needs backward computation.
I1226 08:55:47.982727 119888 net.cpp:305] relu2 needs backward computation.
I1226 08:55:47.982753 119888 net.cpp:305] conv2 needs backward computation.
I1226 08:55:47.982781 119888 net.cpp:305] pool1 needs backward computation.
I1226 08:55:47.982808 119888 net.cpp:305] norm1 needs backward computation.
I1226 08:55:47.982836 119888 net.cpp:305] relu1 needs backward computation.
I1226 08:55:47.982862 119888 net.cpp:305] conv1 needs backward computation.
I1226 08:55:47.982892 119888 net.cpp:307] label_data_1_split does not need backward computation.
I1226 08:55:47.982923 119888 net.cpp:307] data does not need backward computation.
I1226 08:55:47.982947 119888 net.cpp:349] This network produces output accuracy
I1226 08:55:47.982980 119888 net.cpp:349] This network produces output loss
I1226 08:55:47.983080 119888 net.cpp:363] Network initialization done.
I1226 08:55:47.983657 119888 solver.cpp:119] Solver scaffolding done.
I1226 08:55:47.983865 119888 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 08:55:48.657260 89709 caffe.cpp:376] Configuring multinode setup
I1226 08:55:48.658861 89709 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 08:55:48.735250 86102 caffe.cpp:376] Configuring multinode setup
I1226 08:55:48.736704 86102 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 08:55:45.487975 90165 caffe.cpp:376] Configuring multinode setup
I1226 08:55:45.489461 90165 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 08:55:48.886487 92150 caffe.cpp:376] Configuring multinode setup
I1226 08:55:48.887864 92150 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 08:55:52.020531 119888 caffe.cpp:376] Configuring multinode setup
I1226 08:55:52.022114 119888 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 08:56:09.595038 95005 net.cpp:228] Setting up fc6
I1226 08:56:09.595310 95005 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 08:56:09.595371 95005 net.cpp:243] Memory required for data: 2107734016
I1226 08:56:09.595469 95005 layer_factory.hpp:114] Creating layer relu6
I1226 08:56:09.595576 95005 net.cpp:178] Creating Layer relu6
I1226 08:56:09.595628 95005 net.cpp:612] relu6 <- fc6
I1226 08:56:09.595684 95005 net.cpp:573] relu6 -> fc6 (in-place)
I1226 08:56:09.595849 95005 net.cpp:228] Setting up relu6
I1226 08:56:09.595923 95005 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 08:56:09.595984 95005 net.cpp:243] Memory required for data: 2111928320
I1226 08:56:09.596035 95005 layer_factory.hpp:114] Creating layer drop6
I1226 08:56:09.596132 95005 net.cpp:178] Creating Layer drop6
I1226 08:56:09.596191 95005 net.cpp:612] drop6 <- fc6
I1226 08:56:09.596251 95005 net.cpp:573] drop6 -> fc6 (in-place)
I1226 08:56:09.596362 95005 net.cpp:228] Setting up drop6
I1226 08:56:09.596431 95005 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 08:56:09.596478 95005 net.cpp:243] Memory required for data: 2116122624
I1226 08:56:09.596523 95005 layer_factory.hpp:114] Creating layer fc7
I1226 08:56:09.596606 95005 net.cpp:178] Creating Layer fc7
I1226 08:56:09.596655 95005 net.cpp:612] fc7 <- fc6
I1226 08:56:09.596716 95005 net.cpp:586] fc7 -> fc7
I1226 08:56:09.877897 94304 net.cpp:228] Setting up fc6
I1226 08:56:09.878157 94304 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 08:56:09.878204 94304 net.cpp:243] Memory required for data: 2107734016
I1226 08:56:09.878280 94304 layer_factory.hpp:114] Creating layer relu6
I1226 08:56:09.878365 94304 net.cpp:178] Creating Layer relu6
I1226 08:56:09.878413 94304 net.cpp:612] relu6 <- fc6
I1226 08:56:09.878463 94304 net.cpp:573] relu6 -> fc6 (in-place)
I1226 08:56:09.878577 94304 net.cpp:228] Setting up relu6
I1226 08:56:09.878634 94304 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 08:56:09.878666 94304 net.cpp:243] Memory required for data: 2111928320
I1226 08:56:09.878700 94304 layer_factory.hpp:114] Creating layer drop6
I1226 08:56:09.878775 94304 net.cpp:178] Creating Layer drop6
I1226 08:56:09.878847 94304 net.cpp:612] drop6 <- fc6
I1226 08:56:09.878896 94304 net.cpp:573] drop6 -> fc6 (in-place)
I1226 08:56:09.878964 94304 net.cpp:228] Setting up drop6
I1226 08:56:09.879006 94304 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 08:56:09.879034 94304 net.cpp:243] Memory required for data: 2116122624
I1226 08:56:09.879067 94304 layer_factory.hpp:114] Creating layer fc7
I1226 08:56:09.879134 94304 net.cpp:178] Creating Layer fc7
I1226 08:56:09.879166 94304 net.cpp:612] fc7 <- fc6
I1226 08:56:09.879214 94304 net.cpp:586] fc7 -> fc7
I1226 08:56:22.678261 95005 net.cpp:228] Setting up fc7
I1226 08:56:22.678386 95005 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 08:56:22.678438 95005 net.cpp:243] Memory required for data: 2120316928
I1226 08:56:22.678534 95005 layer_factory.hpp:114] Creating layer relu7
I1226 08:56:22.678829 95005 net.cpp:178] Creating Layer relu7
I1226 08:56:22.678910 95005 net.cpp:612] relu7 <- fc7
I1226 08:56:22.678973 95005 net.cpp:573] relu7 -> fc7 (in-place)
I1226 08:56:22.679117 95005 net.cpp:228] Setting up relu7
I1226 08:56:22.679191 95005 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 08:56:22.679230 95005 net.cpp:243] Memory required for data: 2124511232
I1226 08:56:22.679275 95005 layer_factory.hpp:114] Creating layer drop7
I1226 08:56:22.679330 95005 net.cpp:178] Creating Layer drop7
I1226 08:56:22.679379 95005 net.cpp:612] drop7 <- fc7
I1226 08:56:22.679431 95005 net.cpp:573] drop7 -> fc7 (in-place)
I1226 08:56:22.679497 95005 net.cpp:228] Setting up drop7
I1226 08:56:22.679558 95005 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 08:56:22.679590 95005 net.cpp:243] Memory required for data: 2128705536
I1226 08:56:22.679630 95005 layer_factory.hpp:114] Creating layer fc8
I1226 08:56:22.679762 95005 net.cpp:178] Creating Layer fc8
I1226 08:56:22.679819 95005 net.cpp:612] fc8 <- fc7
I1226 08:56:22.679878 95005 net.cpp:586] fc8 -> fc8
I1226 08:56:22.998358 94304 net.cpp:228] Setting up fc7
I1226 08:56:22.998474 94304 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 08:56:22.998512 94304 net.cpp:243] Memory required for data: 2120316928
I1226 08:56:22.998581 94304 layer_factory.hpp:114] Creating layer relu7
I1226 08:56:22.998687 94304 net.cpp:178] Creating Layer relu7
I1226 08:56:22.998762 94304 net.cpp:612] relu7 <- fc7
I1226 08:56:22.998853 94304 net.cpp:573] relu7 -> fc7 (in-place)
I1226 08:56:22.998971 94304 net.cpp:228] Setting up relu7
I1226 08:56:22.999027 94304 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 08:56:22.999056 94304 net.cpp:243] Memory required for data: 2124511232
I1226 08:56:22.999092 94304 layer_factory.hpp:114] Creating layer drop7
I1226 08:56:22.999136 94304 net.cpp:178] Creating Layer drop7
I1226 08:56:22.999168 94304 net.cpp:612] drop7 <- fc7
I1226 08:56:22.999209 94304 net.cpp:573] drop7 -> fc7 (in-place)
I1226 08:56:22.999263 94304 net.cpp:228] Setting up drop7
I1226 08:56:22.999300 94304 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 08:56:22.999326 94304 net.cpp:243] Memory required for data: 2128705536
I1226 08:56:22.999356 94304 layer_factory.hpp:114] Creating layer fc8
I1226 08:56:22.999442 94304 net.cpp:178] Creating Layer fc8
I1226 08:56:22.999488 94304 net.cpp:612] fc8 <- fc7
I1226 08:56:22.999541 94304 net.cpp:586] fc8 -> fc8
I1226 08:56:25.871878 95005 net.cpp:228] Setting up fc8
I1226 08:56:25.872028 95005 net.cpp:235] Top shape: 256 1000 (256000)
I1226 08:56:25.872089 95005 net.cpp:243] Memory required for data: 2129729536
I1226 08:56:25.872177 95005 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 08:56:25.872287 95005 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 08:56:25.872341 95005 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 08:56:25.872398 95005 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 08:56:25.872468 95005 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 08:56:25.872628 95005 net.cpp:228] Setting up fc8_fc8_0_split
I1226 08:56:25.872706 95005 net.cpp:235] Top shape: 256 1000 (256000)
I1226 08:56:25.872792 95005 net.cpp:235] Top shape: 256 1000 (256000)
I1226 08:56:25.872830 95005 net.cpp:243] Memory required for data: 2131777536
I1226 08:56:25.872875 95005 layer_factory.hpp:114] Creating layer accuracy
I1226 08:56:25.872947 95005 net.cpp:178] Creating Layer accuracy
I1226 08:56:25.872987 95005 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 08:56:25.873034 95005 net.cpp:612] accuracy <- label_data_1_split_0
I1226 08:56:25.873111 95005 net.cpp:586] accuracy -> accuracy
I1226 08:56:25.873188 95005 net.cpp:228] Setting up accuracy
I1226 08:56:25.873251 95005 net.cpp:235] Top shape: (1)
I1226 08:56:25.873286 95005 net.cpp:243] Memory required for data: 2131777540
I1226 08:56:25.873325 95005 layer_factory.hpp:114] Creating layer loss
I1226 08:56:25.873530 95005 net.cpp:178] Creating Layer loss
I1226 08:56:25.873600 95005 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 08:56:25.873649 95005 net.cpp:612] loss <- label_data_1_split_1
I1226 08:56:25.873702 95005 net.cpp:586] loss -> loss
I1226 08:56:25.873837 95005 layer_factory.hpp:114] Creating layer loss
I1226 08:56:25.898936 95005 net.cpp:228] Setting up loss
I1226 08:56:25.899085 95005 net.cpp:235] Top shape: (1)
I1226 08:56:25.899128 95005 net.cpp:238]     with loss weight 1
I1226 08:56:25.899452 95005 net.cpp:243] Memory required for data: 2131777544
I1226 08:56:25.899507 95005 net.cpp:305] loss needs backward computation.
I1226 08:56:25.899755 95005 net.cpp:307] accuracy does not need backward computation.
I1226 08:56:25.899807 95005 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 08:56:25.899843 95005 net.cpp:305] fc8 needs backward computation.
I1226 08:56:25.899878 95005 net.cpp:305] drop7 needs backward computation.
I1226 08:56:25.899909 95005 net.cpp:305] relu7 needs backward computation.
I1226 08:56:25.899973 95005 net.cpp:305] fc7 needs backward computation.
I1226 08:56:25.900007 95005 net.cpp:305] drop6 needs backward computation.
I1226 08:56:25.900037 95005 net.cpp:305] relu6 needs backward computation.
I1226 08:56:25.900066 95005 net.cpp:305] fc6 needs backward computation.
I1226 08:56:25.900099 95005 net.cpp:305] pool5 needs backward computation.
I1226 08:56:25.900130 95005 net.cpp:305] relu5 needs backward computation.
I1226 08:56:25.900161 95005 net.cpp:305] conv5 needs backward computation.
I1226 08:56:25.900192 95005 net.cpp:305] relu4 needs backward computation.
I1226 08:56:25.900429 95005 net.cpp:305] conv4 needs backward computation.
I1226 08:56:25.900468 95005 net.cpp:305] relu3 needs backward computation.
I1226 08:56:25.900499 95005 net.cpp:305] conv3 needs backward computation.
I1226 08:56:25.900532 95005 net.cpp:305] pool2 needs backward computation.
I1226 08:56:25.900738 95005 net.cpp:305] norm2 needs backward computation.
I1226 08:56:25.900804 95005 net.cpp:305] relu2 needs backward computation.
I1226 08:56:25.901023 95005 net.cpp:305] conv2 needs backward computation.
I1226 08:56:25.901226 95005 net.cpp:305] pool1 needs backward computation.
I1226 08:56:25.901259 95005 net.cpp:305] norm1 needs backward computation.
I1226 08:56:25.901293 95005 net.cpp:305] relu1 needs backward computation.
I1226 08:56:25.901355 95005 net.cpp:305] conv1 needs backward computation.
I1226 08:56:25.901389 95005 net.cpp:307] label_data_1_split does not need backward computation.
I1226 08:56:25.901422 95005 net.cpp:307] data does not need backward computation.
I1226 08:56:25.901451 95005 net.cpp:349] This network produces output accuracy
I1226 08:56:25.901499 95005 net.cpp:349] This network produces output loss
I1226 08:56:25.901671 95005 net.cpp:363] Network initialization done.
I1226 08:56:25.902242 95005 solver.cpp:119] Solver scaffolding done.
I1226 08:56:25.902499 95005 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 08:56:26.202368 94304 net.cpp:228] Setting up fc8
I1226 08:56:26.202509 94304 net.cpp:235] Top shape: 256 1000 (256000)
I1226 08:56:26.202556 94304 net.cpp:243] Memory required for data: 2129729536
I1226 08:56:26.202631 94304 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 08:56:26.202718 94304 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 08:56:26.202764 94304 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 08:56:26.202847 94304 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 08:56:26.202950 94304 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 08:56:26.203069 94304 net.cpp:228] Setting up fc8_fc8_0_split
I1226 08:56:26.203145 94304 net.cpp:235] Top shape: 256 1000 (256000)
I1226 08:56:26.203187 94304 net.cpp:235] Top shape: 256 1000 (256000)
I1226 08:56:26.203214 94304 net.cpp:243] Memory required for data: 2131777536
I1226 08:56:26.203253 94304 layer_factory.hpp:114] Creating layer accuracy
I1226 08:56:26.203315 94304 net.cpp:178] Creating Layer accuracy
I1226 08:56:26.203352 94304 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 08:56:26.203387 94304 net.cpp:612] accuracy <- label_data_1_split_0
I1226 08:56:26.203426 94304 net.cpp:586] accuracy -> accuracy
I1226 08:56:26.203479 94304 net.cpp:228] Setting up accuracy
I1226 08:56:26.203516 94304 net.cpp:235] Top shape: (1)
I1226 08:56:26.203541 94304 net.cpp:243] Memory required for data: 2131777540
I1226 08:56:26.203569 94304 layer_factory.hpp:114] Creating layer loss
I1226 08:56:26.203739 94304 net.cpp:178] Creating Layer loss
I1226 08:56:26.203783 94304 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 08:56:26.203857 94304 net.cpp:612] loss <- label_data_1_split_1
I1226 08:56:26.203907 94304 net.cpp:586] loss -> loss
I1226 08:56:26.203986 94304 layer_factory.hpp:114] Creating layer loss
I1226 08:56:26.233615 94304 net.cpp:228] Setting up loss
I1226 08:56:26.233731 94304 net.cpp:235] Top shape: (1)
I1226 08:56:26.233772 94304 net.cpp:238]     with loss weight 1
I1226 08:56:26.233933 94304 net.cpp:243] Memory required for data: 2131777544
I1226 08:56:26.233994 94304 net.cpp:305] loss needs backward computation.
I1226 08:56:26.234042 94304 net.cpp:307] accuracy does not need backward computation.
I1226 08:56:26.234083 94304 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 08:56:26.234118 94304 net.cpp:305] fc8 needs backward computation.
I1226 08:56:26.234154 94304 net.cpp:305] drop7 needs backward computation.
I1226 08:56:26.234187 94304 net.cpp:305] relu7 needs backward computation.
I1226 08:56:26.234220 94304 net.cpp:305] fc7 needs backward computation.
I1226 08:56:26.234256 94304 net.cpp:305] drop6 needs backward computation.
I1226 08:56:26.234288 94304 net.cpp:305] relu6 needs backward computation.
I1226 08:56:26.234320 94304 net.cpp:305] fc6 needs backward computation.
I1226 08:56:26.234355 94304 net.cpp:305] pool5 needs backward computation.
I1226 08:56:26.234388 94304 net.cpp:305] relu5 needs backward computation.
I1226 08:56:26.234421 94304 net.cpp:305] conv5 needs backward computation.
I1226 08:56:26.234455 94304 net.cpp:305] relu4 needs backward computation.
I1226 08:56:26.234486 94304 net.cpp:305] conv4 needs backward computation.
I1226 08:56:26.234520 94304 net.cpp:305] relu3 needs backward computation.
I1226 08:56:26.234552 94304 net.cpp:305] conv3 needs backward computation.
I1226 08:56:26.234586 94304 net.cpp:305] pool2 needs backward computation.
I1226 08:56:26.234619 94304 net.cpp:305] norm2 needs backward computation.
I1226 08:56:26.234652 94304 net.cpp:305] relu2 needs backward computation.
I1226 08:56:26.234684 94304 net.cpp:305] conv2 needs backward computation.
I1226 08:56:26.234719 94304 net.cpp:305] pool1 needs backward computation.
I1226 08:56:26.234750 94304 net.cpp:305] norm1 needs backward computation.
I1226 08:56:26.234783 94304 net.cpp:305] relu1 needs backward computation.
I1226 08:56:26.234838 94304 net.cpp:305] conv1 needs backward computation.
I1226 08:56:26.234874 94304 net.cpp:307] label_data_1_split does not need backward computation.
I1226 08:56:26.234908 94304 net.cpp:307] data does not need backward computation.
I1226 08:56:26.234937 94304 net.cpp:349] This network produces output accuracy
I1226 08:56:26.234974 94304 net.cpp:349] This network produces output loss
I1226 08:56:26.235090 94304 net.cpp:363] Network initialization done.
I1226 08:56:26.235577 94304 solver.cpp:119] Solver scaffolding done.
I1226 08:56:26.235831 94304 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 08:56:31.337105 95005 caffe.cpp:376] Configuring multinode setup
I1226 08:56:31.338894 95005 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 08:56:31.799686 94304 caffe.cpp:376] Configuring multinode setup
I1226 08:56:31.801522 94304 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 08:56:31.801749 94304 SynchronousNode.cpp:662] [0] [proc 0] solving
I1226 08:56:31.801885 94304 solver.cpp:366] Solving AlexNet
I1226 08:56:31.801934 94304 solver.cpp:367] Learning Rate Policy: step
I1226 08:56:31.800669 95005 SynchronousNode.cpp:662] [5] [proc 5] solving
I1226 08:56:31.797821 86102 SynchronousNode.cpp:662] [3] [proc 3] solving
I1226 08:56:28.531905 90165 SynchronousNode.cpp:662] [1] [proc 1] solving
I1226 08:56:31.802208 94304 MultiSolver.cpp:76] [0] PROFILING BEGIN[Forward]
I1226 08:56:31.793323 91127 SynchronousNode.cpp:662] [6] [proc 6] solving
I1226 08:56:31.800850 95005 solver.cpp:366] Solving AlexNet
I1226 08:56:31.800904 95005 solver.cpp:367] Learning Rate Policy: step
I1226 08:56:31.798892 92150 SynchronousNode.cpp:662] [2] [proc 2] solving
I1226 08:56:31.798108 86102 solver.cpp:366] Solving AlexNet
I1226 08:56:31.798159 86102 solver.cpp:367] Learning Rate Policy: step
I1226 08:56:28.532183 90165 solver.cpp:366] Solving AlexNet
I1226 08:56:28.532229 90165 solver.cpp:367] Learning Rate Policy: step
I1226 08:56:31.790104 119888 SynchronousNode.cpp:662] [4] [proc 4] solving
I1226 08:56:31.795137 89709 SynchronousNode.cpp:662] [7] [proc 7] solving
I1226 08:56:31.793629 91127 solver.cpp:366] Solving AlexNet
I1226 08:56:31.793674 91127 solver.cpp:367] Learning Rate Policy: step
I1226 08:56:31.801162 95005 MultiSolver.cpp:76] [5] PROFILING BEGIN[Forward]
I1226 08:56:31.799180 92150 solver.cpp:366] Solving AlexNet
I1226 08:56:31.799232 92150 solver.cpp:367] Learning Rate Policy: step
I1226 08:56:31.798395 86102 MultiSolver.cpp:76] [3] PROFILING BEGIN[Forward]
I1226 08:56:28.532457 90165 MultiSolver.cpp:76] [1] PROFILING BEGIN[Forward]
I1226 08:56:31.790395 119888 solver.cpp:366] Solving AlexNet
I1226 08:56:31.790441 119888 solver.cpp:367] Learning Rate Policy: step
I1226 08:56:31.795457 89709 solver.cpp:366] Solving AlexNet
I1226 08:56:31.795500 89709 solver.cpp:367] Learning Rate Policy: step
I1226 08:56:31.793910 91127 MultiSolver.cpp:76] [6] PROFILING BEGIN[Forward]
I1226 08:56:31.799525 92150 MultiSolver.cpp:76] [2] PROFILING BEGIN[Forward]
I1226 08:56:31.790768 119888 MultiSolver.cpp:76] [4] PROFILING BEGIN[Forward]
I1226 08:56:31.795737 89709 MultiSolver.cpp:76] [7] PROFILING BEGIN[Forward]
I1226 08:56:31.790091 85894 async_param_server.cpp:187] PS: Comm loop
I1226 08:56:31.811384 95077 SynchronousNode.cpp:280] [5] Comm thread started 1 1
I1226 08:56:31.810483 85966 async_param_server.cpp:175] PS: Compute loop
I1226 08:56:28.561990 90238 SynchronousNode.cpp:280] [1] Comm thread started 1 1
I1226 08:56:31.833782 94379 SynchronousNode.cpp:280] [0] Comm thread started 1 1
I1226 08:56:31.825341 91199 SynchronousNode.cpp:280] [6] Comm thread started 1 1
I1226 08:56:31.829519 89781 SynchronousNode.cpp:280] [7] Comm thread started 1 1
I1226 08:56:31.824838 119958 SynchronousNode.cpp:280] [4] Comm thread started 1 1
I1226 08:56:31.833758 86176 SynchronousNode.cpp:280] [3] Comm thread started 1 1
I1226 08:56:31.838505 92222 SynchronousNode.cpp:280] [2] Comm thread started 1 1
I1226 08:56:32.716446 92150 MultiSolver.cpp:93] [2] PROFILING END[Forward]
I1226 08:56:32.716536 92150 MultiSolver.cpp:95] [2] PROFILING BEGIN[Backward]
I1226 08:56:32.708649 119888 MultiSolver.cpp:93] [4] PROFILING END[Forward]
I1226 08:56:32.708745 119888 MultiSolver.cpp:95] [4] PROFILING BEGIN[Backward]
I1226 08:56:32.757640 86102 MultiSolver.cpp:93] [3] PROFILING END[Forward]
I1226 08:56:32.757741 86102 MultiSolver.cpp:95] [3] PROFILING BEGIN[Backward]
I1226 08:56:29.504726 90165 MultiSolver.cpp:93] [1] PROFILING END[Forward]
I1226 08:56:29.504899 90165 MultiSolver.cpp:95] [1] PROFILING BEGIN[Backward]
I1226 08:56:32.773542 89709 MultiSolver.cpp:93] [7] PROFILING END[Forward]
I1226 08:56:32.773633 89709 MultiSolver.cpp:95] [7] PROFILING BEGIN[Backward]
I1226 08:56:32.795747 91127 MultiSolver.cpp:93] [6] PROFILING END[Forward]
I1226 08:56:32.795883 91127 MultiSolver.cpp:95] [6] PROFILING BEGIN[Backward]
I1226 08:56:35.013589 91127 MultiSolver.cpp:109] [6] PROFILING END[Backward]
I1226 08:56:35.013672 91127 solver.cpp:303] [6] Iteration 1, loss = 3.33074
I1226 08:56:35.013736 91127 solver.cpp:329]     Train net output #0: accuracy = 0.296875
I1226 08:56:35.013806 91127 solver.cpp:329]     Train net output #1: loss = 3.33074 (* 1 = 3.33074 loss)
I1226 08:56:35.013864 91127 MultiSolver.cpp:76] [6] PROFILING BEGIN[Forward]
I1226 08:56:31.799404 90165 MultiSolver.cpp:109] [1] PROFILING END[Backward]
I1226 08:56:31.799490 90165 solver.cpp:303] [1] Iteration 1, loss = 3.36395
I1226 08:56:31.799556 90165 solver.cpp:329]     Train net output #0: accuracy = 0.242188
I1226 08:56:31.799641 90165 solver.cpp:329]     Train net output #1: loss = 3.36395 (* 1 = 3.36395 loss)
I1226 08:56:31.799700 90165 MultiSolver.cpp:76] [1] PROFILING BEGIN[Forward]
I1226 08:56:35.091789 89709 MultiSolver.cpp:109] [7] PROFILING END[Backward]
I1226 08:56:35.091861 89709 solver.cpp:303] [7] Iteration 1, loss = 3.30462
I1226 08:56:35.091915 89709 solver.cpp:329]     Train net output #0: accuracy = 0.308594
I1226 08:56:35.091965 89709 solver.cpp:329]     Train net output #1: loss = 3.30462 (* 1 = 3.30462 loss)
I1226 08:56:35.092010 89709 MultiSolver.cpp:76] [7] PROFILING BEGIN[Forward]
I1226 08:56:35.096755 92150 MultiSolver.cpp:109] [2] PROFILING END[Backward]
I1226 08:56:35.096835 92150 solver.cpp:303] [2] Iteration 1, loss = 3.34758
I1226 08:56:35.096899 92150 solver.cpp:329]     Train net output #0: accuracy = 0.328125
I1226 08:56:35.096966 92150 solver.cpp:329]     Train net output #1: loss = 3.34758 (* 1 = 3.34758 loss)
I1226 08:56:35.097023 92150 MultiSolver.cpp:76] [2] PROFILING BEGIN[Forward]
I1226 08:56:35.107050 119888 MultiSolver.cpp:109] [4] PROFILING END[Backward]
I1226 08:56:35.107134 119888 solver.cpp:303] [4] Iteration 1, loss = 3.47537
I1226 08:56:35.107197 119888 solver.cpp:329]     Train net output #0: accuracy = 0.292969
I1226 08:56:35.107264 119888 solver.cpp:329]     Train net output #1: loss = 3.47537 (* 1 = 3.47537 loss)
I1226 08:56:35.107331 119888 MultiSolver.cpp:76] [4] PROFILING BEGIN[Forward]
I1226 08:56:35.222612 86102 MultiSolver.cpp:109] [3] PROFILING END[Backward]
I1226 08:56:35.222695 86102 solver.cpp:303] [3] Iteration 1, loss = 3.09322
I1226 08:56:35.222759 86102 solver.cpp:329]     Train net output #0: accuracy = 0.34375
I1226 08:56:35.222826 86102 solver.cpp:329]     Train net output #1: loss = 3.09322 (* 1 = 3.09322 loss)
I1226 08:56:35.222895 86102 MultiSolver.cpp:76] [3] PROFILING BEGIN[Forward]
I1226 08:56:35.443013 91127 MultiSolver.cpp:93] [6] PROFILING END[Forward]
I1226 08:56:35.443099 91127 MultiSolver.cpp:95] [6] PROFILING BEGIN[Backward]
I1226 08:56:32.242388 90165 MultiSolver.cpp:93] [1] PROFILING END[Forward]
I1226 08:56:32.242525 90165 MultiSolver.cpp:95] [1] PROFILING BEGIN[Backward]
I1226 08:56:35.514055 89709 MultiSolver.cpp:93] [7] PROFILING END[Forward]
I1226 08:56:35.514132 89709 MultiSolver.cpp:95] [7] PROFILING BEGIN[Backward]
I1226 08:56:35.528825 92150 MultiSolver.cpp:93] [2] PROFILING END[Forward]
I1226 08:56:35.528916 92150 MultiSolver.cpp:95] [2] PROFILING BEGIN[Backward]
I1226 08:56:35.541538 119888 MultiSolver.cpp:93] [4] PROFILING END[Forward]
I1226 08:56:35.541760 119888 MultiSolver.cpp:95] [4] PROFILING BEGIN[Backward]
I1226 08:56:35.639219 86102 MultiSolver.cpp:93] [3] PROFILING END[Forward]
I1226 08:56:35.639314 86102 MultiSolver.cpp:95] [3] PROFILING BEGIN[Backward]
I1226 08:56:36.848466 91127 MultiSolver.cpp:109] [6] PROFILING END[Backward]
I1226 08:56:36.848548 91127 solver.cpp:303] [6] Iteration 2, loss = 3.24121
I1226 08:56:36.848610 91127 solver.cpp:329]     Train net output #0: accuracy = 0.3125
I1226 08:56:36.848675 91127 solver.cpp:329]     Train net output #1: loss = 3.24121 (* 1 = 3.24121 loss)
I1226 08:56:36.848732 91127 MultiSolver.cpp:76] [6] PROFILING BEGIN[Forward]
I1226 08:56:37.059885 119888 MultiSolver.cpp:109] [4] PROFILING END[Backward]
I1226 08:56:37.059963 119888 solver.cpp:303] [4] Iteration 2, loss = 3.19334
I1226 08:56:37.060045 119888 solver.cpp:329]     Train net output #0: accuracy = 0.351562
I1226 08:56:37.060111 119888 solver.cpp:329]     Train net output #1: loss = 3.19334 (* 1 = 3.19334 loss)
I1226 08:56:37.060178 119888 MultiSolver.cpp:76] [4] PROFILING BEGIN[Forward]
I1226 08:56:37.068884 89709 MultiSolver.cpp:109] [7] PROFILING END[Backward]
I1226 08:56:37.068956 89709 solver.cpp:303] [7] Iteration 2, loss = 3.13283
I1226 08:56:37.069010 89709 solver.cpp:329]     Train net output #0: accuracy = 0.339844
I1226 08:56:37.069061 89709 solver.cpp:329]     Train net output #1: loss = 3.13283 (* 1 = 3.13283 loss)
I1226 08:56:37.069105 89709 MultiSolver.cpp:76] [7] PROFILING BEGIN[Forward]
I1226 08:56:37.073823 92150 MultiSolver.cpp:109] [2] PROFILING END[Backward]
I1226 08:56:37.073906 92150 solver.cpp:303] [2] Iteration 2, loss = 3.23168
I1226 08:56:37.073971 92150 solver.cpp:329]     Train net output #0: accuracy = 0.320312
I1226 08:56:37.074036 92150 solver.cpp:329]     Train net output #1: loss = 3.23168 (* 1 = 3.23168 loss)
I1226 08:56:37.074093 92150 MultiSolver.cpp:76] [2] PROFILING BEGIN[Forward]
I1226 08:56:33.814957 90165 MultiSolver.cpp:109] [1] PROFILING END[Backward]
I1226 08:56:33.815042 90165 solver.cpp:303] [1] Iteration 2, loss = 3.29811
I1226 08:56:33.815105 90165 solver.cpp:329]     Train net output #0: accuracy = 0.28125
I1226 08:56:33.815194 90165 solver.cpp:329]     Train net output #1: loss = 3.29811 (* 1 = 3.29811 loss)
I1226 08:56:33.815260 90165 MultiSolver.cpp:76] [1] PROFILING BEGIN[Forward]
I1226 08:56:37.132419 86102 MultiSolver.cpp:109] [3] PROFILING END[Backward]
I1226 08:56:37.132503 86102 solver.cpp:303] [3] Iteration 2, loss = 3.31662
I1226 08:56:37.132591 86102 solver.cpp:329]     Train net output #0: accuracy = 0.304688
I1226 08:56:37.132658 86102 solver.cpp:329]     Train net output #1: loss = 3.31662 (* 1 = 3.31662 loss)
I1226 08:56:37.132728 86102 MultiSolver.cpp:76] [3] PROFILING BEGIN[Forward]
I1226 08:56:37.289245 91127 MultiSolver.cpp:93] [6] PROFILING END[Forward]
I1226 08:56:37.289362 91127 MultiSolver.cpp:95] [6] PROFILING BEGIN[Backward]
I1226 08:56:34.224355 90165 MultiSolver.cpp:93] [1] PROFILING END[Forward]
I1226 08:56:34.224452 90165 MultiSolver.cpp:95] [1] PROFILING BEGIN[Backward]
I1226 08:56:37.488507 89709 MultiSolver.cpp:93] [7] PROFILING END[Forward]
I1226 08:56:37.488631 89709 MultiSolver.cpp:95] [7] PROFILING BEGIN[Backward]
I1226 08:56:37.488080 119888 MultiSolver.cpp:93] [4] PROFILING END[Forward]
I1226 08:56:37.488178 119888 MultiSolver.cpp:95] [4] PROFILING BEGIN[Backward]
I1226 08:56:37.498576 92150 MultiSolver.cpp:93] [2] PROFILING END[Forward]
I1226 08:56:37.498677 92150 MultiSolver.cpp:95] [2] PROFILING BEGIN[Backward]
I1226 08:56:37.537178 86102 MultiSolver.cpp:93] [3] PROFILING END[Forward]
I1226 08:56:37.537274 86102 MultiSolver.cpp:95] [3] PROFILING BEGIN[Backward]
I1226 08:56:37.798188 95005 MultiSolver.cpp:93] [5] PROFILING END[Forward]
I1226 08:56:37.798287 95005 MultiSolver.cpp:95] [5] PROFILING BEGIN[Backward]
I1226 08:56:37.897264 94304 MultiSolver.cpp:93] [0] PROFILING END[Forward]
I1226 08:56:37.897347 94304 MultiSolver.cpp:95] [0] PROFILING BEGIN[Backward]
I1226 08:56:38.800335 91127 MultiSolver.cpp:109] [6] PROFILING END[Backward]
I1226 08:56:38.800446 91127 solver.cpp:303] [6] Iteration 3, loss = 3.29101
I1226 08:56:38.800509 91127 solver.cpp:329]     Train net output #0: accuracy = 0.289062
I1226 08:56:38.800573 91127 solver.cpp:329]     Train net output #1: loss = 3.29101 (* 1 = 3.29101 loss)
I1226 08:56:38.800833 91127 MultiSolver.cpp:76] [6] PROFILING BEGIN[Forward]
I1226 08:56:39.046041 89709 MultiSolver.cpp:109] [7] PROFILING END[Backward]
I1226 08:56:39.046123 89709 solver.cpp:303] [7] Iteration 3, loss = 3.19566
I1226 08:56:39.046190 89709 solver.cpp:329]     Train net output #0: accuracy = 0.304688
I1226 08:56:39.046241 89709 solver.cpp:329]     Train net output #1: loss = 3.19566 (* 1 = 3.19566 loss)
I1226 08:56:39.046296 89709 MultiSolver.cpp:76] [7] PROFILING BEGIN[Forward]
I1226 08:56:35.816408 90165 MultiSolver.cpp:109] [1] PROFILING END[Backward]
I1226 08:56:35.816490 90165 solver.cpp:303] [1] Iteration 3, loss = 3.42719
I1226 08:56:35.816558 90165 solver.cpp:329]     Train net output #0: accuracy = 0.304688
I1226 08:56:35.816654 90165 solver.cpp:329]     Train net output #1: loss = 3.42719 (* 1 = 3.42719 loss)
I1226 08:56:35.816720 90165 MultiSolver.cpp:76] [1] PROFILING BEGIN[Forward]
I1226 08:56:39.087862 86102 MultiSolver.cpp:109] [3] PROFILING END[Backward]
I1226 08:56:39.087935 86102 solver.cpp:303] [3] Iteration 3, loss = 3.40613
I1226 08:56:39.088001 86102 solver.cpp:329]     Train net output #0: accuracy = 0.300781
I1226 08:56:39.088068 86102 solver.cpp:329]     Train net output #1: loss = 3.40613 (* 1 = 3.40613 loss)
I1226 08:56:39.088320 86102 MultiSolver.cpp:76] [3] PROFILING BEGIN[Forward]
I1226 08:56:39.117156 119888 MultiSolver.cpp:109] [4] PROFILING END[Backward]
I1226 08:56:39.117251 119888 solver.cpp:303] [4] Iteration 3, loss = 2.90254
I1226 08:56:39.117374 119888 solver.cpp:329]     Train net output #0: accuracy = 0.390625
I1226 08:56:39.117811 119888 solver.cpp:329]     Train net output #1: loss = 2.90254 (* 1 = 2.90254 loss)
I1226 08:56:39.118032 119888 MultiSolver.cpp:76] [4] PROFILING BEGIN[Forward]
I1226 08:56:39.166795 92150 MultiSolver.cpp:109] [2] PROFILING END[Backward]
I1226 08:56:39.166911 92150 solver.cpp:303] [2] Iteration 3, loss = 3.35669
I1226 08:56:39.166997 92150 solver.cpp:329]     Train net output #0: accuracy = 0.316406
I1226 08:56:39.167243 92150 solver.cpp:329]     Train net output #1: loss = 3.35669 (* 1 = 3.35669 loss)
I1226 08:56:39.167315 92150 MultiSolver.cpp:76] [2] PROFILING BEGIN[Forward]
I1226 08:56:39.226646 91127 MultiSolver.cpp:93] [6] PROFILING END[Forward]
I1226 08:56:39.226740 91127 MultiSolver.cpp:95] [6] PROFILING BEGIN[Backward]
I1226 08:56:36.216452 90165 MultiSolver.cpp:93] [1] PROFILING END[Forward]
I1226 08:56:36.216681 90165 MultiSolver.cpp:95] [1] PROFILING BEGIN[Backward]
I1226 08:56:39.483572 86102 MultiSolver.cpp:93] [3] PROFILING END[Forward]
I1226 08:56:39.483666 86102 MultiSolver.cpp:95] [3] PROFILING BEGIN[Backward]
I1226 08:56:39.486670 89709 MultiSolver.cpp:93] [7] PROFILING END[Forward]
I1226 08:56:39.486742 89709 MultiSolver.cpp:95] [7] PROFILING BEGIN[Backward]
I1226 08:56:39.523098 119888 MultiSolver.cpp:93] [4] PROFILING END[Forward]
I1226 08:56:39.523192 119888 MultiSolver.cpp:95] [4] PROFILING BEGIN[Backward]
I1226 08:56:39.578718 92150 MultiSolver.cpp:93] [2] PROFILING END[Forward]
I1226 08:56:39.580720 92150 MultiSolver.cpp:95] [2] PROFILING BEGIN[Backward]
I1226 08:56:40.846429 91127 MultiSolver.cpp:109] [6] PROFILING END[Backward]
I1226 08:56:40.846511 91127 solver.cpp:303] [6] Iteration 4, loss = 3.36011
I1226 08:56:40.846575 91127 solver.cpp:329]     Train net output #0: accuracy = 0.269531
I1226 08:56:40.846637 91127 solver.cpp:329]     Train net output #1: loss = 3.36011 (* 1 = 3.36011 loss)
I1226 08:56:40.846693 91127 MultiSolver.cpp:76] [6] PROFILING BEGIN[Forward]
I1226 08:56:37.863378 90165 MultiSolver.cpp:109] [1] PROFILING END[Backward]
I1226 08:56:37.863461 90165 solver.cpp:303] [1] Iteration 4, loss = 3.3095
I1226 08:56:37.863530 90165 solver.cpp:329]     Train net output #0: accuracy = 0.289062
I1226 08:56:37.863682 90165 solver.cpp:329]     Train net output #1: loss = 3.3095 (* 1 = 3.3095 loss)
I1226 08:56:37.863948 90165 MultiSolver.cpp:76] [1] PROFILING BEGIN[Forward]
I1226 08:56:41.135824 89709 MultiSolver.cpp:109] [7] PROFILING END[Backward]
I1226 08:56:41.135895 89709 solver.cpp:303] [7] Iteration 4, loss = 3.20487
I1226 08:56:41.135949 89709 solver.cpp:329]     Train net output #0: accuracy = 0.3125
I1226 08:56:41.135999 89709 solver.cpp:329]     Train net output #1: loss = 3.20487 (* 1 = 3.20487 loss)
I1226 08:56:41.136044 89709 MultiSolver.cpp:76] [7] PROFILING BEGIN[Forward]
I1226 08:56:41.162613 86102 MultiSolver.cpp:109] [3] PROFILING END[Backward]
I1226 08:56:41.162688 86102 solver.cpp:303] [3] Iteration 4, loss = 3.38357
I1226 08:56:41.162752 86102 solver.cpp:329]     Train net output #0: accuracy = 0.300781
I1226 08:56:41.162818 86102 solver.cpp:329]     Train net output #1: loss = 3.38357 (* 1 = 3.38357 loss)
I1226 08:56:41.162874 86102 MultiSolver.cpp:76] [3] PROFILING BEGIN[Forward]
I1226 08:56:41.188184 119888 MultiSolver.cpp:109] [4] PROFILING END[Backward]
I1226 08:56:41.188267 119888 solver.cpp:303] [4] Iteration 4, loss = 3.1621
I1226 08:56:41.188333 119888 solver.cpp:329]     Train net output #0: accuracy = 0.324219
I1226 08:56:41.188401 119888 solver.cpp:329]     Train net output #1: loss = 3.1621 (* 1 = 3.1621 loss)
I1226 08:56:41.188460 119888 MultiSolver.cpp:76] [4] PROFILING BEGIN[Forward]
I1226 08:56:41.267815 91127 MultiSolver.cpp:93] [6] PROFILING END[Forward]
I1226 08:56:41.267907 91127 MultiSolver.cpp:95] [6] PROFILING BEGIN[Backward]
I1226 08:56:41.275925 92150 MultiSolver.cpp:109] [2] PROFILING END[Backward]
I1226 08:56:41.276007 92150 solver.cpp:303] [2] Iteration 4, loss = 3.42208
I1226 08:56:41.276072 92150 solver.cpp:329]     Train net output #0: accuracy = 0.308594
I1226 08:56:41.276132 92150 solver.cpp:329]     Train net output #1: loss = 3.42208 (* 1 = 3.42208 loss)
I1226 08:56:41.276190 92150 MultiSolver.cpp:76] [2] PROFILING BEGIN[Forward]
I1226 08:56:38.286635 90165 MultiSolver.cpp:93] [1] PROFILING END[Forward]
I1226 08:56:38.286772 90165 MultiSolver.cpp:95] [1] PROFILING BEGIN[Backward]
I1226 08:56:41.560626 89709 MultiSolver.cpp:93] [7] PROFILING END[Forward]
I1226 08:56:41.560700 89709 MultiSolver.cpp:95] [7] PROFILING BEGIN[Backward]
I1226 08:56:41.583458 86102 MultiSolver.cpp:93] [3] PROFILING END[Forward]
I1226 08:56:41.583562 86102 MultiSolver.cpp:95] [3] PROFILING BEGIN[Backward]
I1226 08:56:41.599973 119888 MultiSolver.cpp:93] [4] PROFILING END[Forward]
I1226 08:56:41.600067 119888 MultiSolver.cpp:95] [4] PROFILING BEGIN[Backward]
I1226 08:56:41.667839 92150 MultiSolver.cpp:93] [2] PROFILING END[Forward]
I1226 08:56:41.668706 92150 MultiSolver.cpp:95] [2] PROFILING BEGIN[Backward]
I1226 08:56:42.679152 91127 MultiSolver.cpp:109] [6] PROFILING END[Backward]
I1226 08:56:42.679234 91127 solver.cpp:303] [6] Iteration 5, loss = 3.31117
I1226 08:56:42.679297 91127 solver.cpp:329]     Train net output #0: accuracy = 0.328125
I1226 08:56:42.679363 91127 solver.cpp:329]     Train net output #1: loss = 3.31117 (* 1 = 3.31117 loss)
I1226 08:56:42.679440 91127 MultiSolver.cpp:76] [6] PROFILING BEGIN[Forward]
I1226 08:56:43.013240 119888 MultiSolver.cpp:109] [4] PROFILING END[Backward]
I1226 08:56:43.013326 119888 solver.cpp:303] [4] Iteration 5, loss = 3.0906
I1226 08:56:43.013391 119888 solver.cpp:329]     Train net output #0: accuracy = 0.320312
I1226 08:56:43.013458 119888 solver.cpp:329]     Train net output #1: loss = 3.0906 (* 1 = 3.0906 loss)
I1226 08:56:43.013516 119888 MultiSolver.cpp:76] [4] PROFILING BEGIN[Forward]
I1226 08:56:43.041447 89709 MultiSolver.cpp:109] [7] PROFILING END[Backward]
I1226 08:56:39.778309 90165 MultiSolver.cpp:109] [1] PROFILING END[Backward]
I1226 08:56:43.041517 89709 solver.cpp:303] [7] Iteration 5, loss = 3.01329
I1226 08:56:43.041570 89709 solver.cpp:329]     Train net output #0: accuracy = 0.308594
I1226 08:56:43.041620 89709 solver.cpp:329]     Train net output #1: loss = 3.01329 (* 1 = 3.01329 loss)
I1226 08:56:43.041663 89709 MultiSolver.cpp:76] [7] PROFILING BEGIN[Forward]
I1226 08:56:39.778445 90165 solver.cpp:303] [1] Iteration 5, loss = 3.25915
I1226 08:56:39.778645 90165 solver.cpp:329]     Train net output #0: accuracy = 0.300781
I1226 08:56:39.778775 90165 solver.cpp:329]     Train net output #1: loss = 3.25915 (* 1 = 3.25915 loss)
I1226 08:56:39.779016 90165 MultiSolver.cpp:76] [1] PROFILING BEGIN[Forward]
I1226 08:56:43.049921 86102 MultiSolver.cpp:109] [3] PROFILING END[Backward]
I1226 08:56:43.050007 86102 solver.cpp:303] [3] Iteration 5, loss = 3.43715
I1226 08:56:43.050071 86102 solver.cpp:329]     Train net output #0: accuracy = 0.277344
I1226 08:56:43.050138 86102 solver.cpp:329]     Train net output #1: loss = 3.43715 (* 1 = 3.43715 loss)
I1226 08:56:43.050348 86102 MultiSolver.cpp:76] [3] PROFILING BEGIN[Forward]
I1226 08:56:43.126598 91127 MultiSolver.cpp:93] [6] PROFILING END[Forward]
I1226 08:56:43.126694 91127 MultiSolver.cpp:95] [6] PROFILING BEGIN[Backward]
I1226 08:56:43.149324 92150 MultiSolver.cpp:109] [2] PROFILING END[Backward]
I1226 08:56:43.149448 92150 solver.cpp:303] [2] Iteration 5, loss = 3.0988
I1226 08:56:43.149513 92150 solver.cpp:329]     Train net output #0: accuracy = 0.316406
I1226 08:56:43.149585 92150 solver.cpp:329]     Train net output #1: loss = 3.0988 (* 1 = 3.0988 loss)
I1226 08:56:43.149647 92150 MultiSolver.cpp:76] [2] PROFILING BEGIN[Forward]
I1226 08:56:43.429206 89709 MultiSolver.cpp:93] [7] PROFILING END[Forward]
I1226 08:56:43.430163 89709 MultiSolver.cpp:95] [7] PROFILING BEGIN[Backward]
I1226 08:56:43.447079 119888 MultiSolver.cpp:93] [4] PROFILING END[Forward]
I1226 08:56:43.447175 119888 MultiSolver.cpp:95] [4] PROFILING BEGIN[Backward]
I1226 08:56:43.463716 86102 MultiSolver.cpp:93] [3] PROFILING END[Forward]
I1226 08:56:43.463832 86102 MultiSolver.cpp:95] [3] PROFILING BEGIN[Backward]
I1226 08:56:40.198168 90165 MultiSolver.cpp:93] [1] PROFILING END[Forward]
I1226 08:56:40.198268 90165 MultiSolver.cpp:95] [1] PROFILING BEGIN[Backward]
I1226 08:56:43.521240 92150 MultiSolver.cpp:93] [2] PROFILING END[Forward]
I1226 08:56:43.521325 92150 MultiSolver.cpp:95] [2] PROFILING BEGIN[Backward]
I1226 08:56:44.540009 91127 MultiSolver.cpp:109] [6] PROFILING END[Backward]
I1226 08:56:44.540094 91127 solver.cpp:303] [6] Iteration 6, loss = 3.27772
I1226 08:56:44.540158 91127 solver.cpp:329]     Train net output #0: accuracy = 0.320312
I1226 08:56:44.540225 91127 solver.cpp:329]     Train net output #1: loss = 3.27772 (* 1 = 3.27772 loss)
I1226 08:56:44.540282 91127 MultiSolver.cpp:76] [6] PROFILING BEGIN[Forward]
I1226 08:56:44.853286 119888 MultiSolver.cpp:109] [4] PROFILING END[Backward]
I1226 08:56:41.606444 90165 MultiSolver.cpp:109] [1] PROFILING END[Backward]
I1226 08:56:41.606528 90165 solver.cpp:303] [1] Iteration 6, loss = 3.12717
I1226 08:56:41.606608 90165 solver.cpp:329]     Train net output #0: accuracy = 0.324219
I1226 08:56:41.606675 90165 solver.cpp:329]     Train net output #1: loss = 3.12717 (* 1 = 3.12717 loss)
I1226 08:56:41.606735 90165 MultiSolver.cpp:76] [1] PROFILING BEGIN[Forward]
I1226 08:56:44.855372 119888 solver.cpp:303] [4] Iteration 6, loss = 3.30534
I1226 08:56:44.872710 119888 solver.cpp:329]     Train net output #0: accuracy = 0.324219
I1226 08:56:44.872838 119888 solver.cpp:329]     Train net output #1: loss = 3.30534 (* 1 = 3.30534 loss)
I1226 08:56:44.872915 119888 MultiSolver.cpp:76] [4] PROFILING BEGIN[Forward]
I1226 08:56:44.882787 86102 MultiSolver.cpp:109] [3] PROFILING END[Backward]
I1226 08:56:44.882870 86102 solver.cpp:303] [3] Iteration 6, loss = 3.54285
I1226 08:56:44.882936 86102 solver.cpp:329]     Train net output #0: accuracy = 0.269531
I1226 08:56:44.883591 86102 solver.cpp:329]     Train net output #1: loss = 3.54285 (* 1 = 3.54285 loss)
I1226 08:56:44.884074 86102 MultiSolver.cpp:76] [3] PROFILING BEGIN[Forward]
I1226 08:56:44.897891 89709 MultiSolver.cpp:109] [7] PROFILING END[Backward]
I1226 08:56:44.897990 89709 solver.cpp:303] [7] Iteration 6, loss = 3.1987
I1226 08:56:44.898044 89709 solver.cpp:329]     Train net output #0: accuracy = 0.347656
I1226 08:56:44.898094 89709 solver.cpp:329]     Train net output #1: loss = 3.1987 (* 1 = 3.1987 loss)
I1226 08:56:44.898140 89709 MultiSolver.cpp:76] [7] PROFILING BEGIN[Forward]
I1226 08:56:44.985005 92150 MultiSolver.cpp:109] [2] PROFILING END[Backward]
I1226 08:56:44.985083 92150 solver.cpp:303] [2] Iteration 6, loss = 3.45481
I1226 08:56:44.985149 92150 solver.cpp:329]     Train net output #0: accuracy = 0.265625
I1226 08:56:44.985218 92150 solver.cpp:329]     Train net output #1: loss = 3.45481 (* 1 = 3.45481 loss)
I1226 08:56:44.985303 92150 MultiSolver.cpp:76] [2] PROFILING BEGIN[Forward]
I1226 08:56:44.984872 91127 MultiSolver.cpp:93] [6] PROFILING END[Forward]
I1226 08:56:44.984964 91127 MultiSolver.cpp:95] [6] PROFILING BEGIN[Backward]
I1226 08:56:45.278502 119888 MultiSolver.cpp:93] [4] PROFILING END[Forward]
I1226 08:56:42.020453 90165 MultiSolver.cpp:93] [1] PROFILING END[Forward]
I1226 08:56:45.278645 119888 MultiSolver.cpp:95] [4] PROFILING BEGIN[Backward]
I1226 08:56:42.020552 90165 MultiSolver.cpp:95] [1] PROFILING BEGIN[Backward]
I1226 08:56:45.294906 86102 MultiSolver.cpp:93] [3] PROFILING END[Forward]
I1226 08:56:45.295007 86102 MultiSolver.cpp:95] [3] PROFILING BEGIN[Backward]
I1226 08:56:45.319984 89709 MultiSolver.cpp:93] [7] PROFILING END[Forward]
I1226 08:56:45.320081 89709 MultiSolver.cpp:95] [7] PROFILING BEGIN[Backward]
I1226 08:56:45.388970 92150 MultiSolver.cpp:93] [2] PROFILING END[Forward]
I1226 08:56:45.389065 92150 MultiSolver.cpp:95] [2] PROFILING BEGIN[Backward]
I1226 08:56:46.416494 91127 MultiSolver.cpp:109] [6] PROFILING END[Backward]
I1226 08:56:46.416568 91127 solver.cpp:303] [6] Iteration 7, loss = 3.05255
I1226 08:56:46.416631 91127 solver.cpp:329]     Train net output #0: accuracy = 0.363281
I1226 08:56:46.416787 91127 solver.cpp:329]     Train net output #1: loss = 3.05255 (* 1 = 3.05255 loss)
I1226 08:56:46.416853 91127 MultiSolver.cpp:76] [6] PROFILING BEGIN[Forward]
I1226 08:56:46.697060 119888 MultiSolver.cpp:109] [4] PROFILING END[Backward]
I1226 08:56:46.697144 119888 solver.cpp:303] [4] Iteration 7, loss = 3.22644
I1226 08:56:46.697304 119888 solver.cpp:329]     Train net output #0: accuracy = 0.304688
I1226 08:56:46.697376 119888 solver.cpp:329]     Train net output #1: loss = 3.22644 (* 1 = 3.22644 loss)
I1226 08:56:46.697440 119888 MultiSolver.cpp:76] [4] PROFILING BEGIN[Forward]
I1226 08:56:46.706370 86102 MultiSolver.cpp:109] [3] PROFILING END[Backward]
I1226 08:56:46.706454 86102 solver.cpp:303] [3] Iteration 7, loss = 3.2878
I1226 08:56:46.706517 86102 solver.cpp:329]     Train net output #0: accuracy = 0.292969
I1226 08:56:46.706694 86102 solver.cpp:329]     Train net output #1: loss = 3.2878 (* 1 = 3.2878 loss)
I1226 08:56:46.706755 86102 MultiSolver.cpp:76] [3] PROFILING BEGIN[Forward]
I1226 08:56:43.501469 90165 MultiSolver.cpp:109] [1] PROFILING END[Backward]
I1226 08:56:43.501549 90165 solver.cpp:303] [1] Iteration 7, loss = 3.19987
I1226 08:56:43.501796 90165 solver.cpp:329]     Train net output #0: accuracy = 0.300781
I1226 08:56:43.501965 90165 solver.cpp:329]     Train net output #1: loss = 3.19987 (* 1 = 3.19987 loss)
I1226 08:56:43.502058 90165 MultiSolver.cpp:76] [1] PROFILING BEGIN[Forward]
I1226 08:56:46.765540 89709 MultiSolver.cpp:109] [7] PROFILING END[Backward]
I1226 08:56:46.765612 89709 solver.cpp:303] [7] Iteration 7, loss = 3.50259
I1226 08:56:46.765666 89709 solver.cpp:329]     Train net output #0: accuracy = 0.285156
I1226 08:56:46.765806 89709 solver.cpp:329]     Train net output #1: loss = 3.50259 (* 1 = 3.50259 loss)
I1226 08:56:46.765857 89709 MultiSolver.cpp:76] [7] PROFILING BEGIN[Forward]
I1226 08:56:46.821033 92150 MultiSolver.cpp:109] [2] PROFILING END[Backward]
I1226 08:56:46.821115 92150 solver.cpp:303] [2] Iteration 7, loss = 3.14112
I1226 08:56:46.821178 92150 solver.cpp:329]     Train net output #0: accuracy = 0.347656
I1226 08:56:46.821334 92150 solver.cpp:329]     Train net output #1: loss = 3.14112 (* 1 = 3.14112 loss)
I1226 08:56:46.821425 92150 MultiSolver.cpp:76] [2] PROFILING BEGIN[Forward]
I1226 08:56:46.831663 91127 MultiSolver.cpp:93] [6] PROFILING END[Forward]
I1226 08:56:46.831789 91127 MultiSolver.cpp:95] [6] PROFILING BEGIN[Backward]
I1226 08:56:46.901021 95005 MultiSolver.cpp:109] [5] PROFILING END[Backward]
I1226 08:56:46.914844 95005 solver.cpp:303] [5] Iteration 1, loss = 3.26116
I1226 08:56:46.914937 95005 solver.cpp:329]     Train net output #0: accuracy = 0.285156
I1226 08:56:46.915011 95005 solver.cpp:329]     Train net output #1: loss = 3.26116 (* 1 = 3.26116 loss)
I1226 08:56:46.915079 95005 MultiSolver.cpp:76] [5] PROFILING BEGIN[Forward]
I1226 08:56:46.929883 94304 MultiSolver.cpp:109] [0] PROFILING END[Backward]
I1226 08:56:46.930755 94304 solver.cpp:303] [0] Iteration 1, loss = 3.3294
I1226 08:56:46.930848 94304 solver.cpp:329]     Train net output #0: accuracy = 0.304688
I1226 08:56:46.930901 94304 solver.cpp:329]     Train net output #1: loss = 3.3294 (* 1 = 3.3294 loss)
I1226 08:56:46.930948 94304 MultiSolver.cpp:76] [0] PROFILING BEGIN[Forward]
I1226 08:56:47.101585 119888 MultiSolver.cpp:93] [4] PROFILING END[Forward]
I1226 08:56:47.102211 119888 MultiSolver.cpp:95] [4] PROFILING BEGIN[Backward]
I1226 08:56:47.133816 86102 MultiSolver.cpp:93] [3] PROFILING END[Forward]
I1226 08:56:47.133924 86102 MultiSolver.cpp:95] [3] PROFILING BEGIN[Backward]
I1226 08:56:43.931677 90165 MultiSolver.cpp:93] [1] PROFILING END[Forward]
I1226 08:56:43.931830 90165 MultiSolver.cpp:95] [1] PROFILING BEGIN[Backward]
I1226 08:56:47.200881 89709 MultiSolver.cpp:93] [7] PROFILING END[Forward]
I1226 08:56:47.200961 89709 MultiSolver.cpp:95] [7] PROFILING BEGIN[Backward]
I1226 08:56:47.233723 92150 MultiSolver.cpp:93] [2] PROFILING END[Forward]
I1226 08:56:47.233808 92150 MultiSolver.cpp:95] [2] PROFILING BEGIN[Backward]
I1226 08:56:48.247345 91127 MultiSolver.cpp:109] [6] PROFILING END[Backward]
I1226 08:56:48.247452 91127 solver.cpp:303] [6] Iteration 8, loss = 3.0662
I1226 08:56:48.247515 91127 solver.cpp:329]     Train net output #0: accuracy = 0.367188
I1226 08:56:48.247607 91127 solver.cpp:329]     Train net output #1: loss = 3.0662 (* 1 = 3.0662 loss)
I1226 08:56:48.247669 91127 MultiSolver.cpp:76] [6] PROFILING BEGIN[Forward]
I1226 08:56:48.568703 86102 MultiSolver.cpp:109] [3] PROFILING END[Backward]
I1226 08:56:48.568786 86102 solver.cpp:303] [3] Iteration 8, loss = 3.22834
I1226 08:56:48.568850 86102 solver.cpp:329]     Train net output #0: accuracy = 0.308594
I1226 08:56:48.568924 86102 solver.cpp:329]     Train net output #1: loss = 3.22834 (* 1 = 3.22834 loss)
I1226 08:56:48.568987 86102 MultiSolver.cpp:76] [3] PROFILING BEGIN[Forward]
I1226 08:56:48.587450 89709 MultiSolver.cpp:109] [7] PROFILING END[Backward]
I1226 08:56:48.587519 89709 solver.cpp:303] [7] Iteration 8, loss = 3.03738
I1226 08:56:48.587573 89709 solver.cpp:329]     Train net output #0: accuracy = 0.347656
I1226 08:56:48.587621 89709 solver.cpp:329]     Train net output #1: loss = 3.03738 (* 1 = 3.03738 loss)
I1226 08:56:48.587666 89709 MultiSolver.cpp:76] [7] PROFILING BEGIN[Forward]
I1226 08:56:48.584758 119888 MultiSolver.cpp:109] [4] PROFILING END[Backward]
I1226 08:56:48.584839 119888 solver.cpp:303] [4] Iteration 8, loss = 3.29615
I1226 08:56:48.584906 119888 solver.cpp:329]     Train net output #0: accuracy = 0.308594
I1226 08:56:48.584975 119888 solver.cpp:329]     Train net output #1: loss = 3.29615 (* 1 = 3.29615 loss)
I1226 08:56:48.585052 119888 MultiSolver.cpp:76] [4] PROFILING BEGIN[Forward]
I1226 08:56:45.342958 90165 MultiSolver.cpp:109] [1] PROFILING END[Backward]
I1226 08:56:45.343071 90165 solver.cpp:303] [1] Iteration 8, loss = 3.16316
I1226 08:56:45.343569 90165 solver.cpp:329]     Train net output #0: accuracy = 0.277344
I1226 08:56:45.343679 90165 solver.cpp:329]     Train net output #1: loss = 3.16316 (* 1 = 3.16316 loss)
I1226 08:56:45.343746 90165 MultiSolver.cpp:76] [1] PROFILING BEGIN[Forward]
I1226 08:56:48.674945 91127 MultiSolver.cpp:93] [6] PROFILING END[Forward]
I1226 08:56:48.675037 91127 MultiSolver.cpp:95] [6] PROFILING BEGIN[Backward]
I1226 08:56:48.684185 92150 MultiSolver.cpp:109] [2] PROFILING END[Backward]
I1226 08:56:48.684267 92150 solver.cpp:303] [2] Iteration 8, loss = 3.30849
I1226 08:56:48.684334 92150 solver.cpp:329]     Train net output #0: accuracy = 0.316406
I1226 08:56:48.684453 92150 solver.cpp:329]     Train net output #1: loss = 3.30849 (* 1 = 3.30849 loss)
I1226 08:56:48.684540 92150 MultiSolver.cpp:76] [2] PROFILING BEGIN[Forward]
I1226 08:56:48.971823 86102 MultiSolver.cpp:93] [3] PROFILING END[Forward]
I1226 08:56:48.971913 86102 MultiSolver.cpp:95] [3] PROFILING BEGIN[Backward]
I1226 08:56:48.983841 89709 MultiSolver.cpp:93] [7] PROFILING END[Forward]
I1226 08:56:48.983917 89709 MultiSolver.cpp:95] [7] PROFILING BEGIN[Backward]
I1226 08:56:49.002038 119888 MultiSolver.cpp:93] [4] PROFILING END[Forward]
I1226 08:56:49.002133 119888 MultiSolver.cpp:95] [4] PROFILING BEGIN[Backward]
I1226 08:56:45.786931 90165 MultiSolver.cpp:93] [1] PROFILING END[Forward]
I1226 08:56:45.787019 90165 MultiSolver.cpp:95] [1] PROFILING BEGIN[Backward]
I1226 08:56:49.094636 92150 MultiSolver.cpp:93] [2] PROFILING END[Forward]
I1226 08:56:49.094763 92150 MultiSolver.cpp:95] [2] PROFILING BEGIN[Backward]
I1226 08:56:50.168599 91127 MultiSolver.cpp:109] [6] PROFILING END[Backward]
I1226 08:56:50.168678 91127 solver.cpp:303] [6] Iteration 9, loss = 3.58219
I1226 08:56:50.168742 91127 solver.cpp:329]     Train net output #0: accuracy = 0.273438
I1226 08:56:50.168807 91127 solver.cpp:329]     Train net output #1: loss = 3.58219 (* 1 = 3.58219 loss)
I1226 08:56:50.169050 91127 MultiSolver.cpp:76] [6] PROFILING BEGIN[Forward]
I1226 08:56:50.497778 86102 MultiSolver.cpp:109] [3] PROFILING END[Backward]
I1226 08:56:50.497860 86102 solver.cpp:303] [3] Iteration 9, loss = 3.25845
I1226 08:56:50.497925 86102 solver.cpp:329]     Train net output #0: accuracy = 0.277344
I1226 08:56:50.497992 86102 solver.cpp:329]     Train net output #1: loss = 3.25845 (* 1 = 3.25845 loss)
I1226 08:56:50.498601 86102 MultiSolver.cpp:76] [3] PROFILING BEGIN[Forward]
I1226 08:56:50.501088 86174 blocking_queue.cpp:87] Waiting for data
I1226 08:56:50.520153 92150 MultiSolver.cpp:109] [2] PROFILING END[Backward]
I1226 08:56:50.520237 92150 solver.cpp:303] [2] Iteration 9, loss = 3.08847
I1226 08:56:50.520301 92150 solver.cpp:329]     Train net output #0: accuracy = 0.335938
I1226 08:56:50.520391 92150 solver.cpp:329]     Train net output #1: loss = 3.08847 (* 1 = 3.08847 loss)
I1226 08:56:50.520452 92150 MultiSolver.cpp:76] [2] PROFILING BEGIN[Forward]
I1226 08:56:47.255527 90165 MultiSolver.cpp:109] [1] PROFILING END[Backward]
I1226 08:56:47.255641 90165 solver.cpp:303] [1] Iteration 9, loss = 3.55996
I1226 08:56:47.255707 90165 solver.cpp:329]     Train net output #0: accuracy = 0.265625
I1226 08:56:47.255771 90165 solver.cpp:329]     Train net output #1: loss = 3.55996 (* 1 = 3.55996 loss)
I1226 08:56:47.255836 90165 MultiSolver.cpp:76] [1] PROFILING BEGIN[Forward]
I1226 08:56:50.524204 92184 blocking_queue.cpp:87] Waiting for data
I1226 08:56:50.529856 89709 MultiSolver.cpp:109] [7] PROFILING END[Backward]
I1226 08:56:50.529925 89709 solver.cpp:303] [7] Iteration 9, loss = 3.04266
I1226 08:56:50.529981 89709 solver.cpp:329]     Train net output #0: accuracy = 0.339844
I1226 08:56:50.530032 89709 solver.cpp:329]     Train net output #1: loss = 3.04266 (* 1 = 3.04266 loss)
I1226 08:56:50.530077 89709 MultiSolver.cpp:76] [7] PROFILING BEGIN[Forward]
I1226 08:56:50.534541 89776 blocking_queue.cpp:87] Waiting for data
I1226 08:56:50.568892 119888 MultiSolver.cpp:109] [4] PROFILING END[Backward]
I1226 08:56:50.568979 119888 solver.cpp:303] [4] Iteration 9, loss = 3.05573
I1226 08:56:50.569042 119888 solver.cpp:329]     Train net output #0: accuracy = 0.359375
I1226 08:56:50.569110 119888 solver.cpp:329]     Train net output #1: loss = 3.05573 (* 1 = 3.05573 loss)
I1226 08:56:50.569182 119888 MultiSolver.cpp:76] [4] PROFILING BEGIN[Forward]
I1226 08:56:50.606609 91127 MultiSolver.cpp:93] [6] PROFILING END[Forward]
I1226 08:56:50.606701 91127 MultiSolver.cpp:95] [6] PROFILING BEGIN[Backward]
I1226 08:56:47.694026 90165 MultiSolver.cpp:93] [1] PROFILING END[Forward]
I1226 08:56:47.694119 90165 MultiSolver.cpp:95] [1] PROFILING BEGIN[Backward]
I1226 08:56:50.969110 89709 MultiSolver.cpp:93] [7] PROFILING END[Forward]
I1226 08:56:50.969184 89709 MultiSolver.cpp:95] [7] PROFILING BEGIN[Backward]
I1226 08:56:51.008898 119888 MultiSolver.cpp:93] [4] PROFILING END[Forward]
I1226 08:56:51.008981 119888 MultiSolver.cpp:95] [4] PROFILING BEGIN[Backward]
I1226 08:56:51.174865 86102 MultiSolver.cpp:93] [3] PROFILING END[Forward]
I1226 08:56:51.174938 86102 MultiSolver.cpp:95] [3] PROFILING BEGIN[Backward]
I1226 08:56:51.255832 92150 MultiSolver.cpp:93] [2] PROFILING END[Forward]
I1226 08:56:51.255911 92150 MultiSolver.cpp:95] [2] PROFILING BEGIN[Backward]
I1226 08:56:51.985373 95005 MultiSolver.cpp:93] [5] PROFILING END[Forward]
I1226 08:56:51.985463 95005 MultiSolver.cpp:95] [5] PROFILING BEGIN[Backward]
I1226 08:56:52.002081 94304 MultiSolver.cpp:93] [0] PROFILING END[Forward]
I1226 08:56:52.002164 94304 MultiSolver.cpp:95] [0] PROFILING BEGIN[Backward]
I1226 08:56:52.018164 91127 MultiSolver.cpp:109] [6] PROFILING END[Backward]
I1226 08:56:52.018246 91127 solver.cpp:303] [6] Iteration 10, loss = 3.08893
I1226 08:56:52.018311 91127 solver.cpp:329]     Train net output #0: accuracy = 0.355469
I1226 08:56:52.018395 91127 solver.cpp:329]     Train net output #1: loss = 3.08893 (* 1 = 3.08893 loss)
I1226 08:56:52.018452 91127 MultiSolver.cpp:76] [6] PROFILING BEGIN[Forward]
I1226 08:56:52.252565 89709 MultiSolver.cpp:109] [7] PROFILING END[Backward]
I1226 08:56:52.252636 89709 solver.cpp:303] [7] Iteration 10, loss = 3.05823
I1226 08:56:52.252689 89709 solver.cpp:329]     Train net output #0: accuracy = 0.316406
I1226 08:56:52.252739 89709 solver.cpp:329]     Train net output #1: loss = 3.05823 (* 1 = 3.05823 loss)
I1226 08:56:52.252784 89709 MultiSolver.cpp:76] [7] PROFILING BEGIN[Forward]
I1226 08:56:52.403594 119888 MultiSolver.cpp:109] [4] PROFILING END[Backward]
I1226 08:56:52.403734 119888 solver.cpp:303] [4] Iteration 10, loss = 3.24311
I1226 08:56:52.403843 119888 solver.cpp:329]     Train net output #0: accuracy = 0.285156
I1226 08:56:52.403919 119888 solver.cpp:329]     Train net output #1: loss = 3.24311 (* 1 = 3.24311 loss)
I1226 08:56:52.404420 119888 MultiSolver.cpp:76] [4] PROFILING BEGIN[Forward]
I1226 08:56:52.406261 119954 blocking_queue.cpp:87] Waiting for data
I1226 08:56:49.159720 90165 MultiSolver.cpp:109] [1] PROFILING END[Backward]
I1226 08:56:49.159801 90165 solver.cpp:303] [1] Iteration 10, loss = 3.33059
I1226 08:56:49.159873 90165 solver.cpp:329]     Train net output #0: accuracy = 0.289062
I1226 08:56:49.159940 90165 solver.cpp:329]     Train net output #1: loss = 3.33059 (* 1 = 3.33059 loss)
I1226 08:56:49.160010 90165 MultiSolver.cpp:76] [1] PROFILING BEGIN[Forward]
I1226 08:56:49.161902 90231 blocking_queue.cpp:87] Waiting for data
I1226 08:56:52.445857 91127 MultiSolver.cpp:93] [6] PROFILING END[Forward]
I1226 08:56:52.445945 91127 MultiSolver.cpp:95] [6] PROFILING BEGIN[Backward]
I1226 08:56:52.509196 86102 MultiSolver.cpp:109] [3] PROFILING END[Backward]
I1226 08:56:52.509265 86102 solver.cpp:303] [3] Iteration 10, loss = 3.24493
I1226 08:56:52.509318 86102 solver.cpp:329]     Train net output #0: accuracy = 0.351562
I1226 08:56:52.509388 86102 solver.cpp:329]     Train net output #1: loss = 3.24493 (* 1 = 3.24493 loss)
I1226 08:56:52.509436 86102 MultiSolver.cpp:76] [3] PROFILING BEGIN[Forward]
I1226 08:56:52.612742 92150 MultiSolver.cpp:109] [2] PROFILING END[Backward]
I1226 08:56:52.612843 92150 solver.cpp:303] [2] Iteration 10, loss = 3.23863
I1226 08:56:52.612917 92150 solver.cpp:329]     Train net output #0: accuracy = 0.300781
I1226 08:56:52.612968 92150 solver.cpp:329]     Train net output #1: loss = 3.23863 (* 1 = 3.23863 loss)
I1226 08:56:52.613013 92150 MultiSolver.cpp:76] [2] PROFILING BEGIN[Forward]
I1226 08:56:50.201205 90165 MultiSolver.cpp:93] [1] PROFILING END[Forward]
I1226 08:56:50.201340 90165 MultiSolver.cpp:95] [1] PROFILING BEGIN[Backward]
I1226 08:56:53.515228 119888 MultiSolver.cpp:93] [4] PROFILING END[Forward]
I1226 08:56:53.515305 119888 MultiSolver.cpp:95] [4] PROFILING BEGIN[Backward]
I1226 08:56:53.616458 91127 MultiSolver.cpp:109] [6] PROFILING END[Backward]
I1226 08:56:53.616536 91127 solver.cpp:303] [6] Iteration 11, loss = 3.27058
I1226 08:56:53.616600 91127 solver.cpp:329]     Train net output #0: accuracy = 0.308594
I1226 08:56:53.616667 91127 solver.cpp:329]     Train net output #1: loss = 3.27058 (* 1 = 3.27058 loss)
I1226 08:56:53.616722 91127 MultiSolver.cpp:76] [6] PROFILING BEGIN[Forward]
I1226 08:56:54.024979 89709 MultiSolver.cpp:93] [7] PROFILING END[Forward]
I1226 08:56:54.025054 89709 MultiSolver.cpp:95] [7] PROFILING BEGIN[Backward]
I1226 08:56:54.035576 91127 MultiSolver.cpp:93] [6] PROFILING END[Forward]
I1226 08:56:54.035678 91127 MultiSolver.cpp:95] [6] PROFILING BEGIN[Backward]
I1226 08:56:54.183290 86102 MultiSolver.cpp:93] [3] PROFILING END[Forward]
I1226 08:56:54.184087 86102 MultiSolver.cpp:95] [3] PROFILING BEGIN[Backward]
I1226 08:56:54.333865 92150 MultiSolver.cpp:93] [2] PROFILING END[Forward]
I1226 08:56:54.333942 92150 MultiSolver.cpp:95] [2] PROFILING BEGIN[Backward]
I1226 08:56:51.486732 90165 MultiSolver.cpp:109] [1] PROFILING END[Backward]
I1226 08:56:51.486807 90165 solver.cpp:303] [1] Iteration 11, loss = 2.91654
I1226 08:56:51.486862 90165 solver.cpp:329]     Train net output #0: accuracy = 0.34375
I1226 08:56:51.486912 90165 solver.cpp:329]     Train net output #1: loss = 2.91654 (* 1 = 2.91654 loss)
I1226 08:56:51.486961 90165 MultiSolver.cpp:76] [1] PROFILING BEGIN[Forward]
I1226 08:56:54.865275 119888 MultiSolver.cpp:109] [4] PROFILING END[Backward]
I1226 08:56:54.865346 119888 solver.cpp:303] [4] Iteration 11, loss = 3.42639
I1226 08:56:54.865401 119888 solver.cpp:329]     Train net output #0: accuracy = 0.296875
I1226 08:56:54.865453 119888 solver.cpp:329]     Train net output #1: loss = 3.42639 (* 1 = 3.42639 loss)
I1226 08:56:54.865502 119888 MultiSolver.cpp:76] [4] PROFILING BEGIN[Forward]
I1226 08:56:55.270763 91127 MultiSolver.cpp:109] [6] PROFILING END[Backward]
I1226 08:56:55.270846 91127 solver.cpp:303] [6] Iteration 12, loss = 3.07403
I1226 08:56:55.270908 91127 solver.cpp:329]     Train net output #0: accuracy = 0.339844
I1226 08:56:55.270968 91127 solver.cpp:329]     Train net output #1: loss = 3.07403 (* 1 = 3.07403 loss)
I1226 08:56:55.271025 91127 MultiSolver.cpp:76] [6] PROFILING BEGIN[Forward]
I1226 08:56:55.294227 89709 MultiSolver.cpp:109] [7] PROFILING END[Backward]
I1226 08:56:55.294301 89709 solver.cpp:303] [7] Iteration 11, loss = 3.13983
I1226 08:56:55.294355 89709 solver.cpp:329]     Train net output #0: accuracy = 0.335938
I1226 08:56:55.295805 89709 solver.cpp:329]     Train net output #1: loss = 3.13983 (* 1 = 3.13983 loss)
I1226 08:56:55.295876 89709 MultiSolver.cpp:76] [7] PROFILING BEGIN[Forward]
I1226 08:56:55.426717 86102 MultiSolver.cpp:109] [3] PROFILING END[Backward]
I1226 08:56:55.426789 86102 solver.cpp:303] [3] Iteration 11, loss = 3.11418
I1226 08:56:55.426841 86102 solver.cpp:329]     Train net output #0: accuracy = 0.3125
I1226 08:56:55.426892 86102 solver.cpp:329]     Train net output #1: loss = 3.11418 (* 1 = 3.11418 loss)
I1226 08:56:55.426936 86102 MultiSolver.cpp:76] [3] PROFILING BEGIN[Forward]
I1226 08:56:55.627465 92150 MultiSolver.cpp:109] [2] PROFILING END[Backward]
I1226 08:56:55.627537 92150 solver.cpp:303] [2] Iteration 11, loss = 3.34546
I1226 08:56:55.627589 92150 solver.cpp:329]     Train net output #0: accuracy = 0.292969
I1226 08:56:55.627640 92150 solver.cpp:329]     Train net output #1: loss = 3.34546 (* 1 = 3.34546 loss)
I1226 08:56:55.627689 92150 MultiSolver.cpp:76] [2] PROFILING BEGIN[Forward]
I1226 08:56:55.701696 91127 MultiSolver.cpp:93] [6] PROFILING END[Forward]
I1226 08:56:55.701791 91127 MultiSolver.cpp:95] [6] PROFILING BEGIN[Backward]
I1226 08:56:56.222087 119888 MultiSolver.cpp:93] [4] PROFILING END[Forward]
I1226 08:56:56.222167 119888 MultiSolver.cpp:95] [4] PROFILING BEGIN[Backward]
I1226 08:56:52.990805 90165 MultiSolver.cpp:93] [1] PROFILING END[Forward]
I1226 08:56:52.990883 90165 MultiSolver.cpp:95] [1] PROFILING BEGIN[Backward]
I1226 08:56:56.828431 91127 MultiSolver.cpp:109] [6] PROFILING END[Backward]
I1226 08:56:56.828508 91127 solver.cpp:303] [6] Iteration 13, loss = 3.059
I1226 08:56:56.828594 91127 solver.cpp:329]     Train net output #0: accuracy = 0.316406
I1226 08:56:56.828675 91127 solver.cpp:329]     Train net output #1: loss = 3.059 (* 1 = 3.059 loss)
I1226 08:56:56.828737 91127 MultiSolver.cpp:76] [6] PROFILING BEGIN[Forward]
I1226 08:56:56.843628 89709 MultiSolver.cpp:93] [7] PROFILING END[Forward]
I1226 08:56:56.843709 89709 MultiSolver.cpp:95] [7] PROFILING BEGIN[Backward]
I1226 08:56:57.051585 92150 MultiSolver.cpp:93] [2] PROFILING END[Forward]
I1226 08:56:57.051662 92150 MultiSolver.cpp:95] [2] PROFILING BEGIN[Backward]
I1226 08:56:57.089825 86102 MultiSolver.cpp:93] [3] PROFILING END[Forward]
I1226 08:56:57.089941 86102 MultiSolver.cpp:95] [3] PROFILING BEGIN[Backward]
I1226 08:56:57.265820 91127 MultiSolver.cpp:93] [6] PROFILING END[Forward]
I1226 08:56:57.265913 91127 MultiSolver.cpp:95] [6] PROFILING BEGIN[Backward]
I1226 08:56:57.512939 119888 MultiSolver.cpp:109] [4] PROFILING END[Backward]
I1226 08:56:57.513039 119888 solver.cpp:303] [4] Iteration 12, loss = 3.37438
I1226 08:56:57.513095 119888 solver.cpp:329]     Train net output #0: accuracy = 0.261719
I1226 08:56:57.513146 119888 solver.cpp:329]     Train net output #1: loss = 3.37438 (* 1 = 3.37438 loss)
I1226 08:56:57.513505 119888 MultiSolver.cpp:76] [4] PROFILING BEGIN[Forward]
I1226 08:56:54.425364 90165 MultiSolver.cpp:109] [1] PROFILING END[Backward]
I1226 08:56:54.425432 90165 solver.cpp:303] [1] Iteration 12, loss = 3.26665
I1226 08:56:54.425487 90165 solver.cpp:329]     Train net output #0: accuracy = 0.339844
I1226 08:56:54.425537 90165 solver.cpp:329]     Train net output #1: loss = 3.26665 (* 1 = 3.26665 loss)
I1226 08:56:54.425601 90165 MultiSolver.cpp:76] [1] PROFILING BEGIN[Forward]
I1226 08:56:58.091590 89709 MultiSolver.cpp:109] [7] PROFILING END[Backward]
I1226 08:56:58.092079 89709 solver.cpp:303] [7] Iteration 12, loss = 3.16185
I1226 08:56:58.092191 89709 solver.cpp:329]     Train net output #0: accuracy = 0.28125
I1226 08:56:58.092546 89709 solver.cpp:329]     Train net output #1: loss = 3.16185 (* 1 = 3.16185 loss)
I1226 08:56:58.092869 89709 MultiSolver.cpp:76] [7] PROFILING BEGIN[Forward]
I1226 08:56:58.351254 86102 MultiSolver.cpp:109] [3] PROFILING END[Backward]
I1226 08:56:58.351339 86102 solver.cpp:303] [3] Iteration 12, loss = 3.21308
I1226 08:56:58.351394 86102 solver.cpp:329]     Train net output #0: accuracy = 0.351562
I1226 08:56:58.351445 86102 solver.cpp:329]     Train net output #1: loss = 3.21308 (* 1 = 3.21308 loss)
I1226 08:56:58.351513 86102 MultiSolver.cpp:76] [3] PROFILING BEGIN[Forward]
I1226 08:56:58.372783 92150 MultiSolver.cpp:109] [2] PROFILING END[Backward]
I1226 08:56:58.372871 92150 solver.cpp:303] [2] Iteration 12, loss = 3.20355
I1226 08:56:58.372925 92150 solver.cpp:329]     Train net output #0: accuracy = 0.328125
I1226 08:56:58.372977 92150 solver.cpp:329]     Train net output #1: loss = 3.20355 (* 1 = 3.20355 loss)
I1226 08:56:58.373155 92150 MultiSolver.cpp:76] [2] PROFILING BEGIN[Forward]
I1226 08:56:58.405252 91127 MultiSolver.cpp:109] [6] PROFILING END[Backward]
I1226 08:56:58.405344 91127 solver.cpp:303] [6] Iteration 14, loss = 3.12929
I1226 08:56:58.405442 91127 solver.cpp:329]     Train net output #0: accuracy = 0.304688
I1226 08:56:58.405524 91127 solver.cpp:329]     Train net output #1: loss = 3.12929 (* 1 = 3.12929 loss)
I1226 08:56:58.405772 91127 MultiSolver.cpp:76] [6] PROFILING BEGIN[Forward]
I1226 08:56:58.830837 91127 MultiSolver.cpp:93] [6] PROFILING END[Forward]
I1226 08:56:58.830931 91127 MultiSolver.cpp:95] [6] PROFILING BEGIN[Backward]
I1226 08:56:58.933660 119888 MultiSolver.cpp:93] [4] PROFILING END[Forward]
I1226 08:56:58.933739 119888 MultiSolver.cpp:95] [4] PROFILING BEGIN[Backward]
I1226 08:56:55.733779 90165 MultiSolver.cpp:93] [1] PROFILING END[Forward]
I1226 08:56:55.733853 90165 MultiSolver.cpp:95] [1] PROFILING BEGIN[Backward]
I1226 08:56:59.668812 95005 MultiSolver.cpp:109] [5] PROFILING END[Backward]
I1226 08:56:59.668891 95005 solver.cpp:303] [5] Iteration 2, loss = 3.02838
I1226 08:56:59.668954 95005 solver.cpp:329]     Train net output #0: accuracy = 0.34375
I1226 08:56:59.669021 95005 solver.cpp:329]     Train net output #1: loss = 3.02838 (* 1 = 3.02838 loss)
I1226 08:56:59.669267 95005 MultiSolver.cpp:76] [5] PROFILING BEGIN[Forward]
I1226 08:56:59.743517 89709 MultiSolver.cpp:93] [7] PROFILING END[Forward]
I1226 08:56:59.743594 89709 MultiSolver.cpp:95] [7] PROFILING BEGIN[Backward]
I1226 08:56:59.809829 94304 MultiSolver.cpp:109] [0] PROFILING END[Backward]
I1226 08:56:59.809906 94304 solver.cpp:303] [0] Iteration 2, loss = 3.15281
I1226 08:56:59.809963 94304 solver.cpp:329]     Train net output #0: accuracy = 0.308594
I1226 08:56:59.810016 94304 solver.cpp:329]     Train net output #1: loss = 3.15281 (* 1 = 3.15281 loss)
I1226 08:56:59.810063 94304 MultiSolver.cpp:76] [0] PROFILING BEGIN[Forward]
I1226 08:56:59.921054 92150 MultiSolver.cpp:93] [2] PROFILING END[Forward]
I1226 08:56:59.921130 92150 MultiSolver.cpp:95] [2] PROFILING BEGIN[Backward]
I1226 08:57:00.061890 86102 MultiSolver.cpp:93] [3] PROFILING END[Forward]
I1226 08:57:00.061985 86102 MultiSolver.cpp:95] [3] PROFILING BEGIN[Backward]
I1226 08:57:00.204462 91127 MultiSolver.cpp:109] [6] PROFILING END[Backward]
I1226 08:57:00.204540 91127 solver.cpp:303] [6] Iteration 15, loss = 2.95679
I1226 08:57:00.204607 91127 solver.cpp:329]     Train net output #0: accuracy = 0.367188
I1226 08:57:00.204674 91127 solver.cpp:329]     Train net output #1: loss = 2.95679 (* 1 = 2.95679 loss)
I1226 08:57:00.205023 91127 MultiSolver.cpp:76] [6] PROFILING BEGIN[Forward]
I1226 08:57:00.447935 119888 MultiSolver.cpp:109] [4] PROFILING END[Backward]
I1226 08:57:00.448065 119888 solver.cpp:303] [4] Iteration 13, loss = 3.53891
I1226 08:57:00.448127 119888 solver.cpp:329]     Train net output #0: accuracy = 0.269531
I1226 08:57:00.448182 119888 solver.cpp:329]     Train net output #1: loss = 3.53891 (* 1 = 3.53891 loss)
I1226 08:57:00.448369 119888 MultiSolver.cpp:76] [4] PROFILING BEGIN[Forward]
I1226 08:57:00.644793 91127 MultiSolver.cpp:93] [6] PROFILING END[Forward]
I1226 08:57:00.644888 91127 MultiSolver.cpp:95] [6] PROFILING BEGIN[Backward]
I1226 08:56:57.419734 90165 MultiSolver.cpp:109] [1] PROFILING END[Backward]
I1226 08:56:57.419806 90165 solver.cpp:303] [1] Iteration 13, loss = 3.32773
I1226 08:56:57.419865 90165 solver.cpp:329]     Train net output #0: accuracy = 0.316406
I1226 08:56:57.419919 90165 solver.cpp:329]     Train net output #1: loss = 3.32773 (* 1 = 3.32773 loss)
I1226 08:56:57.419970 90165 MultiSolver.cpp:76] [1] PROFILING BEGIN[Forward]
I1226 08:57:01.165189 89709 MultiSolver.cpp:109] [7] PROFILING END[Backward]
I1226 08:57:01.165307 89709 solver.cpp:303] [7] Iteration 13, loss = 3.04575
I1226 08:57:01.165361 89709 solver.cpp:329]     Train net output #0: accuracy = 0.308594
I1226 08:57:01.165436 89709 solver.cpp:329]     Train net output #1: loss = 3.04575 (* 1 = 3.04575 loss)
I1226 08:57:01.165488 89709 MultiSolver.cpp:76] [7] PROFILING BEGIN[Forward]
I1226 08:57:01.343070 86102 MultiSolver.cpp:109] [3] PROFILING END[Backward]
I1226 08:57:01.343143 86102 solver.cpp:303] [3] Iteration 13, loss = 3.21948
I1226 08:57:01.343199 86102 solver.cpp:329]     Train net output #0: accuracy = 0.261719
I1226 08:57:01.343753 86102 solver.cpp:329]     Train net output #1: loss = 3.21948 (* 1 = 3.21948 loss)
I1226 08:57:01.344089 86102 MultiSolver.cpp:76] [3] PROFILING BEGIN[Forward]
I1226 08:57:01.350546 92150 MultiSolver.cpp:109] [2] PROFILING END[Backward]
I1226 08:57:01.350618 92150 solver.cpp:303] [2] Iteration 13, loss = 3.21316
I1226 08:57:01.350672 92150 solver.cpp:329]     Train net output #0: accuracy = 0.332031
I1226 08:57:01.350723 92150 solver.cpp:329]     Train net output #1: loss = 3.21316 (* 1 = 3.21316 loss)
I1226 08:57:01.350765 92150 MultiSolver.cpp:76] [2] PROFILING BEGIN[Forward]
I1226 08:57:01.668534 91127 MultiSolver.cpp:109] [6] PROFILING END[Backward]
I1226 08:57:01.668613 91127 solver.cpp:303] [6] Iteration 16, loss = 3.17937
I1226 08:57:01.668676 91127 solver.cpp:329]     Train net output #0: accuracy = 0.3125
I1226 08:57:01.668742 91127 solver.cpp:329]     Train net output #1: loss = 3.17937 (* 1 = 3.17937 loss)
I1226 08:57:01.668804 91127 MultiSolver.cpp:76] [6] PROFILING BEGIN[Forward]
I1226 08:57:01.904451 119888 MultiSolver.cpp:93] [4] PROFILING END[Forward]
I1226 08:57:01.906225 119888 MultiSolver.cpp:95] [4] PROFILING BEGIN[Backward]
I1226 08:56:58.667615 90165 MultiSolver.cpp:93] [1] PROFILING END[Forward]
I1226 08:56:58.671566 90165 MultiSolver.cpp:95] [1] PROFILING BEGIN[Backward]
I1226 08:57:02.076597 91127 MultiSolver.cpp:93] [6] PROFILING END[Forward]
I1226 08:57:02.079802 91127 MultiSolver.cpp:95] [6] PROFILING BEGIN[Backward]
I1226 08:57:02.705070 89709 MultiSolver.cpp:93] [7] PROFILING END[Forward]
I1226 08:57:02.710134 89709 MultiSolver.cpp:95] [7] PROFILING BEGIN[Backward]
I1226 08:57:02.880628 92150 MultiSolver.cpp:93] [2] PROFILING END[Forward]
I1226 08:57:02.882094 92150 MultiSolver.cpp:95] [2] PROFILING BEGIN[Backward]
I1226 08:57:02.906143 86102 MultiSolver.cpp:93] [3] PROFILING END[Forward]
I1226 08:57:02.907735 86102 MultiSolver.cpp:95] [3] PROFILING BEGIN[Backward]
I1226 08:57:03.201058 119888 MultiSolver.cpp:109] [4] PROFILING END[Backward]
I1226 08:57:03.201318 119888 solver.cpp:303] [4] Iteration 14, loss = 3.32123
I1226 08:57:03.201388 119888 solver.cpp:329]     Train net output #0: accuracy = 0.277344
I1226 08:57:03.201441 119888 solver.cpp:329]     Train net output #1: loss = 3.32123 (* 1 = 3.32123 loss)
I1226 08:57:03.201491 119888 MultiSolver.cpp:76] [4] PROFILING BEGIN[Forward]
I1226 08:57:00.149271 90165 MultiSolver.cpp:109] [1] PROFILING END[Backward]
I1226 08:57:00.149343 90165 solver.cpp:303] [1] Iteration 14, loss = 3.33831
I1226 08:57:00.149399 90165 solver.cpp:329]     Train net output #0: accuracy = 0.304688
I1226 08:57:00.149451 90165 solver.cpp:329]     Train net output #1: loss = 3.33831 (* 1 = 3.33831 loss)
I1226 08:57:00.149852 90165 MultiSolver.cpp:76] [1] PROFILING BEGIN[Forward]
I1226 08:57:03.460247 91127 MultiSolver.cpp:109] [6] PROFILING END[Backward]
I1226 08:57:03.460331 91127 solver.cpp:303] [6] Iteration 17, loss = 3.04561
I1226 08:57:03.460419 91127 solver.cpp:329]     Train net output #0: accuracy = 0.324219
I1226 08:57:03.460484 91127 solver.cpp:329]     Train net output #1: loss = 3.04561 (* 1 = 3.04561 loss)
I1226 08:57:03.460548 91127 MultiSolver.cpp:76] [6] PROFILING BEGIN[Forward]
I1226 08:57:03.918612 91127 MultiSolver.cpp:93] [6] PROFILING END[Forward]
I1226 08:57:03.918701 91127 MultiSolver.cpp:95] [6] PROFILING BEGIN[Backward]
I1226 08:57:03.939982 92150 MultiSolver.cpp:109] [2] PROFILING END[Backward]
I1226 08:57:03.940085 92150 solver.cpp:303] [2] Iteration 14, loss = 3.33026
I1226 08:57:03.940162 92150 solver.cpp:329]     Train net output #0: accuracy = 0.292969
I1226 08:57:03.940222 92150 solver.cpp:329]     Train net output #1: loss = 3.33026 (* 1 = 3.33026 loss)
I1226 08:57:03.950474 92150 MultiSolver.cpp:76] [2] PROFILING BEGIN[Forward]
I1226 08:57:03.956293 89709 MultiSolver.cpp:109] [7] PROFILING END[Backward]
I1226 08:57:03.956365 89709 solver.cpp:303] [7] Iteration 14, loss = 3.48977
I1226 08:57:03.956444 89709 solver.cpp:329]     Train net output #0: accuracy = 0.246094
I1226 08:57:03.956495 89709 solver.cpp:329]     Train net output #1: loss = 3.48977 (* 1 = 3.48977 loss)
I1226 08:57:03.956539 89709 MultiSolver.cpp:76] [7] PROFILING BEGIN[Forward]
I1226 08:57:03.988828 86102 MultiSolver.cpp:109] [3] PROFILING END[Backward]
I1226 08:57:03.988900 86102 solver.cpp:303] [3] Iteration 14, loss = 3.04546
I1226 08:57:03.988955 86102 solver.cpp:329]     Train net output #0: accuracy = 0.34375
I1226 08:57:03.989004 86102 solver.cpp:329]     Train net output #1: loss = 3.04546 (* 1 = 3.04546 loss)
I1226 08:57:03.989048 86102 MultiSolver.cpp:76] [3] PROFILING BEGIN[Forward]
I1226 08:57:04.756407 95005 MultiSolver.cpp:93] [5] PROFILING END[Forward]
I1226 08:57:04.756506 95005 MultiSolver.cpp:95] [5] PROFILING BEGIN[Backward]
I1226 08:57:04.800993 119888 MultiSolver.cpp:93] [4] PROFILING END[Forward]
I1226 08:57:04.801090 119888 MultiSolver.cpp:95] [4] PROFILING BEGIN[Backward]
I1226 08:57:01.557906 90165 MultiSolver.cpp:93] [1] PROFILING END[Forward]
I1226 08:57:01.557998 90165 MultiSolver.cpp:95] [1] PROFILING BEGIN[Backward]
I1226 08:57:04.981256 94304 MultiSolver.cpp:93] [0] PROFILING END[Forward]
I1226 08:57:04.981340 94304 MultiSolver.cpp:95] [0] PROFILING BEGIN[Backward]
I1226 08:57:05.029567 91127 MultiSolver.cpp:109] [6] PROFILING END[Backward]
I1226 08:57:05.029650 91127 solver.cpp:303] [6] Iteration 18, loss = 3.02358
I1226 08:57:05.029716 91127 solver.cpp:329]     Train net output #0: accuracy = 0.347656
I1226 08:57:05.029783 91127 solver.cpp:329]     Train net output #1: loss = 3.02358 (* 1 = 3.02358 loss)
I1226 08:57:05.029840 91127 MultiSolver.cpp:76] [6] PROFILING BEGIN[Forward]
I1226 08:57:05.447422 91127 MultiSolver.cpp:93] [6] PROFILING END[Forward]
I1226 08:57:05.447512 91127 MultiSolver.cpp:95] [6] PROFILING BEGIN[Backward]
I1226 08:57:05.609911 89709 MultiSolver.cpp:93] [7] PROFILING END[Forward]
I1226 08:57:05.609997 89709 MultiSolver.cpp:95] [7] PROFILING BEGIN[Backward]
I1226 08:57:05.798732 92150 MultiSolver.cpp:93] [2] PROFILING END[Forward]
I1226 08:57:05.798806 92150 MultiSolver.cpp:95] [2] PROFILING BEGIN[Backward]
I1226 08:57:05.821333 86102 MultiSolver.cpp:93] [3] PROFILING END[Forward]
I1226 08:57:05.821413 86102 MultiSolver.cpp:95] [3] PROFILING BEGIN[Backward]
I1226 08:57:02.869138 90165 MultiSolver.cpp:109] [1] PROFILING END[Backward]
I1226 08:57:02.869225 90165 solver.cpp:303] [1] Iteration 15, loss = 3.38264
I1226 08:57:02.869280 90165 solver.cpp:329]     Train net output #0: accuracy = 0.3125
I1226 08:57:02.869350 90165 solver.cpp:329]     Train net output #1: loss = 3.38264 (* 1 = 3.38264 loss)
I1226 08:57:02.869405 90165 MultiSolver.cpp:76] [1] PROFILING BEGIN[Forward]
I1226 08:57:06.250928 119888 MultiSolver.cpp:109] [4] PROFILING END[Backward]
I1226 08:57:06.251014 119888 solver.cpp:303] [4] Iteration 15, loss = 3.21401
I1226 08:57:06.251082 119888 solver.cpp:329]     Train net output #0: accuracy = 0.316406
I1226 08:57:06.251446 119888 solver.cpp:329]     Train net output #1: loss = 3.21401 (* 1 = 3.21401 loss)
I1226 08:57:06.251855 119888 MultiSolver.cpp:76] [4] PROFILING BEGIN[Forward]
I1226 08:57:06.707907 91127 MultiSolver.cpp:109] [6] PROFILING END[Backward]
I1226 08:57:06.707981 91127 solver.cpp:303] [6] Iteration 19, loss = 3.30806
I1226 08:57:06.708045 91127 solver.cpp:329]     Train net output #0: accuracy = 0.3125
I1226 08:57:06.708111 91127 solver.cpp:329]     Train net output #1: loss = 3.30806 (* 1 = 3.30806 loss)
I1226 08:57:06.708168 91127 MultiSolver.cpp:76] [6] PROFILING BEGIN[Forward]
I1226 08:57:06.988569 89709 MultiSolver.cpp:109] [7] PROFILING END[Backward]
I1226 08:57:06.988641 89709 solver.cpp:303] [7] Iteration 15, loss = 3.06148
I1226 08:57:06.988695 89709 solver.cpp:329]     Train net output #0: accuracy = 0.339844
I1226 08:57:06.988745 89709 solver.cpp:329]     Train net output #1: loss = 3.06148 (* 1 = 3.06148 loss)
I1226 08:57:06.988790 89709 MultiSolver.cpp:76] [7] PROFILING BEGIN[Forward]
I1226 08:57:07.026557 92150 MultiSolver.cpp:109] [2] PROFILING END[Backward]
I1226 08:57:07.026628 92150 solver.cpp:303] [2] Iteration 15, loss = 3.12538
I1226 08:57:07.026681 92150 solver.cpp:329]     Train net output #0: accuracy = 0.324219
I1226 08:57:07.026732 92150 solver.cpp:329]     Train net output #1: loss = 3.12538 (* 1 = 3.12538 loss)
I1226 08:57:07.026777 92150 MultiSolver.cpp:76] [2] PROFILING BEGIN[Forward]
I1226 08:57:07.082798 86102 MultiSolver.cpp:109] [3] PROFILING END[Backward]
I1226 08:57:07.082868 86102 solver.cpp:303] [3] Iteration 15, loss = 3.24349
I1226 08:57:07.082921 86102 solver.cpp:329]     Train net output #0: accuracy = 0.289062
I1226 08:57:07.082972 86102 solver.cpp:329]     Train net output #1: loss = 3.24349 (* 1 = 3.24349 loss)
I1226 08:57:07.083015 86102 MultiSolver.cpp:76] [3] PROFILING BEGIN[Forward]
I1226 08:57:07.123765 91127 MultiSolver.cpp:93] [6] PROFILING END[Forward]
I1226 08:57:07.123857 91127 MultiSolver.cpp:95] [6] PROFILING BEGIN[Backward]
I1226 08:57:07.756114 119888 MultiSolver.cpp:93] [4] PROFILING END[Forward]
I1226 08:57:07.756191 119888 MultiSolver.cpp:95] [4] PROFILING BEGIN[Backward]
I1226 08:57:04.628823 90165 MultiSolver.cpp:93] [1] PROFILING END[Forward]
I1226 08:57:04.628901 90165 MultiSolver.cpp:95] [1] PROFILING BEGIN[Backward]
I1226 08:57:08.210427 91127 MultiSolver.cpp:109] [6] PROFILING END[Backward]
I1226 08:57:08.210500 91127 solver.cpp:303] [6] Iteration 20, loss = 3.2659
I1226 08:57:08.210566 91127 solver.cpp:329]     Train net output #0: accuracy = 0.320312
I1226 08:57:08.210638 91127 solver.cpp:329]     Train net output #1: loss = 3.2659 (* 1 = 3.2659 loss)
I1226 08:57:08.210703 91127 MultiSolver.cpp:76] [6] PROFILING BEGIN[Forward]
I1226 08:57:08.495890 89709 MultiSolver.cpp:93] [7] PROFILING END[Forward]
I1226 08:57:08.495970 89709 MultiSolver.cpp:95] [7] PROFILING BEGIN[Backward]
I1226 08:57:08.627807 91127 MultiSolver.cpp:93] [6] PROFILING END[Forward]
I1226 08:57:08.627903 91127 MultiSolver.cpp:95] [6] PROFILING BEGIN[Backward]
I1226 08:57:08.635356 92150 MultiSolver.cpp:93] [2] PROFILING END[Forward]
I1226 08:57:08.635455 92150 MultiSolver.cpp:95] [2] PROFILING BEGIN[Backward]
I1226 08:57:08.716773 86102 MultiSolver.cpp:93] [3] PROFILING END[Forward]
I1226 08:57:08.716847 86102 MultiSolver.cpp:95] [3] PROFILING BEGIN[Backward]
I1226 08:57:09.037451 119888 MultiSolver.cpp:109] [4] PROFILING END[Backward]
I1226 08:57:09.037554 119888 solver.cpp:303] [4] Iteration 16, loss = 3.28662
I1226 08:57:09.037701 119888 solver.cpp:329]     Train net output #0: accuracy = 0.285156
I1226 08:57:09.038146 119888 solver.cpp:329]     Train net output #1: loss = 3.28662 (* 1 = 3.28662 loss)
I1226 08:57:09.038213 119888 MultiSolver.cpp:76] [4] PROFILING BEGIN[Forward]
I1226 08:57:06.220199 90165 MultiSolver.cpp:109] [1] PROFILING END[Backward]
I1226 08:57:06.220285 90165 solver.cpp:303] [1] Iteration 16, loss = 3.35366
I1226 08:57:06.228708 90165 solver.cpp:329]     Train net output #0: accuracy = 0.300781
I1226 08:57:06.228790 90165 solver.cpp:329]     Train net output #1: loss = 3.35366 (* 1 = 3.35366 loss)
I1226 08:57:06.228848 90165 MultiSolver.cpp:76] [1] PROFILING BEGIN[Forward]
I1226 08:57:09.793869 89709 MultiSolver.cpp:109] [7] PROFILING END[Backward]
I1226 08:57:09.793962 89709 solver.cpp:303] [7] Iteration 16, loss = 3.207
I1226 08:57:09.794024 89709 solver.cpp:329]     Train net output #0: accuracy = 0.324219
I1226 08:57:09.794353 89709 solver.cpp:329]     Train net output #1: loss = 3.207 (* 1 = 3.207 loss)
I1226 08:57:09.794502 89709 MultiSolver.cpp:76] [7] PROFILING BEGIN[Forward]
I1226 08:57:09.896826 91127 MultiSolver.cpp:109] [6] PROFILING END[Backward]
I1226 08:57:09.896908 91127 solver.cpp:303] [6] Iteration 21, loss = 3.24692
I1226 08:57:09.896971 91127 solver.cpp:329]     Train net output #0: accuracy = 0.3125
I1226 08:57:09.897037 91127 solver.cpp:329]     Train net output #1: loss = 3.24692 (* 1 = 3.24692 loss)
I1226 08:57:09.897094 91127 MultiSolver.cpp:76] [6] PROFILING BEGIN[Forward]
I1226 08:57:09.920346 92150 MultiSolver.cpp:109] [2] PROFILING END[Backward]
I1226 08:57:09.920446 92150 solver.cpp:303] [2] Iteration 16, loss = 3.1493
I1226 08:57:09.920500 92150 solver.cpp:329]     Train net output #0: accuracy = 0.347656
I1226 08:57:09.920552 92150 solver.cpp:329]     Train net output #1: loss = 3.1493 (* 1 = 3.1493 loss)
I1226 08:57:09.920595 92150 MultiSolver.cpp:76] [2] PROFILING BEGIN[Forward]
I1226 08:57:09.987854 86102 MultiSolver.cpp:109] [3] PROFILING END[Backward]
I1226 08:57:09.987933 86102 solver.cpp:303] [3] Iteration 16, loss = 3.18547
I1226 08:57:09.988389 86102 solver.cpp:329]     Train net output #0: accuracy = 0.320312
I1226 08:57:09.988450 86102 solver.cpp:329]     Train net output #1: loss = 3.18547 (* 1 = 3.18547 loss)
I1226 08:57:09.988530 86102 MultiSolver.cpp:76] [3] PROFILING BEGIN[Forward]
I1226 08:57:10.329445 91127 MultiSolver.cpp:93] [6] PROFILING END[Forward]
I1226 08:57:10.329540 91127 MultiSolver.cpp:95] [6] PROFILING BEGIN[Backward]
I1226 08:57:10.752501 119888 MultiSolver.cpp:93] [4] PROFILING END[Forward]
I1226 08:57:10.752578 119888 MultiSolver.cpp:95] [4] PROFILING BEGIN[Backward]
I1226 08:57:07.584465 90165 MultiSolver.cpp:93] [1] PROFILING END[Forward]
I1226 08:57:07.584545 90165 MultiSolver.cpp:95] [1] PROFILING BEGIN[Backward]
I1226 08:57:11.418460 91127 MultiSolver.cpp:109] [6] PROFILING END[Backward]
I1226 08:57:11.418537 91127 solver.cpp:303] [6] Iteration 22, loss = 3.21135
I1226 08:57:11.418601 91127 solver.cpp:329]     Train net output #0: accuracy = 0.335938
I1226 08:57:11.418661 91127 solver.cpp:329]     Train net output #1: loss = 3.21135 (* 1 = 3.21135 loss)
I1226 08:57:11.418720 91127 MultiSolver.cpp:76] [6] PROFILING BEGIN[Forward]
I1226 08:57:11.449503 89709 MultiSolver.cpp:93] [7] PROFILING END[Forward]
I1226 08:57:11.449578 89709 MultiSolver.cpp:95] [7] PROFILING BEGIN[Backward]
I1226 08:57:11.538467 92150 MultiSolver.cpp:93] [2] PROFILING END[Forward]
I1226 08:57:11.538549 92150 MultiSolver.cpp:95] [2] PROFILING BEGIN[Backward]
I1226 08:57:11.725680 86102 MultiSolver.cpp:93] [3] PROFILING END[Forward]
I1226 08:57:11.725787 86102 MultiSolver.cpp:95] [3] PROFILING BEGIN[Backward]
I1226 08:57:11.828608 91127 MultiSolver.cpp:93] [6] PROFILING END[Forward]
I1226 08:57:11.828701 91127 MultiSolver.cpp:95] [6] PROFILING BEGIN[Backward]
I1226 08:57:11.925319 119888 MultiSolver.cpp:109] [4] PROFILING END[Backward]
I1226 08:57:11.925391 119888 solver.cpp:303] [4] Iteration 17, loss = 3.2297
I1226 08:57:11.925446 119888 solver.cpp:329]     Train net output #0: accuracy = 0.292969
I1226 08:57:11.925518 119888 solver.cpp:329]     Train net output #1: loss = 3.2297 (* 1 = 3.2297 loss)
I1226 08:57:11.925817 119888 MultiSolver.cpp:76] [4] PROFILING BEGIN[Forward]
I1226 08:57:08.840759 90165 MultiSolver.cpp:109] [1] PROFILING END[Backward]
I1226 08:57:08.840829 90165 solver.cpp:303] [1] Iteration 17, loss = 3.04389
I1226 08:57:08.840886 90165 solver.cpp:329]     Train net output #0: accuracy = 0.390625
I1226 08:57:08.840937 90165 solver.cpp:329]     Train net output #1: loss = 3.04389 (* 1 = 3.04389 loss)
I1226 08:57:08.840988 90165 MultiSolver.cpp:76] [1] PROFILING BEGIN[Forward]
I1226 08:57:12.441965 95005 MultiSolver.cpp:109] [5] PROFILING END[Backward]
I1226 08:57:12.442049 95005 solver.cpp:303] [5] Iteration 3, loss = 3.1014
I1226 08:57:12.442114 95005 solver.cpp:329]     Train net output #0: accuracy = 0.363281
I1226 08:57:12.442179 95005 solver.cpp:329]     Train net output #1: loss = 3.1014 (* 1 = 3.1014 loss)
I1226 08:57:12.442239 95005 MultiSolver.cpp:76] [5] PROFILING BEGIN[Forward]
I1226 08:57:12.587452 94304 MultiSolver.cpp:109] [0] PROFILING END[Backward]
I1226 08:57:12.587530 94304 solver.cpp:303] [0] Iteration 3, loss = 3.47324
I1226 08:57:12.587584 94304 solver.cpp:329]     Train net output #0: accuracy = 0.296875
I1226 08:57:12.587635 94304 solver.cpp:329]     Train net output #1: loss = 3.47324 (* 1 = 3.47324 loss)
I1226 08:57:12.587678 94304 MultiSolver.cpp:76] [0] PROFILING BEGIN[Forward]
I1226 08:57:12.724495 89709 MultiSolver.cpp:109] [7] PROFILING END[Backward]
I1226 08:57:12.724568 89709 solver.cpp:303] [7] Iteration 17, loss = 3.11684
I1226 08:57:12.724622 89709 solver.cpp:329]     Train net output #0: accuracy = 0.335938
I1226 08:57:12.724680 89709 solver.cpp:329]     Train net output #1: loss = 3.11684 (* 1 = 3.11684 loss)
I1226 08:57:12.724733 89709 MultiSolver.cpp:76] [7] PROFILING BEGIN[Forward]
I1226 08:57:12.880553 92150 MultiSolver.cpp:109] [2] PROFILING END[Backward]
I1226 08:57:12.880623 92150 solver.cpp:303] [2] Iteration 17, loss = 2.95978
I1226 08:57:12.880678 92150 solver.cpp:329]     Train net output #0: accuracy = 0.386719
I1226 08:57:12.880728 92150 solver.cpp:329]     Train net output #1: loss = 2.95978 (* 1 = 2.95978 loss)
I1226 08:57:12.880771 92150 MultiSolver.cpp:76] [2] PROFILING BEGIN[Forward]
I1226 08:57:12.970742 86102 MultiSolver.cpp:109] [3] PROFILING END[Backward]
I1226 08:57:12.970814 86102 solver.cpp:303] [3] Iteration 17, loss = 3.28991
I1226 08:57:12.970868 86102 solver.cpp:329]     Train net output #0: accuracy = 0.296875
I1226 08:57:12.970918 86102 solver.cpp:329]     Train net output #1: loss = 3.28991 (* 1 = 3.28991 loss)
I1226 08:57:12.973587 86102 MultiSolver.cpp:76] [3] PROFILING BEGIN[Forward]
I1226 08:57:13.065291 91127 MultiSolver.cpp:109] [6] PROFILING END[Backward]
I1226 08:57:13.065388 91127 solver.cpp:303] [6] Iteration 23, loss = 3.12664
I1226 08:57:13.065454 91127 solver.cpp:329]     Train net output #0: accuracy = 0.320312
I1226 08:57:13.065517 91127 solver.cpp:329]     Train net output #1: loss = 3.12664 (* 1 = 3.12664 loss)
I1226 08:57:13.065575 91127 MultiSolver.cpp:76] [6] PROFILING BEGIN[Forward]
I1226 08:57:13.495618 91127 MultiSolver.cpp:93] [6] PROFILING END[Forward]
I1226 08:57:13.495710 91127 MultiSolver.cpp:95] [6] PROFILING BEGIN[Backward]
I1226 08:57:10.381810 90165 MultiSolver.cpp:93] [1] PROFILING END[Forward]
I1226 08:57:10.381897 90165 MultiSolver.cpp:95] [1] PROFILING BEGIN[Backward]
I1226 08:57:13.702455 119888 MultiSolver.cpp:93] [4] PROFILING END[Forward]
I1226 08:57:13.702533 119888 MultiSolver.cpp:95] [4] PROFILING BEGIN[Backward]
I1226 08:57:14.233340 89709 MultiSolver.cpp:93] [7] PROFILING END[Forward]
I1226 08:57:14.233620 89709 MultiSolver.cpp:95] [7] PROFILING BEGIN[Backward]
I1226 08:57:14.506849 92150 MultiSolver.cpp:93] [2] PROFILING END[Forward]
I1226 08:57:14.506928 92150 MultiSolver.cpp:95] [2] PROFILING BEGIN[Backward]
I1226 08:57:14.750638 86102 MultiSolver.cpp:93] [3] PROFILING END[Forward]
I1226 08:57:14.750711 86102 MultiSolver.cpp:95] [3] PROFILING BEGIN[Backward]
I1226 08:57:14.750021 91127 MultiSolver.cpp:109] [6] PROFILING END[Backward]
I1226 08:57:14.750097 91127 solver.cpp:303] [6] Iteration 24, loss = 3.16991
I1226 08:57:14.750159 91127 solver.cpp:329]     Train net output #0: accuracy = 0.347656
I1226 08:57:14.750221 91127 solver.cpp:329]     Train net output #1: loss = 3.16991 (* 1 = 3.16991 loss)
I1226 08:57:14.750277 91127 MultiSolver.cpp:76] [6] PROFILING BEGIN[Forward]
I1226 08:57:11.741646 90165 MultiSolver.cpp:109] [1] PROFILING END[Backward]
I1226 08:57:11.741713 90165 solver.cpp:303] [1] Iteration 18, loss = 2.99405
I1226 08:57:11.741766 90165 solver.cpp:329]     Train net output #0: accuracy = 0.351562
I1226 08:57:11.741816 90165 solver.cpp:329]     Train net output #1: loss = 2.99405 (* 1 = 2.99405 loss)
I1226 08:57:11.741859 90165 MultiSolver.cpp:76] [1] PROFILING BEGIN[Forward]
I1226 08:57:15.001659 119888 MultiSolver.cpp:109] [4] PROFILING END[Backward]
I1226 08:57:15.001767 119888 solver.cpp:303] [4] Iteration 18, loss = 3.19732
I1226 08:57:15.002202 119888 solver.cpp:329]     Train net output #0: accuracy = 0.335938
I1226 08:57:15.002316 119888 solver.cpp:329]     Train net output #1: loss = 3.19732 (* 1 = 3.19732 loss)
I1226 08:57:15.002374 119888 MultiSolver.cpp:76] [4] PROFILING BEGIN[Forward]
I1226 08:57:15.192649 91127 MultiSolver.cpp:93] [6] PROFILING END[Forward]
I1226 08:57:15.192744 91127 MultiSolver.cpp:95] [6] PROFILING BEGIN[Backward]
I1226 08:57:15.459591 89709 MultiSolver.cpp:109] [7] PROFILING END[Backward]
I1226 08:57:15.461128 89709 solver.cpp:303] [7] Iteration 18, loss = 2.9903
I1226 08:57:15.461205 89709 solver.cpp:329]     Train net output #0: accuracy = 0.304688
I1226 08:57:15.461259 89709 solver.cpp:329]     Train net output #1: loss = 2.9903 (* 1 = 2.9903 loss)
I1226 08:57:15.461310 89709 MultiSolver.cpp:76] [7] PROFILING BEGIN[Forward]
I1226 08:57:15.695284 92150 MultiSolver.cpp:109] [2] PROFILING END[Backward]
I1226 08:57:15.695356 92150 solver.cpp:303] [2] Iteration 18, loss = 3.073
I1226 08:57:15.695435 92150 solver.cpp:329]     Train net output #0: accuracy = 0.335938
I1226 08:57:15.695507 92150 solver.cpp:329]     Train net output #1: loss = 3.073 (* 1 = 3.073 loss)
I1226 08:57:15.695554 92150 MultiSolver.cpp:76] [2] PROFILING BEGIN[Forward]
I1226 08:57:15.880621 86102 MultiSolver.cpp:109] [3] PROFILING END[Backward]
I1226 08:57:15.880688 86102 solver.cpp:303] [3] Iteration 18, loss = 3.20743
I1226 08:57:15.880741 86102 solver.cpp:329]     Train net output #0: accuracy = 0.277344
I1226 08:57:15.880792 86102 solver.cpp:329]     Train net output #1: loss = 3.20743 (* 1 = 3.20743 loss)
I1226 08:57:15.880841 86102 MultiSolver.cpp:76] [3] PROFILING BEGIN[Forward]
I1226 08:57:16.200034 91127 MultiSolver.cpp:109] [6] PROFILING END[Backward]
I1226 08:57:16.200115 91127 solver.cpp:303] [6] Iteration 25, loss = 2.99533
I1226 08:57:16.200178 91127 solver.cpp:329]     Train net output #0: accuracy = 0.328125
I1226 08:57:16.200243 91127 solver.cpp:329]     Train net output #1: loss = 2.99533 (* 1 = 2.99533 loss)
I1226 08:57:16.200299 91127 MultiSolver.cpp:76] [6] PROFILING BEGIN[Forward]
I1226 08:57:12.978409 90165 MultiSolver.cpp:93] [1] PROFILING END[Forward]
I1226 08:57:12.978483 90165 MultiSolver.cpp:95] [1] PROFILING BEGIN[Backward]
I1226 08:57:16.579716 86102 blocking_queue.cpp:87] Waiting for data
I1226 08:57:16.614786 91127 MultiSolver.cpp:93] [6] PROFILING END[Forward]
I1226 08:57:16.614881 91127 MultiSolver.cpp:95] [6] PROFILING BEGIN[Backward]
I1226 08:57:16.732259 119888 MultiSolver.cpp:93] [4] PROFILING END[Forward]
I1226 08:57:16.732342 119888 MultiSolver.cpp:95] [4] PROFILING BEGIN[Backward]
I1226 08:57:17.274206 89709 MultiSolver.cpp:93] [7] PROFILING END[Forward]
I1226 08:57:17.274770 89709 MultiSolver.cpp:95] [7] PROFILING BEGIN[Backward]
I1226 08:57:14.066215 90165 MultiSolver.cpp:109] [1] PROFILING END[Backward]
I1226 08:57:14.066287 90165 solver.cpp:303] [1] Iteration 19, loss = 3.24574
I1226 08:57:14.066341 90165 solver.cpp:329]     Train net output #0: accuracy = 0.308594
I1226 08:57:14.066391 90165 solver.cpp:329]     Train net output #1: loss = 3.24574 (* 1 = 3.24574 loss)
I1226 08:57:14.066435 90165 MultiSolver.cpp:76] [1] PROFILING BEGIN[Forward]
I1226 08:57:17.422205 92150 MultiSolver.cpp:93] [2] PROFILING END[Forward]
I1226 08:57:17.422282 92150 MultiSolver.cpp:95] [2] PROFILING BEGIN[Backward]
I1226 08:57:17.447186 95005 MultiSolver.cpp:93] [5] PROFILING END[Forward]
I1226 08:57:17.448020 95005 MultiSolver.cpp:95] [5] PROFILING BEGIN[Backward]
I1226 08:57:17.578490 86102 MultiSolver.cpp:93] [3] PROFILING END[Forward]
I1226 08:57:17.578591 86102 MultiSolver.cpp:95] [3] PROFILING BEGIN[Backward]
I1226 08:57:17.677248 94304 MultiSolver.cpp:93] [0] PROFILING END[Forward]
I1226 08:57:17.678169 94304 MultiSolver.cpp:95] [0] PROFILING BEGIN[Backward]
I1226 08:57:17.863672 91127 MultiSolver.cpp:109] [6] PROFILING END[Backward]
I1226 08:57:17.863756 91127 solver.cpp:303] [6] Iteration 26, loss = 3.14207
I1226 08:57:17.863821 91127 solver.cpp:329]     Train net output #0: accuracy = 0.320312
I1226 08:57:17.863886 91127 solver.cpp:329]     Train net output #1: loss = 3.14207 (* 1 = 3.14207 loss)
I1226 08:57:17.863955 91127 MultiSolver.cpp:76] [6] PROFILING BEGIN[Forward]
I1226 08:57:18.143864 119888 MultiSolver.cpp:109] [4] PROFILING END[Backward]
I1226 08:57:18.143937 119888 solver.cpp:303] [4] Iteration 19, loss = 3.28615
I1226 08:57:18.143992 119888 solver.cpp:329]     Train net output #0: accuracy = 0.316406
I1226 08:57:18.144045 119888 solver.cpp:329]     Train net output #1: loss = 3.28615 (* 1 = 3.28615 loss)
I1226 08:57:18.144093 119888 MultiSolver.cpp:76] [4] PROFILING BEGIN[Forward]
I1226 08:57:18.265779 91127 MultiSolver.cpp:93] [6] PROFILING END[Forward]
I1226 08:57:18.265874 91127 MultiSolver.cpp:95] [6] PROFILING BEGIN[Backward]
I1226 08:57:18.863481 89709 MultiSolver.cpp:109] [7] PROFILING END[Backward]
I1226 08:57:18.863551 89709 solver.cpp:303] [7] Iteration 19, loss = 3.322
I1226 08:57:18.863606 89709 solver.cpp:329]     Train net output #0: accuracy = 0.292969
I1226 08:57:18.863661 89709 solver.cpp:329]     Train net output #1: loss = 3.322 (* 1 = 3.322 loss)
I1226 08:57:18.863981 89709 MultiSolver.cpp:76] [7] PROFILING BEGIN[Forward]
I1226 08:57:18.891710 89760 blocking_queue.cpp:87] Waiting for data
I1226 08:57:15.637215 90165 MultiSolver.cpp:93] [1] PROFILING END[Forward]
I1226 08:57:15.637339 90165 MultiSolver.cpp:95] [1] PROFILING BEGIN[Backward]
I1226 08:57:18.946550 92150 MultiSolver.cpp:109] [2] PROFILING END[Backward]
I1226 08:57:18.947038 92150 solver.cpp:303] [2] Iteration 19, loss = 3.18498
I1226 08:57:18.947110 92150 solver.cpp:329]     Train net output #0: accuracy = 0.320312
I1226 08:57:18.947197 92150 solver.cpp:329]     Train net output #1: loss = 3.18498 (* 1 = 3.18498 loss)
I1226 08:57:18.948057 92150 MultiSolver.cpp:76] [2] PROFILING BEGIN[Forward]
I1226 08:57:18.965678 86102 MultiSolver.cpp:109] [3] PROFILING END[Backward]
I1226 08:57:18.965745 86102 solver.cpp:303] [3] Iteration 19, loss = 2.83803
I1226 08:57:18.965798 86102 solver.cpp:329]     Train net output #0: accuracy = 0.40625
I1226 08:57:18.965848 86102 solver.cpp:329]     Train net output #1: loss = 2.83803 (* 1 = 2.83803 loss)
I1226 08:57:18.965893 86102 MultiSolver.cpp:76] [3] PROFILING BEGIN[Forward]
I1226 08:57:19.448704 119888 MultiSolver.cpp:93] [4] PROFILING END[Forward]
I1226 08:57:19.448784 119888 MultiSolver.cpp:95] [4] PROFILING BEGIN[Backward]
I1226 08:57:19.478863 92216 blocking_queue.cpp:87] Waiting for data
I1226 08:57:19.607342 91127 MultiSolver.cpp:109] [6] PROFILING END[Backward]
I1226 08:57:19.607434 91127 solver.cpp:303] [6] Iteration 27, loss = 3.21543
I1226 08:57:19.607523 91127 solver.cpp:329]     Train net output #0: accuracy = 0.332031
I1226 08:57:19.607596 91127 solver.cpp:329]     Train net output #1: loss = 3.21543 (* 1 = 3.21543 loss)
I1226 08:57:19.607672 91127 MultiSolver.cpp:76] [6] PROFILING BEGIN[Forward]
I1226 08:57:16.715431 90165 MultiSolver.cpp:109] [1] PROFILING END[Backward]
I1226 08:57:16.715523 90165 solver.cpp:303] [1] Iteration 20, loss = 3.21707
I1226 08:57:16.715620 90165 solver.cpp:329]     Train net output #0: accuracy = 0.339844
I1226 08:57:16.715673 90165 solver.cpp:329]     Train net output #1: loss = 3.21707 (* 1 = 3.21707 loss)
I1226 08:57:16.715965 90165 MultiSolver.cpp:76] [1] PROFILING BEGIN[Forward]
I1226 08:57:20.032928 91127 MultiSolver.cpp:93] [6] PROFILING END[Forward]
I1226 08:57:20.033015 91127 MultiSolver.cpp:95] [6] PROFILING BEGIN[Backward]
I1226 08:57:20.199326 89709 MultiSolver.cpp:93] [7] PROFILING END[Forward]
I1226 08:57:20.199420 89709 MultiSolver.cpp:95] [7] PROFILING BEGIN[Backward]
I1226 08:57:20.410599 92150 MultiSolver.cpp:93] [2] PROFILING END[Forward]
I1226 08:57:20.410676 92150 MultiSolver.cpp:95] [2] PROFILING BEGIN[Backward]
I1226 08:57:20.502863 86102 MultiSolver.cpp:93] [3] PROFILING END[Forward]
I1226 08:57:20.502938 86102 MultiSolver.cpp:95] [3] PROFILING BEGIN[Backward]
I1226 08:57:20.714794 119888 MultiSolver.cpp:109] [4] PROFILING END[Backward]
I1226 08:57:20.714866 119888 solver.cpp:303] [4] Iteration 20, loss = 3.30717
I1226 08:57:20.714921 119888 solver.cpp:329]     Train net output #0: accuracy = 0.28125
I1226 08:57:20.714974 119888 solver.cpp:329]     Train net output #1: loss = 3.30717 (* 1 = 3.30717 loss)
I1226 08:57:20.715025 119888 MultiSolver.cpp:76] [4] PROFILING BEGIN[Forward]
I1226 08:57:21.144107 119954 blocking_queue.cpp:87] Waiting for data
I1226 08:57:21.275722 91127 MultiSolver.cpp:109] [6] PROFILING END[Backward]
I1226 08:57:21.275804 91127 solver.cpp:303] [6] Iteration 28, loss = 3.21525
I1226 08:57:21.275868 91127 solver.cpp:329]     Train net output #0: accuracy = 0.335938
I1226 08:57:21.275934 91127 solver.cpp:329]     Train net output #1: loss = 3.21525 (* 1 = 3.21525 loss)
I1226 08:57:21.275991 91127 MultiSolver.cpp:76] [6] PROFILING BEGIN[Forward]
I1226 08:57:18.232415 90215 blocking_queue.cpp:87] Waiting for data
I1226 08:57:21.545405 92150 MultiSolver.cpp:109] [2] PROFILING END[Backward]
I1226 08:57:21.545475 92150 solver.cpp:303] [2] Iteration 20, loss = 3.296
I1226 08:57:21.545529 92150 solver.cpp:329]     Train net output #0: accuracy = 0.304688
I1226 08:57:21.545580 92150 solver.cpp:329]     Train net output #1: loss = 3.296 (* 1 = 3.296 loss)
I1226 08:57:21.545624 92150 MultiSolver.cpp:76] [2] PROFILING BEGIN[Forward]
I1226 08:57:21.552119 89709 MultiSolver.cpp:109] [7] PROFILING END[Backward]
I1226 08:57:21.552191 89709 solver.cpp:303] [7] Iteration 20, loss = 3.23474
I1226 08:57:21.552243 89709 solver.cpp:329]     Train net output #0: accuracy = 0.339844
I1226 08:57:21.552294 89709 solver.cpp:329]     Train net output #1: loss = 3.23474 (* 1 = 3.23474 loss)
I1226 08:57:21.552337 89709 MultiSolver.cpp:76] [7] PROFILING BEGIN[Forward]
I1226 08:57:21.625754 86102 MultiSolver.cpp:109] [3] PROFILING END[Backward]
I1226 08:57:21.625828 86102 solver.cpp:303] [3] Iteration 20, loss = 3.24529
I1226 08:57:21.625882 86102 solver.cpp:329]     Train net output #0: accuracy = 0.316406
I1226 08:57:21.625932 86102 solver.cpp:329]     Train net output #1: loss = 3.24529 (* 1 = 3.24529 loss)
I1226 08:57:21.625984 86102 MultiSolver.cpp:76] [3] PROFILING BEGIN[Forward]
I1226 08:57:21.700717 91127 MultiSolver.cpp:93] [6] PROFILING END[Forward]
I1226 08:57:21.700871 91127 MultiSolver.cpp:95] [6] PROFILING BEGIN[Backward]
I1226 08:57:18.716316 90165 MultiSolver.cpp:93] [1] PROFILING END[Forward]
I1226 08:57:18.716526 90165 MultiSolver.cpp:95] [1] PROFILING BEGIN[Backward]
I1226 08:57:22.320719 119888 MultiSolver.cpp:93] [4] PROFILING END[Forward]
I1226 08:57:22.320925 119888 MultiSolver.cpp:95] [4] PROFILING BEGIN[Backward]
I1226 08:57:22.884557 91127 MultiSolver.cpp:109] [6] PROFILING END[Backward]
I1226 08:57:22.884634 91127 solver.cpp:303] [6] Iteration 29, loss = 3.13139
I1226 08:57:22.884698 91127 solver.cpp:329]     Train net output #0: accuracy = 0.335938
I1226 08:57:22.884763 91127 solver.cpp:329]     Train net output #1: loss = 3.13139 (* 1 = 3.13139 loss)
I1226 08:57:22.884819 91127 MultiSolver.cpp:76] [6] PROFILING BEGIN[Forward]
I1226 08:57:23.007033 89709 MultiSolver.cpp:93] [7] PROFILING END[Forward]
I1226 08:57:23.007261 89709 MultiSolver.cpp:95] [7] PROFILING BEGIN[Backward]
I1226 08:57:19.917703 90165 MultiSolver.cpp:109] [1] PROFILING END[Backward]
I1226 08:57:19.917822 90165 solver.cpp:303] [1] Iteration 21, loss = 3.15265
I1226 08:57:19.918236 90165 solver.cpp:329]     Train net output #0: accuracy = 0.300781
I1226 08:57:19.918550 90165 solver.cpp:329]     Train net output #1: loss = 3.15265 (* 1 = 3.15265 loss)
I1226 08:57:19.918634 90165 MultiSolver.cpp:76] [1] PROFILING BEGIN[Forward]
I1226 08:57:23.241334 92150 MultiSolver.cpp:93] [2] PROFILING END[Forward]
I1226 08:57:23.242184 92150 MultiSolver.cpp:95] [2] PROFILING BEGIN[Backward]
I1226 08:57:23.286093 91127 MultiSolver.cpp:93] [6] PROFILING END[Forward]
I1226 08:57:23.286195 91127 MultiSolver.cpp:95] [6] PROFILING BEGIN[Backward]
I1226 08:57:23.485468 86102 MultiSolver.cpp:93] [3] PROFILING END[Forward]
I1226 08:57:23.485795 86102 MultiSolver.cpp:95] [3] PROFILING BEGIN[Backward]
I1226 08:57:23.580564 119888 MultiSolver.cpp:109] [4] PROFILING END[Backward]
I1226 08:57:23.580662 119888 solver.cpp:303] [4] Iteration 21, loss = 3.16839
I1226 08:57:23.580734 119888 solver.cpp:329]     Train net output #0: accuracy = 0.3125
I1226 08:57:23.580785 119888 solver.cpp:329]     Train net output #1: loss = 3.16839 (* 1 = 3.16839 loss)
I1226 08:57:23.581070 119888 MultiSolver.cpp:76] [4] PROFILING BEGIN[Forward]
I1226 08:57:24.299423 89709 MultiSolver.cpp:109] [7] PROFILING END[Backward]
I1226 08:57:24.299494 89709 solver.cpp:303] [7] Iteration 21, loss = 3.09025
I1226 08:57:24.299547 89709 solver.cpp:329]     Train net output #0: accuracy = 0.382812
I1226 08:57:24.299597 89709 solver.cpp:329]     Train net output #1: loss = 3.09025 (* 1 = 3.09025 loss)
I1226 08:57:24.299641 89709 MultiSolver.cpp:76] [7] PROFILING BEGIN[Forward]
I1226 08:57:24.418462 92150 MultiSolver.cpp:109] [2] PROFILING END[Backward]
I1226 08:57:24.418530 92150 solver.cpp:303] [2] Iteration 21, loss = 3.24094
I1226 08:57:24.418584 92150 solver.cpp:329]     Train net output #0: accuracy = 0.28125
I1226 08:57:24.418634 92150 solver.cpp:329]     Train net output #1: loss = 3.24094 (* 1 = 3.24094 loss)
I1226 08:57:24.418684 92150 MultiSolver.cpp:76] [2] PROFILING BEGIN[Forward]
I1226 08:57:24.536042 91127 MultiSolver.cpp:109] [6] PROFILING END[Backward]
I1226 08:57:24.536125 91127 solver.cpp:303] [6] Iteration 30, loss = 3.27887
I1226 08:57:24.536187 91127 solver.cpp:329]     Train net output #0: accuracy = 0.289062
I1226 08:57:24.536253 91127 solver.cpp:329]     Train net output #1: loss = 3.27887 (* 1 = 3.27887 loss)
I1226 08:57:24.536310 91127 MultiSolver.cpp:76] [6] PROFILING BEGIN[Forward]
I1226 08:57:24.619679 86102 MultiSolver.cpp:109] [3] PROFILING END[Backward]
I1226 08:57:24.619750 86102 solver.cpp:303] [3] Iteration 21, loss = 3.1697
I1226 08:57:24.619803 86102 solver.cpp:329]     Train net output #0: accuracy = 0.324219
I1226 08:57:24.619854 86102 solver.cpp:329]     Train net output #1: loss = 3.1697 (* 1 = 3.1697 loss)
I1226 08:57:24.620165 86102 MultiSolver.cpp:76] [3] PROFILING BEGIN[Forward]
I1226 08:57:24.911361 95005 MultiSolver.cpp:109] [5] PROFILING END[Backward]
I1226 08:57:24.911449 95005 solver.cpp:303] [5] Iteration 4, loss = 3.17944
I1226 08:57:24.911514 95005 solver.cpp:329]     Train net output #0: accuracy = 0.359375
I1226 08:57:24.911581 95005 solver.cpp:329]     Train net output #1: loss = 3.17944 (* 1 = 3.17944 loss)
I1226 08:57:24.911654 95005 MultiSolver.cpp:76] [5] PROFILING BEGIN[Forward]
I1226 08:57:21.672462 90165 MultiSolver.cpp:93] [1] PROFILING END[Forward]
I1226 08:57:21.672557 90165 MultiSolver.cpp:95] [1] PROFILING BEGIN[Backward]
I1226 08:57:24.970798 91127 MultiSolver.cpp:93] [6] PROFILING END[Forward]
I1226 08:57:24.970887 91127 MultiSolver.cpp:95] [6] PROFILING BEGIN[Backward]
I1226 08:57:25.455914 94304 MultiSolver.cpp:109] [0] PROFILING END[Backward]
I1226 08:57:25.455986 94304 solver.cpp:303] [0] Iteration 4, loss = 3.30852
I1226 08:57:25.456037 94304 solver.cpp:329]     Train net output #0: accuracy = 0.3125
I1226 08:57:25.456086 94304 solver.cpp:329]     Train net output #1: loss = 3.30852 (* 1 = 3.30852 loss)
I1226 08:57:25.456130 94304 MultiSolver.cpp:76] [0] PROFILING BEGIN[Forward]
I1226 08:57:25.445185 119888 MultiSolver.cpp:93] [4] PROFILING END[Forward]
I1226 08:57:25.445263 119888 MultiSolver.cpp:95] [4] PROFILING BEGIN[Backward]
I1226 08:57:25.962059 89709 MultiSolver.cpp:93] [7] PROFILING END[Forward]
I1226 08:57:25.962127 89709 MultiSolver.cpp:95] [7] PROFILING BEGIN[Backward]
I1226 08:57:26.119693 91127 MultiSolver.cpp:109] [6] PROFILING END[Backward]
I1226 08:57:26.119861 91127 solver.cpp:303] [6] Iteration 31, loss = 3.25409
I1226 08:57:26.119925 91127 solver.cpp:329]     Train net output #0: accuracy = 0.308594
I1226 08:57:26.119992 91127 solver.cpp:329]     Train net output #1: loss = 3.25409 (* 1 = 3.25409 loss)
I1226 08:57:26.120050 91127 MultiSolver.cpp:76] [6] PROFILING BEGIN[Forward]
I1226 08:57:22.881733 90165 MultiSolver.cpp:109] [1] PROFILING END[Backward]
I1226 08:57:22.881808 90165 solver.cpp:303] [1] Iteration 22, loss = 2.97853
I1226 08:57:22.881862 90165 solver.cpp:329]     Train net output #0: accuracy = 0.367188
I1226 08:57:22.881912 90165 solver.cpp:329]     Train net output #1: loss = 2.97853 (* 1 = 2.97853 loss)
I1226 08:57:22.881956 90165 MultiSolver.cpp:76] [1] PROFILING BEGIN[Forward]
I1226 08:57:26.169543 92150 MultiSolver.cpp:93] [2] PROFILING END[Forward]
I1226 08:57:26.169620 92150 MultiSolver.cpp:95] [2] PROFILING BEGIN[Backward]
I1226 08:57:26.403048 86102 MultiSolver.cpp:93] [3] PROFILING END[Forward]
I1226 08:57:26.403121 86102 MultiSolver.cpp:95] [3] PROFILING BEGIN[Backward]
I1226 08:57:26.542332 91127 MultiSolver.cpp:93] [6] PROFILING END[Forward]
I1226 08:57:26.542450 91127 MultiSolver.cpp:95] [6] PROFILING BEGIN[Backward]
I1226 08:57:26.551208 119888 MultiSolver.cpp:109] [4] PROFILING END[Backward]
I1226 08:57:26.551281 119888 solver.cpp:303] [4] Iteration 22, loss = 3.39004
I1226 08:57:26.551338 119888 solver.cpp:329]     Train net output #0: accuracy = 0.289062
I1226 08:57:26.551391 119888 solver.cpp:329]     Train net output #1: loss = 3.39004 (* 1 = 3.39004 loss)
I1226 08:57:26.551442 119888 MultiSolver.cpp:76] [4] PROFILING BEGIN[Forward]
I1226 08:57:27.192868 89709 MultiSolver.cpp:109] [7] PROFILING END[Backward]
I1226 08:57:27.192941 89709 solver.cpp:303] [7] Iteration 22, loss = 3.21927
I1226 08:57:27.192994 89709 solver.cpp:329]     Train net output #0: accuracy = 0.308594
I1226 08:57:27.193045 89709 solver.cpp:329]     Train net output #1: loss = 3.21927 (* 1 = 3.21927 loss)
I1226 08:57:27.193089 89709 MultiSolver.cpp:76] [7] PROFILING BEGIN[Forward]
I1226 08:57:27.528398 86102 MultiSolver.cpp:109] [3] PROFILING END[Backward]
I1226 08:57:27.528471 86102 solver.cpp:303] [3] Iteration 22, loss = 3.14593
I1226 08:57:27.528525 86102 solver.cpp:329]     Train net output #0: accuracy = 0.351562
I1226 08:57:27.528597 86102 solver.cpp:329]     Train net output #1: loss = 3.14593 (* 1 = 3.14593 loss)
I1226 08:57:27.528642 86102 MultiSolver.cpp:76] [3] PROFILING BEGIN[Forward]
I1226 08:57:27.575470 92150 MultiSolver.cpp:109] [2] PROFILING END[Backward]
I1226 08:57:27.575541 92150 solver.cpp:303] [2] Iteration 22, loss = 3.44872
I1226 08:57:27.575595 92150 solver.cpp:329]     Train net output #0: accuracy = 0.25
I1226 08:57:27.575646 92150 solver.cpp:329]     Train net output #1: loss = 3.44872 (* 1 = 3.44872 loss)
I1226 08:57:27.575697 92150 MultiSolver.cpp:76] [2] PROFILING BEGIN[Forward]
User defined signal 2
