Dec 26 13:18:04 2016 98257 3 10.1 NIOS_DEBUG: stdin_fd set to -1
Dec 26 13:18:04 2016 98257 3 10.1 NIOS_DEBUG: fds[0] has a value of -1
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 13:18:07.777731 91029 mpiutil.cpp:166] Process rank 5 from number of 9 processes running on knl-node027
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 13:18:07.778008 91644 mpiutil.cpp:166] Process rank 8 from number of 9 processes running on knl-node019
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 13:18:07.774144 85740 mpiutil.cpp:166] Process rank 4 from number of 9 processes running on knl-node022
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 13:18:07.777992 98267 mpiutil.cpp:166] Process rank 0 from number of 9 processes running on knl-node016
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 13:18:07.778560 94502 mpiutil.cpp:166] Process rank 6 from number of 9 processes running on knl-node084
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 13:18:07.770051 96304 mpiutil.cpp:166] Process rank 2 from number of 9 processes running on knl-node079
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 13:18:07.767101 96696 mpiutil.cpp:166] Process rank 7 from number of 9 processes running on knl-node078
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 13:18:07.762730 93960 mpiutil.cpp:166] Process rank 3 from number of 9 processes running on knl-node015
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 13:18:07.772171 92093 mpiutil.cpp:166] Process rank 1 from number of 9 processes running on knl-node025
I1226 13:18:07.791761 94502 caffe.cpp:316] Use CPU.
I1226 13:18:07.780701 96696 caffe.cpp:316] Use CPU.
I1226 13:18:07.776816 93960 caffe.cpp:316] Use CPU.
I1226 13:18:07.783982 96304 caffe.cpp:316] Use CPU.
I1226 13:18:07.788820 85740 caffe.cpp:316] Use CPU.
I1226 13:18:07.792692 91644 caffe.cpp:316] Use CPU.
I1226 13:18:07.792707 98267 caffe.cpp:316] Use CPU.
I1226 13:18:07.793081 94502 solver.cpp:93] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_7.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 13:18:07.792959 91029 caffe.cpp:316] Use CPU.
I1226 13:18:07.794077 94502 solver.cpp:128] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_7.prototxt
I1226 13:18:07.781962 96696 solver.cpp:93] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_8.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 13:18:07.777984 93960 solver.cpp:93] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_4.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 13:18:07.782956 96696 solver.cpp:128] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_8.prototxt
I1226 13:18:07.789921 85740 solver.cpp:93] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_5.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 13:18:07.778892 93960 solver.cpp:128] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_4.prototxt
I1226 13:18:07.790714 85740 solver.cpp:128] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_5.prototxt
I1226 13:18:07.793895 91644 solver.cpp:93] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_dummy.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 13:18:07.785387 96304 solver.cpp:93] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_3.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 13:18:07.794778 91644 solver.cpp:128] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_dummy.prototxt
I1226 13:18:07.786512 96304 solver.cpp:128] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_3.prototxt
I1226 13:18:07.794070 91029 solver.cpp:93] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_6.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 13:18:07.794976 91029 solver.cpp:128] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_6.prototxt
I1226 13:18:07.794351 98267 solver.cpp:93] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_1.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 13:18:07.795526 98267 solver.cpp:128] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_1.prototxt
I1226 13:18:07.792363 92093 caffe.cpp:316] Use CPU.
I1226 13:18:07.793563 92093 solver.cpp:93] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_2.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 13:18:07.794425 92093 solver.cpp:128] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_2.prototxt
I1226 13:18:07.808667 93960 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 13:18:07.808742 93960 cpu_info.cpp:455] Total number of sockets: 1
I1226 13:18:07.820457 85740 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 13:18:07.820531 85740 cpu_info.cpp:455] Total number of sockets: 1
I1226 13:18:07.808764 93960 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 13:18:07.808784 93960 cpu_info.cpp:461] Total number of processors: 272
I1226 13:18:07.808805 93960 cpu_info.cpp:464] GPU is used: no
I1226 13:18:07.808825 93960 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 13:18:07.820554 85740 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 13:18:07.820572 85740 cpu_info.cpp:461] Total number of processors: 272
I1226 13:18:07.820591 85740 cpu_info.cpp:464] GPU is used: no
I1226 13:18:07.820610 85740 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 13:18:07.808845 93960 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 13:18:07.820672 85740 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 13:18:07.825817 91644 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 13:18:07.825893 91644 cpu_info.cpp:455] Total number of sockets: 1
I1226 13:18:07.825917 91644 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 13:18:07.825937 91644 cpu_info.cpp:461] Total number of processors: 272
I1226 13:18:07.825958 91644 cpu_info.cpp:464] GPU is used: no
I1226 13:18:07.825978 91644 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 13:18:07.825997 91644 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 13:18:07.826730 98267 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 13:18:07.826808 98267 cpu_info.cpp:455] Total number of sockets: 1
I1226 13:18:07.826838 98267 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 13:18:07.826864 98267 cpu_info.cpp:461] Total number of processors: 272
I1226 13:18:07.826889 98267 cpu_info.cpp:464] GPU is used: no
I1226 13:18:07.826916 98267 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 13:18:07.826946 98267 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 13:18:07.827623 94502 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 13:18:07.827710 94502 cpu_info.cpp:455] Total number of sockets: 1
I1226 13:18:07.827736 94502 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 13:18:07.827759 94502 cpu_info.cpp:461] Total number of processors: 272
I1226 13:18:07.827782 94502 cpu_info.cpp:464] GPU is used: no
I1226 13:18:07.816398 96696 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 13:18:07.816489 96696 cpu_info.cpp:455] Total number of sockets: 1
I1226 13:18:07.827805 94502 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 13:18:07.827867 94502 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 13:18:07.816517 96696 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 13:18:07.816540 96696 cpu_info.cpp:461] Total number of processors: 272
I1226 13:18:07.816561 96696 cpu_info.cpp:464] GPU is used: no
I1226 13:18:07.816586 96696 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 13:18:07.816606 96696 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 13:18:07.820849 96304 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 13:18:07.820945 96304 cpu_info.cpp:455] Total number of sockets: 1
I1226 13:18:07.820976 96304 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 13:18:07.821002 96304 cpu_info.cpp:461] Total number of processors: 272
I1226 13:18:07.821086 96304 cpu_info.cpp:464] GPU is used: no
I1226 13:18:07.821140 96304 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 13:18:07.821185 96304 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 13:18:07.831455 91029 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 13:18:07.831531 91029 cpu_info.cpp:455] Total number of sockets: 1
I1226 13:18:07.831552 91029 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 13:18:07.831570 91029 cpu_info.cpp:461] Total number of processors: 272
I1226 13:18:07.831589 91029 cpu_info.cpp:464] GPU is used: no
I1226 13:18:07.831609 91029 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 13:18:07.831627 91029 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 13:18:07.826104 92093 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 13:18:07.826179 92093 cpu_info.cpp:455] Total number of sockets: 1
I1226 13:18:07.826201 92093 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 13:18:07.826222 92093 cpu_info.cpp:461] Total number of processors: 272
I1226 13:18:07.826242 92093 cpu_info.cpp:464] GPU is used: no
I1226 13:18:07.826262 92093 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 13:18:07.826283 92093 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 13:18:07.849342 94502 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 13:18:07.849692 94502 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 13:18:07.838279 96696 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 13:18:07.838631 96696 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 13:18:07.851783 94502 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_6"
    batch_size: 32
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 13:18:07.840713 96696 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_7"
    batch_size: 32
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 13:18:07.860582 98267 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 13:18:07.860885 98267 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 13:18:07.845449 93960 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 13:18:07.845835 93960 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 13:18:07.862498 98267 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_0"
    batch_size: 32
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 13:18:07.847672 93960 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_3"
    batch_size: 32
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 13:18:07.859771 85740 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 13:18:07.860157 85740 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 13:18:07.862442 85740 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_4"
    batch_size: 32
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 13:18:07.866981 91644 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 13:18:07.867450 91644 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 13:18:07.869477 91644 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "DummyData"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  dummy_data_param {
    data_filler {
      type: "gaussian"
      std: 1
    }
    data_filler {
      type: "constant"
      value: 0
    }
    shape {
      dim: 32
      dim: 3
      dim: 227
      dim: 227
    }
    shape {
      dim: 32
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 13:18:07.870765 96304 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 13:18:07.871151 96304 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 13:18:07.873194 96304 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_2"
    batch_size: 32
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 13:18:07.892696 92093 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 13:18:07.893066 92093 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 13:18:07.894780 92093 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_1"
    batch_size: 32
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 13:18:07.903693 91029 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 13:18:07.904153 91029 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 13:18:07.906431 91029 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_5"
    batch_size: 32
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 13:18:07.924996 85740 layer_factory.hpp:114] Creating layer data
I1226 13:18:07.929103 94502 layer_factory.hpp:114] Creating layer data
I1226 13:18:07.935053 93960 layer_factory.hpp:114] Creating layer data
I1226 13:18:07.958106 91644 layer_factory.hpp:114] Creating layer data
I1226 13:18:07.958251 91644 net.cpp:178] Creating Layer data
I1226 13:18:07.958297 91644 net.cpp:586] data -> data
I1226 13:18:07.958436 91644 net.cpp:586] data -> label
I1226 13:18:07.957444 85740 net.cpp:178] Creating Layer data
I1226 13:18:07.957530 85740 net.cpp:586] data -> data
I1226 13:18:07.957617 85740 net.cpp:586] data -> label
I1226 13:18:07.963191 92093 layer_factory.hpp:114] Creating layer data
I1226 13:18:07.971464 94502 net.cpp:178] Creating Layer data
I1226 13:18:07.971637 94502 net.cpp:586] data -> data
I1226 13:18:07.971773 94502 net.cpp:586] data -> label
I1226 13:18:07.964747 93960 net.cpp:178] Creating Layer data
I1226 13:18:07.964835 93960 net.cpp:586] data -> data
I1226 13:18:07.964913 93960 net.cpp:586] data -> label
I1226 13:18:07.980527 85742 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_4
I1226 13:18:07.983741 92093 net.cpp:178] Creating Layer data
I1226 13:18:07.983819 92093 net.cpp:586] data -> data
I1226 13:18:07.983896 92093 net.cpp:586] data -> label
I1226 13:18:07.988126 85740 data_layer.cpp:80] output data size: 32,3,227,227
I1226 13:18:07.993331 94504 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_6
I1226 13:18:07.990562 96304 layer_factory.hpp:114] Creating layer data
I1226 13:18:07.987751 96696 layer_factory.hpp:114] Creating layer data
I1226 13:18:08.001291 94502 data_layer.cpp:80] output data size: 32,3,227,227
I1226 13:18:07.989584 93962 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_3
I1226 13:18:08.008047 98267 layer_factory.hpp:114] Creating layer data
I1226 13:18:08.010852 91644 net.cpp:228] Setting up data
I1226 13:18:08.010998 91644 net.cpp:235] Top shape: 32 3 227 227 (4946784)
I1226 13:18:08.011059 91644 net.cpp:235] Top shape: 32 1 1 1 (32)
I1226 13:18:08.011183 91644 net.cpp:243] Memory required for data: 19787264
I1226 13:18:08.011239 91644 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 13:18:08.011374 91644 net.cpp:178] Creating Layer label_data_1_split
I1226 13:18:08.011409 91644 net.cpp:612] label_data_1_split <- label
I1226 13:18:08.011582 91644 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 13:18:08.011632 91644 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 13:18:08.005852 92095 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_1
I1226 13:18:07.997817 93960 data_layer.cpp:80] output data size: 32,3,227,227
I1226 13:18:08.002676 96696 net.cpp:178] Creating Layer data
I1226 13:18:08.002843 96696 net.cpp:586] data -> data
I1226 13:18:08.002974 96696 net.cpp:586] data -> label
I1226 13:18:08.012679 92093 data_layer.cpp:80] output data size: 32,3,227,227
I1226 13:18:08.023169 91644 net.cpp:228] Setting up label_data_1_split
I1226 13:18:08.023268 91644 net.cpp:235] Top shape: 32 1 1 1 (32)
I1226 13:18:08.023300 91644 net.cpp:235] Top shape: 32 1 1 1 (32)
I1226 13:18:08.023347 91644 net.cpp:243] Memory required for data: 19787520
I1226 13:18:08.023382 91644 layer_factory.hpp:114] Creating layer conv1
I1226 13:18:08.023478 91644 net.cpp:178] Creating Layer conv1
I1226 13:18:08.023509 91644 net.cpp:612] conv1 <- data
I1226 13:18:08.023553 91644 net.cpp:586] conv1 -> conv1
I1226 13:18:08.024225 96304 net.cpp:178] Creating Layer data
I1226 13:18:08.032450 91029 layer_factory.hpp:114] Creating layer data
I1226 13:18:08.024360 96304 net.cpp:586] data -> data
I1226 13:18:08.024489 96304 net.cpp:586] data -> label
I1226 13:18:08.025398 96698 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_7
I1226 13:18:08.037323 98267 net.cpp:178] Creating Layer data
I1226 13:18:08.037408 98267 net.cpp:586] data -> data
I1226 13:18:08.037487 98267 net.cpp:586] data -> label
I1226 13:18:08.042413 91029 net.cpp:178] Creating Layer data
I1226 13:18:08.042495 91029 net.cpp:586] data -> data
I1226 13:18:08.042596 91029 net.cpp:586] data -> label
I1226 13:18:08.033160 96696 data_layer.cpp:80] output data size: 32,3,227,227
I1226 13:18:08.045095 96306 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_2
I1226 13:18:08.054013 96304 data_layer.cpp:80] output data size: 32,3,227,227
I1226 13:18:08.065286 98271 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_0
I1226 13:18:08.065028 85740 net.cpp:228] Setting up data
I1226 13:18:08.068547 91031 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_5
I1226 13:18:08.065145 85740 net.cpp:235] Top shape: 32 3 227 227 (4946784)
I1226 13:18:08.065179 85740 net.cpp:235] Top shape: 32 (32)
I1226 13:18:08.065203 85740 net.cpp:243] Memory required for data: 19787264
I1226 13:18:08.065239 85740 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 13:18:08.065330 85740 net.cpp:178] Creating Layer label_data_1_split
I1226 13:18:08.065451 85740 net.cpp:612] label_data_1_split <- label
I1226 13:18:08.065492 85740 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 13:18:08.065541 85740 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 13:18:08.073860 98267 data_layer.cpp:80] output data size: 32,3,227,227
I1226 13:18:08.077766 91029 data_layer.cpp:80] output data size: 32,3,227,227
I1226 13:18:08.083613 85740 net.cpp:228] Setting up label_data_1_split
I1226 13:18:08.083755 85740 net.cpp:235] Top shape: 32 (32)
I1226 13:18:08.083791 85740 net.cpp:235] Top shape: 32 (32)
I1226 13:18:08.083816 85740 net.cpp:243] Memory required for data: 19787520
I1226 13:18:08.083850 85740 layer_factory.hpp:114] Creating layer conv1
I1226 13:18:08.083956 85740 net.cpp:178] Creating Layer conv1
I1226 13:18:08.083994 85740 net.cpp:612] conv1 <- data
I1226 13:18:08.084036 85740 net.cpp:586] conv1 -> conv1
I1226 13:18:08.077919 93960 net.cpp:228] Setting up data
I1226 13:18:08.078039 93960 net.cpp:235] Top shape: 32 3 227 227 (4946784)
I1226 13:18:08.078079 93960 net.cpp:235] Top shape: 32 (32)
I1226 13:18:08.078132 93960 net.cpp:243] Memory required for data: 19787264
I1226 13:18:08.078191 93960 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 13:18:08.078289 93960 net.cpp:178] Creating Layer label_data_1_split
I1226 13:18:08.078507 93960 net.cpp:612] label_data_1_split <- label
I1226 13:18:08.078624 93960 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 13:18:08.078687 93960 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 13:18:08.089890 92093 net.cpp:228] Setting up data
I1226 13:18:08.090008 92093 net.cpp:235] Top shape: 32 3 227 227 (4946784)
I1226 13:18:08.090045 92093 net.cpp:235] Top shape: 32 (32)
I1226 13:18:08.090098 92093 net.cpp:243] Memory required for data: 19787264
I1226 13:18:08.090136 92093 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 13:18:08.090222 92093 net.cpp:178] Creating Layer label_data_1_split
I1226 13:18:08.090354 92093 net.cpp:612] label_data_1_split <- label
I1226 13:18:08.090457 92093 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 13:18:08.090539 92093 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 13:18:08.106045 92093 net.cpp:228] Setting up label_data_1_split
I1226 13:18:08.106155 92093 net.cpp:235] Top shape: 32 (32)
I1226 13:18:08.106189 92093 net.cpp:235] Top shape: 32 (32)
I1226 13:18:08.106212 92093 net.cpp:243] Memory required for data: 19787520
I1226 13:18:08.106276 92093 layer_factory.hpp:114] Creating layer conv1
I1226 13:18:08.106380 92093 net.cpp:178] Creating Layer conv1
I1226 13:18:08.106468 92093 net.cpp:612] conv1 <- data
I1226 13:18:08.106537 92093 net.cpp:586] conv1 -> conv1
I1226 13:18:08.097731 93960 net.cpp:228] Setting up label_data_1_split
I1226 13:18:08.097877 93960 net.cpp:235] Top shape: 32 (32)
I1226 13:18:08.097940 93960 net.cpp:235] Top shape: 32 (32)
I1226 13:18:08.097975 93960 net.cpp:243] Memory required for data: 19787520
I1226 13:18:08.098192 93960 layer_factory.hpp:114] Creating layer conv1
I1226 13:18:08.098311 93960 net.cpp:178] Creating Layer conv1
I1226 13:18:08.098435 93960 net.cpp:612] conv1 <- data
I1226 13:18:08.098496 93960 net.cpp:586] conv1 -> conv1
I1226 13:18:08.149834 98267 net.cpp:228] Setting up data
I1226 13:18:08.149951 98267 net.cpp:235] Top shape: 32 3 227 227 (4946784)
I1226 13:18:08.149986 98267 net.cpp:235] Top shape: 32 (32)
I1226 13:18:08.150009 98267 net.cpp:243] Memory required for data: 19787264
I1226 13:18:08.150045 98267 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 13:18:08.150162 98267 net.cpp:178] Creating Layer label_data_1_split
I1226 13:18:08.150305 98267 net.cpp:612] label_data_1_split <- label
I1226 13:18:08.150357 98267 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 13:18:08.150410 98267 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 13:18:08.151172 91644 net.cpp:228] Setting up conv1
I1226 13:18:08.151286 91644 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 13:18:08.151330 91644 net.cpp:243] Memory required for data: 56958720
I1226 13:18:08.151440 91644 layer_factory.hpp:114] Creating layer relu1
I1226 13:18:08.151527 91644 net.cpp:178] Creating Layer relu1
I1226 13:18:08.151563 91644 net.cpp:612] relu1 <- conv1
I1226 13:18:08.151612 91644 net.cpp:573] relu1 -> conv1 (in-place)
I1226 13:18:08.151705 91644 net.cpp:228] Setting up relu1
I1226 13:18:08.151808 91644 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 13:18:08.151830 91644 net.cpp:243] Memory required for data: 94129920
I1226 13:18:08.151855 91644 layer_factory.hpp:114] Creating layer norm1
I1226 13:18:08.151919 91644 net.cpp:178] Creating Layer norm1
I1226 13:18:08.151950 91644 net.cpp:612] norm1 <- conv1
I1226 13:18:08.151984 91644 net.cpp:586] norm1 -> norm1
I1226 13:18:08.152065 91644 net.cpp:228] Setting up norm1
I1226 13:18:08.152097 91644 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 13:18:08.152125 91644 net.cpp:243] Memory required for data: 131301120
I1226 13:18:08.152149 91644 layer_factory.hpp:114] Creating layer pool1
I1226 13:18:08.152207 91644 net.cpp:178] Creating Layer pool1
I1226 13:18:08.152240 91644 net.cpp:612] pool1 <- norm1
I1226 13:18:08.152271 91644 net.cpp:586] pool1 -> pool1
I1226 13:18:08.152374 91644 net.cpp:228] Setting up pool1
I1226 13:18:08.152415 91644 net.cpp:235] Top shape: 32 96 27 27 (2239488)
I1226 13:18:08.152441 91644 net.cpp:243] Memory required for data: 140259072
I1226 13:18:08.152465 91644 layer_factory.hpp:114] Creating layer conv2
I1226 13:18:08.152532 91644 net.cpp:178] Creating Layer conv2
I1226 13:18:08.152561 91644 net.cpp:612] conv2 <- pool1
I1226 13:18:08.152604 91644 net.cpp:586] conv2 -> conv2
I1226 13:18:08.154809 94502 net.cpp:228] Setting up data
I1226 13:18:08.154949 94502 net.cpp:235] Top shape: 32 3 227 227 (4946784)
I1226 13:18:08.154997 94502 net.cpp:235] Top shape: 32 (32)
I1226 13:18:08.155026 94502 net.cpp:243] Memory required for data: 19787264
I1226 13:18:08.155083 94502 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 13:18:08.154860 91029 net.cpp:228] Setting up data
I1226 13:18:08.155184 94502 net.cpp:178] Creating Layer label_data_1_split
I1226 13:18:08.154973 91029 net.cpp:235] Top shape: 32 3 227 227 (4946784)
I1226 13:18:08.155007 91029 net.cpp:235] Top shape: 32 (32)
I1226 13:18:08.155333 94502 net.cpp:612] label_data_1_split <- label
I1226 13:18:08.155031 91029 net.cpp:243] Memory required for data: 19787264
I1226 13:18:08.155392 94502 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 13:18:08.155067 91029 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 13:18:08.155472 94502 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 13:18:08.155148 91029 net.cpp:178] Creating Layer label_data_1_split
I1226 13:18:08.155272 91029 net.cpp:612] label_data_1_split <- label
I1226 13:18:08.155311 91029 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 13:18:08.155359 91029 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 13:18:08.166955 98267 net.cpp:228] Setting up label_data_1_split
I1226 13:18:08.167058 98267 net.cpp:235] Top shape: 32 (32)
I1226 13:18:08.167119 98267 net.cpp:235] Top shape: 32 (32)
I1226 13:18:08.167143 98267 net.cpp:243] Memory required for data: 19787520
I1226 13:18:08.167179 98267 layer_factory.hpp:114] Creating layer conv1
I1226 13:18:08.167307 98267 net.cpp:178] Creating Layer conv1
I1226 13:18:08.167374 98267 net.cpp:612] conv1 <- data
I1226 13:18:08.167512 98267 net.cpp:586] conv1 -> conv1
I1226 13:18:08.171767 91029 net.cpp:228] Setting up label_data_1_split
I1226 13:18:08.171910 91029 net.cpp:235] Top shape: 32 (32)
I1226 13:18:08.171943 91029 net.cpp:235] Top shape: 32 (32)
I1226 13:18:08.171993 91029 net.cpp:243] Memory required for data: 19787520
I1226 13:18:08.172030 91029 layer_factory.hpp:114] Creating layer conv1
I1226 13:18:08.172204 91029 net.cpp:178] Creating Layer conv1
I1226 13:18:08.172240 91029 net.cpp:612] conv1 <- data
I1226 13:18:08.172282 91029 net.cpp:586] conv1 -> conv1
I1226 13:18:08.175849 94502 net.cpp:228] Setting up label_data_1_split
I1226 13:18:08.175990 94502 net.cpp:235] Top shape: 32 (32)
I1226 13:18:08.176066 94502 net.cpp:235] Top shape: 32 (32)
I1226 13:18:08.176219 94502 net.cpp:243] Memory required for data: 19787520
I1226 13:18:08.176268 94502 layer_factory.hpp:114] Creating layer conv1
I1226 13:18:08.176391 94502 net.cpp:178] Creating Layer conv1
I1226 13:18:08.176452 94502 net.cpp:612] conv1 <- data
I1226 13:18:08.176512 94502 net.cpp:586] conv1 -> conv1
I1226 13:18:08.188977 96696 net.cpp:228] Setting up data
I1226 13:18:08.189103 96696 net.cpp:235] Top shape: 32 3 227 227 (4946784)
I1226 13:18:08.189152 96696 net.cpp:235] Top shape: 32 (32)
I1226 13:18:08.189183 96696 net.cpp:243] Memory required for data: 19787264
I1226 13:18:08.189244 96696 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 13:18:08.189342 96696 net.cpp:178] Creating Layer label_data_1_split
I1226 13:18:08.189491 96696 net.cpp:612] label_data_1_split <- label
I1226 13:18:08.189553 96696 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 13:18:08.189620 96696 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 13:18:08.217299 96304 net.cpp:228] Setting up data
I1226 13:18:08.217425 96304 net.cpp:235] Top shape: 32 3 227 227 (4946784)
I1226 13:18:08.217475 96304 net.cpp:235] Top shape: 32 (32)
I1226 13:18:08.217504 96304 net.cpp:243] Memory required for data: 19787264
I1226 13:18:08.217545 96304 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 13:18:08.217669 96304 net.cpp:178] Creating Layer label_data_1_split
I1226 13:18:08.217912 96304 net.cpp:612] label_data_1_split <- label
I1226 13:18:08.217985 96304 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 13:18:08.218058 96304 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 13:18:08.223691 96696 net.cpp:228] Setting up label_data_1_split
I1226 13:18:08.223851 96696 net.cpp:235] Top shape: 32 (32)
I1226 13:18:08.223896 96696 net.cpp:235] Top shape: 32 (32)
I1226 13:18:08.223922 96696 net.cpp:243] Memory required for data: 19787520
I1226 13:18:08.223960 96696 layer_factory.hpp:114] Creating layer conv1
I1226 13:18:08.224066 96696 net.cpp:178] Creating Layer conv1
I1226 13:18:08.224123 96696 net.cpp:612] conv1 <- data
I1226 13:18:08.224174 96696 net.cpp:586] conv1 -> conv1
I1226 13:18:08.245437 85740 net.cpp:228] Setting up conv1
I1226 13:18:08.245589 85740 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 13:18:08.245626 85740 net.cpp:243] Memory required for data: 56958720
I1226 13:18:08.245828 85740 layer_factory.hpp:114] Creating layer relu1
I1226 13:18:08.245950 85740 net.cpp:178] Creating Layer relu1
I1226 13:18:08.246417 85740 net.cpp:612] relu1 <- conv1
I1226 13:18:08.246562 85740 net.cpp:573] relu1 -> conv1 (in-place)
I1226 13:18:08.246785 85740 net.cpp:228] Setting up relu1
I1226 13:18:08.246851 85740 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 13:18:08.246877 85740 net.cpp:243] Memory required for data: 94129920
I1226 13:18:08.246932 85740 layer_factory.hpp:114] Creating layer norm1
I1226 13:18:08.247134 85740 net.cpp:178] Creating Layer norm1
I1226 13:18:08.247201 85740 net.cpp:612] norm1 <- conv1
I1226 13:18:08.247290 85740 net.cpp:586] norm1 -> norm1
I1226 13:18:08.247439 85740 net.cpp:228] Setting up norm1
I1226 13:18:08.247509 85740 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 13:18:08.247561 85740 net.cpp:243] Memory required for data: 131301120
I1226 13:18:08.247594 85740 layer_factory.hpp:114] Creating layer pool1
I1226 13:18:08.247701 85740 net.cpp:178] Creating Layer pool1
I1226 13:18:08.248100 85740 net.cpp:612] pool1 <- norm1
I1226 13:18:08.248179 85740 net.cpp:586] pool1 -> pool1
I1226 13:18:08.248356 85740 net.cpp:228] Setting up pool1
I1226 13:18:08.248446 85740 net.cpp:235] Top shape: 32 96 27 27 (2239488)
I1226 13:18:08.248479 85740 net.cpp:243] Memory required for data: 140259072
I1226 13:18:08.248533 85740 layer_factory.hpp:114] Creating layer conv2
I1226 13:18:08.248631 85740 net.cpp:178] Creating Layer conv2
I1226 13:18:08.248690 85740 net.cpp:612] conv2 <- pool1
I1226 13:18:08.248736 85740 net.cpp:586] conv2 -> conv2
I1226 13:18:08.244765 96304 net.cpp:228] Setting up label_data_1_split
I1226 13:18:08.244920 96304 net.cpp:235] Top shape: 32 (32)
I1226 13:18:08.244958 96304 net.cpp:235] Top shape: 32 (32)
I1226 13:18:08.244985 96304 net.cpp:243] Memory required for data: 19787520
I1226 13:18:08.245023 96304 layer_factory.hpp:114] Creating layer conv1
I1226 13:18:08.245260 96304 net.cpp:178] Creating Layer conv1
I1226 13:18:08.245365 96304 net.cpp:612] conv1 <- data
I1226 13:18:08.245414 96304 net.cpp:586] conv1 -> conv1
I1226 13:18:08.250571 92093 net.cpp:228] Setting up conv1
I1226 13:18:08.250691 92093 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 13:18:08.250723 92093 net.cpp:243] Memory required for data: 56958720
I1226 13:18:08.250839 92093 layer_factory.hpp:114] Creating layer relu1
I1226 13:18:08.250941 92093 net.cpp:178] Creating Layer relu1
I1226 13:18:08.250988 92093 net.cpp:612] relu1 <- conv1
I1226 13:18:08.251037 92093 net.cpp:573] relu1 -> conv1 (in-place)
I1226 13:18:08.251150 92093 net.cpp:228] Setting up relu1
I1226 13:18:08.251199 92093 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 13:18:08.251230 92093 net.cpp:243] Memory required for data: 94129920
I1226 13:18:08.251269 92093 layer_factory.hpp:114] Creating layer norm1
I1226 13:18:08.251343 92093 net.cpp:178] Creating Layer norm1
I1226 13:18:08.251371 92093 net.cpp:612] norm1 <- conv1
I1226 13:18:08.251417 92093 net.cpp:586] norm1 -> norm1
I1226 13:18:08.251538 92093 net.cpp:228] Setting up norm1
I1226 13:18:08.251597 92093 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 13:18:08.251622 92093 net.cpp:243] Memory required for data: 131301120
I1226 13:18:08.251654 92093 layer_factory.hpp:114] Creating layer pool1
I1226 13:18:08.251735 92093 net.cpp:178] Creating Layer pool1
I1226 13:18:08.251771 92093 net.cpp:612] pool1 <- norm1
I1226 13:18:08.251809 92093 net.cpp:586] pool1 -> pool1
I1226 13:18:08.251899 92093 net.cpp:228] Setting up pool1
I1226 13:18:08.251941 92093 net.cpp:235] Top shape: 32 96 27 27 (2239488)
I1226 13:18:08.251965 92093 net.cpp:243] Memory required for data: 140259072
I1226 13:18:08.251996 92093 layer_factory.hpp:114] Creating layer conv2
I1226 13:18:08.252075 92093 net.cpp:178] Creating Layer conv2
I1226 13:18:08.252104 92093 net.cpp:612] conv2 <- pool1
I1226 13:18:08.252146 92093 net.cpp:586] conv2 -> conv2
I1226 13:18:08.259465 93960 net.cpp:228] Setting up conv1
I1226 13:18:08.259609 93960 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 13:18:08.259644 93960 net.cpp:243] Memory required for data: 56958720
I1226 13:18:08.259789 93960 layer_factory.hpp:114] Creating layer relu1
I1226 13:18:08.259917 93960 net.cpp:178] Creating Layer relu1
I1226 13:18:08.259979 93960 net.cpp:612] relu1 <- conv1
I1226 13:18:08.260031 93960 net.cpp:573] relu1 -> conv1 (in-place)
I1226 13:18:08.260169 93960 net.cpp:228] Setting up relu1
I1226 13:18:08.260243 93960 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 13:18:08.260274 93960 net.cpp:243] Memory required for data: 94129920
I1226 13:18:08.260311 93960 layer_factory.hpp:114] Creating layer norm1
I1226 13:18:08.260445 93960 net.cpp:178] Creating Layer norm1
I1226 13:18:08.260509 93960 net.cpp:612] norm1 <- conv1
I1226 13:18:08.260574 93960 net.cpp:586] norm1 -> norm1
I1226 13:18:08.260710 93960 net.cpp:228] Setting up norm1
I1226 13:18:08.260783 93960 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 13:18:08.260813 93960 net.cpp:243] Memory required for data: 131301120
I1226 13:18:08.260848 93960 layer_factory.hpp:114] Creating layer pool1
I1226 13:18:08.260939 93960 net.cpp:178] Creating Layer pool1
I1226 13:18:08.260980 93960 net.cpp:612] pool1 <- norm1
I1226 13:18:08.261039 93960 net.cpp:586] pool1 -> pool1
I1226 13:18:08.261556 93960 net.cpp:228] Setting up pool1
I1226 13:18:08.261675 93960 net.cpp:235] Top shape: 32 96 27 27 (2239488)
I1226 13:18:08.261713 93960 net.cpp:243] Memory required for data: 140259072
I1226 13:18:08.261745 93960 layer_factory.hpp:114] Creating layer conv2
I1226 13:18:08.261862 93960 net.cpp:178] Creating Layer conv2
I1226 13:18:08.261909 93960 net.cpp:612] conv2 <- pool1
I1226 13:18:08.261955 93960 net.cpp:586] conv2 -> conv2
I1226 13:18:08.288427 91644 net.cpp:228] Setting up conv2
I1226 13:18:08.288538 91644 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 13:18:08.288565 91644 net.cpp:243] Memory required for data: 164146944
I1226 13:18:08.288638 91644 layer_factory.hpp:114] Creating layer relu2
I1226 13:18:08.288705 91644 net.cpp:178] Creating Layer relu2
I1226 13:18:08.288740 91644 net.cpp:612] relu2 <- conv2
I1226 13:18:08.288794 91644 net.cpp:573] relu2 -> conv2 (in-place)
I1226 13:18:08.288884 91644 net.cpp:228] Setting up relu2
I1226 13:18:08.288924 91644 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 13:18:08.288949 91644 net.cpp:243] Memory required for data: 188034816
I1226 13:18:08.288976 91644 layer_factory.hpp:114] Creating layer norm2
I1226 13:18:08.289038 91644 net.cpp:178] Creating Layer norm2
I1226 13:18:08.289070 91644 net.cpp:612] norm2 <- conv2
I1226 13:18:08.289106 91644 net.cpp:586] norm2 -> norm2
I1226 13:18:08.289185 91644 net.cpp:228] Setting up norm2
I1226 13:18:08.289227 91644 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 13:18:08.289255 91644 net.cpp:243] Memory required for data: 211922688
I1226 13:18:08.289283 91644 layer_factory.hpp:114] Creating layer pool2
I1226 13:18:08.289356 91644 net.cpp:178] Creating Layer pool2
I1226 13:18:08.289386 91644 net.cpp:612] pool2 <- norm2
I1226 13:18:08.289433 91644 net.cpp:586] pool2 -> pool2
I1226 13:18:08.289515 91644 net.cpp:228] Setting up pool2
I1226 13:18:08.289556 91644 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 13:18:08.289675 91644 net.cpp:243] Memory required for data: 217460480
I1226 13:18:08.289710 91644 layer_factory.hpp:114] Creating layer conv3
I1226 13:18:08.289788 91644 net.cpp:178] Creating Layer conv3
I1226 13:18:08.289822 91644 net.cpp:612] conv3 <- pool2
I1226 13:18:08.289868 91644 net.cpp:586] conv3 -> conv3
I1226 13:18:08.327294 98267 net.cpp:228] Setting up conv1
I1226 13:18:08.327438 98267 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 13:18:08.327473 98267 net.cpp:243] Memory required for data: 56958720
I1226 13:18:08.327595 98267 layer_factory.hpp:114] Creating layer relu1
I1226 13:18:08.327419 91029 net.cpp:228] Setting up conv1
I1226 13:18:08.327896 98267 net.cpp:178] Creating Layer relu1
I1226 13:18:08.327533 91029 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 13:18:08.327559 91029 net.cpp:243] Memory required for data: 56958720
I1226 13:18:08.327942 98267 net.cpp:612] relu1 <- conv1
I1226 13:18:08.327687 91029 layer_factory.hpp:114] Creating layer relu1
I1226 13:18:08.328018 98267 net.cpp:573] relu1 -> conv1 (in-place)
I1226 13:18:08.327803 91029 net.cpp:178] Creating Layer relu1
I1226 13:18:08.328171 98267 net.cpp:228] Setting up relu1
I1226 13:18:08.327945 91029 net.cpp:612] relu1 <- conv1
I1226 13:18:08.328352 98267 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 13:18:08.327988 91029 net.cpp:573] relu1 -> conv1 (in-place)
I1226 13:18:08.328380 98267 net.cpp:243] Memory required for data: 94129920
I1226 13:18:08.328413 98267 layer_factory.hpp:114] Creating layer norm1
I1226 13:18:08.328213 91029 net.cpp:228] Setting up relu1
I1226 13:18:08.328488 98267 net.cpp:178] Creating Layer norm1
I1226 13:18:08.328284 91029 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 13:18:08.328307 91029 net.cpp:243] Memory required for data: 94129920
I1226 13:18:08.328529 98267 net.cpp:612] norm1 <- conv1
I1226 13:18:08.328353 91029 layer_factory.hpp:114] Creating layer norm1
I1226 13:18:08.328601 98267 net.cpp:586] norm1 -> norm1
I1226 13:18:08.328447 91029 net.cpp:178] Creating Layer norm1
I1226 13:18:08.328699 98267 net.cpp:228] Setting up norm1
I1226 13:18:08.328845 98267 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 13:18:08.328869 98267 net.cpp:243] Memory required for data: 131301120
I1226 13:18:08.328898 98267 layer_factory.hpp:114] Creating layer pool1
I1226 13:18:08.328965 98267 net.cpp:178] Creating Layer pool1
I1226 13:18:08.329010 98267 net.cpp:612] pool1 <- norm1
I1226 13:18:08.329056 98267 net.cpp:586] pool1 -> pool1
I1226 13:18:08.328500 91029 net.cpp:612] norm1 <- conv1
I1226 13:18:08.329967 91029 net.cpp:586] norm1 -> norm1
I1226 13:18:08.330183 91029 net.cpp:228] Setting up norm1
I1226 13:18:08.330243 91029 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 13:18:08.330268 91029 net.cpp:243] Memory required for data: 131301120
I1226 13:18:08.330302 91029 layer_factory.hpp:114] Creating layer pool1
I1226 13:18:08.330399 91029 net.cpp:178] Creating Layer pool1
I1226 13:18:08.330570 98267 net.cpp:228] Setting up pool1
I1226 13:18:08.330440 91029 net.cpp:612] pool1 <- norm1
I1226 13:18:08.330660 98267 net.cpp:235] Top shape: 32 96 27 27 (2239488)
I1226 13:18:08.330687 98267 net.cpp:243] Memory required for data: 140259072
I1226 13:18:08.330718 98267 layer_factory.hpp:114] Creating layer conv2
I1226 13:18:08.330484 91029 net.cpp:586] pool1 -> pool1
I1226 13:18:08.330857 98267 net.cpp:178] Creating Layer conv2
I1226 13:18:08.330586 91029 net.cpp:228] Setting up pool1
I1226 13:18:08.330904 98267 net.cpp:612] conv2 <- pool1
I1226 13:18:08.330631 91029 net.cpp:235] Top shape: 32 96 27 27 (2239488)
I1226 13:18:08.330984 98267 net.cpp:586] conv2 -> conv2
I1226 13:18:08.330660 91029 net.cpp:243] Memory required for data: 140259072
I1226 13:18:08.330687 91029 layer_factory.hpp:114] Creating layer conv2
I1226 13:18:08.330790 91029 net.cpp:178] Creating Layer conv2
I1226 13:18:08.330824 91029 net.cpp:612] conv2 <- pool1
I1226 13:18:08.330862 91029 net.cpp:586] conv2 -> conv2
I1226 13:18:08.474020 91644 net.cpp:228] Setting up conv3
I1226 13:18:08.474130 91644 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:18:08.474155 91644 net.cpp:243] Memory required for data: 225767168
I1226 13:18:08.474252 91644 layer_factory.hpp:114] Creating layer relu3
I1226 13:18:08.474347 91644 net.cpp:178] Creating Layer relu3
I1226 13:18:08.474408 91644 net.cpp:612] relu3 <- conv3
I1226 13:18:08.474486 91644 net.cpp:573] relu3 -> conv3 (in-place)
I1226 13:18:08.474575 91644 net.cpp:228] Setting up relu3
I1226 13:18:08.474616 91644 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:18:08.474643 91644 net.cpp:243] Memory required for data: 234073856
I1226 13:18:08.474669 91644 layer_factory.hpp:114] Creating layer conv4
I1226 13:18:08.474731 91644 net.cpp:178] Creating Layer conv4
I1226 13:18:08.474756 91644 net.cpp:612] conv4 <- conv3
I1226 13:18:08.474792 91644 net.cpp:586] conv4 -> conv4
I1226 13:18:08.497869 92093 net.cpp:228] Setting up conv2
I1226 13:18:08.497980 92093 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 13:18:08.498008 92093 net.cpp:243] Memory required for data: 164146944
I1226 13:18:08.498107 92093 layer_factory.hpp:114] Creating layer relu2
I1226 13:18:08.498174 92093 net.cpp:178] Creating Layer relu2
I1226 13:18:08.498203 92093 net.cpp:612] relu2 <- conv2
I1226 13:18:08.498240 92093 net.cpp:573] relu2 -> conv2 (in-place)
I1226 13:18:08.498340 92093 net.cpp:228] Setting up relu2
I1226 13:18:08.498391 92093 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 13:18:08.498420 92093 net.cpp:243] Memory required for data: 188034816
I1226 13:18:08.498450 92093 layer_factory.hpp:114] Creating layer norm2
I1226 13:18:08.498538 92093 net.cpp:178] Creating Layer norm2
I1226 13:18:08.498579 92093 net.cpp:612] norm2 <- conv2
I1226 13:18:08.498616 92093 net.cpp:586] norm2 -> norm2
I1226 13:18:08.498709 92093 net.cpp:228] Setting up norm2
I1226 13:18:08.498759 92093 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 13:18:08.498782 92093 net.cpp:243] Memory required for data: 211922688
I1226 13:18:08.498811 92093 layer_factory.hpp:114] Creating layer pool2
I1226 13:18:08.498853 92093 net.cpp:178] Creating Layer pool2
I1226 13:18:08.498884 92093 net.cpp:612] pool2 <- norm2
I1226 13:18:08.498927 92093 net.cpp:586] pool2 -> pool2
I1226 13:18:08.499011 92093 net.cpp:228] Setting up pool2
I1226 13:18:08.499052 92093 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 13:18:08.499171 92093 net.cpp:243] Memory required for data: 217460480
I1226 13:18:08.499210 92093 layer_factory.hpp:114] Creating layer conv3
I1226 13:18:08.499290 92093 net.cpp:178] Creating Layer conv3
I1226 13:18:08.499326 92093 net.cpp:612] conv3 <- pool2
I1226 13:18:08.499368 92093 net.cpp:586] conv3 -> conv3
I1226 13:18:08.497689 93960 net.cpp:228] Setting up conv2
I1226 13:18:08.522004 85740 net.cpp:228] Setting up conv2
I1226 13:18:08.510449 93960 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 13:18:08.522114 85740 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 13:18:08.522138 85740 net.cpp:243] Memory required for data: 164146944
I1226 13:18:08.510534 93960 net.cpp:243] Memory required for data: 164146944
I1226 13:18:08.522239 85740 layer_factory.hpp:114] Creating layer relu2
I1226 13:18:08.510639 93960 layer_factory.hpp:114] Creating layer relu2
I1226 13:18:08.522331 85740 net.cpp:178] Creating Layer relu2
I1226 13:18:08.510702 93960 net.cpp:178] Creating Layer relu2
I1226 13:18:08.522372 85740 net.cpp:612] relu2 <- conv2
I1226 13:18:08.510742 93960 net.cpp:612] relu2 <- conv2
I1226 13:18:08.522413 85740 net.cpp:573] relu2 -> conv2 (in-place)
I1226 13:18:08.510783 93960 net.cpp:573] relu2 -> conv2 (in-place)
I1226 13:18:08.522516 85740 net.cpp:228] Setting up relu2
I1226 13:18:08.510890 93960 net.cpp:228] Setting up relu2
I1226 13:18:08.522560 85740 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 13:18:08.522584 85740 net.cpp:243] Memory required for data: 188034816
I1226 13:18:08.510941 93960 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 13:18:08.510965 93960 net.cpp:243] Memory required for data: 188034816
I1226 13:18:08.522614 85740 layer_factory.hpp:114] Creating layer norm2
I1226 13:18:08.510994 93960 layer_factory.hpp:114] Creating layer norm2
I1226 13:18:08.522706 85740 net.cpp:178] Creating Layer norm2
I1226 13:18:08.511060 93960 net.cpp:178] Creating Layer norm2
I1226 13:18:08.511090 93960 net.cpp:612] norm2 <- conv2
I1226 13:18:08.522747 85740 net.cpp:612] norm2 <- conv2
I1226 13:18:08.511128 93960 net.cpp:586] norm2 -> norm2
I1226 13:18:08.522791 85740 net.cpp:586] norm2 -> norm2
I1226 13:18:08.511219 93960 net.cpp:228] Setting up norm2
I1226 13:18:08.522878 85740 net.cpp:228] Setting up norm2
I1226 13:18:08.511278 93960 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 13:18:08.511303 93960 net.cpp:243] Memory required for data: 211922688
I1226 13:18:08.522938 85740 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 13:18:08.511332 93960 layer_factory.hpp:114] Creating layer pool2
I1226 13:18:08.522967 85740 net.cpp:243] Memory required for data: 211922688
I1226 13:18:08.522995 85740 layer_factory.hpp:114] Creating layer pool2
I1226 13:18:08.511404 93960 net.cpp:178] Creating Layer pool2
I1226 13:18:08.511436 93960 net.cpp:612] pool2 <- norm2
I1226 13:18:08.523041 85740 net.cpp:178] Creating Layer pool2
I1226 13:18:08.511481 93960 net.cpp:586] pool2 -> pool2
I1226 13:18:08.523125 85740 net.cpp:612] pool2 <- norm2
I1226 13:18:08.511575 93960 net.cpp:228] Setting up pool2
I1226 13:18:08.523174 85740 net.cpp:586] pool2 -> pool2
I1226 13:18:08.511617 93960 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 13:18:08.523265 85740 net.cpp:228] Setting up pool2
I1226 13:18:08.511736 93960 net.cpp:243] Memory required for data: 217460480
I1226 13:18:08.523301 85740 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 13:18:08.511772 93960 layer_factory.hpp:114] Creating layer conv3
I1226 13:18:08.523427 85740 net.cpp:243] Memory required for data: 217460480
I1226 13:18:08.511854 93960 net.cpp:178] Creating Layer conv3
I1226 13:18:08.523465 85740 layer_factory.hpp:114] Creating layer conv3
I1226 13:18:08.511890 93960 net.cpp:612] conv3 <- pool2
I1226 13:18:08.523550 85740 net.cpp:178] Creating Layer conv3
I1226 13:18:08.511934 93960 net.cpp:586] conv3 -> conv3
I1226 13:18:08.523586 85740 net.cpp:612] conv3 <- pool2
I1226 13:18:08.523632 85740 net.cpp:586] conv3 -> conv3
I1226 13:18:08.537133 91029 net.cpp:228] Setting up conv2
I1226 13:18:08.537245 91029 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 13:18:08.537271 91029 net.cpp:243] Memory required for data: 164146944
I1226 13:18:08.537370 91029 layer_factory.hpp:114] Creating layer relu2
I1226 13:18:08.537446 91029 net.cpp:178] Creating Layer relu2
I1226 13:18:08.537482 91029 net.cpp:612] relu2 <- conv2
I1226 13:18:08.537523 91029 net.cpp:573] relu2 -> conv2 (in-place)
I1226 13:18:08.537623 91029 net.cpp:228] Setting up relu2
I1226 13:18:08.537683 91029 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 13:18:08.537735 91029 net.cpp:243] Memory required for data: 188034816
I1226 13:18:08.537768 91029 layer_factory.hpp:114] Creating layer norm2
I1226 13:18:08.537823 91029 net.cpp:178] Creating Layer norm2
I1226 13:18:08.537858 91029 net.cpp:612] norm2 <- conv2
I1226 13:18:08.537894 91029 net.cpp:586] norm2 -> norm2
I1226 13:18:08.537986 91029 net.cpp:228] Setting up norm2
I1226 13:18:08.538033 91029 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 13:18:08.538056 91029 net.cpp:243] Memory required for data: 211922688
I1226 13:18:08.538084 91029 layer_factory.hpp:114] Creating layer pool2
I1226 13:18:08.538133 91029 net.cpp:178] Creating Layer pool2
I1226 13:18:08.538159 91029 net.cpp:612] pool2 <- norm2
I1226 13:18:08.538206 91029 net.cpp:586] pool2 -> pool2
I1226 13:18:08.538302 91029 net.cpp:228] Setting up pool2
I1226 13:18:08.538341 91029 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 13:18:08.538460 91029 net.cpp:243] Memory required for data: 217460480
I1226 13:18:08.538496 91029 layer_factory.hpp:114] Creating layer conv3
I1226 13:18:08.538589 91029 net.cpp:178] Creating Layer conv3
I1226 13:18:08.538625 91029 net.cpp:612] conv3 <- pool2
I1226 13:18:08.538676 91029 net.cpp:586] conv3 -> conv3
I1226 13:18:08.561913 98267 net.cpp:228] Setting up conv2
I1226 13:18:08.562022 98267 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 13:18:08.562049 98267 net.cpp:243] Memory required for data: 164146944
I1226 13:18:08.562172 98267 layer_factory.hpp:114] Creating layer relu2
I1226 13:18:08.562273 98267 net.cpp:178] Creating Layer relu2
I1226 13:18:08.562312 98267 net.cpp:612] relu2 <- conv2
I1226 13:18:08.562357 98267 net.cpp:573] relu2 -> conv2 (in-place)
I1226 13:18:08.562461 98267 net.cpp:228] Setting up relu2
I1226 13:18:08.562525 98267 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 13:18:08.562552 98267 net.cpp:243] Memory required for data: 188034816
I1226 13:18:08.562584 98267 layer_factory.hpp:114] Creating layer norm2
I1226 13:18:08.562651 98267 net.cpp:178] Creating Layer norm2
I1226 13:18:08.562680 98267 net.cpp:612] norm2 <- conv2
I1226 13:18:08.562718 98267 net.cpp:586] norm2 -> norm2
I1226 13:18:08.562813 98267 net.cpp:228] Setting up norm2
I1226 13:18:08.562860 98267 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 13:18:08.562881 98267 net.cpp:243] Memory required for data: 211922688
I1226 13:18:08.562911 98267 layer_factory.hpp:114] Creating layer pool2
I1226 13:18:08.562958 98267 net.cpp:178] Creating Layer pool2
I1226 13:18:08.562985 98267 net.cpp:612] pool2 <- norm2
I1226 13:18:08.563030 98267 net.cpp:586] pool2 -> pool2
I1226 13:18:08.563176 98267 net.cpp:228] Setting up pool2
I1226 13:18:08.563235 98267 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 13:18:08.563379 98267 net.cpp:243] Memory required for data: 217460480
I1226 13:18:08.563416 98267 layer_factory.hpp:114] Creating layer conv3
I1226 13:18:08.563518 98267 net.cpp:178] Creating Layer conv3
I1226 13:18:08.563558 98267 net.cpp:612] conv3 <- pool2
I1226 13:18:08.563602 98267 net.cpp:586] conv3 -> conv3
I1226 13:18:08.600574 94502 net.cpp:228] Setting up conv1
I1226 13:18:08.600693 94502 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 13:18:08.600726 94502 net.cpp:243] Memory required for data: 56958720
I1226 13:18:08.600911 94502 layer_factory.hpp:114] Creating layer relu1
I1226 13:18:08.601075 94502 net.cpp:178] Creating Layer relu1
I1226 13:18:08.601276 94502 net.cpp:612] relu1 <- conv1
I1226 13:18:08.601356 94502 net.cpp:573] relu1 -> conv1 (in-place)
I1226 13:18:08.601984 94502 net.cpp:228] Setting up relu1
I1226 13:18:08.602159 94502 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 13:18:08.602193 94502 net.cpp:243] Memory required for data: 94129920
I1226 13:18:08.602233 94502 layer_factory.hpp:114] Creating layer norm1
I1226 13:18:08.602345 94502 net.cpp:178] Creating Layer norm1
I1226 13:18:08.602391 94502 net.cpp:612] norm1 <- conv1
I1226 13:18:08.602437 94502 net.cpp:586] norm1 -> norm1
I1226 13:18:08.602566 94502 net.cpp:228] Setting up norm1
I1226 13:18:08.602630 94502 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 13:18:08.602658 94502 net.cpp:243] Memory required for data: 131301120
I1226 13:18:08.602699 94502 layer_factory.hpp:114] Creating layer pool1
I1226 13:18:08.602766 94502 net.cpp:178] Creating Layer pool1
I1226 13:18:08.602804 94502 net.cpp:612] pool1 <- norm1
I1226 13:18:08.602939 94502 net.cpp:586] pool1 -> pool1
I1226 13:18:08.603088 94502 net.cpp:228] Setting up pool1
I1226 13:18:08.603170 94502 net.cpp:235] Top shape: 32 96 27 27 (2239488)
I1226 13:18:08.603200 94502 net.cpp:243] Memory required for data: 140259072
I1226 13:18:08.603260 94502 layer_factory.hpp:114] Creating layer conv2
I1226 13:18:08.603365 94502 net.cpp:178] Creating Layer conv2
I1226 13:18:08.603425 94502 net.cpp:612] conv2 <- pool1
I1226 13:18:08.604116 94502 net.cpp:586] conv2 -> conv2
I1226 13:18:08.630522 91644 net.cpp:228] Setting up conv4
I1226 13:18:08.630630 91644 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:18:08.630653 91644 net.cpp:243] Memory required for data: 242380544
I1226 13:18:08.630735 91644 layer_factory.hpp:114] Creating layer relu4
I1226 13:18:08.630811 91644 net.cpp:178] Creating Layer relu4
I1226 13:18:08.630847 91644 net.cpp:612] relu4 <- conv4
I1226 13:18:08.630885 91644 net.cpp:573] relu4 -> conv4 (in-place)
I1226 13:18:08.630975 91644 net.cpp:228] Setting up relu4
I1226 13:18:08.631016 91644 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:18:08.631048 91644 net.cpp:243] Memory required for data: 250687232
I1226 13:18:08.631075 91644 layer_factory.hpp:114] Creating layer conv5
I1226 13:18:08.631153 91644 net.cpp:178] Creating Layer conv5
I1226 13:18:08.631186 91644 net.cpp:612] conv5 <- conv4
I1226 13:18:08.631223 91644 net.cpp:586] conv5 -> conv5
I1226 13:18:08.659855 96304 net.cpp:228] Setting up conv1
I1226 13:18:08.659986 96304 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 13:18:08.660027 96304 net.cpp:243] Memory required for data: 56958720
I1226 13:18:08.660158 96304 layer_factory.hpp:114] Creating layer relu1
I1226 13:18:08.660276 96304 net.cpp:178] Creating Layer relu1
I1226 13:18:08.660323 96304 net.cpp:612] relu1 <- conv1
I1226 13:18:08.660374 96304 net.cpp:573] relu1 -> conv1 (in-place)
I1226 13:18:08.660511 96304 net.cpp:228] Setting up relu1
I1226 13:18:08.660588 96304 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 13:18:08.660621 96304 net.cpp:243] Memory required for data: 94129920
I1226 13:18:08.660658 96304 layer_factory.hpp:114] Creating layer norm1
I1226 13:18:08.660753 96304 net.cpp:178] Creating Layer norm1
I1226 13:18:08.660837 96304 net.cpp:612] norm1 <- conv1
I1226 13:18:08.660895 96304 net.cpp:586] norm1 -> norm1
I1226 13:18:08.661034 96304 net.cpp:228] Setting up norm1
I1226 13:18:08.661108 96304 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 13:18:08.661135 96304 net.cpp:243] Memory required for data: 131301120
I1226 13:18:08.661172 96304 layer_factory.hpp:114] Creating layer pool1
I1226 13:18:08.661248 96304 net.cpp:178] Creating Layer pool1
I1226 13:18:08.661308 96304 net.cpp:612] pool1 <- norm1
I1226 13:18:08.661376 96304 net.cpp:586] pool1 -> pool1
I1226 13:18:08.658828 96696 net.cpp:228] Setting up conv1
I1226 13:18:08.661516 96304 net.cpp:228] Setting up pool1
I1226 13:18:08.658951 96696 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 13:18:08.658984 96696 net.cpp:243] Memory required for data: 56958720
I1226 13:18:08.662053 96304 net.cpp:235] Top shape: 32 96 27 27 (2239488)
I1226 13:18:08.659099 96696 layer_factory.hpp:114] Creating layer relu1
I1226 13:18:08.662184 96304 net.cpp:243] Memory required for data: 140259072
I1226 13:18:08.659190 96696 net.cpp:178] Creating Layer relu1
I1226 13:18:08.659237 96696 net.cpp:612] relu1 <- conv1
I1226 13:18:08.659283 96696 net.cpp:573] relu1 -> conv1 (in-place)
I1226 13:18:08.662281 96304 layer_factory.hpp:114] Creating layer conv2
I1226 13:18:08.662554 96304 net.cpp:178] Creating Layer conv2
I1226 13:18:08.659407 96696 net.cpp:228] Setting up relu1
I1226 13:18:08.662611 96304 net.cpp:612] conv2 <- pool1
I1226 13:18:08.659459 96696 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 13:18:08.662677 96304 net.cpp:586] conv2 -> conv2
I1226 13:18:08.659502 96696 net.cpp:243] Memory required for data: 94129920
I1226 13:18:08.659543 96696 layer_factory.hpp:114] Creating layer norm1
I1226 13:18:08.659610 96696 net.cpp:178] Creating Layer norm1
I1226 13:18:08.659646 96696 net.cpp:612] norm1 <- conv1
I1226 13:18:08.659690 96696 net.cpp:586] norm1 -> norm1
I1226 13:18:08.659826 96696 net.cpp:228] Setting up norm1
I1226 13:18:08.659884 96696 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 13:18:08.659912 96696 net.cpp:243] Memory required for data: 131301120
I1226 13:18:08.659945 96696 layer_factory.hpp:114] Creating layer pool1
I1226 13:18:08.660008 96696 net.cpp:178] Creating Layer pool1
I1226 13:18:08.660043 96696 net.cpp:612] pool1 <- norm1
I1226 13:18:08.660105 96696 net.cpp:586] pool1 -> pool1
I1226 13:18:08.660210 96696 net.cpp:228] Setting up pool1
I1226 13:18:08.660257 96696 net.cpp:235] Top shape: 32 96 27 27 (2239488)
I1226 13:18:08.660287 96696 net.cpp:243] Memory required for data: 140259072
I1226 13:18:08.660325 96696 layer_factory.hpp:114] Creating layer conv2
I1226 13:18:08.660409 96696 net.cpp:178] Creating Layer conv2
I1226 13:18:08.660445 96696 net.cpp:612] conv2 <- pool1
I1226 13:18:08.660485 96696 net.cpp:586] conv2 -> conv2
I1226 13:18:08.741888 91644 net.cpp:228] Setting up conv5
I1226 13:18:08.741994 91644 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 13:18:08.742017 91644 net.cpp:243] Memory required for data: 256225024
I1226 13:18:08.742130 91644 layer_factory.hpp:114] Creating layer relu5
I1226 13:18:08.742192 91644 net.cpp:178] Creating Layer relu5
I1226 13:18:08.742219 91644 net.cpp:612] relu5 <- conv5
I1226 13:18:08.742269 91644 net.cpp:573] relu5 -> conv5 (in-place)
I1226 13:18:08.742378 91644 net.cpp:228] Setting up relu5
I1226 13:18:08.742422 91644 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 13:18:08.742445 91644 net.cpp:243] Memory required for data: 261762816
I1226 13:18:08.742477 91644 layer_factory.hpp:114] Creating layer pool5
I1226 13:18:08.742532 91644 net.cpp:178] Creating Layer pool5
I1226 13:18:08.742558 91644 net.cpp:612] pool5 <- conv5
I1226 13:18:08.742593 91644 net.cpp:586] pool5 -> pool5
I1226 13:18:08.742673 91644 net.cpp:228] Setting up pool5
I1226 13:18:08.742710 91644 net.cpp:235] Top shape: 32 256 6 6 (294912)
I1226 13:18:08.742733 91644 net.cpp:243] Memory required for data: 262942464
I1226 13:18:08.742765 91644 layer_factory.hpp:114] Creating layer fc6
I1226 13:18:08.742828 91644 net.cpp:178] Creating Layer fc6
I1226 13:18:08.742859 91644 net.cpp:612] fc6 <- pool5
I1226 13:18:08.742900 91644 net.cpp:586] fc6 -> fc6
I1226 13:18:08.833168 92093 net.cpp:228] Setting up conv3
I1226 13:18:08.833297 92093 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:18:08.833325 92093 net.cpp:243] Memory required for data: 225767168
I1226 13:18:08.833398 92093 layer_factory.hpp:114] Creating layer relu3
I1226 13:18:08.833461 92093 net.cpp:178] Creating Layer relu3
I1226 13:18:08.833515 92093 net.cpp:612] relu3 <- conv3
I1226 13:18:08.833554 92093 net.cpp:573] relu3 -> conv3 (in-place)
I1226 13:18:08.833653 92093 net.cpp:228] Setting up relu3
I1226 13:18:08.833694 92093 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:18:08.833719 92093 net.cpp:243] Memory required for data: 234073856
I1226 13:18:08.833746 92093 layer_factory.hpp:114] Creating layer conv4
I1226 13:18:08.833832 92093 net.cpp:178] Creating Layer conv4
I1226 13:18:08.833866 92093 net.cpp:612] conv4 <- conv3
I1226 13:18:08.833906 92093 net.cpp:586] conv4 -> conv4
I1226 13:18:08.829840 93960 net.cpp:228] Setting up conv3
I1226 13:18:08.829957 93960 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:18:08.829987 93960 net.cpp:243] Memory required for data: 225767168
I1226 13:18:08.830087 93960 layer_factory.hpp:114] Creating layer relu3
I1226 13:18:08.830166 93960 net.cpp:178] Creating Layer relu3
I1226 13:18:08.830202 93960 net.cpp:612] relu3 <- conv3
I1226 13:18:08.830250 93960 net.cpp:573] relu3 -> conv3 (in-place)
I1226 13:18:08.830355 93960 net.cpp:228] Setting up relu3
I1226 13:18:08.830435 93960 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:18:08.830464 93960 net.cpp:243] Memory required for data: 234073856
I1226 13:18:08.830498 93960 layer_factory.hpp:114] Creating layer conv4
I1226 13:18:08.830601 93960 net.cpp:178] Creating Layer conv4
I1226 13:18:08.830646 93960 net.cpp:612] conv4 <- conv3
I1226 13:18:08.830691 93960 net.cpp:586] conv4 -> conv4
I1226 13:18:08.858805 91029 net.cpp:228] Setting up conv3
I1226 13:18:08.859230 91029 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:18:08.859339 91029 net.cpp:243] Memory required for data: 225767168
I1226 13:18:08.859449 91029 layer_factory.hpp:114] Creating layer relu3
I1226 13:18:08.859513 91029 net.cpp:178] Creating Layer relu3
I1226 13:18:08.859555 91029 net.cpp:612] relu3 <- conv3
I1226 13:18:08.859597 91029 net.cpp:573] relu3 -> conv3 (in-place)
I1226 13:18:08.859733 91029 net.cpp:228] Setting up relu3
I1226 13:18:08.859789 91029 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:18:08.859814 91029 net.cpp:243] Memory required for data: 234073856
I1226 13:18:08.859844 91029 layer_factory.hpp:114] Creating layer conv4
I1226 13:18:08.859966 91029 net.cpp:178] Creating Layer conv4
I1226 13:18:08.860005 91029 net.cpp:612] conv4 <- conv3
I1226 13:18:08.860182 91029 net.cpp:586] conv4 -> conv4
I1226 13:18:08.872517 98267 net.cpp:228] Setting up conv3
I1226 13:18:08.872660 98267 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:18:08.872694 98267 net.cpp:243] Memory required for data: 225767168
I1226 13:18:08.872776 98267 layer_factory.hpp:114] Creating layer relu3
I1226 13:18:08.872844 98267 net.cpp:178] Creating Layer relu3
I1226 13:18:08.872918 98267 net.cpp:612] relu3 <- conv3
I1226 13:18:08.872961 98267 net.cpp:573] relu3 -> conv3 (in-place)
I1226 13:18:08.873092 98267 net.cpp:228] Setting up relu3
I1226 13:18:08.873142 98267 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:18:08.873167 98267 net.cpp:243] Memory required for data: 234073856
I1226 13:18:08.873194 98267 layer_factory.hpp:114] Creating layer conv4
I1226 13:18:08.873317 98267 net.cpp:178] Creating Layer conv4
I1226 13:18:08.873360 98267 net.cpp:612] conv4 <- conv3
I1226 13:18:08.873409 98267 net.cpp:586] conv4 -> conv4
I1226 13:18:08.961720 85740 net.cpp:228] Setting up conv3
I1226 13:18:08.961833 85740 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:18:08.961863 85740 net.cpp:243] Memory required for data: 225767168
I1226 13:18:08.961973 85740 layer_factory.hpp:114] Creating layer relu3
I1226 13:18:08.962090 85740 net.cpp:178] Creating Layer relu3
I1226 13:18:08.962131 85740 net.cpp:612] relu3 <- conv3
I1226 13:18:08.962195 85740 net.cpp:573] relu3 -> conv3 (in-place)
I1226 13:18:08.962468 85740 net.cpp:228] Setting up relu3
I1226 13:18:08.962553 85740 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:18:08.962585 85740 net.cpp:243] Memory required for data: 234073856
I1226 13:18:08.962622 85740 layer_factory.hpp:114] Creating layer conv4
I1226 13:18:08.962769 85740 net.cpp:178] Creating Layer conv4
I1226 13:18:08.964460 85740 net.cpp:612] conv4 <- conv3
I1226 13:18:08.964613 85740 net.cpp:586] conv4 -> conv4
I1226 13:18:09.110354 93960 net.cpp:228] Setting up conv4
I1226 13:18:09.110998 93960 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:18:09.111083 93960 net.cpp:243] Memory required for data: 242380544
I1226 13:18:09.111178 93960 layer_factory.hpp:114] Creating layer relu4
I1226 13:18:09.111341 93960 net.cpp:178] Creating Layer relu4
I1226 13:18:09.111451 93960 net.cpp:612] relu4 <- conv4
I1226 13:18:09.111512 93960 net.cpp:573] relu4 -> conv4 (in-place)
I1226 13:18:09.111634 93960 net.cpp:228] Setting up relu4
I1226 13:18:09.111702 93960 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:18:09.111735 93960 net.cpp:243] Memory required for data: 250687232
I1226 13:18:09.111775 93960 layer_factory.hpp:114] Creating layer conv5
I1226 13:18:09.111908 93960 net.cpp:178] Creating Layer conv5
I1226 13:18:09.112123 93960 net.cpp:612] conv5 <- conv4
I1226 13:18:09.112201 93960 net.cpp:586] conv5 -> conv5
I1226 13:18:09.128859 92093 net.cpp:228] Setting up conv4
I1226 13:18:09.128968 92093 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:18:09.128994 92093 net.cpp:243] Memory required for data: 242380544
I1226 13:18:09.129053 92093 layer_factory.hpp:114] Creating layer relu4
I1226 13:18:09.129132 92093 net.cpp:178] Creating Layer relu4
I1226 13:18:09.129173 92093 net.cpp:612] relu4 <- conv4
I1226 13:18:09.129216 92093 net.cpp:573] relu4 -> conv4 (in-place)
I1226 13:18:09.129315 92093 net.cpp:228] Setting up relu4
I1226 13:18:09.129362 92093 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:18:09.129385 92093 net.cpp:243] Memory required for data: 250687232
I1226 13:18:09.129413 92093 layer_factory.hpp:114] Creating layer conv5
I1226 13:18:09.129516 92093 net.cpp:178] Creating Layer conv5
I1226 13:18:09.129547 92093 net.cpp:612] conv5 <- conv4
I1226 13:18:09.129597 92093 net.cpp:586] conv5 -> conv5
I1226 13:18:09.153347 91029 net.cpp:228] Setting up conv4
I1226 13:18:09.153460 91029 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:18:09.153487 91029 net.cpp:243] Memory required for data: 242380544
I1226 13:18:09.153544 91029 layer_factory.hpp:114] Creating layer relu4
I1226 13:18:09.153632 91029 net.cpp:178] Creating Layer relu4
I1226 13:18:09.153666 91029 net.cpp:612] relu4 <- conv4
I1226 13:18:09.153733 91029 net.cpp:573] relu4 -> conv4 (in-place)
I1226 13:18:09.153841 91029 net.cpp:228] Setting up relu4
I1226 13:18:09.153882 91029 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:18:09.153904 91029 net.cpp:243] Memory required for data: 250687232
I1226 13:18:09.153940 91029 layer_factory.hpp:114] Creating layer conv5
I1226 13:18:09.154021 91029 net.cpp:178] Creating Layer conv5
I1226 13:18:09.154057 91029 net.cpp:612] conv5 <- conv4
I1226 13:18:09.154112 91029 net.cpp:586] conv5 -> conv5
I1226 13:18:09.172843 98267 net.cpp:228] Setting up conv4
I1226 13:18:09.172955 98267 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:18:09.172986 98267 net.cpp:243] Memory required for data: 242380544
I1226 13:18:09.173048 98267 layer_factory.hpp:114] Creating layer relu4
I1226 13:18:09.173192 98267 net.cpp:178] Creating Layer relu4
I1226 13:18:09.173251 98267 net.cpp:612] relu4 <- conv4
I1226 13:18:09.173305 98267 net.cpp:573] relu4 -> conv4 (in-place)
I1226 13:18:09.173421 98267 net.cpp:228] Setting up relu4
I1226 13:18:09.173465 98267 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:18:09.173488 98267 net.cpp:243] Memory required for data: 250687232
I1226 13:18:09.173537 98267 layer_factory.hpp:114] Creating layer conv5
I1226 13:18:09.173641 98267 net.cpp:178] Creating Layer conv5
I1226 13:18:09.173859 98267 net.cpp:612] conv5 <- conv4
I1226 13:18:09.173941 98267 net.cpp:586] conv5 -> conv5
I1226 13:18:09.302507 93960 net.cpp:228] Setting up conv5
I1226 13:18:09.302860 93960 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 13:18:09.302932 93960 net.cpp:243] Memory required for data: 256225024
I1226 13:18:09.303056 93960 layer_factory.hpp:114] Creating layer relu5
I1226 13:18:09.303788 93960 net.cpp:178] Creating Layer relu5
I1226 13:18:09.313225 92093 net.cpp:228] Setting up conv5
I1226 13:18:09.303910 93960 net.cpp:612] relu5 <- conv5
I1226 13:18:09.313336 92093 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 13:18:09.313365 92093 net.cpp:243] Memory required for data: 256225024
I1226 13:18:09.303973 93960 net.cpp:573] relu5 -> conv5 (in-place)
I1226 13:18:09.313465 92093 layer_factory.hpp:114] Creating layer relu5
I1226 13:18:09.304299 93960 net.cpp:228] Setting up relu5
I1226 13:18:09.313580 92093 net.cpp:178] Creating Layer relu5
I1226 13:18:09.304414 93960 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 13:18:09.313637 92093 net.cpp:612] relu5 <- conv5
I1226 13:18:09.304455 93960 net.cpp:243] Memory required for data: 261762816
I1226 13:18:09.313773 92093 net.cpp:573] relu5 -> conv5 (in-place)
I1226 13:18:09.304693 93960 layer_factory.hpp:114] Creating layer pool5
I1226 13:18:09.313969 92093 net.cpp:228] Setting up relu5
I1226 13:18:09.304821 93960 net.cpp:178] Creating Layer pool5
I1226 13:18:09.314062 92093 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 13:18:09.305018 93960 net.cpp:612] pool5 <- conv5
I1226 13:18:09.314414 92093 net.cpp:243] Memory required for data: 261762816
I1226 13:18:09.305094 93960 net.cpp:586] pool5 -> pool5
I1226 13:18:09.314513 92093 layer_factory.hpp:114] Creating layer pool5
I1226 13:18:09.314692 92093 net.cpp:178] Creating Layer pool5
I1226 13:18:09.314736 92093 net.cpp:612] pool5 <- conv5
I1226 13:18:09.314793 92093 net.cpp:586] pool5 -> pool5
I1226 13:18:09.305624 93960 net.cpp:228] Setting up pool5
I1226 13:18:09.314904 92093 net.cpp:228] Setting up pool5
I1226 13:18:09.305784 93960 net.cpp:235] Top shape: 32 256 6 6 (294912)
I1226 13:18:09.314954 92093 net.cpp:235] Top shape: 32 256 6 6 (294912)
I1226 13:18:09.305871 93960 net.cpp:243] Memory required for data: 262942464
I1226 13:18:09.314981 92093 net.cpp:243] Memory required for data: 262942464
I1226 13:18:09.315011 92093 layer_factory.hpp:114] Creating layer fc6
I1226 13:18:09.305929 93960 layer_factory.hpp:114] Creating layer fc6
I1226 13:18:09.315066 92093 net.cpp:178] Creating Layer fc6
I1226 13:18:09.306012 93960 net.cpp:178] Creating Layer fc6
I1226 13:18:09.315119 92093 net.cpp:612] fc6 <- pool5
I1226 13:18:09.315158 92093 net.cpp:586] fc6 -> fc6
I1226 13:18:09.306049 93960 net.cpp:612] fc6 <- pool5
I1226 13:18:09.306107 93960 net.cpp:586] fc6 -> fc6
I1226 13:18:09.317863 85740 net.cpp:228] Setting up conv4
I1226 13:18:09.317996 85740 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:18:09.318032 85740 net.cpp:243] Memory required for data: 242380544
I1226 13:18:09.318112 85740 layer_factory.hpp:114] Creating layer relu4
I1226 13:18:09.318522 85740 net.cpp:178] Creating Layer relu4
I1226 13:18:09.318572 85740 net.cpp:612] relu4 <- conv4
I1226 13:18:09.318706 85740 net.cpp:573] relu4 -> conv4 (in-place)
I1226 13:18:09.318897 85740 net.cpp:228] Setting up relu4
I1226 13:18:09.319069 85740 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:18:09.319106 85740 net.cpp:243] Memory required for data: 250687232
I1226 13:18:09.319149 85740 layer_factory.hpp:114] Creating layer conv5
I1226 13:18:09.319269 85740 net.cpp:178] Creating Layer conv5
I1226 13:18:09.319329 85740 net.cpp:612] conv5 <- conv4
I1226 13:18:09.319404 85740 net.cpp:586] conv5 -> conv5
I1226 13:18:09.345934 91029 net.cpp:228] Setting up conv5
I1226 13:18:09.346048 91029 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 13:18:09.346076 91029 net.cpp:243] Memory required for data: 256225024
I1226 13:18:09.346530 91029 layer_factory.hpp:114] Creating layer relu5
I1226 13:18:09.346765 91029 net.cpp:178] Creating Layer relu5
I1226 13:18:09.346842 91029 net.cpp:612] relu5 <- conv5
I1226 13:18:09.346917 91029 net.cpp:573] relu5 -> conv5 (in-place)
I1226 13:18:09.347028 91029 net.cpp:228] Setting up relu5
I1226 13:18:09.347072 91029 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 13:18:09.347198 91029 net.cpp:243] Memory required for data: 261762816
I1226 13:18:09.347234 91029 layer_factory.hpp:114] Creating layer pool5
I1226 13:18:09.347313 91029 net.cpp:178] Creating Layer pool5
I1226 13:18:09.347429 91029 net.cpp:612] pool5 <- conv5
I1226 13:18:09.347498 91029 net.cpp:586] pool5 -> pool5
I1226 13:18:09.347646 91029 net.cpp:228] Setting up pool5
I1226 13:18:09.348170 91029 net.cpp:235] Top shape: 32 256 6 6 (294912)
I1226 13:18:09.348235 91029 net.cpp:243] Memory required for data: 262942464
I1226 13:18:09.348309 91029 layer_factory.hpp:114] Creating layer fc6
I1226 13:18:09.348386 91029 net.cpp:178] Creating Layer fc6
I1226 13:18:09.348446 91029 net.cpp:612] fc6 <- pool5
I1226 13:18:09.348490 91029 net.cpp:586] fc6 -> fc6
I1226 13:18:09.370263 98267 net.cpp:228] Setting up conv5
I1226 13:18:09.370373 98267 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 13:18:09.370420 98267 net.cpp:243] Memory required for data: 256225024
I1226 13:18:09.370710 98267 layer_factory.hpp:114] Creating layer relu5
I1226 13:18:09.370892 98267 net.cpp:178] Creating Layer relu5
I1226 13:18:09.370965 98267 net.cpp:612] relu5 <- conv5
I1226 13:18:09.371018 98267 net.cpp:573] relu5 -> conv5 (in-place)
I1226 13:18:09.371536 98267 net.cpp:228] Setting up relu5
I1226 13:18:09.371664 98267 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 13:18:09.371712 98267 net.cpp:243] Memory required for data: 261762816
I1226 13:18:09.371754 98267 layer_factory.hpp:114] Creating layer pool5
I1226 13:18:09.371845 98267 net.cpp:178] Creating Layer pool5
I1226 13:18:09.371886 98267 net.cpp:612] pool5 <- conv5
I1226 13:18:09.371942 98267 net.cpp:586] pool5 -> pool5
I1226 13:18:09.372092 98267 net.cpp:228] Setting up pool5
I1226 13:18:09.372174 98267 net.cpp:235] Top shape: 32 256 6 6 (294912)
I1226 13:18:09.372223 98267 net.cpp:243] Memory required for data: 262942464
I1226 13:18:09.372261 98267 layer_factory.hpp:114] Creating layer fc6
I1226 13:18:09.372324 98267 net.cpp:178] Creating Layer fc6
I1226 13:18:09.372354 98267 net.cpp:612] fc6 <- pool5
I1226 13:18:09.372395 98267 net.cpp:586] fc6 -> fc6
I1226 13:18:09.478978 85740 net.cpp:228] Setting up conv5
I1226 13:18:09.479095 85740 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 13:18:09.479127 85740 net.cpp:243] Memory required for data: 256225024
I1226 13:18:09.479244 85740 layer_factory.hpp:114] Creating layer relu5
I1226 13:18:09.479322 85740 net.cpp:178] Creating Layer relu5
I1226 13:18:09.479360 85740 net.cpp:612] relu5 <- conv5
I1226 13:18:09.479427 85740 net.cpp:573] relu5 -> conv5 (in-place)
I1226 13:18:09.479547 85740 net.cpp:228] Setting up relu5
I1226 13:18:09.479606 85740 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 13:18:09.479637 85740 net.cpp:243] Memory required for data: 261762816
I1226 13:18:09.479701 85740 layer_factory.hpp:114] Creating layer pool5
I1226 13:18:09.479790 85740 net.cpp:178] Creating Layer pool5
I1226 13:18:09.479838 85740 net.cpp:612] pool5 <- conv5
I1226 13:18:09.479888 85740 net.cpp:586] pool5 -> pool5
I1226 13:18:09.480000 85740 net.cpp:228] Setting up pool5
I1226 13:18:09.480072 85740 net.cpp:235] Top shape: 32 256 6 6 (294912)
I1226 13:18:09.480103 85740 net.cpp:243] Memory required for data: 262942464
I1226 13:18:09.480139 85740 layer_factory.hpp:114] Creating layer fc6
I1226 13:18:09.480216 85740 net.cpp:178] Creating Layer fc6
I1226 13:18:09.480257 85740 net.cpp:612] fc6 <- pool5
I1226 13:18:09.480307 85740 net.cpp:586] fc6 -> fc6
I1226 13:18:09.502512 94502 net.cpp:228] Setting up conv2
I1226 13:18:09.502634 94502 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 13:18:09.502663 94502 net.cpp:243] Memory required for data: 164146944
I1226 13:18:09.502742 94502 layer_factory.hpp:114] Creating layer relu2
I1226 13:18:09.502815 94502 net.cpp:178] Creating Layer relu2
I1226 13:18:09.502873 94502 net.cpp:612] relu2 <- conv2
I1226 13:18:09.502941 94502 net.cpp:573] relu2 -> conv2 (in-place)
I1226 13:18:09.503047 94502 net.cpp:228] Setting up relu2
I1226 13:18:09.503111 94502 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 13:18:09.503144 94502 net.cpp:243] Memory required for data: 188034816
I1226 13:18:09.503185 94502 layer_factory.hpp:114] Creating layer norm2
I1226 13:18:09.503247 94502 net.cpp:178] Creating Layer norm2
I1226 13:18:09.503284 94502 net.cpp:612] norm2 <- conv2
I1226 13:18:09.503327 94502 net.cpp:586] norm2 -> norm2
I1226 13:18:09.503420 94502 net.cpp:228] Setting up norm2
I1226 13:18:09.503473 94502 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 13:18:09.503499 94502 net.cpp:243] Memory required for data: 211922688
I1226 13:18:09.503538 94502 layer_factory.hpp:114] Creating layer pool2
I1226 13:18:09.503593 94502 net.cpp:178] Creating Layer pool2
I1226 13:18:09.503628 94502 net.cpp:612] pool2 <- norm2
I1226 13:18:09.503681 94502 net.cpp:586] pool2 -> pool2
I1226 13:18:09.503775 94502 net.cpp:228] Setting up pool2
I1226 13:18:09.503855 94502 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 13:18:09.503994 94502 net.cpp:243] Memory required for data: 217460480
I1226 13:18:09.504031 94502 layer_factory.hpp:114] Creating layer conv3
I1226 13:18:09.504128 94502 net.cpp:178] Creating Layer conv3
I1226 13:18:09.504168 94502 net.cpp:612] conv3 <- pool2
I1226 13:18:09.504233 94502 net.cpp:586] conv3 -> conv3
I1226 13:18:09.535778 96696 net.cpp:228] Setting up conv2
I1226 13:18:09.535898 96696 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 13:18:09.535929 96696 net.cpp:243] Memory required for data: 164146944
I1226 13:18:09.536005 96696 layer_factory.hpp:114] Creating layer relu2
I1226 13:18:09.536079 96696 net.cpp:178] Creating Layer relu2
I1226 13:18:09.536123 96696 net.cpp:612] relu2 <- conv2
I1226 13:18:09.536190 96696 net.cpp:573] relu2 -> conv2 (in-place)
I1226 13:18:09.536309 96696 net.cpp:228] Setting up relu2
I1226 13:18:09.536366 96696 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 13:18:09.536499 96696 net.cpp:243] Memory required for data: 188034816
I1226 13:18:09.536543 96696 layer_factory.hpp:114] Creating layer norm2
I1226 13:18:09.536600 96696 net.cpp:178] Creating Layer norm2
I1226 13:18:09.536628 96696 net.cpp:612] norm2 <- conv2
I1226 13:18:09.536675 96696 net.cpp:586] norm2 -> norm2
I1226 13:18:09.536808 96696 net.cpp:228] Setting up norm2
I1226 13:18:09.536869 96696 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 13:18:09.536893 96696 net.cpp:243] Memory required for data: 211922688
I1226 13:18:09.536926 96696 layer_factory.hpp:114] Creating layer pool2
I1226 13:18:09.536984 96696 net.cpp:178] Creating Layer pool2
I1226 13:18:09.537021 96696 net.cpp:612] pool2 <- norm2
I1226 13:18:09.537080 96696 net.cpp:586] pool2 -> pool2
I1226 13:18:09.537175 96696 net.cpp:228] Setting up pool2
I1226 13:18:09.537222 96696 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 13:18:09.537343 96696 net.cpp:243] Memory required for data: 217460480
I1226 13:18:09.537380 96696 layer_factory.hpp:114] Creating layer conv3
I1226 13:18:09.537468 96696 net.cpp:178] Creating Layer conv3
I1226 13:18:09.537508 96696 net.cpp:612] conv3 <- pool2
I1226 13:18:09.537564 96696 net.cpp:586] conv3 -> conv3
I1226 13:18:09.559952 96304 net.cpp:228] Setting up conv2
I1226 13:18:09.560075 96304 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 13:18:09.560111 96304 net.cpp:243] Memory required for data: 164146944
I1226 13:18:09.560209 96304 layer_factory.hpp:114] Creating layer relu2
I1226 13:18:09.560459 96304 net.cpp:178] Creating Layer relu2
I1226 13:18:09.560511 96304 net.cpp:612] relu2 <- conv2
I1226 13:18:09.560593 96304 net.cpp:573] relu2 -> conv2 (in-place)
I1226 13:18:09.560725 96304 net.cpp:228] Setting up relu2
I1226 13:18:09.560816 96304 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 13:18:09.560855 96304 net.cpp:243] Memory required for data: 188034816
I1226 13:18:09.560901 96304 layer_factory.hpp:114] Creating layer norm2
I1226 13:18:09.560973 96304 net.cpp:178] Creating Layer norm2
I1226 13:18:09.561022 96304 net.cpp:612] norm2 <- conv2
I1226 13:18:09.561079 96304 net.cpp:586] norm2 -> norm2
I1226 13:18:09.561223 96304 net.cpp:228] Setting up norm2
I1226 13:18:09.561290 96304 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 13:18:09.561326 96304 net.cpp:243] Memory required for data: 211922688
I1226 13:18:09.561365 96304 layer_factory.hpp:114] Creating layer pool2
I1226 13:18:09.561426 96304 net.cpp:178] Creating Layer pool2
I1226 13:18:09.561466 96304 net.cpp:612] pool2 <- norm2
I1226 13:18:09.561537 96304 net.cpp:586] pool2 -> pool2
I1226 13:18:09.561657 96304 net.cpp:228] Setting up pool2
I1226 13:18:09.561729 96304 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 13:18:09.561918 96304 net.cpp:243] Memory required for data: 217460480
I1226 13:18:09.561975 96304 layer_factory.hpp:114] Creating layer conv3
I1226 13:18:09.562093 96304 net.cpp:178] Creating Layer conv3
I1226 13:18:09.562150 96304 net.cpp:612] conv3 <- pool2
I1226 13:18:09.562229 96304 net.cpp:586] conv3 -> conv3
I1226 13:18:10.381356 94502 net.cpp:228] Setting up conv3
I1226 13:18:10.381474 94502 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:18:10.381503 94502 net.cpp:243] Memory required for data: 225767168
I1226 13:18:10.381605 94502 layer_factory.hpp:114] Creating layer relu3
I1226 13:18:10.381736 94502 net.cpp:178] Creating Layer relu3
I1226 13:18:10.381783 94502 net.cpp:612] relu3 <- conv3
I1226 13:18:10.381856 94502 net.cpp:573] relu3 -> conv3 (in-place)
I1226 13:18:10.381969 94502 net.cpp:228] Setting up relu3
I1226 13:18:10.382042 94502 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:18:10.382077 94502 net.cpp:243] Memory required for data: 234073856
I1226 13:18:10.382122 94502 layer_factory.hpp:114] Creating layer conv4
I1226 13:18:10.382215 94502 net.cpp:178] Creating Layer conv4
I1226 13:18:10.382254 94502 net.cpp:612] conv4 <- conv3
I1226 13:18:10.382302 94502 net.cpp:586] conv4 -> conv4
I1226 13:18:10.413938 96696 net.cpp:228] Setting up conv3
I1226 13:18:10.414075 96696 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:18:10.414114 96696 net.cpp:243] Memory required for data: 225767168
I1226 13:18:10.414228 96696 layer_factory.hpp:114] Creating layer relu3
I1226 13:18:10.414314 96696 net.cpp:178] Creating Layer relu3
I1226 13:18:10.414353 96696 net.cpp:612] relu3 <- conv3
I1226 13:18:10.414440 96696 net.cpp:573] relu3 -> conv3 (in-place)
I1226 13:18:10.414551 96696 net.cpp:228] Setting up relu3
I1226 13:18:10.414646 96696 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:18:10.414757 96696 net.cpp:243] Memory required for data: 234073856
I1226 13:18:10.414799 96696 layer_factory.hpp:114] Creating layer conv4
I1226 13:18:10.414909 96696 net.cpp:178] Creating Layer conv4
I1226 13:18:10.414983 96696 net.cpp:612] conv4 <- conv3
I1226 13:18:10.415329 96696 net.cpp:586] conv4 -> conv4
I1226 13:18:10.437153 96304 net.cpp:228] Setting up conv3
I1226 13:18:10.437283 96304 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:18:10.437325 96304 net.cpp:243] Memory required for data: 225767168
I1226 13:18:10.437458 96304 layer_factory.hpp:114] Creating layer relu3
I1226 13:18:10.437549 96304 net.cpp:178] Creating Layer relu3
I1226 13:18:10.437595 96304 net.cpp:612] relu3 <- conv3
I1226 13:18:10.437656 96304 net.cpp:573] relu3 -> conv3 (in-place)
I1226 13:18:10.437819 96304 net.cpp:228] Setting up relu3
I1226 13:18:10.437911 96304 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:18:10.437968 96304 net.cpp:243] Memory required for data: 234073856
I1226 13:18:10.438007 96304 layer_factory.hpp:114] Creating layer conv4
I1226 13:18:10.438127 96304 net.cpp:178] Creating Layer conv4
I1226 13:18:10.438177 96304 net.cpp:612] conv4 <- conv3
I1226 13:18:10.438248 96304 net.cpp:586] conv4 -> conv4
I1226 13:18:11.107771 94502 net.cpp:228] Setting up conv4
I1226 13:18:11.107911 94502 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:18:11.107944 94502 net.cpp:243] Memory required for data: 242380544
I1226 13:18:11.108011 94502 layer_factory.hpp:114] Creating layer relu4
I1226 13:18:11.108108 94502 net.cpp:178] Creating Layer relu4
I1226 13:18:11.108160 94502 net.cpp:612] relu4 <- conv4
I1226 13:18:11.108202 94502 net.cpp:573] relu4 -> conv4 (in-place)
I1226 13:18:11.108304 94502 net.cpp:228] Setting up relu4
I1226 13:18:11.108357 94502 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:18:11.108384 94502 net.cpp:243] Memory required for data: 250687232
I1226 13:18:11.108417 94502 layer_factory.hpp:114] Creating layer conv5
I1226 13:18:11.108516 94502 net.cpp:178] Creating Layer conv5
I1226 13:18:11.108566 94502 net.cpp:612] conv5 <- conv4
I1226 13:18:11.108623 94502 net.cpp:586] conv5 -> conv5
I1226 13:18:11.137792 96696 net.cpp:228] Setting up conv4
I1226 13:18:11.137908 96696 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:18:11.137940 96696 net.cpp:243] Memory required for data: 242380544
I1226 13:18:11.138001 96696 layer_factory.hpp:114] Creating layer relu4
I1226 13:18:11.138186 96696 net.cpp:178] Creating Layer relu4
I1226 13:18:11.138234 96696 net.cpp:612] relu4 <- conv4
I1226 13:18:11.138276 96696 net.cpp:573] relu4 -> conv4 (in-place)
I1226 13:18:11.138381 96696 net.cpp:228] Setting up relu4
I1226 13:18:11.138434 96696 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:18:11.138468 96696 net.cpp:243] Memory required for data: 250687232
I1226 13:18:11.138506 96696 layer_factory.hpp:114] Creating layer conv5
I1226 13:18:11.138609 96696 net.cpp:178] Creating Layer conv5
I1226 13:18:11.138648 96696 net.cpp:612] conv5 <- conv4
I1226 13:18:11.138695 96696 net.cpp:586] conv5 -> conv5
I1226 13:18:11.163193 96304 net.cpp:228] Setting up conv4
I1226 13:18:11.163322 96304 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:18:11.163363 96304 net.cpp:243] Memory required for data: 242380544
I1226 13:18:11.163446 96304 layer_factory.hpp:114] Creating layer relu4
I1226 13:18:11.163528 96304 net.cpp:178] Creating Layer relu4
I1226 13:18:11.163579 96304 net.cpp:612] relu4 <- conv4
I1226 13:18:11.163631 96304 net.cpp:573] relu4 -> conv4 (in-place)
I1226 13:18:11.163730 96304 net.cpp:228] Setting up relu4
I1226 13:18:11.163827 96304 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:18:11.163869 96304 net.cpp:243] Memory required for data: 250687232
I1226 13:18:11.163913 96304 layer_factory.hpp:114] Creating layer conv5
I1226 13:18:11.164018 96304 net.cpp:178] Creating Layer conv5
I1226 13:18:11.164075 96304 net.cpp:612] conv5 <- conv4
I1226 13:18:11.164136 96304 net.cpp:586] conv5 -> conv5
I1226 13:18:11.610710 94502 net.cpp:228] Setting up conv5
I1226 13:18:11.610851 94502 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 13:18:11.610884 94502 net.cpp:243] Memory required for data: 256225024
I1226 13:18:11.610970 94502 layer_factory.hpp:114] Creating layer relu5
I1226 13:18:11.611141 94502 net.cpp:178] Creating Layer relu5
I1226 13:18:11.611197 94502 net.cpp:612] relu5 <- conv5
I1226 13:18:11.611263 94502 net.cpp:573] relu5 -> conv5 (in-place)
I1226 13:18:11.611366 94502 net.cpp:228] Setting up relu5
I1226 13:18:11.611424 94502 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 13:18:11.611459 94502 net.cpp:243] Memory required for data: 261762816
I1226 13:18:11.611491 94502 layer_factory.hpp:114] Creating layer pool5
I1226 13:18:11.611546 94502 net.cpp:178] Creating Layer pool5
I1226 13:18:11.611587 94502 net.cpp:612] pool5 <- conv5
I1226 13:18:11.611644 94502 net.cpp:586] pool5 -> pool5
I1226 13:18:11.611749 94502 net.cpp:228] Setting up pool5
I1226 13:18:11.611801 94502 net.cpp:235] Top shape: 32 256 6 6 (294912)
I1226 13:18:11.611855 94502 net.cpp:243] Memory required for data: 262942464
I1226 13:18:11.611893 94502 layer_factory.hpp:114] Creating layer fc6
I1226 13:18:11.611969 94502 net.cpp:178] Creating Layer fc6
I1226 13:18:11.612006 94502 net.cpp:612] fc6 <- pool5
I1226 13:18:11.612051 94502 net.cpp:586] fc6 -> fc6
I1226 13:18:11.640035 96696 net.cpp:228] Setting up conv5
I1226 13:18:11.640156 96696 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 13:18:11.640189 96696 net.cpp:243] Memory required for data: 256225024
I1226 13:18:11.640272 96696 layer_factory.hpp:114] Creating layer relu5
I1226 13:18:11.640372 96696 net.cpp:178] Creating Layer relu5
I1226 13:18:11.640424 96696 net.cpp:612] relu5 <- conv5
I1226 13:18:11.640481 96696 net.cpp:573] relu5 -> conv5 (in-place)
I1226 13:18:11.640594 96696 net.cpp:228] Setting up relu5
I1226 13:18:11.640645 96696 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 13:18:11.640677 96696 net.cpp:243] Memory required for data: 261762816
I1226 13:18:11.640718 96696 layer_factory.hpp:114] Creating layer pool5
I1226 13:18:11.640825 96696 net.cpp:178] Creating Layer pool5
I1226 13:18:11.640861 96696 net.cpp:612] pool5 <- conv5
I1226 13:18:11.640909 96696 net.cpp:586] pool5 -> pool5
I1226 13:18:11.641022 96696 net.cpp:228] Setting up pool5
I1226 13:18:11.641075 96696 net.cpp:235] Top shape: 32 256 6 6 (294912)
I1226 13:18:11.641100 96696 net.cpp:243] Memory required for data: 262942464
I1226 13:18:11.641130 96696 layer_factory.hpp:114] Creating layer fc6
I1226 13:18:11.641212 96696 net.cpp:178] Creating Layer fc6
I1226 13:18:11.641247 96696 net.cpp:612] fc6 <- pool5
I1226 13:18:11.641301 96696 net.cpp:586] fc6 -> fc6
I1226 13:18:11.665156 96304 net.cpp:228] Setting up conv5
I1226 13:18:11.665283 96304 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 13:18:11.665323 96304 net.cpp:243] Memory required for data: 256225024
I1226 13:18:11.665431 96304 layer_factory.hpp:114] Creating layer relu5
I1226 13:18:11.665541 96304 net.cpp:178] Creating Layer relu5
I1226 13:18:11.665607 96304 net.cpp:612] relu5 <- conv5
I1226 13:18:11.665680 96304 net.cpp:573] relu5 -> conv5 (in-place)
I1226 13:18:11.665828 96304 net.cpp:228] Setting up relu5
I1226 13:18:11.665897 96304 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 13:18:11.665930 96304 net.cpp:243] Memory required for data: 261762816
I1226 13:18:11.665971 96304 layer_factory.hpp:114] Creating layer pool5
I1226 13:18:11.666065 96304 net.cpp:178] Creating Layer pool5
I1226 13:18:11.666111 96304 net.cpp:612] pool5 <- conv5
I1226 13:18:11.666162 96304 net.cpp:586] pool5 -> pool5
I1226 13:18:11.666283 96304 net.cpp:228] Setting up pool5
I1226 13:18:11.666347 96304 net.cpp:235] Top shape: 32 256 6 6 (294912)
I1226 13:18:11.666378 96304 net.cpp:243] Memory required for data: 262942464
I1226 13:18:11.666417 96304 layer_factory.hpp:114] Creating layer fc6
I1226 13:18:11.666512 96304 net.cpp:178] Creating Layer fc6
I1226 13:18:11.666558 96304 net.cpp:612] fc6 <- pool5
I1226 13:18:11.666612 96304 net.cpp:586] fc6 -> fc6
I1226 13:18:13.857357 91644 net.cpp:228] Setting up fc6
I1226 13:18:13.857465 91644 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:18:13.857491 91644 net.cpp:243] Memory required for data: 263466752
I1226 13:18:13.857547 91644 layer_factory.hpp:114] Creating layer relu6
I1226 13:18:13.857640 91644 net.cpp:178] Creating Layer relu6
I1226 13:18:13.857683 91644 net.cpp:612] relu6 <- fc6
I1226 13:18:13.857728 91644 net.cpp:573] relu6 -> fc6 (in-place)
I1226 13:18:13.857808 91644 net.cpp:228] Setting up relu6
I1226 13:18:13.857848 91644 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:18:13.857976 91644 net.cpp:243] Memory required for data: 263991040
I1226 13:18:13.858011 91644 layer_factory.hpp:114] Creating layer drop6
I1226 13:18:13.858078 91644 net.cpp:178] Creating Layer drop6
I1226 13:18:13.858110 91644 net.cpp:612] drop6 <- fc6
I1226 13:18:13.858146 91644 net.cpp:573] drop6 -> fc6 (in-place)
I1226 13:18:13.858199 91644 net.cpp:228] Setting up drop6
I1226 13:18:13.858239 91644 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:18:13.858265 91644 net.cpp:243] Memory required for data: 264515328
I1226 13:18:13.858292 91644 layer_factory.hpp:114] Creating layer fc7
I1226 13:18:13.858373 91644 net.cpp:178] Creating Layer fc7
I1226 13:18:13.858402 91644 net.cpp:612] fc7 <- fc6
I1226 13:18:13.858451 91644 net.cpp:586] fc7 -> fc7
I1226 13:18:14.447908 92093 net.cpp:228] Setting up fc6
I1226 13:18:14.448021 92093 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:18:14.448052 92093 net.cpp:243] Memory required for data: 263466752
I1226 13:18:14.448107 92093 layer_factory.hpp:114] Creating layer relu6
I1226 13:18:14.448187 92093 net.cpp:178] Creating Layer relu6
I1226 13:18:14.448304 92093 net.cpp:612] relu6 <- fc6
I1226 13:18:14.448357 92093 net.cpp:573] relu6 -> fc6 (in-place)
I1226 13:18:14.448446 92093 net.cpp:228] Setting up relu6
I1226 13:18:14.448643 92093 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:18:14.448683 92093 net.cpp:243] Memory required for data: 263991040
I1226 13:18:14.448716 92093 layer_factory.hpp:114] Creating layer drop6
I1226 13:18:14.448770 92093 net.cpp:178] Creating Layer drop6
I1226 13:18:14.448796 92093 net.cpp:612] drop6 <- fc6
I1226 13:18:14.448832 92093 net.cpp:573] drop6 -> fc6 (in-place)
I1226 13:18:14.448887 92093 net.cpp:228] Setting up drop6
I1226 13:18:14.448935 92093 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:18:14.448966 92093 net.cpp:243] Memory required for data: 264515328
I1226 13:18:14.448992 92093 layer_factory.hpp:114] Creating layer fc7
I1226 13:18:14.449065 92093 net.cpp:178] Creating Layer fc7
I1226 13:18:14.449100 92093 net.cpp:612] fc7 <- fc6
I1226 13:18:14.449142 92093 net.cpp:586] fc7 -> fc7
I1226 13:18:14.499027 91029 net.cpp:228] Setting up fc6
I1226 13:18:14.499141 91029 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:18:14.499172 91029 net.cpp:243] Memory required for data: 263466752
I1226 13:18:14.499255 91029 layer_factory.hpp:114] Creating layer relu6
I1226 13:18:14.499336 91029 net.cpp:178] Creating Layer relu6
I1226 13:18:14.499379 91029 net.cpp:612] relu6 <- fc6
I1226 13:18:14.499434 91029 net.cpp:573] relu6 -> fc6 (in-place)
I1226 13:18:14.499526 91029 net.cpp:228] Setting up relu6
I1226 13:18:14.499675 91029 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:18:14.499733 91029 net.cpp:243] Memory required for data: 263991040
I1226 13:18:14.499773 91029 layer_factory.hpp:114] Creating layer drop6
I1226 13:18:14.499838 91029 net.cpp:178] Creating Layer drop6
I1226 13:18:14.499867 91029 net.cpp:612] drop6 <- fc6
I1226 13:18:14.499905 91029 net.cpp:573] drop6 -> fc6 (in-place)
I1226 13:18:14.499969 91029 net.cpp:228] Setting up drop6
I1226 13:18:14.500007 91029 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:18:14.500036 91029 net.cpp:243] Memory required for data: 264515328
I1226 13:18:14.500062 91029 layer_factory.hpp:114] Creating layer fc7
I1226 13:18:14.500139 91029 net.cpp:178] Creating Layer fc7
I1226 13:18:14.500176 91029 net.cpp:612] fc7 <- fc6
I1226 13:18:14.500213 91029 net.cpp:586] fc7 -> fc7
I1226 13:18:14.496441 93960 net.cpp:228] Setting up fc6
I1226 13:18:14.496561 93960 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:18:14.496649 93960 net.cpp:243] Memory required for data: 263466752
I1226 13:18:14.496907 93960 layer_factory.hpp:114] Creating layer relu6
I1226 13:18:14.497113 93960 net.cpp:178] Creating Layer relu6
I1226 13:18:14.497167 93960 net.cpp:612] relu6 <- fc6
I1226 13:18:14.497249 93960 net.cpp:573] relu6 -> fc6 (in-place)
I1226 13:18:14.497411 93960 net.cpp:228] Setting up relu6
I1226 13:18:14.497620 93960 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:18:14.497967 93960 net.cpp:243] Memory required for data: 263991040
I1226 13:18:14.498019 93960 layer_factory.hpp:114] Creating layer drop6
I1226 13:18:14.498100 93960 net.cpp:178] Creating Layer drop6
I1226 13:18:14.498142 93960 net.cpp:612] drop6 <- fc6
I1226 13:18:14.498203 93960 net.cpp:573] drop6 -> fc6 (in-place)
I1226 13:18:14.498282 93960 net.cpp:228] Setting up drop6
I1226 13:18:14.498332 93960 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:18:14.498363 93960 net.cpp:243] Memory required for data: 264515328
I1226 13:18:14.498468 93960 layer_factory.hpp:114] Creating layer fc7
I1226 13:18:14.498571 93960 net.cpp:178] Creating Layer fc7
I1226 13:18:14.498621 93960 net.cpp:612] fc7 <- fc6
I1226 13:18:14.498680 93960 net.cpp:586] fc7 -> fc7
I1226 13:18:14.574439 98267 net.cpp:228] Setting up fc6
I1226 13:18:14.574550 98267 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:18:14.574580 98267 net.cpp:243] Memory required for data: 263466752
I1226 13:18:14.574640 98267 layer_factory.hpp:114] Creating layer relu6
I1226 13:18:14.574717 98267 net.cpp:178] Creating Layer relu6
I1226 13:18:14.574759 98267 net.cpp:612] relu6 <- fc6
I1226 13:18:14.574820 98267 net.cpp:573] relu6 -> fc6 (in-place)
I1226 13:18:14.574916 98267 net.cpp:228] Setting up relu6
I1226 13:18:14.575095 98267 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:18:14.575135 98267 net.cpp:243] Memory required for data: 263991040
I1226 13:18:14.575170 98267 layer_factory.hpp:114] Creating layer drop6
I1226 13:18:14.575232 98267 net.cpp:178] Creating Layer drop6
I1226 13:18:14.575265 98267 net.cpp:612] drop6 <- fc6
I1226 13:18:14.575304 98267 net.cpp:573] drop6 -> fc6 (in-place)
I1226 13:18:14.575371 98267 net.cpp:228] Setting up drop6
I1226 13:18:14.575422 98267 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:18:14.575448 98267 net.cpp:243] Memory required for data: 264515328
I1226 13:18:14.575476 98267 layer_factory.hpp:114] Creating layer fc7
I1226 13:18:14.575554 98267 net.cpp:178] Creating Layer fc7
I1226 13:18:14.575588 98267 net.cpp:612] fc7 <- fc6
I1226 13:18:14.575626 98267 net.cpp:586] fc7 -> fc7
I1226 13:18:16.136449 91644 net.cpp:228] Setting up fc7
I1226 13:18:16.136561 91644 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:18:16.136590 91644 net.cpp:243] Memory required for data: 265039616
I1226 13:18:16.136648 91644 layer_factory.hpp:114] Creating layer relu7
I1226 13:18:16.136729 91644 net.cpp:178] Creating Layer relu7
I1226 13:18:16.136761 91644 net.cpp:612] relu7 <- fc7
I1226 13:18:16.136806 91644 net.cpp:573] relu7 -> fc7 (in-place)
I1226 13:18:16.136898 91644 net.cpp:228] Setting up relu7
I1226 13:18:16.136948 91644 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:18:16.136978 91644 net.cpp:243] Memory required for data: 265563904
I1226 13:18:16.137007 91644 layer_factory.hpp:114] Creating layer drop7
I1226 13:18:16.137044 91644 net.cpp:178] Creating Layer drop7
I1226 13:18:16.137070 91644 net.cpp:612] drop7 <- fc7
I1226 13:18:16.137110 91644 net.cpp:573] drop7 -> fc7 (in-place)
I1226 13:18:16.137159 91644 net.cpp:228] Setting up drop7
I1226 13:18:16.137192 91644 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:18:16.137214 91644 net.cpp:243] Memory required for data: 266088192
I1226 13:18:16.137244 91644 layer_factory.hpp:114] Creating layer fc8
I1226 13:18:16.137346 91644 net.cpp:178] Creating Layer fc8
I1226 13:18:16.137387 91644 net.cpp:612] fc8 <- fc7
I1226 13:18:16.137425 91644 net.cpp:586] fc8 -> fc8
I1226 13:18:16.706287 91644 net.cpp:228] Setting up fc8
I1226 13:18:16.706429 91644 net.cpp:235] Top shape: 32 1000 (32000)
I1226 13:18:16.706455 91644 net.cpp:243] Memory required for data: 266216192
I1226 13:18:16.706537 91644 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 13:18:16.706627 91644 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 13:18:16.706661 91644 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 13:18:16.706712 91644 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 13:18:16.706773 91644 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 13:18:16.706863 91644 net.cpp:228] Setting up fc8_fc8_0_split
I1226 13:18:16.706912 91644 net.cpp:235] Top shape: 32 1000 (32000)
I1226 13:18:16.706943 91644 net.cpp:235] Top shape: 32 1000 (32000)
I1226 13:18:16.706965 91644 net.cpp:243] Memory required for data: 266472192
I1226 13:18:16.706998 91644 layer_factory.hpp:114] Creating layer accuracy
I1226 13:18:16.707056 91644 net.cpp:178] Creating Layer accuracy
I1226 13:18:16.707083 91644 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 13:18:16.707113 91644 net.cpp:612] accuracy <- label_data_1_split_0
I1226 13:18:16.707159 91644 net.cpp:586] accuracy -> accuracy
I1226 13:18:16.707212 91644 net.cpp:228] Setting up accuracy
I1226 13:18:16.707252 91644 net.cpp:235] Top shape: (1)
I1226 13:18:16.707273 91644 net.cpp:243] Memory required for data: 266472196
I1226 13:18:16.707298 91644 layer_factory.hpp:114] Creating layer loss
I1226 13:18:16.707365 91644 net.cpp:178] Creating Layer loss
I1226 13:18:16.707391 91644 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 13:18:16.707420 91644 net.cpp:612] loss <- label_data_1_split_1
I1226 13:18:16.707460 91644 net.cpp:586] loss -> loss
I1226 13:18:16.707521 91644 layer_factory.hpp:114] Creating layer loss
I1226 13:18:16.732900 91644 net.cpp:228] Setting up loss
I1226 13:18:16.733013 91644 net.cpp:235] Top shape: (1)
I1226 13:18:16.733050 91644 net.cpp:238]     with loss weight 1
I1226 13:18:16.733301 91644 net.cpp:243] Memory required for data: 266472200
I1226 13:18:16.733392 91644 net.cpp:305] loss needs backward computation.
I1226 13:18:16.733431 91644 net.cpp:307] accuracy does not need backward computation.
I1226 13:18:16.733464 91644 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 13:18:16.733492 91644 net.cpp:305] fc8 needs backward computation.
I1226 13:18:16.733520 91644 net.cpp:305] drop7 needs backward computation.
I1226 13:18:16.733549 91644 net.cpp:305] relu7 needs backward computation.
I1226 13:18:16.733582 91644 net.cpp:305] fc7 needs backward computation.
I1226 13:18:16.733613 91644 net.cpp:305] drop6 needs backward computation.
I1226 13:18:16.733642 91644 net.cpp:305] relu6 needs backward computation.
I1226 13:18:16.733672 91644 net.cpp:305] fc6 needs backward computation.
I1226 13:18:16.733703 91644 net.cpp:305] pool5 needs backward computation.
I1226 13:18:16.733752 91644 net.cpp:305] relu5 needs backward computation.
I1226 13:18:16.733783 91644 net.cpp:305] conv5 needs backward computation.
I1226 13:18:16.733814 91644 net.cpp:305] relu4 needs backward computation.
I1226 13:18:16.733844 91644 net.cpp:305] conv4 needs backward computation.
I1226 13:18:16.733882 91644 net.cpp:305] relu3 needs backward computation.
I1226 13:18:16.733912 91644 net.cpp:305] conv3 needs backward computation.
I1226 13:18:16.733952 91644 net.cpp:305] pool2 needs backward computation.
I1226 13:18:16.733984 91644 net.cpp:305] norm2 needs backward computation.
I1226 13:18:16.734025 91644 net.cpp:305] relu2 needs backward computation.
I1226 13:18:16.734055 91644 net.cpp:305] conv2 needs backward computation.
I1226 13:18:16.734092 91644 net.cpp:305] pool1 needs backward computation.
I1226 13:18:16.734163 91644 net.cpp:305] norm1 needs backward computation.
I1226 13:18:16.734196 91644 net.cpp:305] relu1 needs backward computation.
I1226 13:18:16.734225 91644 net.cpp:305] conv1 needs backward computation.
I1226 13:18:16.734259 91644 net.cpp:307] label_data_1_split does not need backward computation.
I1226 13:18:16.734299 91644 net.cpp:307] data does not need backward computation.
I1226 13:18:16.734347 91644 net.cpp:349] This network produces output accuracy
I1226 13:18:16.734385 91644 net.cpp:349] This network produces output loss
I1226 13:18:16.734472 91644 net.cpp:363] Network initialization done.
I1226 13:18:16.734923 91644 solver.cpp:107] Solver scaffolding done.
I1226 13:18:16.735131 91644 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 13:18:16.731884 92093 net.cpp:228] Setting up fc7
I1226 13:18:16.731995 92093 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:18:16.732022 92093 net.cpp:243] Memory required for data: 265039616
I1226 13:18:16.732105 92093 layer_factory.hpp:114] Creating layer relu7
I1226 13:18:16.732201 92093 net.cpp:178] Creating Layer relu7
I1226 13:18:16.732244 92093 net.cpp:612] relu7 <- fc7
I1226 13:18:16.732280 92093 net.cpp:573] relu7 -> fc7 (in-place)
I1226 13:18:16.732367 92093 net.cpp:228] Setting up relu7
I1226 13:18:16.732417 92093 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:18:16.732440 92093 net.cpp:243] Memory required for data: 265563904
I1226 13:18:16.732470 92093 layer_factory.hpp:114] Creating layer drop7
I1226 13:18:16.732553 92093 net.cpp:178] Creating Layer drop7
I1226 13:18:16.732583 92093 net.cpp:612] drop7 <- fc7
I1226 13:18:16.732620 92093 net.cpp:573] drop7 -> fc7 (in-place)
I1226 13:18:16.732676 92093 net.cpp:228] Setting up drop7
I1226 13:18:16.732717 92093 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:18:16.732739 92093 net.cpp:243] Memory required for data: 266088192
I1226 13:18:16.732765 92093 layer_factory.hpp:114] Creating layer fc8
I1226 13:18:16.732821 92093 net.cpp:178] Creating Layer fc8
I1226 13:18:16.732853 92093 net.cpp:612] fc8 <- fc7
I1226 13:18:16.732908 92093 net.cpp:586] fc8 -> fc8
I1226 13:18:16.762965 91029 net.cpp:228] Setting up fc7
I1226 13:18:16.763074 91029 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:18:16.763104 91029 net.cpp:243] Memory required for data: 265039616
I1226 13:18:16.763164 91029 layer_factory.hpp:114] Creating layer relu7
I1226 13:18:16.763263 91029 net.cpp:178] Creating Layer relu7
I1226 13:18:16.763366 91029 net.cpp:612] relu7 <- fc7
I1226 13:18:16.763407 91029 net.cpp:573] relu7 -> fc7 (in-place)
I1226 13:18:16.763494 91029 net.cpp:228] Setting up relu7
I1226 13:18:16.763540 91029 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:18:16.763566 91029 net.cpp:243] Memory required for data: 265563904
I1226 13:18:16.763594 91029 layer_factory.hpp:114] Creating layer drop7
I1226 13:18:16.763653 91029 net.cpp:178] Creating Layer drop7
I1226 13:18:16.763685 91029 net.cpp:612] drop7 <- fc7
I1226 13:18:16.763746 91029 net.cpp:573] drop7 -> fc7 (in-place)
I1226 13:18:16.763903 91029 net.cpp:228] Setting up drop7
I1226 13:18:16.763938 91029 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:18:16.763960 91029 net.cpp:243] Memory required for data: 266088192
I1226 13:18:16.763988 91029 layer_factory.hpp:114] Creating layer fc8
I1226 13:18:16.764050 91029 net.cpp:178] Creating Layer fc8
I1226 13:18:16.764081 91029 net.cpp:612] fc8 <- fc7
I1226 13:18:16.764117 91029 net.cpp:586] fc8 -> fc8
I1226 13:18:16.770936 85740 net.cpp:228] Setting up fc6
I1226 13:18:16.771054 85740 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:18:16.771090 85740 net.cpp:243] Memory required for data: 263466752
I1226 13:18:16.771170 85740 layer_factory.hpp:114] Creating layer relu6
I1226 13:18:16.771256 85740 net.cpp:178] Creating Layer relu6
I1226 13:18:16.771461 85740 net.cpp:612] relu6 <- fc6
I1226 13:18:16.771528 85740 net.cpp:573] relu6 -> fc6 (in-place)
I1226 13:18:16.771651 85740 net.cpp:228] Setting up relu6
I1226 13:18:16.771847 85740 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:18:16.771890 85740 net.cpp:243] Memory required for data: 263991040
I1226 13:18:16.771948 85740 layer_factory.hpp:114] Creating layer drop6
I1226 13:18:16.772016 85740 net.cpp:178] Creating Layer drop6
I1226 13:18:16.772053 85740 net.cpp:612] drop6 <- fc6
I1226 13:18:16.772114 85740 net.cpp:573] drop6 -> fc6 (in-place)
I1226 13:18:16.772195 85740 net.cpp:228] Setting up drop6
I1226 13:18:16.772245 85740 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:18:16.772274 85740 net.cpp:243] Memory required for data: 264515328
I1226 13:18:16.772305 85740 layer_factory.hpp:114] Creating layer fc7
I1226 13:18:16.772392 85740 net.cpp:178] Creating Layer fc7
I1226 13:18:16.772433 85740 net.cpp:612] fc7 <- fc6
I1226 13:18:16.772482 85740 net.cpp:586] fc7 -> fc7
I1226 13:18:16.779500 93960 net.cpp:228] Setting up fc7
I1226 13:18:16.779623 93960 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:18:16.779662 93960 net.cpp:243] Memory required for data: 265039616
I1226 13:18:16.779749 93960 layer_factory.hpp:114] Creating layer relu7
I1226 13:18:16.779865 93960 net.cpp:178] Creating Layer relu7
I1226 13:18:16.779923 93960 net.cpp:612] relu7 <- fc7
I1226 13:18:16.779975 93960 net.cpp:573] relu7 -> fc7 (in-place)
I1226 13:18:16.780092 93960 net.cpp:228] Setting up relu7
I1226 13:18:16.780149 93960 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:18:16.780184 93960 net.cpp:243] Memory required for data: 265563904
I1226 13:18:16.780226 93960 layer_factory.hpp:114] Creating layer drop7
I1226 13:18:16.780305 93960 net.cpp:178] Creating Layer drop7
I1226 13:18:16.780349 93960 net.cpp:612] drop7 <- fc7
I1226 13:18:16.780436 93960 net.cpp:573] drop7 -> fc7 (in-place)
I1226 13:18:16.780505 93960 net.cpp:228] Setting up drop7
I1226 13:18:16.780558 93960 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:18:16.780591 93960 net.cpp:243] Memory required for data: 266088192
I1226 13:18:16.780629 93960 layer_factory.hpp:114] Creating layer fc8
I1226 13:18:16.780709 93960 net.cpp:178] Creating Layer fc8
I1226 13:18:16.780791 93960 net.cpp:612] fc8 <- fc7
I1226 13:18:16.780844 93960 net.cpp:586] fc8 -> fc8
I1226 13:18:16.861111 98267 net.cpp:228] Setting up fc7
I1226 13:18:16.861219 98267 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:18:16.861249 98267 net.cpp:243] Memory required for data: 265039616
I1226 13:18:16.861307 98267 layer_factory.hpp:114] Creating layer relu7
I1226 13:18:16.861402 98267 net.cpp:178] Creating Layer relu7
I1226 13:18:16.861450 98267 net.cpp:612] relu7 <- fc7
I1226 13:18:16.861536 98267 net.cpp:573] relu7 -> fc7 (in-place)
I1226 13:18:16.861685 98267 net.cpp:228] Setting up relu7
I1226 13:18:16.861773 98267 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:18:16.861799 98267 net.cpp:243] Memory required for data: 265563904
I1226 13:18:16.861831 98267 layer_factory.hpp:114] Creating layer drop7
I1226 13:18:16.861891 98267 net.cpp:178] Creating Layer drop7
I1226 13:18:16.861927 98267 net.cpp:612] drop7 <- fc7
I1226 13:18:16.861970 98267 net.cpp:573] drop7 -> fc7 (in-place)
I1226 13:18:16.862027 98267 net.cpp:228] Setting up drop7
I1226 13:18:16.862105 98267 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:18:16.862134 98267 net.cpp:243] Memory required for data: 266088192
I1226 13:18:16.862164 98267 layer_factory.hpp:114] Creating layer fc8
I1226 13:18:16.862256 98267 net.cpp:178] Creating Layer fc8
I1226 13:18:16.862478 98267 net.cpp:612] fc8 <- fc7
I1226 13:18:16.862527 98267 net.cpp:586] fc8 -> fc8
I1226 13:18:17.302418 92093 net.cpp:228] Setting up fc8
I1226 13:18:17.302568 92093 net.cpp:235] Top shape: 32 1000 (32000)
I1226 13:18:17.302603 92093 net.cpp:243] Memory required for data: 266216192
I1226 13:18:17.302698 92093 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 13:18:17.302829 92093 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 13:18:17.302867 92093 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 13:18:17.302906 92093 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 13:18:17.302966 92093 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 13:18:17.303046 92093 net.cpp:228] Setting up fc8_fc8_0_split
I1226 13:18:17.303095 92093 net.cpp:235] Top shape: 32 1000 (32000)
I1226 13:18:17.303125 92093 net.cpp:235] Top shape: 32 1000 (32000)
I1226 13:18:17.303148 92093 net.cpp:243] Memory required for data: 266472192
I1226 13:18:17.303182 92093 layer_factory.hpp:114] Creating layer accuracy
I1226 13:18:17.303232 92093 net.cpp:178] Creating Layer accuracy
I1226 13:18:17.303263 92093 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 13:18:17.303300 92093 net.cpp:612] accuracy <- label_data_1_split_0
I1226 13:18:17.303333 92093 net.cpp:586] accuracy -> accuracy
I1226 13:18:17.303375 92093 net.cpp:228] Setting up accuracy
I1226 13:18:17.303409 92093 net.cpp:235] Top shape: (1)
I1226 13:18:17.303436 92093 net.cpp:243] Memory required for data: 266472196
I1226 13:18:17.303463 92093 layer_factory.hpp:114] Creating layer loss
I1226 13:18:17.303551 92093 net.cpp:178] Creating Layer loss
I1226 13:18:17.303592 92093 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 13:18:17.303622 92093 net.cpp:612] loss <- label_data_1_split_1
I1226 13:18:17.303655 92093 net.cpp:586] loss -> loss
I1226 13:18:17.303715 92093 layer_factory.hpp:114] Creating layer loss
I1226 13:18:17.316285 91029 net.cpp:228] Setting up fc8
I1226 13:18:17.316411 91029 net.cpp:235] Top shape: 32 1000 (32000)
I1226 13:18:17.316442 91029 net.cpp:243] Memory required for data: 266216192
I1226 13:18:17.316527 91029 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 13:18:17.316601 91029 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 13:18:17.316634 91029 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 13:18:17.316684 91029 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 13:18:17.316784 91029 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 13:18:17.316889 91029 net.cpp:228] Setting up fc8_fc8_0_split
I1226 13:18:17.316941 91029 net.cpp:235] Top shape: 32 1000 (32000)
I1226 13:18:17.316972 91029 net.cpp:235] Top shape: 32 1000 (32000)
I1226 13:18:17.316994 91029 net.cpp:243] Memory required for data: 266472192
I1226 13:18:17.317023 91029 layer_factory.hpp:114] Creating layer accuracy
I1226 13:18:17.317073 91029 net.cpp:178] Creating Layer accuracy
I1226 13:18:17.317107 91029 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 13:18:17.317138 91029 net.cpp:612] accuracy <- label_data_1_split_0
I1226 13:18:17.317173 91029 net.cpp:586] accuracy -> accuracy
I1226 13:18:17.317224 91029 net.cpp:228] Setting up accuracy
I1226 13:18:17.317262 91029 net.cpp:235] Top shape: (1)
I1226 13:18:17.317286 91029 net.cpp:243] Memory required for data: 266472196
I1226 13:18:17.317312 91029 layer_factory.hpp:114] Creating layer loss
I1226 13:18:17.317355 91029 net.cpp:178] Creating Layer loss
I1226 13:18:17.317387 91029 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 13:18:17.317431 91029 net.cpp:612] loss <- label_data_1_split_1
I1226 13:18:17.317468 91029 net.cpp:586] loss -> loss
I1226 13:18:17.317529 91029 layer_factory.hpp:114] Creating layer loss
I1226 13:18:17.328502 92093 net.cpp:228] Setting up loss
I1226 13:18:17.328615 92093 net.cpp:235] Top shape: (1)
I1226 13:18:17.328776 92093 net.cpp:238]     with loss weight 1
I1226 13:18:17.328920 92093 net.cpp:243] Memory required for data: 266472200
I1226 13:18:17.328963 92093 net.cpp:305] loss needs backward computation.
I1226 13:18:17.329006 92093 net.cpp:307] accuracy does not need backward computation.
I1226 13:18:17.329048 92093 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 13:18:17.329082 92093 net.cpp:305] fc8 needs backward computation.
I1226 13:18:17.329111 92093 net.cpp:305] drop7 needs backward computation.
I1226 13:18:17.329140 92093 net.cpp:305] relu7 needs backward computation.
I1226 13:18:17.329170 92093 net.cpp:305] fc7 needs backward computation.
I1226 13:18:17.329202 92093 net.cpp:305] drop6 needs backward computation.
I1226 13:18:17.329242 92093 net.cpp:305] relu6 needs backward computation.
I1226 13:18:17.329272 92093 net.cpp:305] fc6 needs backward computation.
I1226 13:18:17.329311 92093 net.cpp:305] pool5 needs backward computation.
I1226 13:18:17.329351 92093 net.cpp:305] relu5 needs backward computation.
I1226 13:18:17.329386 92093 net.cpp:305] conv5 needs backward computation.
I1226 13:18:17.329422 92093 net.cpp:305] relu4 needs backward computation.
I1226 13:18:17.329459 92093 net.cpp:305] conv4 needs backward computation.
I1226 13:18:17.329512 92093 net.cpp:305] relu3 needs backward computation.
I1226 13:18:17.329543 92093 net.cpp:305] conv3 needs backward computation.
I1226 13:18:17.329584 92093 net.cpp:305] pool2 needs backward computation.
I1226 13:18:17.329615 92093 net.cpp:305] norm2 needs backward computation.
I1226 13:18:17.329653 92093 net.cpp:305] relu2 needs backward computation.
I1226 13:18:17.329689 92093 net.cpp:305] conv2 needs backward computation.
I1226 13:18:17.329720 92093 net.cpp:305] pool1 needs backward computation.
I1226 13:18:17.329773 92093 net.cpp:305] norm1 needs backward computation.
I1226 13:18:17.329816 92093 net.cpp:305] relu1 needs backward computation.
I1226 13:18:17.329851 92093 net.cpp:305] conv1 needs backward computation.
I1226 13:18:17.329885 92093 net.cpp:307] label_data_1_split does not need backward computation.
I1226 13:18:17.329924 92093 net.cpp:307] data does not need backward computation.
I1226 13:18:17.329959 92093 net.cpp:349] This network produces output accuracy
I1226 13:18:17.330000 92093 net.cpp:349] This network produces output loss
I1226 13:18:17.330101 92093 net.cpp:363] Network initialization done.
I1226 13:18:17.330557 92093 solver.cpp:107] Solver scaffolding done.
I1226 13:18:17.330773 92093 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 13:18:17.343507 91029 net.cpp:228] Setting up loss
I1226 13:18:17.343617 91029 net.cpp:235] Top shape: (1)
I1226 13:18:17.343816 91029 net.cpp:238]     with loss weight 1
I1226 13:18:17.343971 91029 net.cpp:243] Memory required for data: 266472200
I1226 13:18:17.344022 91029 net.cpp:305] loss needs backward computation.
I1226 13:18:17.344065 91029 net.cpp:307] accuracy does not need backward computation.
I1226 13:18:17.344100 91029 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 13:18:17.344133 91029 net.cpp:305] fc8 needs backward computation.
I1226 13:18:17.344166 91029 net.cpp:305] drop7 needs backward computation.
I1226 13:18:17.344195 91029 net.cpp:305] relu7 needs backward computation.
I1226 13:18:17.344228 91029 net.cpp:305] fc7 needs backward computation.
I1226 13:18:17.344259 91029 net.cpp:305] drop6 needs backward computation.
I1226 13:18:17.344286 91029 net.cpp:305] relu6 needs backward computation.
I1226 13:18:17.344326 91029 net.cpp:305] fc6 needs backward computation.
I1226 13:18:17.344357 91029 net.cpp:305] pool5 needs backward computation.
I1226 13:18:17.344395 91029 net.cpp:305] relu5 needs backward computation.
I1226 13:18:17.344426 91029 net.cpp:305] conv5 needs backward computation.
I1226 13:18:17.344458 91029 net.cpp:305] relu4 needs backward computation.
I1226 13:18:17.344486 91029 net.cpp:305] conv4 needs backward computation.
I1226 13:18:17.344519 91029 net.cpp:305] relu3 needs backward computation.
I1226 13:18:17.344549 91029 net.cpp:305] conv3 needs backward computation.
I1226 13:18:17.344581 91029 net.cpp:305] pool2 needs backward computation.
I1226 13:18:17.344612 91029 net.cpp:305] norm2 needs backward computation.
I1226 13:18:17.344643 91029 net.cpp:305] relu2 needs backward computation.
I1226 13:18:17.344673 91029 net.cpp:305] conv2 needs backward computation.
I1226 13:18:17.344738 91029 net.cpp:305] pool1 needs backward computation.
I1226 13:18:17.344774 91029 net.cpp:305] norm1 needs backward computation.
I1226 13:18:17.344827 91029 net.cpp:305] relu1 needs backward computation.
I1226 13:18:17.344867 91029 net.cpp:305] conv1 needs backward computation.
I1226 13:18:17.344900 91029 net.cpp:307] label_data_1_split does not need backward computation.
I1226 13:18:17.344939 91029 net.cpp:307] data does not need backward computation.
I1226 13:18:17.344969 91029 net.cpp:349] This network produces output accuracy
I1226 13:18:17.345011 91029 net.cpp:349] This network produces output loss
I1226 13:18:17.345100 91029 net.cpp:363] Network initialization done.
I1226 13:18:17.345530 91029 solver.cpp:107] Solver scaffolding done.
I1226 13:18:17.345760 91029 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 13:18:17.359508 93960 net.cpp:228] Setting up fc8
I1226 13:18:17.359676 93960 net.cpp:235] Top shape: 32 1000 (32000)
I1226 13:18:17.359719 93960 net.cpp:243] Memory required for data: 266216192
I1226 13:18:17.359825 93960 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 13:18:17.359907 93960 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 13:18:17.360010 93960 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 13:18:17.360066 93960 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 13:18:17.360121 93960 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 13:18:17.360234 93960 net.cpp:228] Setting up fc8_fc8_0_split
I1226 13:18:17.360301 93960 net.cpp:235] Top shape: 32 1000 (32000)
I1226 13:18:17.360344 93960 net.cpp:235] Top shape: 32 1000 (32000)
I1226 13:18:17.360400 93960 net.cpp:243] Memory required for data: 266472192
I1226 13:18:17.360448 93960 layer_factory.hpp:114] Creating layer accuracy
I1226 13:18:17.360515 93960 net.cpp:178] Creating Layer accuracy
I1226 13:18:17.360559 93960 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 13:18:17.360601 93960 net.cpp:612] accuracy <- label_data_1_split_0
I1226 13:18:17.360647 93960 net.cpp:586] accuracy -> accuracy
I1226 13:18:17.360714 93960 net.cpp:228] Setting up accuracy
I1226 13:18:17.360769 93960 net.cpp:235] Top shape: (1)
I1226 13:18:17.360798 93960 net.cpp:243] Memory required for data: 266472196
I1226 13:18:17.360838 93960 layer_factory.hpp:114] Creating layer loss
I1226 13:18:17.360926 93960 net.cpp:178] Creating Layer loss
I1226 13:18:17.360970 93960 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 13:18:17.361034 93960 net.cpp:612] loss <- label_data_1_split_1
I1226 13:18:17.361096 93960 net.cpp:586] loss -> loss
I1226 13:18:17.361187 93960 layer_factory.hpp:114] Creating layer loss
I1226 13:18:17.392217 93960 net.cpp:228] Setting up loss
I1226 13:18:17.392331 93960 net.cpp:235] Top shape: (1)
I1226 13:18:17.392534 93960 net.cpp:238]     with loss weight 1
I1226 13:18:17.392689 93960 net.cpp:243] Memory required for data: 266472200
I1226 13:18:17.392741 93960 net.cpp:305] loss needs backward computation.
I1226 13:18:17.392786 93960 net.cpp:307] accuracy does not need backward computation.
I1226 13:18:17.392824 93960 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 13:18:17.392859 93960 net.cpp:305] fc8 needs backward computation.
I1226 13:18:17.392901 93960 net.cpp:305] drop7 needs backward computation.
I1226 13:18:17.392935 93960 net.cpp:305] relu7 needs backward computation.
I1226 13:18:17.392966 93960 net.cpp:305] fc7 needs backward computation.
I1226 13:18:17.392999 93960 net.cpp:305] drop6 needs backward computation.
I1226 13:18:17.393029 93960 net.cpp:305] relu6 needs backward computation.
I1226 13:18:17.393061 93960 net.cpp:305] fc6 needs backward computation.
I1226 13:18:17.393092 93960 net.cpp:305] pool5 needs backward computation.
I1226 13:18:17.393123 93960 net.cpp:305] relu5 needs backward computation.
I1226 13:18:17.393153 93960 net.cpp:305] conv5 needs backward computation.
I1226 13:18:17.393183 93960 net.cpp:305] relu4 needs backward computation.
I1226 13:18:17.393215 93960 net.cpp:305] conv4 needs backward computation.
I1226 13:18:17.393245 93960 net.cpp:305] relu3 needs backward computation.
I1226 13:18:17.393285 93960 net.cpp:305] conv3 needs backward computation.
I1226 13:18:17.393316 93960 net.cpp:305] pool2 needs backward computation.
I1226 13:18:17.393348 93960 net.cpp:305] norm2 needs backward computation.
I1226 13:18:17.393429 93960 net.cpp:305] relu2 needs backward computation.
I1226 13:18:17.393472 93960 net.cpp:305] conv2 needs backward computation.
I1226 13:18:17.393512 93960 net.cpp:305] pool1 needs backward computation.
I1226 13:18:17.393554 93960 net.cpp:305] norm1 needs backward computation.
I1226 13:18:17.393586 93960 net.cpp:305] relu1 needs backward computation.
I1226 13:18:17.393615 93960 net.cpp:305] conv1 needs backward computation.
I1226 13:18:17.393651 93960 net.cpp:307] label_data_1_split does not need backward computation.
I1226 13:18:17.393707 93960 net.cpp:307] data does not need backward computation.
I1226 13:18:17.393743 93960 net.cpp:349] This network produces output accuracy
I1226 13:18:17.393779 93960 net.cpp:349] This network produces output loss
I1226 13:18:17.393864 93960 net.cpp:363] Network initialization done.
I1226 13:18:17.394299 93960 solver.cpp:107] Solver scaffolding done.
I1226 13:18:17.394541 93960 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 13:18:17.416582 98267 net.cpp:228] Setting up fc8
I1226 13:18:17.416705 98267 net.cpp:235] Top shape: 32 1000 (32000)
I1226 13:18:17.416733 98267 net.cpp:243] Memory required for data: 266216192
I1226 13:18:17.416786 98267 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 13:18:17.416854 98267 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 13:18:17.416889 98267 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 13:18:17.416929 98267 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 13:18:17.416993 98267 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 13:18:17.417136 98267 net.cpp:228] Setting up fc8_fc8_0_split
I1226 13:18:17.417192 98267 net.cpp:235] Top shape: 32 1000 (32000)
I1226 13:18:17.417227 98267 net.cpp:235] Top shape: 32 1000 (32000)
I1226 13:18:17.417249 98267 net.cpp:243] Memory required for data: 266472192
I1226 13:18:17.417280 98267 layer_factory.hpp:114] Creating layer accuracy
I1226 13:18:17.417336 98267 net.cpp:178] Creating Layer accuracy
I1226 13:18:17.417366 98267 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 13:18:17.417397 98267 net.cpp:612] accuracy <- label_data_1_split_0
I1226 13:18:17.417429 98267 net.cpp:586] accuracy -> accuracy
I1226 13:18:17.417477 98267 net.cpp:228] Setting up accuracy
I1226 13:18:17.417512 98267 net.cpp:235] Top shape: (1)
I1226 13:18:17.417536 98267 net.cpp:243] Memory required for data: 266472196
I1226 13:18:17.417563 98267 layer_factory.hpp:114] Creating layer loss
I1226 13:18:17.417610 98267 net.cpp:178] Creating Layer loss
I1226 13:18:17.417636 98267 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 13:18:17.417681 98267 net.cpp:612] loss <- label_data_1_split_1
I1226 13:18:17.417719 98267 net.cpp:586] loss -> loss
I1226 13:18:17.417781 98267 layer_factory.hpp:114] Creating layer loss
I1226 13:18:17.448660 98267 net.cpp:228] Setting up loss
I1226 13:18:17.448776 98267 net.cpp:235] Top shape: (1)
I1226 13:18:17.448925 98267 net.cpp:238]     with loss weight 1
I1226 13:18:17.449216 98267 net.cpp:243] Memory required for data: 266472200
I1226 13:18:17.449270 98267 net.cpp:305] loss needs backward computation.
I1226 13:18:17.449309 98267 net.cpp:307] accuracy does not need backward computation.
I1226 13:18:17.449342 98267 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 13:18:17.449373 98267 net.cpp:305] fc8 needs backward computation.
I1226 13:18:17.449401 98267 net.cpp:305] drop7 needs backward computation.
I1226 13:18:17.449430 98267 net.cpp:305] relu7 needs backward computation.
I1226 13:18:17.449460 98267 net.cpp:305] fc7 needs backward computation.
I1226 13:18:17.449491 98267 net.cpp:305] drop6 needs backward computation.
I1226 13:18:17.449524 98267 net.cpp:305] relu6 needs backward computation.
I1226 13:18:17.449554 98267 net.cpp:305] fc6 needs backward computation.
I1226 13:18:17.449589 98267 net.cpp:305] pool5 needs backward computation.
I1226 13:18:17.449626 98267 net.cpp:305] relu5 needs backward computation.
I1226 13:18:17.449661 98267 net.cpp:305] conv5 needs backward computation.
I1226 13:18:17.449693 98267 net.cpp:305] relu4 needs backward computation.
I1226 13:18:17.449723 98267 net.cpp:305] conv4 needs backward computation.
I1226 13:18:17.449755 98267 net.cpp:305] relu3 needs backward computation.
I1226 13:18:17.449786 98267 net.cpp:305] conv3 needs backward computation.
I1226 13:18:17.449818 98267 net.cpp:305] pool2 needs backward computation.
I1226 13:18:17.449851 98267 net.cpp:305] norm2 needs backward computation.
I1226 13:18:17.449882 98267 net.cpp:305] relu2 needs backward computation.
I1226 13:18:17.449913 98267 net.cpp:305] conv2 needs backward computation.
I1226 13:18:17.449945 98267 net.cpp:305] pool1 needs backward computation.
I1226 13:18:17.449977 98267 net.cpp:305] norm1 needs backward computation.
I1226 13:18:17.450037 98267 net.cpp:305] relu1 needs backward computation.
I1226 13:18:17.450093 98267 net.cpp:305] conv1 needs backward computation.
I1226 13:18:17.450132 98267 net.cpp:307] label_data_1_split does not need backward computation.
I1226 13:18:17.450166 98267 net.cpp:307] data does not need backward computation.
I1226 13:18:17.450196 98267 net.cpp:349] This network produces output accuracy
I1226 13:18:17.450232 98267 net.cpp:349] This network produces output loss
I1226 13:18:17.450327 98267 net.cpp:363] Network initialization done.
I1226 13:18:17.450778 98267 solver.cpp:107] Solver scaffolding done.
I1226 13:18:17.450995 98267 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 13:18:20.035356 85740 net.cpp:228] Setting up fc7
I1226 13:18:20.035477 85740 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:18:20.035511 85740 net.cpp:243] Memory required for data: 265039616
I1226 13:18:20.035590 85740 layer_factory.hpp:114] Creating layer relu7
I1226 13:18:20.035725 85740 net.cpp:178] Creating Layer relu7
I1226 13:18:20.035780 85740 net.cpp:612] relu7 <- fc7
I1226 13:18:20.035845 85740 net.cpp:573] relu7 -> fc7 (in-place)
I1226 13:18:20.035953 85740 net.cpp:228] Setting up relu7
I1226 13:18:20.036007 85740 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:18:20.036038 85740 net.cpp:243] Memory required for data: 265563904
I1226 13:18:20.036077 85740 layer_factory.hpp:114] Creating layer drop7
I1226 13:18:20.036155 85740 net.cpp:178] Creating Layer drop7
I1226 13:18:20.036195 85740 net.cpp:612] drop7 <- fc7
I1226 13:18:20.036244 85740 net.cpp:573] drop7 -> fc7 (in-place)
I1226 13:18:20.036311 85740 net.cpp:228] Setting up drop7
I1226 13:18:20.036357 85740 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:18:20.036389 85740 net.cpp:243] Memory required for data: 266088192
I1226 13:18:20.036425 85740 layer_factory.hpp:114] Creating layer fc8
I1226 13:18:20.036509 85740 net.cpp:178] Creating Layer fc8
I1226 13:18:20.036545 85740 net.cpp:612] fc8 <- fc7
I1226 13:18:20.036597 85740 net.cpp:586] fc8 -> fc8
I1226 13:18:20.672031 93960 caffe.cpp:376] Configuring multinode setup
I1226 13:18:20.673555 93960 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 13:18:20.794473 91644 caffe.cpp:376] Configuring multinode setup
I1226 13:18:20.796735 91644 caffe.cpp:386] Starting parameter server in mpi environment
I1226 13:18:20.830943 85740 net.cpp:228] Setting up fc8
I1226 13:18:20.831084 85740 net.cpp:235] Top shape: 32 1000 (32000)
I1226 13:18:20.831125 85740 net.cpp:243] Memory required for data: 266216192
I1226 13:18:20.831202 85740 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 13:18:20.831284 85740 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 13:18:20.831328 85740 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 13:18:20.831375 85740 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 13:18:20.831436 85740 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 13:18:20.831552 85740 net.cpp:228] Setting up fc8_fc8_0_split
I1226 13:18:20.831612 85740 net.cpp:235] Top shape: 32 1000 (32000)
I1226 13:18:20.831650 85740 net.cpp:235] Top shape: 32 1000 (32000)
I1226 13:18:20.831713 85740 net.cpp:243] Memory required for data: 266472192
I1226 13:18:20.831758 85740 layer_factory.hpp:114] Creating layer accuracy
I1226 13:18:20.831825 85740 net.cpp:178] Creating Layer accuracy
I1226 13:18:20.831861 85740 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 13:18:20.831902 85740 net.cpp:612] accuracy <- label_data_1_split_0
I1226 13:18:20.831946 85740 net.cpp:586] accuracy -> accuracy
I1226 13:18:20.832005 85740 net.cpp:228] Setting up accuracy
I1226 13:18:20.832068 85740 net.cpp:235] Top shape: (1)
I1226 13:18:20.832099 85740 net.cpp:243] Memory required for data: 266472196
I1226 13:18:20.832140 85740 layer_factory.hpp:114] Creating layer loss
I1226 13:18:20.832216 85740 net.cpp:178] Creating Layer loss
I1226 13:18:20.832259 85740 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 13:18:20.832320 85740 net.cpp:612] loss <- label_data_1_split_1
I1226 13:18:20.832373 85740 net.cpp:586] loss -> loss
I1226 13:18:20.832466 85740 layer_factory.hpp:114] Creating layer loss
I1226 13:18:20.859086 85740 net.cpp:228] Setting up loss
I1226 13:18:20.859195 85740 net.cpp:235] Top shape: (1)
I1226 13:18:20.859366 85740 net.cpp:238]     with loss weight 1
I1226 13:18:20.859519 85740 net.cpp:243] Memory required for data: 266472200
I1226 13:18:20.859565 85740 net.cpp:305] loss needs backward computation.
I1226 13:18:20.859607 85740 net.cpp:307] accuracy does not need backward computation.
I1226 13:18:20.859642 85740 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 13:18:20.859712 85740 net.cpp:305] fc8 needs backward computation.
I1226 13:18:20.859746 85740 net.cpp:305] drop7 needs backward computation.
I1226 13:18:20.859777 85740 net.cpp:305] relu7 needs backward computation.
I1226 13:18:20.859817 85740 net.cpp:305] fc7 needs backward computation.
I1226 13:18:20.859848 85740 net.cpp:305] drop6 needs backward computation.
I1226 13:18:20.859877 85740 net.cpp:305] relu6 needs backward computation.
I1226 13:18:20.859906 85740 net.cpp:305] fc6 needs backward computation.
I1226 13:18:20.859935 85740 net.cpp:305] pool5 needs backward computation.
I1226 13:18:20.859973 85740 net.cpp:305] relu5 needs backward computation.
I1226 13:18:20.860003 85740 net.cpp:305] conv5 needs backward computation.
I1226 13:18:20.860039 85740 net.cpp:305] relu4 needs backward computation.
I1226 13:18:20.860069 85740 net.cpp:305] conv4 needs backward computation.
I1226 13:18:20.860100 85740 net.cpp:305] relu3 needs backward computation.
I1226 13:18:20.860129 85740 net.cpp:305] conv3 needs backward computation.
I1226 13:18:20.860169 85740 net.cpp:305] pool2 needs backward computation.
I1226 13:18:20.860206 85740 net.cpp:305] norm2 needs backward computation.
I1226 13:18:20.860237 85740 net.cpp:305] relu2 needs backward computation.
I1226 13:18:20.860272 85740 net.cpp:305] conv2 needs backward computation.
I1226 13:18:20.860302 85740 net.cpp:305] pool1 needs backward computation.
I1226 13:18:20.860339 85740 net.cpp:305] norm1 needs backward computation.
I1226 13:18:20.860368 85740 net.cpp:305] relu1 needs backward computation.
I1226 13:18:20.860397 85740 net.cpp:305] conv1 needs backward computation.
I1226 13:18:20.860453 85740 net.cpp:307] label_data_1_split does not need backward computation.
I1226 13:18:20.860491 85740 net.cpp:307] data does not need backward computation.
I1226 13:18:20.860518 85740 net.cpp:349] This network produces output accuracy
I1226 13:18:20.860553 85740 net.cpp:349] This network produces output loss
I1226 13:18:20.860641 85740 net.cpp:363] Network initialization done.
I1226 13:18:20.861094 85740 solver.cpp:107] Solver scaffolding done.
I1226 13:18:20.861304 85740 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 13:18:21.387545 91029 caffe.cpp:376] Configuring multinode setup
I1226 13:18:21.389111 91029 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 13:18:21.394212 92093 caffe.cpp:376] Configuring multinode setup
I1226 13:18:21.395985 92093 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 13:18:21.539729 98267 caffe.cpp:376] Configuring multinode setup
I1226 13:18:21.541254 98267 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 13:18:24.111063 85740 caffe.cpp:376] Configuring multinode setup
I1226 13:18:24.112582 85740 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 13:18:41.010046 94502 net.cpp:228] Setting up fc6
I1226 13:18:41.010318 94502 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:18:41.010361 94502 net.cpp:243] Memory required for data: 263466752
I1226 13:18:41.010428 94502 layer_factory.hpp:114] Creating layer relu6
I1226 13:18:41.010519 94502 net.cpp:178] Creating Layer relu6
I1226 13:18:41.010567 94502 net.cpp:612] relu6 <- fc6
I1226 13:18:41.010620 94502 net.cpp:573] relu6 -> fc6 (in-place)
I1226 13:18:41.010727 94502 net.cpp:228] Setting up relu6
I1226 13:18:41.010793 94502 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:18:41.010841 94502 net.cpp:243] Memory required for data: 263991040
I1226 13:18:41.010881 94502 layer_factory.hpp:114] Creating layer drop6
I1226 13:18:41.010941 94502 net.cpp:178] Creating Layer drop6
I1226 13:18:41.010984 94502 net.cpp:612] drop6 <- fc6
I1226 13:18:41.011039 94502 net.cpp:573] drop6 -> fc6 (in-place)
I1226 13:18:41.011107 94502 net.cpp:228] Setting up drop6
I1226 13:18:41.011150 94502 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:18:41.011184 94502 net.cpp:243] Memory required for data: 264515328
I1226 13:18:41.011214 94502 layer_factory.hpp:114] Creating layer fc7
I1226 13:18:41.011302 94502 net.cpp:178] Creating Layer fc7
I1226 13:18:41.011343 94502 net.cpp:612] fc7 <- fc6
I1226 13:18:41.011386 94502 net.cpp:586] fc7 -> fc7
I1226 13:18:41.058398 96696 net.cpp:228] Setting up fc6
I1226 13:18:41.058656 96696 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:18:41.058703 96696 net.cpp:243] Memory required for data: 263466752
I1226 13:18:41.058802 96696 layer_factory.hpp:114] Creating layer relu6
I1226 13:18:41.058980 96696 net.cpp:178] Creating Layer relu6
I1226 13:18:41.059031 96696 net.cpp:612] relu6 <- fc6
I1226 13:18:41.059073 96696 net.cpp:573] relu6 -> fc6 (in-place)
I1226 13:18:41.059180 96696 net.cpp:228] Setting up relu6
I1226 13:18:41.059228 96696 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:18:41.059253 96696 net.cpp:243] Memory required for data: 263991040
I1226 13:18:41.059285 96696 layer_factory.hpp:114] Creating layer drop6
I1226 13:18:41.059341 96696 net.cpp:178] Creating Layer drop6
I1226 13:18:41.059387 96696 net.cpp:612] drop6 <- fc6
I1226 13:18:41.059429 96696 net.cpp:573] drop6 -> fc6 (in-place)
I1226 13:18:41.059494 96696 net.cpp:228] Setting up drop6
I1226 13:18:41.059541 96696 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:18:41.059567 96696 net.cpp:243] Memory required for data: 264515328
I1226 13:18:41.059598 96696 layer_factory.hpp:114] Creating layer fc7
I1226 13:18:41.059684 96696 net.cpp:178] Creating Layer fc7
I1226 13:18:41.059746 96696 net.cpp:612] fc7 <- fc6
I1226 13:18:41.059798 96696 net.cpp:586] fc7 -> fc7
I1226 13:18:41.091333 96304 net.cpp:228] Setting up fc6
I1226 13:18:41.091670 96304 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:18:41.091770 96304 net.cpp:243] Memory required for data: 263466752
I1226 13:18:41.092054 96304 layer_factory.hpp:114] Creating layer relu6
I1226 13:18:41.092345 96304 net.cpp:178] Creating Layer relu6
I1226 13:18:41.092512 96304 net.cpp:612] relu6 <- fc6
I1226 13:18:41.092567 96304 net.cpp:573] relu6 -> fc6 (in-place)
I1226 13:18:41.092710 96304 net.cpp:228] Setting up relu6
I1226 13:18:41.092808 96304 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:18:41.092849 96304 net.cpp:243] Memory required for data: 263991040
I1226 13:18:41.092895 96304 layer_factory.hpp:114] Creating layer drop6
I1226 13:18:41.092965 96304 net.cpp:178] Creating Layer drop6
I1226 13:18:41.093014 96304 net.cpp:612] drop6 <- fc6
I1226 13:18:41.093065 96304 net.cpp:573] drop6 -> fc6 (in-place)
I1226 13:18:41.093142 96304 net.cpp:228] Setting up drop6
I1226 13:18:41.093199 96304 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:18:41.093231 96304 net.cpp:243] Memory required for data: 264515328
I1226 13:18:41.093267 96304 layer_factory.hpp:114] Creating layer fc7
I1226 13:18:41.093369 96304 net.cpp:178] Creating Layer fc7
I1226 13:18:41.093420 96304 net.cpp:612] fc7 <- fc6
I1226 13:18:41.093474 96304 net.cpp:586] fc7 -> fc7
I1226 13:18:54.081807 94502 net.cpp:228] Setting up fc7
I1226 13:18:54.081946 94502 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:18:54.081979 94502 net.cpp:243] Memory required for data: 265039616
I1226 13:18:54.082046 94502 layer_factory.hpp:114] Creating layer relu7
I1226 13:18:54.082247 94502 net.cpp:178] Creating Layer relu7
I1226 13:18:54.082303 94502 net.cpp:612] relu7 <- fc7
I1226 13:18:54.082345 94502 net.cpp:573] relu7 -> fc7 (in-place)
I1226 13:18:54.082454 94502 net.cpp:228] Setting up relu7
I1226 13:18:54.082509 94502 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:18:54.082541 94502 net.cpp:243] Memory required for data: 265563904
I1226 13:18:54.082573 94502 layer_factory.hpp:114] Creating layer drop7
I1226 13:18:54.082623 94502 net.cpp:178] Creating Layer drop7
I1226 13:18:54.082659 94502 net.cpp:612] drop7 <- fc7
I1226 13:18:54.082715 94502 net.cpp:573] drop7 -> fc7 (in-place)
I1226 13:18:54.082770 94502 net.cpp:228] Setting up drop7
I1226 13:18:54.082815 94502 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:18:54.082868 94502 net.cpp:243] Memory required for data: 266088192
I1226 13:18:54.082902 94502 layer_factory.hpp:114] Creating layer fc8
I1226 13:18:54.082965 94502 net.cpp:178] Creating Layer fc8
I1226 13:18:54.083009 94502 net.cpp:612] fc8 <- fc7
I1226 13:18:54.083063 94502 net.cpp:586] fc8 -> fc8
I1226 13:18:54.129906 96696 net.cpp:228] Setting up fc7
I1226 13:18:54.130023 96696 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:18:54.130062 96696 net.cpp:243] Memory required for data: 265039616
I1226 13:18:54.130125 96696 layer_factory.hpp:114] Creating layer relu7
I1226 13:18:54.130229 96696 net.cpp:178] Creating Layer relu7
I1226 13:18:54.130285 96696 net.cpp:612] relu7 <- fc7
I1226 13:18:54.130331 96696 net.cpp:573] relu7 -> fc7 (in-place)
I1226 13:18:54.130439 96696 net.cpp:228] Setting up relu7
I1226 13:18:54.130499 96696 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:18:54.130528 96696 net.cpp:243] Memory required for data: 265563904
I1226 13:18:54.130563 96696 layer_factory.hpp:114] Creating layer drop7
I1226 13:18:54.130614 96696 net.cpp:178] Creating Layer drop7
I1226 13:18:54.130653 96696 net.cpp:612] drop7 <- fc7
I1226 13:18:54.130717 96696 net.cpp:573] drop7 -> fc7 (in-place)
I1226 13:18:54.130815 96696 net.cpp:228] Setting up drop7
I1226 13:18:54.130859 96696 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:18:54.130895 96696 net.cpp:243] Memory required for data: 266088192
I1226 13:18:54.130926 96696 layer_factory.hpp:114] Creating layer fc8
I1226 13:18:54.130990 96696 net.cpp:178] Creating Layer fc8
I1226 13:18:54.131027 96696 net.cpp:612] fc8 <- fc7
I1226 13:18:54.131069 96696 net.cpp:586] fc8 -> fc8
I1226 13:18:54.176018 96304 net.cpp:228] Setting up fc7
I1226 13:18:54.176141 96304 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:18:54.176187 96304 net.cpp:243] Memory required for data: 265039616
I1226 13:18:54.176272 96304 layer_factory.hpp:114] Creating layer relu7
I1226 13:18:54.176525 96304 net.cpp:178] Creating Layer relu7
I1226 13:18:54.176597 96304 net.cpp:612] relu7 <- fc7
I1226 13:18:54.176654 96304 net.cpp:573] relu7 -> fc7 (in-place)
I1226 13:18:54.176772 96304 net.cpp:228] Setting up relu7
I1226 13:18:54.176877 96304 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:18:54.176916 96304 net.cpp:243] Memory required for data: 265563904
I1226 13:18:54.176959 96304 layer_factory.hpp:114] Creating layer drop7
I1226 13:18:54.177022 96304 net.cpp:178] Creating Layer drop7
I1226 13:18:54.177067 96304 net.cpp:612] drop7 <- fc7
I1226 13:18:54.177135 96304 net.cpp:573] drop7 -> fc7 (in-place)
I1226 13:18:54.177201 96304 net.cpp:228] Setting up drop7
I1226 13:18:54.177263 96304 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:18:54.177304 96304 net.cpp:243] Memory required for data: 266088192
I1226 13:18:54.177340 96304 layer_factory.hpp:114] Creating layer fc8
I1226 13:18:54.177413 96304 net.cpp:178] Creating Layer fc8
I1226 13:18:54.177459 96304 net.cpp:612] fc8 <- fc7
I1226 13:18:54.177510 96304 net.cpp:586] fc8 -> fc8
I1226 13:18:57.273111 94502 net.cpp:228] Setting up fc8
I1226 13:18:57.273228 94502 net.cpp:235] Top shape: 32 1000 (32000)
I1226 13:18:57.273262 94502 net.cpp:243] Memory required for data: 266216192
I1226 13:18:57.273325 94502 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 13:18:57.273432 94502 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 13:18:57.273484 94502 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 13:18:57.273541 94502 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 13:18:57.273599 94502 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 13:18:57.273699 94502 net.cpp:228] Setting up fc8_fc8_0_split
I1226 13:18:57.273762 94502 net.cpp:235] Top shape: 32 1000 (32000)
I1226 13:18:57.273805 94502 net.cpp:235] Top shape: 32 1000 (32000)
I1226 13:18:57.273862 94502 net.cpp:243] Memory required for data: 266472192
I1226 13:18:57.273896 94502 layer_factory.hpp:114] Creating layer accuracy
I1226 13:18:57.273957 94502 net.cpp:178] Creating Layer accuracy
I1226 13:18:57.273998 94502 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 13:18:57.274040 94502 net.cpp:612] accuracy <- label_data_1_split_0
I1226 13:18:57.274081 94502 net.cpp:586] accuracy -> accuracy
I1226 13:18:57.274135 94502 net.cpp:228] Setting up accuracy
I1226 13:18:57.274181 94502 net.cpp:235] Top shape: (1)
I1226 13:18:57.274214 94502 net.cpp:243] Memory required for data: 266472196
I1226 13:18:57.274241 94502 layer_factory.hpp:114] Creating layer loss
I1226 13:18:57.274411 94502 net.cpp:178] Creating Layer loss
I1226 13:18:57.274458 94502 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 13:18:57.274493 94502 net.cpp:612] loss <- label_data_1_split_1
I1226 13:18:57.274549 94502 net.cpp:586] loss -> loss
I1226 13:18:57.274628 94502 layer_factory.hpp:114] Creating layer loss
I1226 13:18:57.304744 94502 net.cpp:228] Setting up loss
I1226 13:18:57.304878 94502 net.cpp:235] Top shape: (1)
I1226 13:18:57.304922 94502 net.cpp:238]     with loss weight 1
I1226 13:18:57.305064 94502 net.cpp:243] Memory required for data: 266472200
I1226 13:18:57.305137 94502 net.cpp:305] loss needs backward computation.
I1226 13:18:57.305194 94502 net.cpp:307] accuracy does not need backward computation.
I1226 13:18:57.305241 94502 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 13:18:57.305276 94502 net.cpp:305] fc8 needs backward computation.
I1226 13:18:57.305310 94502 net.cpp:305] drop7 needs backward computation.
I1226 13:18:57.305341 94502 net.cpp:305] relu7 needs backward computation.
I1226 13:18:57.305382 94502 net.cpp:305] fc7 needs backward computation.
I1226 13:18:57.305413 94502 net.cpp:305] drop6 needs backward computation.
I1226 13:18:57.305444 94502 net.cpp:305] relu6 needs backward computation.
I1226 13:18:57.305483 94502 net.cpp:305] fc6 needs backward computation.
I1226 13:18:57.305523 94502 net.cpp:305] pool5 needs backward computation.
I1226 13:18:57.305555 94502 net.cpp:305] relu5 needs backward computation.
I1226 13:18:57.305593 94502 net.cpp:305] conv5 needs backward computation.
I1226 13:18:57.305634 94502 net.cpp:305] relu4 needs backward computation.
I1226 13:18:57.305665 94502 net.cpp:305] conv4 needs backward computation.
I1226 13:18:57.305696 94502 net.cpp:305] relu3 needs backward computation.
I1226 13:18:57.305726 94502 net.cpp:305] conv3 needs backward computation.
I1226 13:18:57.305764 94502 net.cpp:305] pool2 needs backward computation.
I1226 13:18:57.305795 94502 net.cpp:305] norm2 needs backward computation.
I1226 13:18:57.305845 94502 net.cpp:305] relu2 needs backward computation.
I1226 13:18:57.305878 94502 net.cpp:305] conv2 needs backward computation.
I1226 13:18:57.305919 94502 net.cpp:305] pool1 needs backward computation.
I1226 13:18:57.305960 94502 net.cpp:305] norm1 needs backward computation.
I1226 13:18:57.305992 94502 net.cpp:305] relu1 needs backward computation.
I1226 13:18:57.306031 94502 net.cpp:305] conv1 needs backward computation.
I1226 13:18:57.306064 94502 net.cpp:307] label_data_1_split does not need backward computation.
I1226 13:18:57.306098 94502 net.cpp:307] data does not need backward computation.
I1226 13:18:57.306126 94502 net.cpp:349] This network produces output accuracy
I1226 13:18:57.306170 94502 net.cpp:349] This network produces output loss
I1226 13:18:57.306279 94502 net.cpp:363] Network initialization done.
I1226 13:18:57.306733 94502 solver.cpp:107] Solver scaffolding done.
I1226 13:18:57.306974 94502 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 13:18:57.326453 96696 net.cpp:228] Setting up fc8
I1226 13:18:57.326575 96696 net.cpp:235] Top shape: 32 1000 (32000)
I1226 13:18:57.326611 96696 net.cpp:243] Memory required for data: 266216192
I1226 13:18:57.326673 96696 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 13:18:57.326804 96696 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 13:18:57.326870 96696 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 13:18:57.326918 96696 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 13:18:57.326977 96696 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 13:18:57.327095 96696 net.cpp:228] Setting up fc8_fc8_0_split
I1226 13:18:57.327160 96696 net.cpp:235] Top shape: 32 1000 (32000)
I1226 13:18:57.327195 96696 net.cpp:235] Top shape: 32 1000 (32000)
I1226 13:18:57.327229 96696 net.cpp:243] Memory required for data: 266472192
I1226 13:18:57.327260 96696 layer_factory.hpp:114] Creating layer accuracy
I1226 13:18:57.327319 96696 net.cpp:178] Creating Layer accuracy
I1226 13:18:57.327350 96696 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 13:18:57.327383 96696 net.cpp:612] accuracy <- label_data_1_split_0
I1226 13:18:57.327430 96696 net.cpp:586] accuracy -> accuracy
I1226 13:18:57.327486 96696 net.cpp:228] Setting up accuracy
I1226 13:18:57.327528 96696 net.cpp:235] Top shape: (1)
I1226 13:18:57.327558 96696 net.cpp:243] Memory required for data: 266472196
I1226 13:18:57.327596 96696 layer_factory.hpp:114] Creating layer loss
I1226 13:18:57.327781 96696 net.cpp:178] Creating Layer loss
I1226 13:18:57.327826 96696 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 13:18:57.327862 96696 net.cpp:612] loss <- label_data_1_split_1
I1226 13:18:57.327939 96696 net.cpp:586] loss -> loss
I1226 13:18:57.328024 96696 layer_factory.hpp:114] Creating layer loss
I1226 13:18:57.366907 96696 net.cpp:228] Setting up loss
I1226 13:18:57.369913 96304 net.cpp:228] Setting up fc8
I1226 13:18:57.367028 96696 net.cpp:235] Top shape: (1)
I1226 13:18:57.370040 96304 net.cpp:235] Top shape: 32 1000 (32000)
I1226 13:18:57.370087 96304 net.cpp:243] Memory required for data: 266216192
I1226 13:18:57.367074 96696 net.cpp:238]     with loss weight 1
I1226 13:18:57.370174 96304 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 13:18:57.367233 96696 net.cpp:243] Memory required for data: 266472200
I1226 13:18:57.370299 96304 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 13:18:57.367287 96696 net.cpp:305] loss needs backward computation.
I1226 13:18:57.370368 96304 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 13:18:57.370425 96304 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 13:18:57.370489 96304 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 13:18:57.370615 96304 net.cpp:228] Setting up fc8_fc8_0_split
I1226 13:18:57.367347 96696 net.cpp:307] accuracy does not need backward computation.
I1226 13:18:57.370687 96304 net.cpp:235] Top shape: 32 1000 (32000)
I1226 13:18:57.367398 96696 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 13:18:57.370735 96304 net.cpp:235] Top shape: 32 1000 (32000)
I1226 13:18:57.367445 96696 net.cpp:305] fc8 needs backward computation.
I1226 13:18:57.370767 96304 net.cpp:243] Memory required for data: 266472192
I1226 13:18:57.370837 96304 layer_factory.hpp:114] Creating layer accuracy
I1226 13:18:57.367488 96696 net.cpp:305] drop7 needs backward computation.
I1226 13:18:57.370913 96304 net.cpp:178] Creating Layer accuracy
I1226 13:18:57.367528 96696 net.cpp:305] relu7 needs backward computation.
I1226 13:18:57.367566 96696 net.cpp:305] fc7 needs backward computation.
I1226 13:18:57.370967 96304 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 13:18:57.371012 96304 net.cpp:612] accuracy <- label_data_1_split_0
I1226 13:18:57.367609 96696 net.cpp:305] drop6 needs backward computation.
I1226 13:18:57.371068 96304 net.cpp:586] accuracy -> accuracy
I1226 13:18:57.367646 96696 net.cpp:305] relu6 needs backward computation.
I1226 13:18:57.371146 96304 net.cpp:228] Setting up accuracy
I1226 13:18:57.367686 96696 net.cpp:305] fc6 needs backward computation.
I1226 13:18:57.367746 96696 net.cpp:305] pool5 needs backward computation.
I1226 13:18:57.367786 96696 net.cpp:305] relu5 needs backward computation.
I1226 13:18:57.367815 96696 net.cpp:305] conv5 needs backward computation.
I1226 13:18:57.367844 96696 net.cpp:305] relu4 needs backward computation.
I1226 13:18:57.367872 96696 net.cpp:305] conv4 needs backward computation.
I1226 13:18:57.367902 96696 net.cpp:305] relu3 needs backward computation.
I1226 13:18:57.367928 96696 net.cpp:305] conv3 needs backward computation.
I1226 13:18:57.367956 96696 net.cpp:305] pool2 needs backward computation.
I1226 13:18:57.367985 96696 net.cpp:305] norm2 needs backward computation.
I1226 13:18:57.368012 96696 net.cpp:305] relu2 needs backward computation.
I1226 13:18:57.368041 96696 net.cpp:305] conv2 needs backward computation.
I1226 13:18:57.368070 96696 net.cpp:305] pool1 needs backward computation.
I1226 13:18:57.368098 96696 net.cpp:305] norm1 needs backward computation.
I1226 13:18:57.368127 96696 net.cpp:305] relu1 needs backward computation.
I1226 13:18:57.368155 96696 net.cpp:305] conv1 needs backward computation.
I1226 13:18:57.368185 96696 net.cpp:307] label_data_1_split does not need backward computation.
I1226 13:18:57.368214 96696 net.cpp:307] data does not need backward computation.
I1226 13:18:57.368240 96696 net.cpp:349] This network produces output accuracy
I1226 13:18:57.368273 96696 net.cpp:349] This network produces output loss
I1226 13:18:57.371208 96304 net.cpp:235] Top shape: (1)
I1226 13:18:57.371243 96304 net.cpp:243] Memory required for data: 266472196
I1226 13:18:57.368383 96696 net.cpp:363] Network initialization done.
I1226 13:18:57.371285 96304 layer_factory.hpp:114] Creating layer loss
I1226 13:18:57.371477 96304 net.cpp:178] Creating Layer loss
I1226 13:18:57.371528 96304 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 13:18:57.371573 96304 net.cpp:612] loss <- label_data_1_split_1
I1226 13:18:57.371659 96304 net.cpp:586] loss -> loss
I1226 13:18:57.371780 96304 layer_factory.hpp:114] Creating layer loss
I1226 13:18:57.368875 96696 solver.cpp:107] Solver scaffolding done.
I1226 13:18:57.369109 96696 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 13:18:57.408092 96304 net.cpp:228] Setting up loss
I1226 13:18:57.408202 96304 net.cpp:235] Top shape: (1)
I1226 13:18:57.408237 96304 net.cpp:238]     with loss weight 1
I1226 13:18:57.408500 96304 net.cpp:243] Memory required for data: 266472200
I1226 13:18:57.408548 96304 net.cpp:305] loss needs backward computation.
I1226 13:18:57.408591 96304 net.cpp:307] accuracy does not need backward computation.
I1226 13:18:57.408637 96304 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 13:18:57.408668 96304 net.cpp:305] fc8 needs backward computation.
I1226 13:18:57.408699 96304 net.cpp:305] drop7 needs backward computation.
I1226 13:18:57.408730 96304 net.cpp:305] relu7 needs backward computation.
I1226 13:18:57.408759 96304 net.cpp:305] fc7 needs backward computation.
I1226 13:18:57.408814 96304 net.cpp:305] drop6 needs backward computation.
I1226 13:18:57.408849 96304 net.cpp:305] relu6 needs backward computation.
I1226 13:18:57.408879 96304 net.cpp:305] fc6 needs backward computation.
I1226 13:18:57.408917 96304 net.cpp:305] pool5 needs backward computation.
I1226 13:18:57.408951 96304 net.cpp:305] relu5 needs backward computation.
I1226 13:18:57.408982 96304 net.cpp:305] conv5 needs backward computation.
I1226 13:18:57.409021 96304 net.cpp:305] relu4 needs backward computation.
I1226 13:18:57.409051 96304 net.cpp:305] conv4 needs backward computation.
I1226 13:18:57.409081 96304 net.cpp:305] relu3 needs backward computation.
I1226 13:18:57.409111 96304 net.cpp:305] conv3 needs backward computation.
I1226 13:18:57.409152 96304 net.cpp:305] pool2 needs backward computation.
I1226 13:18:57.409193 96304 net.cpp:305] norm2 needs backward computation.
I1226 13:18:57.409225 96304 net.cpp:305] relu2 needs backward computation.
I1226 13:18:57.409255 96304 net.cpp:305] conv2 needs backward computation.
I1226 13:18:57.409297 96304 net.cpp:305] pool1 needs backward computation.
I1226 13:18:57.409328 96304 net.cpp:305] norm1 needs backward computation.
I1226 13:18:57.409358 96304 net.cpp:305] relu1 needs backward computation.
I1226 13:18:57.409397 96304 net.cpp:305] conv1 needs backward computation.
I1226 13:18:57.409430 96304 net.cpp:307] label_data_1_split does not need backward computation.
I1226 13:18:57.409472 96304 net.cpp:307] data does not need backward computation.
I1226 13:18:57.409502 96304 net.cpp:349] This network produces output accuracy
I1226 13:18:57.409535 96304 net.cpp:349] This network produces output loss
I1226 13:18:57.409644 96304 net.cpp:363] Network initialization done.
I1226 13:18:57.410121 96304 solver.cpp:107] Solver scaffolding done.
I1226 13:18:57.410338 96304 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 13:19:01.895277 94502 caffe.cpp:376] Configuring multinode setup
I1226 13:19:01.897143 94502 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 13:19:01.987457 96304 caffe.cpp:376] Configuring multinode setup
I1226 13:19:01.989732 96304 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 13:19:02.846612 96696 caffe.cpp:376] Configuring multinode setup
I1226 13:19:02.860162 94502 SynchronousNode.cpp:674] [6] [proc 6] solving
I1226 13:19:02.848506 96696 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 13:19:02.856142 85740 SynchronousNode.cpp:674] [4] [proc 4] solving
I1226 13:19:02.859917 98267 SynchronousNode.cpp:674] [0] [proc 0] solving
I1226 13:19:02.860301 94502 solver.cpp:354] Solving AlexNet
I1226 13:19:02.860357 94502 solver.cpp:355] Learning Rate Policy: step
I1226 13:19:02.851625 96304 SynchronousNode.cpp:674] [2] [proc 2] solving
I1226 13:19:02.848779 96696 SynchronousNode.cpp:674] [7] [proc 7] solving
I1226 13:19:02.844497 93960 SynchronousNode.cpp:674] [3] [proc 3] solving
I1226 13:19:02.859824 91029 SynchronousNode.cpp:674] [5] [proc 5] solving
I1226 13:19:02.853858 92093 SynchronousNode.cpp:674] [1] [proc 1] solving
I1226 13:19:02.856426 85740 solver.cpp:354] Solving AlexNet
I1226 13:19:02.860234 98267 solver.cpp:354] Solving AlexNet
I1226 13:19:02.860285 98267 solver.cpp:355] Learning Rate Policy: step
I1226 13:19:02.851816 96304 solver.cpp:354] Solving AlexNet
I1226 13:19:02.848892 96696 solver.cpp:354] Solving AlexNet
I1226 13:19:02.848937 96696 solver.cpp:355] Learning Rate Policy: step
I1226 13:19:02.844774 93960 solver.cpp:354] Solving AlexNet
I1226 13:19:02.844825 93960 solver.cpp:355] Learning Rate Policy: step
I1226 13:19:02.860100 91029 solver.cpp:354] Solving AlexNet
I1226 13:19:02.854142 92093 solver.cpp:354] Solving AlexNet
I1226 13:19:02.854187 92093 solver.cpp:355] Learning Rate Policy: step
I1226 13:19:02.856477 85740 solver.cpp:355] Learning Rate Policy: step
I1226 13:19:02.851878 96304 solver.cpp:355] Learning Rate Policy: step
I1226 13:19:02.860154 91029 solver.cpp:355] Learning Rate Policy: step
I1226 13:19:02.862092 96376 SynchronousNode.cpp:292] [2] Comm thread started 0 0
I1226 13:19:02.871109 91102 SynchronousNode.cpp:292] [5] Comm thread started 1 0
I1226 13:19:02.867560 85812 SynchronousNode.cpp:292] [4] Comm thread started 1 0
I1226 13:19:02.872375 98341 SynchronousNode.cpp:292] [0] Comm thread started 0 1
I1226 13:19:02.867019 96766 SynchronousNode.cpp:292] [7] Comm thread started 1 0
I1226 13:19:02.865021 94029 SynchronousNode.cpp:292] [3] Comm thread started 0 0
I1226 13:19:02.876945 92166 SynchronousNode.cpp:292] [1] Comm thread started 0 0
I1226 13:19:02.897038 94574 SynchronousNode.cpp:292] [6] Comm thread started 1 0
I1226 13:19:02.902125 98341 SynchronousNode.cpp:478] [0] initialized root of cluster with nodes: 9 and the total iter size is: 8
I1226 13:19:03.131327 98267 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 13:19:03.131503 98267 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 13:19:03.431107 98267 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 13:19:03.431197 98267 solver.cpp:291] [0] Iteration 1, loss = 2.42712
I1226 13:19:03.431262 98267 solver.cpp:317]     Train net output #0: accuracy = 0.40625
I1226 13:19:03.431341 98267 solver.cpp:317]     Train net output #1: loss = 2.42712 (* 1 = 2.42712 loss)
I1226 13:19:05.178990 96376 SynchronousNode.cpp:246] [2] PROFILING BEGIN[Forward]
I1226 13:19:05.181172 92166 SynchronousNode.cpp:246] [1] PROFILING BEGIN[Forward]
I1226 13:19:05.183655 85812 SynchronousNode.cpp:246] [4] PROFILING BEGIN[Forward]
I1226 13:19:05.172044 94029 SynchronousNode.cpp:246] [3] PROFILING BEGIN[Forward]
I1226 13:19:05.176379 96766 SynchronousNode.cpp:246] [7] PROFILING BEGIN[Forward]
I1226 13:19:05.264765 92093 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 13:19:05.264878 92093 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 13:19:05.258620 93960 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 13:19:05.258734 93960 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 13:19:05.343832 85740 MultiSolver.cpp:92] [4] PROFILING END[Forward]
I1226 13:19:05.343945 85740 MultiSolver.cpp:94] [4] PROFILING BEGIN[Backward]
I1226 13:19:05.355492 93960 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 13:19:05.355582 93960 solver.cpp:291] [3] Iteration 1, loss = 2.56674
I1226 13:19:05.355651 93960 solver.cpp:317]     Train net output #0: accuracy = 0.3125
I1226 13:19:05.355720 93960 solver.cpp:317]     Train net output #1: loss = 2.56674 (* 1 = 2.56674 loss)
I1226 13:19:05.367342 92093 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 13:19:05.367435 92093 solver.cpp:291] [1] Iteration 1, loss = 2.71618
I1226 13:19:05.367574 92093 solver.cpp:317]     Train net output #0: accuracy = 0.46875
I1226 13:19:05.367805 92093 solver.cpp:317]     Train net output #1: loss = 2.71618 (* 1 = 2.71618 loss)
I1226 13:19:05.555084 91102 SynchronousNode.cpp:246] [5] PROFILING BEGIN[Forward]
I1226 13:19:05.555662 94574 SynchronousNode.cpp:246] [6] PROFILING BEGIN[Forward]
I1226 13:19:05.597465 91029 MultiSolver.cpp:92] [5] PROFILING END[Forward]
I1226 13:19:05.597578 91029 MultiSolver.cpp:94] [5] PROFILING BEGIN[Backward]
I1226 13:19:05.609031 85740 MultiSolver.cpp:108] [4] PROFILING END[Backward]
I1226 13:19:05.609127 85740 solver.cpp:291] [4] Iteration 1, loss = 3.1287
I1226 13:19:05.609203 85740 solver.cpp:317]     Train net output #0: accuracy = 0.375
I1226 13:19:05.609279 85740 solver.cpp:317]     Train net output #1: loss = 3.1287 (* 1 = 3.1287 loss)
I1226 13:19:05.727018 94502 MultiSolver.cpp:92] [6] PROFILING END[Forward]
I1226 13:19:05.727136 94502 MultiSolver.cpp:94] [6] PROFILING BEGIN[Backward]
I1226 13:19:05.730701 96696 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 13:19:05.730845 96696 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 13:19:05.781569 91029 MultiSolver.cpp:108] [5] PROFILING END[Backward]
I1226 13:19:05.781668 91029 solver.cpp:291] [5] Iteration 1, loss = 3.61876
I1226 13:19:05.781783 91029 solver.cpp:317]     Train net output #0: accuracy = 0.25
I1226 13:19:05.781864 91029 solver.cpp:317]     Train net output #1: loss = 3.61876 (* 1 = 3.61876 loss)
I1226 13:19:05.797143 96304 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 13:19:05.797255 96304 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 13:19:06.594717 94502 MultiSolver.cpp:108] [6] PROFILING END[Backward]
I1226 13:19:06.594856 94502 solver.cpp:291] [6] Iteration 1, loss = 3.13817
I1226 13:19:06.594931 94502 solver.cpp:317]     Train net output #0: accuracy = 0.40625
I1226 13:19:06.595010 94502 solver.cpp:317]     Train net output #1: loss = 3.13817 (* 1 = 3.13817 loss)
I1226 13:19:06.616307 96696 MultiSolver.cpp:108] [7] PROFILING END[Backward]
I1226 13:19:06.616412 96696 solver.cpp:291] [7] Iteration 1, loss = 3.47717
I1226 13:19:06.616490 96696 solver.cpp:317]     Train net output #0: accuracy = 0.34375
I1226 13:19:06.616569 96696 solver.cpp:317]     Train net output #1: loss = 3.47717 (* 1 = 3.47717 loss)
I1226 13:19:06.629585 96304 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 13:19:06.629685 96304 solver.cpp:291] [2] Iteration 1, loss = 2.80211
I1226 13:19:06.629751 96304 solver.cpp:317]     Train net output #0: accuracy = 0.34375
I1226 13:19:06.629847 96304 solver.cpp:317]     Train net output #1: loss = 2.80211 (* 1 = 2.80211 loss)
I1226 13:19:13.060109 98267 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 13:19:13.060225 98267 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 13:19:13.127126 98267 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 13:19:13.127209 98267 solver.cpp:291] [0] Iteration 2, loss = 3.89807
I1226 13:19:13.127274 98267 solver.cpp:317]     Train net output #0: accuracy = 0.25
I1226 13:19:13.127351 98267 solver.cpp:317]     Train net output #1: loss = 3.89807 (* 1 = 3.89807 loss)
I1226 13:19:14.347920 85812 SynchronousNode.cpp:246] [4] PROFILING BEGIN[Forward]
I1226 13:19:14.345631 92166 SynchronousNode.cpp:246] [1] PROFILING BEGIN[Forward]
I1226 13:19:14.336287 94029 SynchronousNode.cpp:246] [3] PROFILING BEGIN[Forward]
I1226 13:19:14.340659 96766 SynchronousNode.cpp:246] [7] PROFILING BEGIN[Forward]
I1226 13:19:14.343904 96376 SynchronousNode.cpp:246] [2] PROFILING BEGIN[Forward]
I1226 13:19:14.350723 85740 MultiSolver.cpp:92] [4] PROFILING END[Forward]
I1226 13:19:14.339046 93960 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 13:19:14.348300 92093 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 13:19:14.350841 85740 MultiSolver.cpp:94] [4] PROFILING BEGIN[Backward]
I1226 13:19:14.339157 93960 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 13:19:14.348410 92093 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 13:19:14.352329 96304 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 13:19:14.352445 96304 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 13:19:14.349766 96696 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 13:19:14.349896 96696 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 13:19:14.400519 92093 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 13:19:14.400635 92093 solver.cpp:291] [1] Iteration 2, loss = 2.65419
I1226 13:19:14.400710 92093 solver.cpp:317]     Train net output #0: accuracy = 0.40625
I1226 13:19:14.400781 92093 solver.cpp:317]     Train net output #1: loss = 2.65419 (* 1 = 2.65419 loss)
I1226 13:19:14.395809 93960 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 13:19:14.395894 93960 solver.cpp:291] [3] Iteration 2, loss = 3.45473
I1226 13:19:14.395961 93960 solver.cpp:317]     Train net output #0: accuracy = 0.34375
I1226 13:19:14.396026 93960 solver.cpp:317]     Train net output #1: loss = 3.45473 (* 1 = 3.45473 loss)
I1226 13:19:14.459745 85740 MultiSolver.cpp:108] [4] PROFILING END[Backward]
I1226 13:19:14.459844 85740 solver.cpp:291] [4] Iteration 2, loss = 3.10913
I1226 13:19:14.459920 85740 solver.cpp:317]     Train net output #0: accuracy = 0.25
I1226 13:19:14.459996 85740 solver.cpp:317]     Train net output #1: loss = 3.10913 (* 1 = 3.10913 loss)
I1226 13:19:14.836349 96304 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 13:19:14.836570 96304 solver.cpp:291] [2] Iteration 2, loss = 3.79598
I1226 13:19:14.836637 96304 solver.cpp:317]     Train net output #0: accuracy = 0.21875
I1226 13:19:14.836701 96304 solver.cpp:317]     Train net output #1: loss = 3.79598 (* 1 = 3.79598 loss)
I1226 13:19:14.841615 96696 MultiSolver.cpp:108] [7] PROFILING END[Backward]
I1226 13:19:14.841755 96696 solver.cpp:291] [7] Iteration 2, loss = 2.76296
I1226 13:19:14.841831 96696 solver.cpp:317]     Train net output #0: accuracy = 0.34375
I1226 13:19:14.841915 96696 solver.cpp:317]     Train net output #1: loss = 2.76296 (* 1 = 2.76296 loss)
I1226 13:19:14.889458 91102 SynchronousNode.cpp:246] [5] PROFILING BEGIN[Forward]
I1226 13:19:14.890097 94574 SynchronousNode.cpp:246] [6] PROFILING BEGIN[Forward]
I1226 13:19:14.892233 91029 MultiSolver.cpp:92] [5] PROFILING END[Forward]
I1226 13:19:14.892339 91029 MultiSolver.cpp:94] [5] PROFILING BEGIN[Backward]
I1226 13:19:14.899698 94502 MultiSolver.cpp:92] [6] PROFILING END[Forward]
I1226 13:19:14.899847 94502 MultiSolver.cpp:94] [6] PROFILING BEGIN[Backward]
I1226 13:19:15.001201 91029 MultiSolver.cpp:108] [5] PROFILING END[Backward]
I1226 13:19:15.001292 91029 solver.cpp:291] [5] Iteration 2, loss = 2.98442
I1226 13:19:15.001372 91029 solver.cpp:317]     Train net output #0: accuracy = 0.34375
I1226 13:19:15.001448 91029 solver.cpp:317]     Train net output #1: loss = 2.98442 (* 1 = 2.98442 loss)
I1226 13:19:15.424175 94502 MultiSolver.cpp:108] [6] PROFILING END[Backward]
I1226 13:19:15.424273 94502 solver.cpp:291] [6] Iteration 2, loss = 3.66563
I1226 13:19:15.424350 94502 solver.cpp:317]     Train net output #0: accuracy = 0.15625
I1226 13:19:15.424427 94502 solver.cpp:317]     Train net output #1: loss = 3.66563 (* 1 = 3.66563 loss)
I1226 13:19:21.857386 98267 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 13:19:21.857499 98267 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 13:19:21.924700 98267 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 13:19:21.924792 98267 solver.cpp:291] [0] Iteration 3, loss = 3.40008
I1226 13:19:21.924854 98267 solver.cpp:317]     Train net output #0: accuracy = 0.34375
I1226 13:19:21.924931 98267 solver.cpp:317]     Train net output #1: loss = 3.40008 (* 1 = 3.40008 loss)
I1226 13:19:23.749267 85812 SynchronousNode.cpp:246] [4] PROFILING BEGIN[Forward]
I1226 13:19:23.744910 96376 SynchronousNode.cpp:246] [2] PROFILING BEGIN[Forward]
I1226 13:19:23.742036 96766 SynchronousNode.cpp:246] [7] PROFILING BEGIN[Forward]
I1226 13:19:23.737627 94029 SynchronousNode.cpp:246] [3] PROFILING BEGIN[Forward]
I1226 13:19:23.746979 92166 SynchronousNode.cpp:246] [1] PROFILING BEGIN[Forward]
I1226 13:19:23.752090 85740 MultiSolver.cpp:92] [4] PROFILING END[Forward]
I1226 13:19:23.740398 93960 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 13:19:23.749735 92093 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 13:19:23.752218 85740 MultiSolver.cpp:94] [4] PROFILING BEGIN[Backward]
I1226 13:19:23.740514 93960 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 13:19:23.749852 92093 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 13:19:23.750493 96696 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 13:19:23.750612 96696 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 13:19:23.753702 96304 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 13:19:23.753882 96304 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 13:19:23.802435 92093 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 13:19:23.802547 92093 solver.cpp:291] [1] Iteration 3, loss = 3.70068
I1226 13:19:23.802613 92093 solver.cpp:317]     Train net output #0: accuracy = 0.25
I1226 13:19:23.802711 92093 solver.cpp:317]     Train net output #1: loss = 3.70068 (* 1 = 3.70068 loss)
I1226 13:19:23.795939 93960 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 13:19:23.796026 93960 solver.cpp:291] [3] Iteration 3, loss = 2.72106
I1226 13:19:23.796095 93960 solver.cpp:317]     Train net output #0: accuracy = 0.5625
I1226 13:19:23.796162 93960 solver.cpp:317]     Train net output #1: loss = 2.72106 (* 1 = 2.72106 loss)
I1226 13:19:23.853997 85740 MultiSolver.cpp:108] [4] PROFILING END[Backward]
I1226 13:19:23.854087 85740 solver.cpp:291] [4] Iteration 3, loss = 3.71122
I1226 13:19:23.854166 85740 solver.cpp:317]     Train net output #0: accuracy = 0.1875
I1226 13:19:23.854243 85740 solver.cpp:317]     Train net output #1: loss = 3.71122 (* 1 = 3.71122 loss)
I1226 13:19:23.994412 91102 SynchronousNode.cpp:246] [5] PROFILING BEGIN[Forward]
I1226 13:19:23.995007 94574 SynchronousNode.cpp:246] [6] PROFILING BEGIN[Forward]
I1226 13:19:23.999382 91029 MultiSolver.cpp:92] [5] PROFILING END[Forward]
I1226 13:19:23.999495 91029 MultiSolver.cpp:94] [5] PROFILING BEGIN[Backward]
I1226 13:19:24.058286 94502 MultiSolver.cpp:92] [6] PROFILING END[Forward]
I1226 13:19:24.058459 94502 MultiSolver.cpp:94] [6] PROFILING BEGIN[Backward]
I1226 13:19:24.110528 91029 MultiSolver.cpp:108] [5] PROFILING END[Backward]
I1226 13:19:24.110620 91029 solver.cpp:291] [5] Iteration 3, loss = 3.63087
I1226 13:19:24.110698 91029 solver.cpp:317]     Train net output #0: accuracy = 0.28125
I1226 13:19:24.110811 91029 solver.cpp:317]     Train net output #1: loss = 3.63087 (* 1 = 3.63087 loss)
I1226 13:19:24.235086 96304 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 13:19:24.235183 96304 solver.cpp:291] [2] Iteration 3, loss = 3.2739
I1226 13:19:24.235258 96304 solver.cpp:317]     Train net output #0: accuracy = 0.28125
I1226 13:19:24.235333 96304 solver.cpp:317]     Train net output #1: loss = 3.2739 (* 1 = 3.2739 loss)
I1226 13:19:24.243114 96696 MultiSolver.cpp:108] [7] PROFILING END[Backward]
I1226 13:19:24.243223 96696 solver.cpp:291] [7] Iteration 3, loss = 3.37359
I1226 13:19:24.243300 96696 solver.cpp:317]     Train net output #0: accuracy = 0.1875
I1226 13:19:24.243381 96696 solver.cpp:317]     Train net output #1: loss = 3.37359 (* 1 = 3.37359 loss)
I1226 13:19:24.604862 94502 MultiSolver.cpp:108] [6] PROFILING END[Backward]
I1226 13:19:24.604959 94502 solver.cpp:291] [6] Iteration 3, loss = 2.76625
I1226 13:19:24.605036 94502 solver.cpp:317]     Train net output #0: accuracy = 0.40625
I1226 13:19:24.605113 94502 solver.cpp:317]     Train net output #1: loss = 2.76625 (* 1 = 2.76625 loss)
I1226 13:19:31.230176 98267 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 13:19:31.230286 98267 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 13:19:31.293902 98267 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 13:19:31.293985 98267 solver.cpp:291] [0] Iteration 4, loss = 3.63404
I1226 13:19:31.294045 98267 solver.cpp:317]     Train net output #0: accuracy = 0.25
I1226 13:19:31.294155 98267 solver.cpp:317]     Train net output #1: loss = 3.63404 (* 1 = 3.63404 loss)
I1226 13:19:32.693873 85812 SynchronousNode.cpp:246] [4] PROFILING BEGIN[Forward]
I1226 13:19:32.686656 96766 SynchronousNode.cpp:246] [7] PROFILING BEGIN[Forward]
I1226 13:19:32.682221 94029 SynchronousNode.cpp:246] [3] PROFILING BEGIN[Forward]
I1226 13:19:32.691593 92166 SynchronousNode.cpp:246] [1] PROFILING BEGIN[Forward]
I1226 13:19:32.689716 96376 SynchronousNode.cpp:246] [2] PROFILING BEGIN[Forward]
I1226 13:19:32.694320 92093 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 13:19:32.696642 85740 MultiSolver.cpp:92] [4] PROFILING END[Forward]
I1226 13:19:32.696782 85740 MultiSolver.cpp:94] [4] PROFILING BEGIN[Backward]
I1226 13:19:32.694432 92093 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 13:19:32.685556 93960 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 13:19:32.685664 93960 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 13:19:32.695142 96696 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 13:19:32.695272 96696 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 13:19:32.698602 96304 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 13:19:32.698714 96304 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 13:19:32.750285 92093 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 13:19:32.750383 92093 solver.cpp:291] [1] Iteration 4, loss = 3.72911
I1226 13:19:32.750452 92093 solver.cpp:317]     Train net output #0: accuracy = 0.3125
I1226 13:19:32.750556 92093 solver.cpp:317]     Train net output #1: loss = 3.72911 (* 1 = 3.72911 loss)
I1226 13:19:32.750483 93960 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 13:19:32.750617 93960 solver.cpp:291] [3] Iteration 4, loss = 2.87451
I1226 13:19:32.750689 93960 solver.cpp:317]     Train net output #0: accuracy = 0.3125
I1226 13:19:32.750762 93960 solver.cpp:317]     Train net output #1: loss = 2.87451 (* 1 = 2.87451 loss)
I1226 13:19:32.800180 85740 MultiSolver.cpp:108] [4] PROFILING END[Backward]
I1226 13:19:32.800272 85740 solver.cpp:291] [4] Iteration 4, loss = 4.29976
I1226 13:19:32.800348 85740 solver.cpp:317]     Train net output #0: accuracy = 0.15625
I1226 13:19:32.800426 85740 solver.cpp:317]     Train net output #1: loss = 4.29976 (* 1 = 4.29976 loss)
I1226 13:19:33.181705 96304 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 13:19:33.181815 96304 solver.cpp:291] [2] Iteration 4, loss = 3.37844
I1226 13:19:33.181880 96304 solver.cpp:317]     Train net output #0: accuracy = 0.28125
I1226 13:19:33.181946 96304 solver.cpp:317]     Train net output #1: loss = 3.37844 (* 1 = 3.37844 loss)
I1226 13:19:33.187252 96696 MultiSolver.cpp:108] [7] PROFILING END[Backward]
I1226 13:19:33.187355 96696 solver.cpp:291] [7] Iteration 4, loss = 3.15874
I1226 13:19:33.187434 96696 solver.cpp:317]     Train net output #0: accuracy = 0.34375
I1226 13:19:33.187512 96696 solver.cpp:317]     Train net output #1: loss = 3.15874 (* 1 = 3.15874 loss)
I1226 13:19:33.240376 94574 SynchronousNode.cpp:246] [6] PROFILING BEGIN[Forward]
I1226 13:19:33.239794 91102 SynchronousNode.cpp:246] [5] PROFILING BEGIN[Forward]
I1226 13:19:33.242943 91029 MultiSolver.cpp:92] [5] PROFILING END[Forward]
I1226 13:19:33.243068 91029 MultiSolver.cpp:94] [5] PROFILING BEGIN[Backward]
I1226 13:19:33.318212 94502 MultiSolver.cpp:92] [6] PROFILING END[Forward]
I1226 13:19:33.318337 94502 MultiSolver.cpp:94] [6] PROFILING BEGIN[Backward]
I1226 13:19:33.355180 91029 MultiSolver.cpp:108] [5] PROFILING END[Backward]
I1226 13:19:33.355270 91029 solver.cpp:291] [5] Iteration 4, loss = 3.13412
I1226 13:19:33.355347 91029 solver.cpp:317]     Train net output #0: accuracy = 0.3125
I1226 13:19:33.355425 91029 solver.cpp:317]     Train net output #1: loss = 3.13412 (* 1 = 3.13412 loss)
I1226 13:19:33.859539 94502 MultiSolver.cpp:108] [6] PROFILING END[Backward]
I1226 13:19:33.859643 94502 solver.cpp:291] [6] Iteration 4, loss = 2.99947
I1226 13:19:33.859721 94502 solver.cpp:317]     Train net output #0: accuracy = 0.28125
I1226 13:19:33.859797 94502 solver.cpp:317]     Train net output #1: loss = 2.99947 (* 1 = 2.99947 loss)
I1226 13:19:40.463524 98267 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 13:19:40.464717 98267 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 13:19:40.532323 98267 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 13:19:40.532403 98267 solver.cpp:291] [0] Iteration 5, loss = 3.26155
I1226 13:19:40.532464 98267 solver.cpp:317]     Train net output #0: accuracy = 0.375
I1226 13:19:40.532541 98267 solver.cpp:317]     Train net output #1: loss = 3.26155 (* 1 = 3.26155 loss)
I1226 13:19:42.055974 96766 SynchronousNode.cpp:246] [7] PROFILING BEGIN[Forward]
I1226 13:19:42.058830 96376 SynchronousNode.cpp:246] [2] PROFILING BEGIN[Forward]
I1226 13:19:42.063159 85812 SynchronousNode.cpp:246] [4] PROFILING BEGIN[Forward]
I1226 13:19:42.060875 92166 SynchronousNode.cpp:246] [1] PROFILING BEGIN[Forward]
I1226 13:19:42.051503 94029 SynchronousNode.cpp:246] [3] PROFILING BEGIN[Forward]
I1226 13:19:42.066205 85740 MultiSolver.cpp:92] [4] PROFILING END[Forward]
I1226 13:19:42.066314 85740 MultiSolver.cpp:94] [4] PROFILING BEGIN[Backward]
I1226 13:19:42.055359 93960 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 13:19:42.064765 92093 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 13:19:42.055549 93960 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 13:19:42.064877 92093 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 13:19:42.067255 96304 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 13:19:42.064434 96696 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 13:19:42.067500 96304 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 13:19:42.064553 96696 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 13:19:42.119701 92093 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 13:19:42.119791 92093 solver.cpp:291] [1] Iteration 5, loss = 3.6784
I1226 13:19:42.119856 92093 solver.cpp:317]     Train net output #0: accuracy = 0.21875
I1226 13:19:42.119922 92093 solver.cpp:317]     Train net output #1: loss = 3.6784 (* 1 = 3.6784 loss)
I1226 13:19:42.110947 93960 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 13:19:42.111085 93960 solver.cpp:291] [3] Iteration 5, loss = 2.36119
I1226 13:19:42.111304 93960 solver.cpp:317]     Train net output #0: accuracy = 0.5
I1226 13:19:42.111404 93960 solver.cpp:317]     Train net output #1: loss = 2.36119 (* 1 = 2.36119 loss)
I1226 13:19:42.179409 85740 MultiSolver.cpp:108] [4] PROFILING END[Backward]
I1226 13:19:42.179502 85740 solver.cpp:291] [4] Iteration 5, loss = 2.62481
I1226 13:19:42.179579 85740 solver.cpp:317]     Train net output #0: accuracy = 0.34375
I1226 13:19:42.179656 85740 solver.cpp:317]     Train net output #1: loss = 2.62481 (* 1 = 2.62481 loss)
I1226 13:19:42.548722 96304 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 13:19:42.548866 96304 solver.cpp:291] [2] Iteration 5, loss = 2.92559
I1226 13:19:42.548935 96304 solver.cpp:317]     Train net output #0: accuracy = 0.4375
I1226 13:19:42.549010 96304 solver.cpp:317]     Train net output #1: loss = 2.92559 (* 1 = 2.92559 loss)
I1226 13:19:42.557260 96696 MultiSolver.cpp:108] [7] PROFILING END[Backward]
I1226 13:19:42.557368 96696 solver.cpp:291] [7] Iteration 5, loss = 3.46742
I1226 13:19:42.557447 96696 solver.cpp:317]     Train net output #0: accuracy = 0.25
I1226 13:19:42.557528 96696 solver.cpp:317]     Train net output #1: loss = 3.46742 (* 1 = 3.46742 loss)
I1226 13:19:42.598690 91102 SynchronousNode.cpp:246] [5] PROFILING BEGIN[Forward]
I1226 13:19:42.599339 94574 SynchronousNode.cpp:246] [6] PROFILING BEGIN[Forward]
I1226 13:19:42.601485 91029 MultiSolver.cpp:92] [5] PROFILING END[Forward]
I1226 13:19:42.601605 91029 MultiSolver.cpp:94] [5] PROFILING BEGIN[Backward]
I1226 13:19:42.607750 94502 MultiSolver.cpp:92] [6] PROFILING END[Forward]
I1226 13:19:42.607906 94502 MultiSolver.cpp:94] [6] PROFILING BEGIN[Backward]
I1226 13:19:42.715874 91029 MultiSolver.cpp:108] [5] PROFILING END[Backward]
I1226 13:19:42.715968 91029 solver.cpp:291] [5] Iteration 5, loss = 3.23647
I1226 13:19:42.716042 91029 solver.cpp:317]     Train net output #0: accuracy = 0.40625
I1226 13:19:42.716120 91029 solver.cpp:317]     Train net output #1: loss = 3.23647 (* 1 = 3.23647 loss)
I1226 13:19:43.129667 94502 MultiSolver.cpp:108] [6] PROFILING END[Backward]
I1226 13:19:43.129765 94502 solver.cpp:291] [6] Iteration 5, loss = 3.26753
I1226 13:19:43.129873 94502 solver.cpp:317]     Train net output #0: accuracy = 0.3125
I1226 13:19:43.129953 94502 solver.cpp:317]     Train net output #1: loss = 3.26753 (* 1 = 3.26753 loss)
I1226 13:19:49.668328 98267 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 13:19:49.668434 98267 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 13:19:49.738466 98267 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 13:19:49.738554 98267 solver.cpp:291] [0] Iteration 6, loss = 3.30444
I1226 13:19:49.738612 98267 solver.cpp:317]     Train net output #0: accuracy = 0.3125
I1226 13:19:49.738688 98267 solver.cpp:317]     Train net output #1: loss = 3.30444 (* 1 = 3.30444 loss)
I1226 13:19:51.237715 85812 SynchronousNode.cpp:246] [4] PROFILING BEGIN[Forward]
I1226 13:19:51.226016 94029 SynchronousNode.cpp:246] [3] PROFILING BEGIN[Forward]
I1226 13:19:51.235420 92166 SynchronousNode.cpp:246] [1] PROFILING BEGIN[Forward]
I1226 13:19:51.233335 96376 SynchronousNode.cpp:246] [2] PROFILING BEGIN[Forward]
I1226 13:19:51.230518 96766 SynchronousNode.cpp:246] [7] PROFILING BEGIN[Forward]
I1226 13:19:51.240484 85740 MultiSolver.cpp:92] [4] PROFILING END[Forward]
I1226 13:19:51.228773 93960 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 13:19:51.238189 92093 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 13:19:51.240593 85740 MultiSolver.cpp:94] [4] PROFILING BEGIN[Backward]
I1226 13:19:51.228904 93960 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 13:19:51.238312 92093 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 13:19:51.241894 96304 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 13:19:51.239024 96696 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 13:19:51.242003 96304 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 13:19:51.239145 96696 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 13:19:51.293463 92093 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 13:19:51.293601 92093 solver.cpp:291] [1] Iteration 6, loss = 3.5865
I1226 13:19:51.293668 92093 solver.cpp:317]     Train net output #0: accuracy = 0.09375
I1226 13:19:51.293735 92093 solver.cpp:317]     Train net output #1: loss = 3.5865 (* 1 = 3.5865 loss)
I1226 13:19:51.284633 93960 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 13:19:51.284718 93960 solver.cpp:291] [3] Iteration 6, loss = 2.75636
I1226 13:19:51.284786 93960 solver.cpp:317]     Train net output #0: accuracy = 0.40625
I1226 13:19:51.284853 93960 solver.cpp:317]     Train net output #1: loss = 2.75636 (* 1 = 2.75636 loss)
I1226 13:19:51.350906 85740 MultiSolver.cpp:108] [4] PROFILING END[Backward]
I1226 13:19:51.350999 85740 solver.cpp:291] [4] Iteration 6, loss = 3.55864
I1226 13:19:51.351078 85740 solver.cpp:317]     Train net output #0: accuracy = 0.28125
I1226 13:19:51.351153 85740 solver.cpp:317]     Train net output #1: loss = 3.55864 (* 1 = 3.55864 loss)
I1226 13:19:51.717200 91102 SynchronousNode.cpp:246] [5] PROFILING BEGIN[Forward]
I1226 13:19:51.717793 94574 SynchronousNode.cpp:246] [6] PROFILING BEGIN[Forward]
I1226 13:19:51.721851 91029 MultiSolver.cpp:92] [5] PROFILING END[Forward]
I1226 13:19:51.721974 91029 MultiSolver.cpp:94] [5] PROFILING BEGIN[Backward]
I1226 13:19:51.722297 96304 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 13:19:51.722394 96304 solver.cpp:291] [2] Iteration 6, loss = 2.64045
I1226 13:19:51.722460 96304 solver.cpp:317]     Train net output #0: accuracy = 0.5625
I1226 13:19:51.722527 96304 solver.cpp:317]     Train net output #1: loss = 2.64045 (* 1 = 2.64045 loss)
I1226 13:19:51.731081 96696 MultiSolver.cpp:108] [7] PROFILING END[Backward]
I1226 13:19:51.731171 96696 solver.cpp:291] [7] Iteration 6, loss = 3.2829
I1226 13:19:51.731248 96696 solver.cpp:317]     Train net output #0: accuracy = 0.28125
I1226 13:19:51.731325 96696 solver.cpp:317]     Train net output #1: loss = 3.2829 (* 1 = 3.2829 loss)
I1226 13:19:51.807657 94502 MultiSolver.cpp:92] [6] PROFILING END[Forward]
I1226 13:19:51.807799 94502 MultiSolver.cpp:94] [6] PROFILING BEGIN[Backward]
I1226 13:19:51.828598 91029 MultiSolver.cpp:108] [5] PROFILING END[Backward]
I1226 13:19:51.828744 91029 solver.cpp:291] [5] Iteration 6, loss = 3.4553
I1226 13:19:51.828822 91029 solver.cpp:317]     Train net output #0: accuracy = 0.28125
I1226 13:19:51.828897 91029 solver.cpp:317]     Train net output #1: loss = 3.4553 (* 1 = 3.4553 loss)
I1226 13:19:52.330773 94502 MultiSolver.cpp:108] [6] PROFILING END[Backward]
I1226 13:19:52.330901 94502 solver.cpp:291] [6] Iteration 6, loss = 3.07783
I1226 13:19:52.330976 94502 solver.cpp:317]     Train net output #0: accuracy = 0.34375
I1226 13:19:52.331051 94502 solver.cpp:317]     Train net output #1: loss = 3.07783 (* 1 = 3.07783 loss)
I1226 13:19:58.848320 98267 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 13:19:58.848426 98267 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 13:19:58.915854 98267 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 13:19:58.915947 98267 solver.cpp:291] [0] Iteration 7, loss = 3.06267
I1226 13:19:58.916008 98267 solver.cpp:317]     Train net output #0: accuracy = 0.34375
I1226 13:19:58.916115 98267 solver.cpp:317]     Train net output #1: loss = 3.06267 (* 1 = 3.06267 loss)
I1226 13:20:00.404062 85812 SynchronousNode.cpp:246] [4] PROFILING BEGIN[Forward]
I1226 13:20:00.392400 94029 SynchronousNode.cpp:246] [3] PROFILING BEGIN[Forward]
I1226 13:20:00.401795 92166 SynchronousNode.cpp:246] [1] PROFILING BEGIN[Forward]
I1226 13:20:00.396908 96766 SynchronousNode.cpp:246] [7] PROFILING BEGIN[Forward]
I1226 13:20:00.399706 96376 SynchronousNode.cpp:246] [2] PROFILING BEGIN[Forward]
I1226 13:20:00.395077 93960 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 13:20:00.404562 92093 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 13:20:00.395191 93960 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 13:20:00.404683 92093 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 13:20:00.407611 85740 MultiSolver.cpp:92] [4] PROFILING END[Forward]
I1226 13:20:00.407768 85740 MultiSolver.cpp:94] [4] PROFILING BEGIN[Backward]
I1226 13:20:00.408046 96304 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 13:20:00.405361 96696 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 13:20:00.408331 96304 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 13:20:00.405481 96696 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 13:20:00.461040 92093 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 13:20:00.461127 92093 solver.cpp:291] [1] Iteration 7, loss = 3.64025
I1226 13:20:00.461194 92093 solver.cpp:317]     Train net output #0: accuracy = 0.1875
I1226 13:20:00.461259 92093 solver.cpp:317]     Train net output #1: loss = 3.64025 (* 1 = 3.64025 loss)
I1226 13:20:00.453778 93960 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 13:20:00.453863 93960 solver.cpp:291] [3] Iteration 7, loss = 3.39041
I1226 13:20:00.453932 93960 solver.cpp:317]     Train net output #0: accuracy = 0.34375
I1226 13:20:00.453996 93960 solver.cpp:317]     Train net output #1: loss = 3.39041 (* 1 = 3.39041 loss)
I1226 13:20:00.513654 85740 MultiSolver.cpp:108] [4] PROFILING END[Backward]
I1226 13:20:00.513778 85740 solver.cpp:291] [4] Iteration 7, loss = 3.96673
I1226 13:20:00.513878 85740 solver.cpp:317]     Train net output #0: accuracy = 0.1875
I1226 13:20:00.513980 85740 solver.cpp:317]     Train net output #1: loss = 3.96673 (* 1 = 3.96673 loss)
I1226 13:20:00.844010 91102 SynchronousNode.cpp:246] [5] PROFILING BEGIN[Forward]
I1226 13:20:00.844677 94574 SynchronousNode.cpp:246] [6] PROFILING BEGIN[Forward]
I1226 13:20:00.847136 91029 MultiSolver.cpp:92] [5] PROFILING END[Forward]
I1226 13:20:00.847257 91029 MultiSolver.cpp:94] [5] PROFILING BEGIN[Backward]
I1226 13:20:00.853251 94502 MultiSolver.cpp:92] [6] PROFILING END[Forward]
I1226 13:20:00.853374 94502 MultiSolver.cpp:94] [6] PROFILING BEGIN[Backward]
I1226 13:20:00.888314 96304 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 13:20:00.888412 96304 solver.cpp:291] [2] Iteration 7, loss = 3.70765
I1226 13:20:00.888478 96304 solver.cpp:317]     Train net output #0: accuracy = 0.28125
I1226 13:20:00.888542 96304 solver.cpp:317]     Train net output #1: loss = 3.70765 (* 1 = 3.70765 loss)
I1226 13:20:00.898994 96696 MultiSolver.cpp:108] [7] PROFILING END[Backward]
I1226 13:20:00.899132 96696 solver.cpp:291] [7] Iteration 7, loss = 3.11149
I1226 13:20:00.899211 96696 solver.cpp:317]     Train net output #0: accuracy = 0.3125
I1226 13:20:00.899330 96696 solver.cpp:317]     Train net output #1: loss = 3.11149 (* 1 = 3.11149 loss)
I1226 13:20:00.966267 91029 MultiSolver.cpp:108] [5] PROFILING END[Backward]
I1226 13:20:00.966359 91029 solver.cpp:291] [5] Iteration 7, loss = 2.78765
I1226 13:20:00.966436 91029 solver.cpp:317]     Train net output #0: accuracy = 0.34375
I1226 13:20:00.966512 91029 solver.cpp:317]     Train net output #1: loss = 2.78765 (* 1 = 2.78765 loss)
I1226 13:20:01.380725 94502 MultiSolver.cpp:108] [6] PROFILING END[Backward]
I1226 13:20:01.380857 94502 solver.cpp:291] [6] Iteration 7, loss = 3.47645
I1226 13:20:01.380935 94502 solver.cpp:317]     Train net output #0: accuracy = 0.34375
I1226 13:20:01.381014 94502 solver.cpp:317]     Train net output #1: loss = 3.47645 (* 1 = 3.47645 loss)
User defined signal 2
