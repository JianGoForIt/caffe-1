Dec 26 15:03:55 2016 98064 3 10.1 NIOS_DEBUG: stdin_fd set to -1
Dec 26 15:03:55 2016 98064 3 10.1 NIOS_DEBUG: fds[0] has a value of -1
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 15:04:02.753747 92056 mpiutil.cpp:166] Process rank 7 from number of 9 processes running on knl-node026
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 15:04:02.746544 93129 mpiutil.cpp:166] Process rank 1 from number of 9 processes running on knl-node060
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 15:04:02.731509 98074 mpiutil.cpp:166] Process rank 0 from number of 9 processes running on knl-node078
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 15:04:02.739981 95917 mpiutil.cpp:166] Process rank 2 from number of 9 processes running on knl-node084
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 15:04:02.749245 88631 mpiutil.cpp:166] Process rank 3 from number of 9 processes running on knl-node072
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 15:04:02.740530 92253 mpiutil.cpp:166] Process rank 6 from number of 9 processes running on knl-node047
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 15:04:02.749065 118847 mpiutil.cpp:166] Process rank 4 from number of 9 processes running on knl-node042
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 15:04:02.746237 92722 mpiutil.cpp:166] Process rank 5 from number of 9 processes running on knl-node019
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 15:04:02.731488 52875 mpiutil.cpp:166] Process rank 8 from number of 9 processes running on knl-node017
I1226 15:04:02.767416 92056 caffe.cpp:316] Use CPU.
I1226 15:04:02.760367 93129 caffe.cpp:316] Use CPU.
I1226 15:04:02.760080 92722 caffe.cpp:316] Use CPU.
I1226 15:04:02.768637 92056 solver.cpp:93] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_8.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 15:04:02.768762 92056 solver.cpp:128] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_8.prototxt
I1226 15:04:02.761505 93129 solver.cpp:93] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_2.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 15:04:02.753736 95917 caffe.cpp:316] Use CPU.
I1226 15:04:02.761600 93129 solver.cpp:128] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_2.prototxt
I1226 15:04:02.755084 95917 solver.cpp:93] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_3.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 15:04:02.755233 95917 solver.cpp:128] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_3.prototxt
I1226 15:04:02.761234 92722 solver.cpp:93] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_6.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 15:04:02.762210 92722 solver.cpp:128] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_6.prototxt
I1226 15:04:02.756495 92253 caffe.cpp:316] Use CPU.
I1226 15:04:02.765671 118847 caffe.cpp:316] Use CPU.
I1226 15:04:02.758079 92253 solver.cpp:93] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_7.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 15:04:02.758267 92253 solver.cpp:128] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_7.prototxt
I1226 15:04:02.748598 98074 caffe.cpp:316] Use CPU.
I1226 15:04:02.767069 88631 caffe.cpp:316] Use CPU.
I1226 15:04:02.766834 118847 solver.cpp:93] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_5.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 15:04:02.767882 118847 solver.cpp:128] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_5.prototxt
I1226 15:04:02.768226 88631 solver.cpp:93] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_4.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 15:04:02.768333 88631 solver.cpp:128] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_4.prototxt
I1226 15:04:02.749948 98074 solver.cpp:93] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_1.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 15:04:02.750977 98074 solver.cpp:128] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_1.prototxt
I1226 15:04:02.752616 52875 caffe.cpp:316] Use CPU.
I1226 15:04:02.753825 52875 solver.cpp:93] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_dummy.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 15:04:02.754760 52875 solver.cpp:128] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_dummy.prototxt
I1226 15:04:02.799263 92056 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 15:04:02.799341 92056 cpu_info.cpp:455] Total number of sockets: 1
I1226 15:04:02.799365 92056 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 15:04:02.799386 92056 cpu_info.cpp:461] Total number of processors: 272
I1226 15:04:02.799406 92056 cpu_info.cpp:464] GPU is used: no
I1226 15:04:02.792371 93129 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 15:04:02.799427 92056 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 15:04:02.792441 93129 cpu_info.cpp:455] Total number of sockets: 1
I1226 15:04:02.792466 93129 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 15:04:02.792486 93129 cpu_info.cpp:461] Total number of processors: 272
I1226 15:04:02.792506 93129 cpu_info.cpp:464] GPU is used: no
I1226 15:04:02.799446 92056 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 15:04:02.792526 93129 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 15:04:02.792546 93129 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 15:04:02.794049 92722 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 15:04:02.794126 92722 cpu_info.cpp:455] Total number of sockets: 1
I1226 15:04:02.794148 92722 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 15:04:02.794169 92722 cpu_info.cpp:461] Total number of processors: 272
I1226 15:04:02.794189 92722 cpu_info.cpp:464] GPU is used: no
I1226 15:04:02.794209 92722 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 15:04:02.794229 92722 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 15:04:02.789942 92253 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 15:04:02.790014 92253 cpu_info.cpp:455] Total number of sockets: 1
I1226 15:04:02.790037 92253 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 15:04:02.790057 92253 cpu_info.cpp:461] Total number of processors: 272
I1226 15:04:02.790077 92253 cpu_info.cpp:464] GPU is used: no
I1226 15:04:02.790097 92253 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 15:04:02.790117 92253 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 15:04:02.799497 88631 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 15:04:02.799572 88631 cpu_info.cpp:455] Total number of sockets: 1
I1226 15:04:02.799595 88631 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 15:04:02.799617 88631 cpu_info.cpp:461] Total number of processors: 272
I1226 15:04:02.799636 88631 cpu_info.cpp:464] GPU is used: no
I1226 15:04:02.799656 88631 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 15:04:02.799677 88631 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 15:04:02.800050 118847 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 15:04:02.800122 118847 cpu_info.cpp:455] Total number of sockets: 1
I1226 15:04:02.800145 118847 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 15:04:02.800165 118847 cpu_info.cpp:461] Total number of processors: 272
I1226 15:04:02.800186 118847 cpu_info.cpp:464] GPU is used: no
I1226 15:04:02.800206 118847 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 15:04:02.800227 118847 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 15:04:02.784559 98074 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 15:04:02.784651 98074 cpu_info.cpp:455] Total number of sockets: 1
I1226 15:04:02.784679 98074 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 15:04:02.784703 98074 cpu_info.cpp:461] Total number of processors: 272
I1226 15:04:02.784752 98074 cpu_info.cpp:464] GPU is used: no
I1226 15:04:02.784785 98074 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 15:04:02.784811 98074 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 15:04:02.785151 52875 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 15:04:02.785224 52875 cpu_info.cpp:455] Total number of sockets: 1
I1226 15:04:02.785248 52875 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 15:04:02.785267 52875 cpu_info.cpp:461] Total number of processors: 272
I1226 15:04:02.785287 52875 cpu_info.cpp:464] GPU is used: no
I1226 15:04:02.785307 52875 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 15:04:02.785327 52875 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 15:04:02.799278 95917 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 15:04:02.799372 95917 cpu_info.cpp:455] Total number of sockets: 1
I1226 15:04:02.799401 95917 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 15:04:02.799424 95917 cpu_info.cpp:461] Total number of processors: 272
I1226 15:04:02.799448 95917 cpu_info.cpp:464] GPU is used: no
I1226 15:04:02.799469 95917 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 15:04:02.799492 95917 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 15:04:02.828327 92253 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 15:04:02.828654 92253 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 15:04:02.830840 92253 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_6"
    batch_size: 32
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 15:04:02.831085 92253 layer_factory.hpp:114] Creating layer data
I1226 15:04:02.840129 118847 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 15:04:02.840414 118847 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 15:04:02.845934 92056 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 15:04:02.846320 92056 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 15:04:02.823626 98074 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 15:04:02.832381 95917 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 15:04:02.824019 98074 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 15:04:02.842046 118847 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_4"
    batch_size: 32
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 15:04:02.832772 95917 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 15:04:02.834106 92253 net.cpp:178] Creating Layer data
I1226 15:04:02.834184 92253 net.cpp:586] data -> data
I1226 15:04:02.834312 92253 net.cpp:586] data -> label
I1226 15:04:02.847978 92056 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_7"
    batch_size: 32
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 15:04:02.848168 92056 layer_factory.hpp:114] Creating layer data
I1226 15:04:02.826093 98074 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_0"
    batch_size: 32
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 15:04:02.835314 95917 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_2"
    batch_size: 32
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 15:04:02.845049 88631 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 15:04:02.835650 95917 layer_factory.hpp:114] Creating layer data
I1226 15:04:02.845429 88631 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 15:04:02.850431 92056 net.cpp:178] Creating Layer data
I1226 15:04:02.850507 92056 net.cpp:586] data -> data
I1226 15:04:02.850602 92056 net.cpp:586] data -> label
I1226 15:04:02.847681 88631 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_3"
    batch_size: 32
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 15:04:02.847954 88631 layer_factory.hpp:114] Creating layer data
I1226 15:04:02.838452 95917 net.cpp:178] Creating Layer data
I1226 15:04:02.838582 95917 net.cpp:586] data -> data
I1226 15:04:02.838747 95917 net.cpp:586] data -> label
I1226 15:04:02.850976 88631 net.cpp:178] Creating Layer data
I1226 15:04:02.851083 88631 net.cpp:586] data -> data
I1226 15:04:02.851183 88631 net.cpp:586] data -> label
I1226 15:04:02.851886 93129 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 15:04:02.852288 93129 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 15:04:02.854328 93129 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_1"
    batch_size: 32
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 15:04:02.854569 93129 layer_factory.hpp:114] Creating layer data
I1226 15:04:02.864568 92059 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_7
I1226 15:04:02.857777 93129 net.cpp:178] Creating Layer data
I1226 15:04:02.857956 93129 net.cpp:586] data -> data
I1226 15:04:02.858073 93129 net.cpp:586] data -> label
I1226 15:04:02.868420 92056 data_layer.cpp:80] output data size: 32,3,227,227
I1226 15:04:02.847676 52875 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 15:04:02.847987 52875 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 15:04:02.865171 92722 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 15:04:02.850252 52875 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "DummyData"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  dummy_data_param {
    data_filler {
      type: "gaussian"
      std: 1
    }
    data_filler {
      type: "constant"
      value: 0
    }
    shape {
      dim: 32
      dim: 3
      dim: 227
      dim: 227
    }
    shape {
      dim: 32
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 15:04:02.865594 92722 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 15:04:02.867871 92722 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_5"
    batch_size: 32
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 15:04:02.870950 118847 layer_factory.hpp:114] Creating layer data
I1226 15:04:02.867754 92256 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_6
I1226 15:04:02.892436 88634 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_3
I1226 15:04:02.890750 92722 layer_factory.hpp:114] Creating layer data
I1226 15:04:02.885097 95920 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_2
I1226 15:04:02.877400 52875 layer_factory.hpp:114] Creating layer data
I1226 15:04:02.895052 118847 net.cpp:178] Creating Layer data
I1226 15:04:02.877542 52875 net.cpp:178] Creating Layer data
I1226 15:04:02.895141 118847 net.cpp:586] data -> data
I1226 15:04:02.877610 52875 net.cpp:586] data -> data
I1226 15:04:02.877710 52875 net.cpp:586] data -> label
I1226 15:04:02.895215 118847 net.cpp:586] data -> label
I1226 15:04:02.890275 92253 data_layer.cpp:80] output data size: 32,3,227,227
I1226 15:04:02.881306 98074 layer_factory.hpp:114] Creating layer data
I1226 15:04:02.900846 88631 data_layer.cpp:80] output data size: 32,3,227,227
I1226 15:04:02.892982 95917 data_layer.cpp:80] output data size: 32,3,227,227
I1226 15:04:02.901628 92722 net.cpp:178] Creating Layer data
I1226 15:04:02.901731 92722 net.cpp:586] data -> data
I1226 15:04:02.901820 92722 net.cpp:586] data -> label
I1226 15:04:02.909911 93133 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_1
I1226 15:04:02.895452 98074 net.cpp:178] Creating Layer data
I1226 15:04:02.895583 98074 net.cpp:586] data -> data
I1226 15:04:02.895707 98074 net.cpp:586] data -> label
I1226 15:04:02.923061 93129 data_layer.cpp:80] output data size: 32,3,227,227
I1226 15:04:02.926273 118850 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_4
I1226 15:04:02.915529 52875 net.cpp:228] Setting up data
I1226 15:04:02.915643 52875 net.cpp:235] Top shape: 32 3 227 227 (4946784)
I1226 15:04:02.915678 52875 net.cpp:235] Top shape: 32 1 1 1 (32)
I1226 15:04:02.915700 52875 net.cpp:243] Memory required for data: 19787264
I1226 15:04:02.915735 52875 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 15:04:02.915820 52875 net.cpp:178] Creating Layer label_data_1_split
I1226 15:04:02.915853 52875 net.cpp:612] label_data_1_split <- label
I1226 15:04:02.915969 52875 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 15:04:02.916014 52875 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 15:04:02.916532 98077 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_0
I1226 15:04:02.936110 118847 data_layer.cpp:80] output data size: 32,3,227,227
I1226 15:04:02.945808 92056 net.cpp:228] Setting up data
I1226 15:04:02.945953 92056 net.cpp:235] Top shape: 32 3 227 227 (4946784)
I1226 15:04:02.946008 92056 net.cpp:235] Top shape: 32 (32)
I1226 15:04:02.946195 92056 net.cpp:243] Memory required for data: 19787264
I1226 15:04:02.938666 92725 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_5
I1226 15:04:02.946245 92056 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 15:04:02.946344 92056 net.cpp:178] Creating Layer label_data_1_split
I1226 15:04:02.946501 92056 net.cpp:612] label_data_1_split <- label
I1226 15:04:02.946565 92056 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 15:04:02.946715 92056 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 15:04:02.924825 98074 data_layer.cpp:80] output data size: 32,3,227,227
I1226 15:04:02.926934 52875 net.cpp:228] Setting up label_data_1_split
I1226 15:04:02.927037 52875 net.cpp:235] Top shape: 32 1 1 1 (32)
I1226 15:04:02.927068 52875 net.cpp:235] Top shape: 32 1 1 1 (32)
I1226 15:04:02.927090 52875 net.cpp:243] Memory required for data: 19787520
I1226 15:04:02.927145 52875 layer_factory.hpp:114] Creating layer conv1
I1226 15:04:02.927242 52875 net.cpp:178] Creating Layer conv1
I1226 15:04:02.927273 52875 net.cpp:612] conv1 <- data
I1226 15:04:02.927320 52875 net.cpp:586] conv1 -> conv1
I1226 15:04:02.948220 92722 data_layer.cpp:80] output data size: 32,3,227,227
I1226 15:04:02.972751 92056 net.cpp:228] Setting up label_data_1_split
I1226 15:04:02.972901 92056 net.cpp:235] Top shape: 32 (32)
I1226 15:04:02.972940 92056 net.cpp:235] Top shape: 32 (32)
I1226 15:04:02.972966 92056 net.cpp:243] Memory required for data: 19787520
I1226 15:04:02.973157 92056 layer_factory.hpp:114] Creating layer conv1
I1226 15:04:02.973263 92056 net.cpp:178] Creating Layer conv1
I1226 15:04:02.973394 92056 net.cpp:612] conv1 <- data
I1226 15:04:02.973446 92056 net.cpp:586] conv1 -> conv1
I1226 15:04:02.962754 92253 net.cpp:228] Setting up data
I1226 15:04:02.962884 92253 net.cpp:235] Top shape: 32 3 227 227 (4946784)
I1226 15:04:02.962954 92253 net.cpp:235] Top shape: 32 (32)
I1226 15:04:02.962981 92253 net.cpp:243] Memory required for data: 19787264
I1226 15:04:02.963030 92253 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 15:04:02.963119 92253 net.cpp:178] Creating Layer label_data_1_split
I1226 15:04:02.963294 92253 net.cpp:612] label_data_1_split <- label
I1226 15:04:02.963351 92253 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 15:04:02.963405 92253 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 15:04:02.975719 88631 net.cpp:228] Setting up data
I1226 15:04:02.975850 88631 net.cpp:235] Top shape: 32 3 227 227 (4946784)
I1226 15:04:02.975888 88631 net.cpp:235] Top shape: 32 (32)
I1226 15:04:02.975913 88631 net.cpp:243] Memory required for data: 19787264
I1226 15:04:02.975975 88631 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 15:04:02.976056 88631 net.cpp:178] Creating Layer label_data_1_split
I1226 15:04:02.976179 88631 net.cpp:612] label_data_1_split <- label
I1226 15:04:02.976223 88631 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 15:04:02.976274 88631 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 15:04:02.982506 92253 net.cpp:228] Setting up label_data_1_split
I1226 15:04:02.982612 92253 net.cpp:235] Top shape: 32 (32)
I1226 15:04:02.982645 92253 net.cpp:235] Top shape: 32 (32)
I1226 15:04:02.982669 92253 net.cpp:243] Memory required for data: 19787520
I1226 15:04:02.982733 92253 layer_factory.hpp:114] Creating layer conv1
I1226 15:04:02.982838 92253 net.cpp:178] Creating Layer conv1
I1226 15:04:02.982877 92253 net.cpp:612] conv1 <- data
I1226 15:04:02.982920 92253 net.cpp:586] conv1 -> conv1
I1226 15:04:02.998755 88631 net.cpp:228] Setting up label_data_1_split
I1226 15:04:02.998894 88631 net.cpp:235] Top shape: 32 (32)
I1226 15:04:02.998973 88631 net.cpp:235] Top shape: 32 (32)
I1226 15:04:02.999009 88631 net.cpp:243] Memory required for data: 19787520
I1226 15:04:02.999068 88631 layer_factory.hpp:114] Creating layer conv1
I1226 15:04:02.999167 88631 net.cpp:178] Creating Layer conv1
I1226 15:04:02.999217 88631 net.cpp:612] conv1 <- data
I1226 15:04:02.999279 88631 net.cpp:586] conv1 -> conv1
I1226 15:04:03.000651 93129 net.cpp:228] Setting up data
I1226 15:04:03.000780 93129 net.cpp:235] Top shape: 32 3 227 227 (4946784)
I1226 15:04:03.000857 93129 net.cpp:235] Top shape: 32 (32)
I1226 15:04:03.000888 93129 net.cpp:243] Memory required for data: 19787264
I1226 15:04:03.000931 93129 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 15:04:03.001169 93129 net.cpp:178] Creating Layer label_data_1_split
I1226 15:04:03.001315 93129 net.cpp:612] label_data_1_split <- label
I1226 15:04:03.001372 93129 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 15:04:03.001431 93129 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 15:04:03.010465 118847 net.cpp:228] Setting up data
I1226 15:04:03.010648 118847 net.cpp:235] Top shape: 32 3 227 227 (4946784)
I1226 15:04:03.010685 118847 net.cpp:235] Top shape: 32 (32)
I1226 15:04:03.010710 118847 net.cpp:243] Memory required for data: 19787264
I1226 15:04:03.010746 118847 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 15:04:03.010977 118847 net.cpp:178] Creating Layer label_data_1_split
I1226 15:04:03.011016 118847 net.cpp:612] label_data_1_split <- label
I1226 15:04:03.011055 118847 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 15:04:03.011102 118847 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 15:04:03.015223 93129 net.cpp:228] Setting up label_data_1_split
I1226 15:04:03.015328 93129 net.cpp:235] Top shape: 32 (32)
I1226 15:04:03.015365 93129 net.cpp:235] Top shape: 32 (32)
I1226 15:04:03.015390 93129 net.cpp:243] Memory required for data: 19787520
I1226 15:04:03.015426 93129 layer_factory.hpp:114] Creating layer conv1
I1226 15:04:03.015557 93129 net.cpp:178] Creating Layer conv1
I1226 15:04:03.015635 93129 net.cpp:612] conv1 <- data
I1226 15:04:03.015689 93129 net.cpp:586] conv1 -> conv1
I1226 15:04:03.024081 92722 net.cpp:228] Setting up data
I1226 15:04:03.024210 92722 net.cpp:235] Top shape: 32 3 227 227 (4946784)
I1226 15:04:03.024246 92722 net.cpp:235] Top shape: 32 (32)
I1226 15:04:03.024298 92722 net.cpp:243] Memory required for data: 19787264
I1226 15:04:03.024379 92722 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 15:04:03.024472 92722 net.cpp:178] Creating Layer label_data_1_split
I1226 15:04:03.024610 92722 net.cpp:612] label_data_1_split <- label
I1226 15:04:03.024722 92722 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 15:04:03.024852 92722 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 15:04:03.030510 118847 net.cpp:228] Setting up label_data_1_split
I1226 15:04:03.030673 118847 net.cpp:235] Top shape: 32 (32)
I1226 15:04:03.030707 118847 net.cpp:235] Top shape: 32 (32)
I1226 15:04:03.030731 118847 net.cpp:243] Memory required for data: 19787520
I1226 15:04:03.030881 118847 layer_factory.hpp:114] Creating layer conv1
I1226 15:04:03.030977 118847 net.cpp:178] Creating Layer conv1
I1226 15:04:03.031015 118847 net.cpp:612] conv1 <- data
I1226 15:04:03.031059 118847 net.cpp:586] conv1 -> conv1
I1226 15:04:03.021939 52875 net.cpp:228] Setting up conv1
I1226 15:04:03.022049 52875 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 15:04:03.022075 52875 net.cpp:243] Memory required for data: 56958720
I1226 15:04:03.022213 52875 layer_factory.hpp:114] Creating layer relu1
I1226 15:04:03.022289 52875 net.cpp:178] Creating Layer relu1
I1226 15:04:03.022331 52875 net.cpp:612] relu1 <- conv1
I1226 15:04:03.022367 52875 net.cpp:573] relu1 -> conv1 (in-place)
I1226 15:04:03.022465 52875 net.cpp:228] Setting up relu1
I1226 15:04:03.022503 52875 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 15:04:03.022531 52875 net.cpp:243] Memory required for data: 94129920
I1226 15:04:03.022557 52875 layer_factory.hpp:114] Creating layer norm1
I1226 15:04:03.022619 52875 net.cpp:178] Creating Layer norm1
I1226 15:04:03.022650 52875 net.cpp:612] norm1 <- conv1
I1226 15:04:03.022683 52875 net.cpp:586] norm1 -> norm1
I1226 15:04:03.022769 52875 net.cpp:228] Setting up norm1
I1226 15:04:03.022807 52875 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 15:04:03.022830 52875 net.cpp:243] Memory required for data: 131301120
I1226 15:04:03.022855 52875 layer_factory.hpp:114] Creating layer pool1
I1226 15:04:03.022905 52875 net.cpp:178] Creating Layer pool1
I1226 15:04:03.022934 52875 net.cpp:612] pool1 <- norm1
I1226 15:04:03.022966 52875 net.cpp:586] pool1 -> pool1
I1226 15:04:03.031214 95917 net.cpp:228] Setting up data
I1226 15:04:03.023051 52875 net.cpp:228] Setting up pool1
I1226 15:04:03.031428 95917 net.cpp:235] Top shape: 32 3 227 227 (4946784)
I1226 15:04:03.023084 52875 net.cpp:235] Top shape: 32 96 27 27 (2239488)
I1226 15:04:03.031498 95917 net.cpp:235] Top shape: 32 (32)
I1226 15:04:03.023111 52875 net.cpp:243] Memory required for data: 140259072
I1226 15:04:03.031637 95917 net.cpp:243] Memory required for data: 19787264
I1226 15:04:03.031689 95917 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 15:04:03.023167 52875 layer_factory.hpp:114] Creating layer conv2
I1226 15:04:03.023234 52875 net.cpp:178] Creating Layer conv2
I1226 15:04:03.023268 52875 net.cpp:612] conv2 <- pool1
I1226 15:04:03.023313 52875 net.cpp:586] conv2 -> conv2
I1226 15:04:03.031849 95917 net.cpp:178] Creating Layer label_data_1_split
I1226 15:04:03.032084 95917 net.cpp:612] label_data_1_split <- label
I1226 15:04:03.032505 95917 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 15:04:03.032677 95917 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 15:04:03.046185 92722 net.cpp:228] Setting up label_data_1_split
I1226 15:04:03.046342 92722 net.cpp:235] Top shape: 32 (32)
I1226 15:04:03.046380 92722 net.cpp:235] Top shape: 32 (32)
I1226 15:04:03.046402 92722 net.cpp:243] Memory required for data: 19787520
I1226 15:04:03.046489 92722 layer_factory.hpp:114] Creating layer conv1
I1226 15:04:03.046592 92722 net.cpp:178] Creating Layer conv1
I1226 15:04:03.046675 92722 net.cpp:612] conv1 <- data
I1226 15:04:03.046726 92722 net.cpp:586] conv1 -> conv1
I1226 15:04:03.055462 98074 net.cpp:228] Setting up data
I1226 15:04:03.055649 98074 net.cpp:235] Top shape: 32 3 227 227 (4946784)
I1226 15:04:03.055824 98074 net.cpp:235] Top shape: 32 (32)
I1226 15:04:03.055883 98074 net.cpp:243] Memory required for data: 19787264
I1226 15:04:03.055941 98074 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 15:04:03.056147 98074 net.cpp:178] Creating Layer label_data_1_split
I1226 15:04:03.056310 98074 net.cpp:612] label_data_1_split <- label
I1226 15:04:03.056381 98074 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 15:04:03.056890 98074 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 15:04:03.070268 95917 net.cpp:228] Setting up label_data_1_split
I1226 15:04:03.070387 95917 net.cpp:235] Top shape: 32 (32)
I1226 15:04:03.070428 95917 net.cpp:235] Top shape: 32 (32)
I1226 15:04:03.070456 95917 net.cpp:243] Memory required for data: 19787520
I1226 15:04:03.070497 95917 layer_factory.hpp:114] Creating layer conv1
I1226 15:04:03.070616 95917 net.cpp:178] Creating Layer conv1
I1226 15:04:03.070662 95917 net.cpp:612] conv1 <- data
I1226 15:04:03.070719 95917 net.cpp:586] conv1 -> conv1
I1226 15:04:03.083982 98074 net.cpp:228] Setting up label_data_1_split
I1226 15:04:03.084094 98074 net.cpp:235] Top shape: 32 (32)
I1226 15:04:03.084131 98074 net.cpp:235] Top shape: 32 (32)
I1226 15:04:03.084157 98074 net.cpp:243] Memory required for data: 19787520
I1226 15:04:03.084197 98074 layer_factory.hpp:114] Creating layer conv1
I1226 15:04:03.084307 98074 net.cpp:178] Creating Layer conv1
I1226 15:04:03.084518 98074 net.cpp:612] conv1 <- data
I1226 15:04:03.084578 98074 net.cpp:586] conv1 -> conv1
I1226 15:04:03.135761 92056 net.cpp:228] Setting up conv1
I1226 15:04:03.135917 92056 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 15:04:03.135954 92056 net.cpp:243] Memory required for data: 56958720
I1226 15:04:03.136070 92056 layer_factory.hpp:114] Creating layer relu1
I1226 15:04:03.136178 92056 net.cpp:178] Creating Layer relu1
I1226 15:04:03.136232 92056 net.cpp:612] relu1 <- conv1
I1226 15:04:03.136276 92056 net.cpp:573] relu1 -> conv1 (in-place)
I1226 15:04:03.136405 92056 net.cpp:228] Setting up relu1
I1226 15:04:03.136463 92056 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 15:04:03.136488 92056 net.cpp:243] Memory required for data: 94129920
I1226 15:04:03.136521 92056 layer_factory.hpp:114] Creating layer norm1
I1226 15:04:03.136600 92056 net.cpp:178] Creating Layer norm1
I1226 15:04:03.136642 92056 net.cpp:612] norm1 <- conv1
I1226 15:04:03.136682 92056 net.cpp:586] norm1 -> norm1
I1226 15:04:03.136781 92056 net.cpp:228] Setting up norm1
I1226 15:04:03.136875 92056 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 15:04:03.136901 92056 net.cpp:243] Memory required for data: 131301120
I1226 15:04:03.136934 92056 layer_factory.hpp:114] Creating layer pool1
I1226 15:04:03.137017 92056 net.cpp:178] Creating Layer pool1
I1226 15:04:03.137053 92056 net.cpp:612] pool1 <- norm1
I1226 15:04:03.137106 92056 net.cpp:586] pool1 -> pool1
I1226 15:04:03.137222 92056 net.cpp:228] Setting up pool1
I1226 15:04:03.137271 92056 net.cpp:235] Top shape: 32 96 27 27 (2239488)
I1226 15:04:03.137295 92056 net.cpp:243] Memory required for data: 140259072
I1226 15:04:03.137323 92056 layer_factory.hpp:114] Creating layer conv2
I1226 15:04:03.137408 92056 net.cpp:178] Creating Layer conv2
I1226 15:04:03.137440 92056 net.cpp:612] conv2 <- pool1
I1226 15:04:03.137486 92056 net.cpp:586] conv2 -> conv2
I1226 15:04:03.130453 92253 net.cpp:228] Setting up conv1
I1226 15:04:03.130563 92253 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 15:04:03.130591 92253 net.cpp:243] Memory required for data: 56958720
I1226 15:04:03.130728 92253 layer_factory.hpp:114] Creating layer relu1
I1226 15:04:03.130825 92253 net.cpp:178] Creating Layer relu1
I1226 15:04:03.130864 92253 net.cpp:612] relu1 <- conv1
I1226 15:04:03.130919 92253 net.cpp:573] relu1 -> conv1 (in-place)
I1226 15:04:03.131026 92253 net.cpp:228] Setting up relu1
I1226 15:04:03.131074 92253 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 15:04:03.131111 92253 net.cpp:243] Memory required for data: 94129920
I1226 15:04:03.131142 92253 layer_factory.hpp:114] Creating layer norm1
I1226 15:04:03.131242 92253 net.cpp:178] Creating Layer norm1
I1226 15:04:03.131368 92253 net.cpp:612] norm1 <- conv1
I1226 15:04:03.131427 92253 net.cpp:586] norm1 -> norm1
I1226 15:04:03.131528 92253 net.cpp:228] Setting up norm1
I1226 15:04:03.131577 92253 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 15:04:03.131602 92253 net.cpp:243] Memory required for data: 131301120
I1226 15:04:03.131631 92253 layer_factory.hpp:114] Creating layer pool1
I1226 15:04:03.131695 92253 net.cpp:178] Creating Layer pool1
I1226 15:04:03.131727 92253 net.cpp:612] pool1 <- norm1
I1226 15:04:03.131762 92253 net.cpp:586] pool1 -> pool1
I1226 15:04:03.131857 92253 net.cpp:228] Setting up pool1
I1226 15:04:03.131901 92253 net.cpp:235] Top shape: 32 96 27 27 (2239488)
I1226 15:04:03.131923 92253 net.cpp:243] Memory required for data: 140259072
I1226 15:04:03.131950 92253 layer_factory.hpp:114] Creating layer conv2
I1226 15:04:03.132020 92253 net.cpp:178] Creating Layer conv2
I1226 15:04:03.132055 92253 net.cpp:612] conv2 <- pool1
I1226 15:04:03.132097 92253 net.cpp:586] conv2 -> conv2
I1226 15:04:03.152745 88631 net.cpp:228] Setting up conv1
I1226 15:04:03.152863 88631 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 15:04:03.152889 88631 net.cpp:243] Memory required for data: 56958720
I1226 15:04:03.153023 88631 layer_factory.hpp:114] Creating layer relu1
I1226 15:04:03.153133 88631 net.cpp:178] Creating Layer relu1
I1226 15:04:03.153174 88631 net.cpp:612] relu1 <- conv1
I1226 15:04:03.153223 88631 net.cpp:573] relu1 -> conv1 (in-place)
I1226 15:04:03.153337 88631 net.cpp:228] Setting up relu1
I1226 15:04:03.153383 88631 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 15:04:03.153406 88631 net.cpp:243] Memory required for data: 94129920
I1226 15:04:03.153435 88631 layer_factory.hpp:114] Creating layer norm1
I1226 15:04:03.153504 88631 net.cpp:178] Creating Layer norm1
I1226 15:04:03.153532 88631 net.cpp:612] norm1 <- conv1
I1226 15:04:03.153574 88631 net.cpp:586] norm1 -> norm1
I1226 15:04:03.153673 88631 net.cpp:228] Setting up norm1
I1226 15:04:03.153715 88631 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 15:04:03.153738 88631 net.cpp:243] Memory required for data: 131301120
I1226 15:04:03.153766 88631 layer_factory.hpp:114] Creating layer pool1
I1226 15:04:03.153834 88631 net.cpp:178] Creating Layer pool1
I1226 15:04:03.153861 88631 net.cpp:612] pool1 <- norm1
I1226 15:04:03.153898 88631 net.cpp:586] pool1 -> pool1
I1226 15:04:03.154013 88631 net.cpp:228] Setting up pool1
I1226 15:04:03.154062 88631 net.cpp:235] Top shape: 32 96 27 27 (2239488)
I1226 15:04:03.154089 88631 net.cpp:243] Memory required for data: 140259072
I1226 15:04:03.154124 88631 layer_factory.hpp:114] Creating layer conv2
I1226 15:04:03.154197 88631 net.cpp:178] Creating Layer conv2
I1226 15:04:03.154230 88631 net.cpp:612] conv2 <- pool1
I1226 15:04:03.154274 88631 net.cpp:586] conv2 -> conv2
I1226 15:04:03.156630 52875 net.cpp:228] Setting up conv2
I1226 15:04:03.156743 52875 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 15:04:03.156766 52875 net.cpp:243] Memory required for data: 164146944
I1226 15:04:03.156863 52875 layer_factory.hpp:114] Creating layer relu2
I1226 15:04:03.156944 52875 net.cpp:178] Creating Layer relu2
I1226 15:04:03.156975 52875 net.cpp:612] relu2 <- conv2
I1226 15:04:03.157024 52875 net.cpp:573] relu2 -> conv2 (in-place)
I1226 15:04:03.157142 52875 net.cpp:228] Setting up relu2
I1226 15:04:03.157192 52875 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 15:04:03.157217 52875 net.cpp:243] Memory required for data: 188034816
I1226 15:04:03.157245 52875 layer_factory.hpp:114] Creating layer norm2
I1226 15:04:03.157299 52875 net.cpp:178] Creating Layer norm2
I1226 15:04:03.157323 52875 net.cpp:612] norm2 <- conv2
I1226 15:04:03.157357 52875 net.cpp:586] norm2 -> norm2
I1226 15:04:03.157443 52875 net.cpp:228] Setting up norm2
I1226 15:04:03.157486 52875 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 15:04:03.157510 52875 net.cpp:243] Memory required for data: 211922688
I1226 15:04:03.157536 52875 layer_factory.hpp:114] Creating layer pool2
I1226 15:04:03.157584 52875 net.cpp:178] Creating Layer pool2
I1226 15:04:03.157614 52875 net.cpp:612] pool2 <- norm2
I1226 15:04:03.157660 52875 net.cpp:586] pool2 -> pool2
I1226 15:04:03.157732 52875 net.cpp:228] Setting up pool2
I1226 15:04:03.157769 52875 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 15:04:03.157886 52875 net.cpp:243] Memory required for data: 217460480
I1226 15:04:03.157917 52875 layer_factory.hpp:114] Creating layer conv3
I1226 15:04:03.157990 52875 net.cpp:178] Creating Layer conv3
I1226 15:04:03.158023 52875 net.cpp:612] conv3 <- pool2
I1226 15:04:03.158062 52875 net.cpp:586] conv3 -> conv3
I1226 15:04:03.175155 93129 net.cpp:228] Setting up conv1
I1226 15:04:03.175272 93129 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 15:04:03.175307 93129 net.cpp:243] Memory required for data: 56958720
I1226 15:04:03.175421 93129 layer_factory.hpp:114] Creating layer relu1
I1226 15:04:03.175524 93129 net.cpp:178] Creating Layer relu1
I1226 15:04:03.175570 93129 net.cpp:612] relu1 <- conv1
I1226 15:04:03.175613 93129 net.cpp:573] relu1 -> conv1 (in-place)
I1226 15:04:03.175729 93129 net.cpp:228] Setting up relu1
I1226 15:04:03.175786 93129 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 15:04:03.176188 93129 net.cpp:243] Memory required for data: 94129920
I1226 15:04:03.176245 93129 layer_factory.hpp:114] Creating layer norm1
I1226 15:04:03.176393 93129 net.cpp:178] Creating Layer norm1
I1226 15:04:03.176455 93129 net.cpp:612] norm1 <- conv1
I1226 15:04:03.176555 93129 net.cpp:586] norm1 -> norm1
I1226 15:04:03.176743 93129 net.cpp:228] Setting up norm1
I1226 15:04:03.176826 93129 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 15:04:03.176853 93129 net.cpp:243] Memory required for data: 131301120
I1226 15:04:03.176890 93129 layer_factory.hpp:114] Creating layer pool1
I1226 15:04:03.176959 93129 net.cpp:178] Creating Layer pool1
I1226 15:04:03.176997 93129 net.cpp:612] pool1 <- norm1
I1226 15:04:03.177043 93129 net.cpp:586] pool1 -> pool1
I1226 15:04:03.177148 93129 net.cpp:228] Setting up pool1
I1226 15:04:03.177197 93129 net.cpp:235] Top shape: 32 96 27 27 (2239488)
I1226 15:04:03.177220 93129 net.cpp:243] Memory required for data: 140259072
I1226 15:04:03.177259 93129 layer_factory.hpp:114] Creating layer conv2
I1226 15:04:03.177340 93129 net.cpp:178] Creating Layer conv2
I1226 15:04:03.177371 93129 net.cpp:612] conv2 <- pool1
I1226 15:04:03.177412 93129 net.cpp:586] conv2 -> conv2
I1226 15:04:03.189318 118847 net.cpp:228] Setting up conv1
I1226 15:04:03.189435 118847 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 15:04:03.189465 118847 net.cpp:243] Memory required for data: 56958720
I1226 15:04:03.189605 118847 layer_factory.hpp:114] Creating layer relu1
I1226 15:04:03.189712 118847 net.cpp:178] Creating Layer relu1
I1226 15:04:03.189757 118847 net.cpp:612] relu1 <- conv1
I1226 15:04:03.189800 118847 net.cpp:573] relu1 -> conv1 (in-place)
I1226 15:04:03.189911 118847 net.cpp:228] Setting up relu1
I1226 15:04:03.189955 118847 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 15:04:03.189987 118847 net.cpp:243] Memory required for data: 94129920
I1226 15:04:03.190017 118847 layer_factory.hpp:114] Creating layer norm1
I1226 15:04:03.190088 118847 net.cpp:178] Creating Layer norm1
I1226 15:04:03.190124 118847 net.cpp:612] norm1 <- conv1
I1226 15:04:03.190168 118847 net.cpp:586] norm1 -> norm1
I1226 15:04:03.190259 118847 net.cpp:228] Setting up norm1
I1226 15:04:03.190310 118847 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 15:04:03.190332 118847 net.cpp:243] Memory required for data: 131301120
I1226 15:04:03.190362 118847 layer_factory.hpp:114] Creating layer pool1
I1226 15:04:03.190421 118847 net.cpp:178] Creating Layer pool1
I1226 15:04:03.190454 118847 net.cpp:612] pool1 <- norm1
I1226 15:04:03.190500 118847 net.cpp:586] pool1 -> pool1
I1226 15:04:03.190616 118847 net.cpp:228] Setting up pool1
I1226 15:04:03.190665 118847 net.cpp:235] Top shape: 32 96 27 27 (2239488)
I1226 15:04:03.190688 118847 net.cpp:243] Memory required for data: 140259072
I1226 15:04:03.190716 118847 layer_factory.hpp:114] Creating layer conv2
I1226 15:04:03.190789 118847 net.cpp:178] Creating Layer conv2
I1226 15:04:03.190824 118847 net.cpp:612] conv2 <- pool1
I1226 15:04:03.190865 118847 net.cpp:586] conv2 -> conv2
I1226 15:04:03.212409 92722 net.cpp:228] Setting up conv1
I1226 15:04:03.212832 92722 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 15:04:03.212882 92722 net.cpp:243] Memory required for data: 56958720
I1226 15:04:03.213069 92722 layer_factory.hpp:114] Creating layer relu1
I1226 15:04:03.213228 92722 net.cpp:178] Creating Layer relu1
I1226 15:04:03.213383 92722 net.cpp:612] relu1 <- conv1
I1226 15:04:03.213443 92722 net.cpp:573] relu1 -> conv1 (in-place)
I1226 15:04:03.213603 92722 net.cpp:228] Setting up relu1
I1226 15:04:03.213758 92722 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 15:04:03.213791 92722 net.cpp:243] Memory required for data: 94129920
I1226 15:04:03.213853 92722 layer_factory.hpp:114] Creating layer norm1
I1226 15:04:03.213954 92722 net.cpp:178] Creating Layer norm1
I1226 15:04:03.214057 92722 net.cpp:612] norm1 <- conv1
I1226 15:04:03.214782 92722 net.cpp:586] norm1 -> norm1
I1226 15:04:03.214989 92722 net.cpp:228] Setting up norm1
I1226 15:04:03.215086 92722 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 15:04:03.215112 92722 net.cpp:243] Memory required for data: 131301120
I1226 15:04:03.215147 92722 layer_factory.hpp:114] Creating layer pool1
I1226 15:04:03.215236 92722 net.cpp:178] Creating Layer pool1
I1226 15:04:03.215286 92722 net.cpp:612] pool1 <- norm1
I1226 15:04:03.215373 92722 net.cpp:586] pool1 -> pool1
I1226 15:04:03.215510 92722 net.cpp:228] Setting up pool1
I1226 15:04:03.215592 92722 net.cpp:235] Top shape: 32 96 27 27 (2239488)
I1226 15:04:03.215628 92722 net.cpp:243] Memory required for data: 140259072
I1226 15:04:03.215680 92722 layer_factory.hpp:114] Creating layer conv2
I1226 15:04:03.215797 92722 net.cpp:178] Creating Layer conv2
I1226 15:04:03.215840 92722 net.cpp:612] conv2 <- pool1
I1226 15:04:03.215889 92722 net.cpp:586] conv2 -> conv2
I1226 15:04:03.331280 92253 net.cpp:228] Setting up conv2
I1226 15:04:03.331390 92253 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 15:04:03.331421 92253 net.cpp:243] Memory required for data: 164146944
I1226 15:04:03.331498 92253 layer_factory.hpp:114] Creating layer relu2
I1226 15:04:03.331570 92253 net.cpp:178] Creating Layer relu2
I1226 15:04:03.331614 92253 net.cpp:612] relu2 <- conv2
I1226 15:04:03.331667 92253 net.cpp:573] relu2 -> conv2 (in-place)
I1226 15:04:03.331768 92253 net.cpp:228] Setting up relu2
I1226 15:04:03.331830 92253 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 15:04:03.331863 92253 net.cpp:243] Memory required for data: 188034816
I1226 15:04:03.331893 92253 layer_factory.hpp:114] Creating layer norm2
I1226 15:04:03.331944 92253 net.cpp:178] Creating Layer norm2
I1226 15:04:03.331979 92253 net.cpp:612] norm2 <- conv2
I1226 15:04:03.332034 92253 net.cpp:586] norm2 -> norm2
I1226 15:04:03.332124 92253 net.cpp:228] Setting up norm2
I1226 15:04:03.332168 92253 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 15:04:03.332198 92253 net.cpp:243] Memory required for data: 211922688
I1226 15:04:03.332259 92253 layer_factory.hpp:114] Creating layer pool2
I1226 15:04:03.332322 92253 net.cpp:178] Creating Layer pool2
I1226 15:04:03.332360 92253 net.cpp:612] pool2 <- norm2
I1226 15:04:03.332399 92253 net.cpp:586] pool2 -> pool2
I1226 15:04:03.332494 92253 net.cpp:228] Setting up pool2
I1226 15:04:03.332535 92253 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 15:04:03.332651 92253 net.cpp:243] Memory required for data: 217460480
I1226 15:04:03.332686 92253 layer_factory.hpp:114] Creating layer conv3
I1226 15:04:03.332769 92253 net.cpp:178] Creating Layer conv3
I1226 15:04:03.332803 92253 net.cpp:612] conv3 <- pool2
I1226 15:04:03.332864 92253 net.cpp:586] conv3 -> conv3
I1226 15:04:03.341192 52875 net.cpp:228] Setting up conv3
I1226 15:04:03.341305 52875 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 15:04:03.341332 52875 net.cpp:243] Memory required for data: 225767168
I1226 15:04:03.341403 52875 layer_factory.hpp:114] Creating layer relu3
I1226 15:04:03.341471 52875 net.cpp:178] Creating Layer relu3
I1226 15:04:03.341511 52875 net.cpp:612] relu3 <- conv3
I1226 15:04:03.341550 52875 net.cpp:573] relu3 -> conv3 (in-place)
I1226 15:04:03.341622 52875 net.cpp:228] Setting up relu3
I1226 15:04:03.341665 52875 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 15:04:03.341688 52875 net.cpp:243] Memory required for data: 234073856
I1226 15:04:03.341716 52875 layer_factory.hpp:114] Creating layer conv4
I1226 15:04:03.341779 52875 net.cpp:178] Creating Layer conv4
I1226 15:04:03.341812 52875 net.cpp:612] conv4 <- conv3
I1226 15:04:03.341850 52875 net.cpp:586] conv4 -> conv4
I1226 15:04:03.379675 88631 net.cpp:228] Setting up conv2
I1226 15:04:03.379782 88631 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 15:04:03.379808 88631 net.cpp:243] Memory required for data: 164146944
I1226 15:04:03.379878 88631 layer_factory.hpp:114] Creating layer relu2
I1226 15:04:03.379961 88631 net.cpp:178] Creating Layer relu2
I1226 15:04:03.379992 88631 net.cpp:612] relu2 <- conv2
I1226 15:04:03.380036 88631 net.cpp:573] relu2 -> conv2 (in-place)
I1226 15:04:03.380138 88631 net.cpp:228] Setting up relu2
I1226 15:04:03.380179 88631 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 15:04:03.380211 88631 net.cpp:243] Memory required for data: 188034816
I1226 15:04:03.380240 88631 layer_factory.hpp:114] Creating layer norm2
I1226 15:04:03.380307 88631 net.cpp:178] Creating Layer norm2
I1226 15:04:03.380340 88631 net.cpp:612] norm2 <- conv2
I1226 15:04:03.380376 88631 net.cpp:586] norm2 -> norm2
I1226 15:04:03.380460 88631 net.cpp:228] Setting up norm2
I1226 15:04:03.380506 88631 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 15:04:03.380539 88631 net.cpp:243] Memory required for data: 211922688
I1226 15:04:03.380568 88631 layer_factory.hpp:114] Creating layer pool2
I1226 15:04:03.380610 88631 net.cpp:178] Creating Layer pool2
I1226 15:04:03.380635 88631 net.cpp:612] pool2 <- norm2
I1226 15:04:03.380676 88631 net.cpp:586] pool2 -> pool2
I1226 15:04:03.380754 88631 net.cpp:228] Setting up pool2
I1226 15:04:03.380795 88631 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 15:04:03.380910 88631 net.cpp:243] Memory required for data: 217460480
I1226 15:04:03.380967 88631 layer_factory.hpp:114] Creating layer conv3
I1226 15:04:03.381050 88631 net.cpp:178] Creating Layer conv3
I1226 15:04:03.381088 88631 net.cpp:612] conv3 <- pool2
I1226 15:04:03.381142 88631 net.cpp:586] conv3 -> conv3
I1226 15:04:03.381816 118847 net.cpp:228] Setting up conv2
I1226 15:04:03.381927 118847 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 15:04:03.381954 118847 net.cpp:243] Memory required for data: 164146944
I1226 15:04:03.382026 118847 layer_factory.hpp:114] Creating layer relu2
I1226 15:04:03.382087 118847 net.cpp:178] Creating Layer relu2
I1226 15:04:03.382128 118847 net.cpp:612] relu2 <- conv2
I1226 15:04:03.382171 118847 net.cpp:573] relu2 -> conv2 (in-place)
I1226 15:04:03.382294 118847 net.cpp:228] Setting up relu2
I1226 15:04:03.382359 118847 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 15:04:03.382511 118847 net.cpp:243] Memory required for data: 188034816
I1226 15:04:03.382575 118847 layer_factory.hpp:114] Creating layer norm2
I1226 15:04:03.382634 118847 net.cpp:178] Creating Layer norm2
I1226 15:04:03.382661 118847 net.cpp:612] norm2 <- conv2
I1226 15:04:03.382856 118847 net.cpp:586] norm2 -> norm2
I1226 15:04:03.383087 118847 net.cpp:228] Setting up norm2
I1226 15:04:03.383234 118847 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 15:04:03.383288 118847 net.cpp:243] Memory required for data: 211922688
I1226 15:04:03.383338 118847 layer_factory.hpp:114] Creating layer pool2
I1226 15:04:03.383450 118847 net.cpp:178] Creating Layer pool2
I1226 15:04:03.383644 118847 net.cpp:612] pool2 <- norm2
I1226 15:04:03.383735 118847 net.cpp:586] pool2 -> pool2
I1226 15:04:03.384008 118847 net.cpp:228] Setting up pool2
I1226 15:04:03.384136 118847 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 15:04:03.384174 118847 net.cpp:243] Memory required for data: 217460480
I1226 15:04:03.384215 118847 layer_factory.hpp:114] Creating layer conv3
I1226 15:04:03.384340 118847 net.cpp:178] Creating Layer conv3
I1226 15:04:03.384390 118847 net.cpp:612] conv3 <- pool2
I1226 15:04:03.384521 118847 net.cpp:586] conv3 -> conv3
I1226 15:04:03.394042 93129 net.cpp:228] Setting up conv2
I1226 15:04:03.394158 93129 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 15:04:03.394194 93129 net.cpp:243] Memory required for data: 164146944
I1226 15:04:03.394333 93129 layer_factory.hpp:114] Creating layer relu2
I1226 15:04:03.394409 93129 net.cpp:178] Creating Layer relu2
I1226 15:04:03.394554 93129 net.cpp:612] relu2 <- conv2
I1226 15:04:03.394623 93129 net.cpp:573] relu2 -> conv2 (in-place)
I1226 15:04:03.394847 93129 net.cpp:228] Setting up relu2
I1226 15:04:03.394995 93129 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 15:04:03.402231 92056 net.cpp:228] Setting up conv2
I1226 15:04:03.395040 93129 net.cpp:243] Memory required for data: 188034816
I1226 15:04:03.402338 92056 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 15:04:03.402366 92056 net.cpp:243] Memory required for data: 164146944
I1226 15:04:03.395092 93129 layer_factory.hpp:114] Creating layer norm2
I1226 15:04:03.402441 92056 layer_factory.hpp:114] Creating layer relu2
I1226 15:04:03.395303 93129 net.cpp:178] Creating Layer norm2
I1226 15:04:03.402534 92056 net.cpp:178] Creating Layer relu2
I1226 15:04:03.395375 93129 net.cpp:612] norm2 <- conv2
I1226 15:04:03.395429 93129 net.cpp:586] norm2 -> norm2
I1226 15:04:03.402580 92056 net.cpp:612] relu2 <- conv2
I1226 15:04:03.402623 92056 net.cpp:573] relu2 -> conv2 (in-place)
I1226 15:04:03.402736 92056 net.cpp:228] Setting up relu2
I1226 15:04:03.395576 93129 net.cpp:228] Setting up norm2
I1226 15:04:03.402817 92056 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 15:04:03.395678 93129 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 15:04:03.402855 92056 net.cpp:243] Memory required for data: 188034816
I1226 15:04:03.402886 92056 layer_factory.hpp:114] Creating layer norm2
I1226 15:04:03.395725 93129 net.cpp:243] Memory required for data: 211922688
I1226 15:04:03.402940 92056 net.cpp:178] Creating Layer norm2
I1226 15:04:03.402982 92056 net.cpp:612] norm2 <- conv2
I1226 15:04:03.403038 92056 net.cpp:586] norm2 -> norm2
I1226 15:04:03.403136 92056 net.cpp:228] Setting up norm2
I1226 15:04:03.395788 93129 layer_factory.hpp:114] Creating layer pool2
I1226 15:04:03.403182 92056 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 15:04:03.396014 93129 net.cpp:178] Creating Layer pool2
I1226 15:04:03.403211 92056 net.cpp:243] Memory required for data: 211922688
I1226 15:04:03.396087 93129 net.cpp:612] pool2 <- norm2
I1226 15:04:03.403240 92056 layer_factory.hpp:114] Creating layer pool2
I1226 15:04:03.396184 93129 net.cpp:586] pool2 -> pool2
I1226 15:04:03.403301 92056 net.cpp:178] Creating Layer pool2
I1226 15:04:03.403342 92056 net.cpp:612] pool2 <- norm2
I1226 15:04:03.396363 93129 net.cpp:228] Setting up pool2
I1226 15:04:03.403383 92056 net.cpp:586] pool2 -> pool2
I1226 15:04:03.403470 92056 net.cpp:228] Setting up pool2
I1226 15:04:03.403514 92056 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 15:04:03.396456 93129 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 15:04:03.403638 92056 net.cpp:243] Memory required for data: 217460480
I1226 15:04:03.396625 93129 net.cpp:243] Memory required for data: 217460480
I1226 15:04:03.403683 92056 layer_factory.hpp:114] Creating layer conv3
I1226 15:04:03.396688 93129 layer_factory.hpp:114] Creating layer conv3
I1226 15:04:03.403772 92056 net.cpp:178] Creating Layer conv3
I1226 15:04:03.403833 92056 net.cpp:612] conv3 <- pool2
I1226 15:04:03.403882 92056 net.cpp:586] conv3 -> conv3
I1226 15:04:03.396852 93129 net.cpp:178] Creating Layer conv3
I1226 15:04:03.397467 93129 net.cpp:612] conv3 <- pool2
I1226 15:04:03.397740 93129 net.cpp:586] conv3 -> conv3
I1226 15:04:03.435369 92722 net.cpp:228] Setting up conv2
I1226 15:04:03.435490 92722 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 15:04:03.435524 92722 net.cpp:243] Memory required for data: 164146944
I1226 15:04:03.435598 92722 layer_factory.hpp:114] Creating layer relu2
I1226 15:04:03.435673 92722 net.cpp:178] Creating Layer relu2
I1226 15:04:03.435719 92722 net.cpp:612] relu2 <- conv2
I1226 15:04:03.435762 92722 net.cpp:573] relu2 -> conv2 (in-place)
I1226 15:04:03.435861 92722 net.cpp:228] Setting up relu2
I1226 15:04:03.435925 92722 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 15:04:03.435952 92722 net.cpp:243] Memory required for data: 188034816
I1226 15:04:03.435984 92722 layer_factory.hpp:114] Creating layer norm2
I1226 15:04:03.436069 92722 net.cpp:178] Creating Layer norm2
I1226 15:04:03.436106 92722 net.cpp:612] norm2 <- conv2
I1226 15:04:03.436146 92722 net.cpp:586] norm2 -> norm2
I1226 15:04:03.436251 92722 net.cpp:228] Setting up norm2
I1226 15:04:03.436305 92722 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 15:04:03.436368 92722 net.cpp:243] Memory required for data: 211922688
I1226 15:04:03.436403 92722 layer_factory.hpp:114] Creating layer pool2
I1226 15:04:03.436451 92722 net.cpp:178] Creating Layer pool2
I1226 15:04:03.436493 92722 net.cpp:612] pool2 <- norm2
I1226 15:04:03.436553 92722 net.cpp:586] pool2 -> pool2
I1226 15:04:03.436658 92722 net.cpp:228] Setting up pool2
I1226 15:04:03.436710 92722 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 15:04:03.436833 92722 net.cpp:243] Memory required for data: 217460480
I1226 15:04:03.436882 92722 layer_factory.hpp:114] Creating layer conv3
I1226 15:04:03.436975 92722 net.cpp:178] Creating Layer conv3
I1226 15:04:03.437013 92722 net.cpp:612] conv3 <- pool2
I1226 15:04:03.437062 92722 net.cpp:586] conv3 -> conv3
I1226 15:04:03.457330 98074 net.cpp:228] Setting up conv1
I1226 15:04:03.457455 98074 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 15:04:03.457494 98074 net.cpp:243] Memory required for data: 56958720
I1226 15:04:03.457620 98074 layer_factory.hpp:114] Creating layer relu1
I1226 15:04:03.457753 98074 net.cpp:178] Creating Layer relu1
I1226 15:04:03.457813 98074 net.cpp:612] relu1 <- conv1
I1226 15:04:03.457897 98074 net.cpp:573] relu1 -> conv1 (in-place)
I1226 15:04:03.458046 98074 net.cpp:228] Setting up relu1
I1226 15:04:03.458130 98074 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 15:04:03.458163 98074 net.cpp:243] Memory required for data: 94129920
I1226 15:04:03.458204 98074 layer_factory.hpp:114] Creating layer norm1
I1226 15:04:03.458286 98074 net.cpp:178] Creating Layer norm1
I1226 15:04:03.458322 98074 net.cpp:612] norm1 <- conv1
I1226 15:04:03.458387 98074 net.cpp:586] norm1 -> norm1
I1226 15:04:03.458531 98074 net.cpp:228] Setting up norm1
I1226 15:04:03.458607 98074 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 15:04:03.458636 98074 net.cpp:243] Memory required for data: 131301120
I1226 15:04:03.458673 98074 layer_factory.hpp:114] Creating layer pool1
I1226 15:04:03.458786 98074 net.cpp:178] Creating Layer pool1
I1226 15:04:03.458827 98074 net.cpp:612] pool1 <- norm1
I1226 15:04:03.458873 98074 net.cpp:586] pool1 -> pool1
I1226 15:04:03.458992 98074 net.cpp:228] Setting up pool1
I1226 15:04:03.459054 98074 net.cpp:235] Top shape: 32 96 27 27 (2239488)
I1226 15:04:03.459081 98074 net.cpp:243] Memory required for data: 140259072
I1226 15:04:03.459115 98074 layer_factory.hpp:114] Creating layer conv2
I1226 15:04:03.459203 98074 net.cpp:178] Creating Layer conv2
I1226 15:04:03.459237 98074 net.cpp:612] conv2 <- pool1
I1226 15:04:03.459283 98074 net.cpp:586] conv2 -> conv2
I1226 15:04:03.481397 95917 net.cpp:228] Setting up conv1
I1226 15:04:03.481549 95917 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 15:04:03.481612 95917 net.cpp:243] Memory required for data: 56958720
I1226 15:04:03.481798 95917 layer_factory.hpp:114] Creating layer relu1
I1226 15:04:03.481997 95917 net.cpp:178] Creating Layer relu1
I1226 15:04:03.482071 95917 net.cpp:612] relu1 <- conv1
I1226 15:04:03.482249 95917 net.cpp:573] relu1 -> conv1 (in-place)
I1226 15:04:03.482739 95917 net.cpp:228] Setting up relu1
I1226 15:04:03.482959 95917 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 15:04:03.483119 95917 net.cpp:243] Memory required for data: 94129920
I1226 15:04:03.483196 95917 layer_factory.hpp:114] Creating layer norm1
I1226 15:04:03.483392 95917 net.cpp:178] Creating Layer norm1
I1226 15:04:03.483477 95917 net.cpp:612] norm1 <- conv1
I1226 15:04:03.483566 95917 net.cpp:586] norm1 -> norm1
I1226 15:04:03.483925 95917 net.cpp:228] Setting up norm1
I1226 15:04:03.484037 95917 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 15:04:03.484472 95917 net.cpp:243] Memory required for data: 131301120
I1226 15:04:03.484617 95917 layer_factory.hpp:114] Creating layer pool1
I1226 15:04:03.484735 95917 net.cpp:178] Creating Layer pool1
I1226 15:04:03.484781 95917 net.cpp:612] pool1 <- norm1
I1226 15:04:03.484871 95917 net.cpp:586] pool1 -> pool1
I1226 15:04:03.485021 95917 net.cpp:228] Setting up pool1
I1226 15:04:03.485080 95917 net.cpp:235] Top shape: 32 96 27 27 (2239488)
I1226 15:04:03.485131 95917 net.cpp:243] Memory required for data: 140259072
I1226 15:04:03.485184 95917 layer_factory.hpp:114] Creating layer conv2
I1226 15:04:03.485278 95917 net.cpp:178] Creating Layer conv2
I1226 15:04:03.485342 95917 net.cpp:612] conv2 <- pool1
I1226 15:04:03.485416 95917 net.cpp:586] conv2 -> conv2
I1226 15:04:03.500730 52875 net.cpp:228] Setting up conv4
I1226 15:04:03.500846 52875 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 15:04:03.500876 52875 net.cpp:243] Memory required for data: 242380544
I1226 15:04:03.500933 52875 layer_factory.hpp:114] Creating layer relu4
I1226 15:04:03.501008 52875 net.cpp:178] Creating Layer relu4
I1226 15:04:03.501055 52875 net.cpp:612] relu4 <- conv4
I1226 15:04:03.501098 52875 net.cpp:573] relu4 -> conv4 (in-place)
I1226 15:04:03.501229 52875 net.cpp:228] Setting up relu4
I1226 15:04:03.501298 52875 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 15:04:03.501323 52875 net.cpp:243] Memory required for data: 250687232
I1226 15:04:03.501361 52875 layer_factory.hpp:114] Creating layer conv5
I1226 15:04:03.501457 52875 net.cpp:178] Creating Layer conv5
I1226 15:04:03.501497 52875 net.cpp:612] conv5 <- conv4
I1226 15:04:03.501539 52875 net.cpp:586] conv5 -> conv5
I1226 15:04:03.609521 52875 net.cpp:228] Setting up conv5
I1226 15:04:03.609629 52875 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 15:04:03.609654 52875 net.cpp:243] Memory required for data: 256225024
I1226 15:04:03.609771 52875 layer_factory.hpp:114] Creating layer relu5
I1226 15:04:03.609840 52875 net.cpp:178] Creating Layer relu5
I1226 15:04:03.609879 52875 net.cpp:612] relu5 <- conv5
I1226 15:04:03.609935 52875 net.cpp:573] relu5 -> conv5 (in-place)
I1226 15:04:03.610028 52875 net.cpp:228] Setting up relu5
I1226 15:04:03.610074 52875 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 15:04:03.610098 52875 net.cpp:243] Memory required for data: 261762816
I1226 15:04:03.610152 52875 layer_factory.hpp:114] Creating layer pool5
I1226 15:04:03.610219 52875 net.cpp:178] Creating Layer pool5
I1226 15:04:03.610257 52875 net.cpp:612] pool5 <- conv5
I1226 15:04:03.610296 52875 net.cpp:586] pool5 -> pool5
I1226 15:04:03.610388 52875 net.cpp:228] Setting up pool5
I1226 15:04:03.610433 52875 net.cpp:235] Top shape: 32 256 6 6 (294912)
I1226 15:04:03.610455 52875 net.cpp:243] Memory required for data: 262942464
I1226 15:04:03.610482 52875 layer_factory.hpp:114] Creating layer fc6
I1226 15:04:03.610559 52875 net.cpp:178] Creating Layer fc6
I1226 15:04:03.610594 52875 net.cpp:612] fc6 <- pool5
I1226 15:04:03.610642 52875 net.cpp:586] fc6 -> fc6
I1226 15:04:03.620249 92253 net.cpp:228] Setting up conv3
I1226 15:04:03.620378 92253 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 15:04:03.620432 92253 net.cpp:243] Memory required for data: 225767168
I1226 15:04:03.620517 92253 layer_factory.hpp:114] Creating layer relu3
I1226 15:04:03.621244 92253 net.cpp:178] Creating Layer relu3
I1226 15:04:03.621336 92253 net.cpp:612] relu3 <- conv3
I1226 15:04:03.621460 92253 net.cpp:573] relu3 -> conv3 (in-place)
I1226 15:04:03.621616 92253 net.cpp:228] Setting up relu3
I1226 15:04:03.621736 92253 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 15:04:03.621763 92253 net.cpp:243] Memory required for data: 234073856
I1226 15:04:03.621796 92253 layer_factory.hpp:114] Creating layer conv4
I1226 15:04:03.621919 92253 net.cpp:178] Creating Layer conv4
I1226 15:04:03.622046 92253 net.cpp:612] conv4 <- conv3
I1226 15:04:03.622113 92253 net.cpp:586] conv4 -> conv4
I1226 15:04:03.657057 118847 net.cpp:228] Setting up conv3
I1226 15:04:03.657609 118847 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 15:04:03.657677 118847 net.cpp:243] Memory required for data: 225767168
I1226 15:04:03.657773 118847 layer_factory.hpp:114] Creating layer relu3
I1226 15:04:03.657894 118847 net.cpp:178] Creating Layer relu3
I1226 15:04:03.657946 118847 net.cpp:612] relu3 <- conv3
I1226 15:04:03.658054 118847 net.cpp:573] relu3 -> conv3 (in-place)
I1226 15:04:03.658372 118847 net.cpp:228] Setting up relu3
I1226 15:04:03.658510 118847 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 15:04:03.658542 118847 net.cpp:243] Memory required for data: 234073856
I1226 15:04:03.658612 118847 layer_factory.hpp:114] Creating layer conv4
I1226 15:04:03.658777 118847 net.cpp:178] Creating Layer conv4
I1226 15:04:03.658835 118847 net.cpp:612] conv4 <- conv3
I1226 15:04:03.658908 118847 net.cpp:586] conv4 -> conv4
I1226 15:04:03.677587 88631 net.cpp:228] Setting up conv3
I1226 15:04:03.677729 88631 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 15:04:03.677784 88631 net.cpp:243] Memory required for data: 225767168
I1226 15:04:03.677860 88631 layer_factory.hpp:114] Creating layer relu3
I1226 15:04:03.677999 88631 net.cpp:178] Creating Layer relu3
I1226 15:04:03.678537 88631 net.cpp:612] relu3 <- conv3
I1226 15:04:03.678635 88631 net.cpp:573] relu3 -> conv3 (in-place)
I1226 15:04:03.678793 88631 net.cpp:228] Setting up relu3
I1226 15:04:03.678886 88631 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 15:04:03.678961 88631 net.cpp:243] Memory required for data: 234073856
I1226 15:04:03.678999 88631 layer_factory.hpp:114] Creating layer conv4
I1226 15:04:03.679105 88631 net.cpp:178] Creating Layer conv4
I1226 15:04:03.679158 88631 net.cpp:612] conv4 <- conv3
I1226 15:04:03.679358 88631 net.cpp:586] conv4 -> conv4
I1226 15:04:03.691488 92056 net.cpp:228] Setting up conv3
I1226 15:04:03.691598 92056 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 15:04:03.691627 92056 net.cpp:243] Memory required for data: 225767168
I1226 15:04:03.691704 92056 layer_factory.hpp:114] Creating layer relu3
I1226 15:04:03.691823 92056 net.cpp:178] Creating Layer relu3
I1226 15:04:03.691879 92056 net.cpp:612] relu3 <- conv3
I1226 15:04:03.691927 92056 net.cpp:573] relu3 -> conv3 (in-place)
I1226 15:04:03.692028 92056 net.cpp:228] Setting up relu3
I1226 15:04:03.692085 92056 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 15:04:03.692111 92056 net.cpp:243] Memory required for data: 234073856
I1226 15:04:03.692153 92056 layer_factory.hpp:114] Creating layer conv4
I1226 15:04:03.692257 92056 net.cpp:178] Creating Layer conv4
I1226 15:04:03.692296 92056 net.cpp:612] conv4 <- conv3
I1226 15:04:03.692358 92056 net.cpp:586] conv4 -> conv4
I1226 15:04:03.721273 93129 net.cpp:228] Setting up conv3
I1226 15:04:03.721397 93129 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 15:04:03.721431 93129 net.cpp:243] Memory required for data: 225767168
I1226 15:04:03.721539 93129 layer_factory.hpp:114] Creating layer relu3
I1226 15:04:03.721720 93129 net.cpp:178] Creating Layer relu3
I1226 15:04:03.721861 93129 net.cpp:612] relu3 <- conv3
I1226 15:04:03.721925 93129 net.cpp:573] relu3 -> conv3 (in-place)
I1226 15:04:03.722059 93129 net.cpp:228] Setting up relu3
I1226 15:04:03.722550 93129 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 15:04:03.722635 93129 net.cpp:243] Memory required for data: 234073856
I1226 15:04:03.722743 93129 layer_factory.hpp:114] Creating layer conv4
I1226 15:04:03.722934 93129 net.cpp:178] Creating Layer conv4
I1226 15:04:03.722988 93129 net.cpp:612] conv4 <- conv3
I1226 15:04:03.723038 93129 net.cpp:586] conv4 -> conv4
I1226 15:04:03.727186 92722 net.cpp:228] Setting up conv3
I1226 15:04:03.727330 92722 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 15:04:03.727365 92722 net.cpp:243] Memory required for data: 225767168
I1226 15:04:03.727466 92722 layer_factory.hpp:114] Creating layer relu3
I1226 15:04:03.727538 92722 net.cpp:178] Creating Layer relu3
I1226 15:04:03.727571 92722 net.cpp:612] relu3 <- conv3
I1226 15:04:03.727619 92722 net.cpp:573] relu3 -> conv3 (in-place)
I1226 15:04:03.727715 92722 net.cpp:228] Setting up relu3
I1226 15:04:03.727764 92722 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 15:04:03.727787 92722 net.cpp:243] Memory required for data: 234073856
I1226 15:04:03.727818 92722 layer_factory.hpp:114] Creating layer conv4
I1226 15:04:03.727910 92722 net.cpp:178] Creating Layer conv4
I1226 15:04:03.727941 92722 net.cpp:612] conv4 <- conv3
I1226 15:04:03.727990 92722 net.cpp:586] conv4 -> conv4
I1226 15:04:03.846731 92056 net.cpp:228] Setting up conv4
I1226 15:04:03.846880 92056 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 15:04:03.846909 92056 net.cpp:243] Memory required for data: 242380544
I1226 15:04:03.846968 92056 layer_factory.hpp:114] Creating layer relu4
I1226 15:04:03.847054 92056 net.cpp:178] Creating Layer relu4
I1226 15:04:03.847100 92056 net.cpp:612] relu4 <- conv4
I1226 15:04:03.847142 92056 net.cpp:573] relu4 -> conv4 (in-place)
I1226 15:04:03.847234 92056 net.cpp:228] Setting up relu4
I1226 15:04:03.847293 92056 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 15:04:03.847329 92056 net.cpp:243] Memory required for data: 250687232
I1226 15:04:03.847359 92056 layer_factory.hpp:114] Creating layer conv5
I1226 15:04:03.847439 92056 net.cpp:178] Creating Layer conv5
I1226 15:04:03.847476 92056 net.cpp:612] conv5 <- conv4
I1226 15:04:03.847520 92056 net.cpp:586] conv5 -> conv5
I1226 15:04:03.896090 92253 net.cpp:228] Setting up conv4
I1226 15:04:03.896201 92253 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 15:04:03.896257 92253 net.cpp:243] Memory required for data: 242380544
I1226 15:04:03.896319 92253 layer_factory.hpp:114] Creating layer relu4
I1226 15:04:03.896390 92253 net.cpp:178] Creating Layer relu4
I1226 15:04:03.896422 92253 net.cpp:612] relu4 <- conv4
I1226 15:04:03.896508 92253 net.cpp:573] relu4 -> conv4 (in-place)
I1226 15:04:03.896613 92253 net.cpp:228] Setting up relu4
I1226 15:04:03.896661 92253 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 15:04:03.896688 92253 net.cpp:243] Memory required for data: 250687232
I1226 15:04:03.896721 92253 layer_factory.hpp:114] Creating layer conv5
I1226 15:04:03.896814 92253 net.cpp:178] Creating Layer conv5
I1226 15:04:03.896847 92253 net.cpp:612] conv5 <- conv4
I1226 15:04:03.896906 92253 net.cpp:586] conv5 -> conv5
I1226 15:04:03.929911 88631 net.cpp:228] Setting up conv4
I1226 15:04:03.930068 88631 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 15:04:03.930099 88631 net.cpp:243] Memory required for data: 242380544
I1226 15:04:03.930183 88631 layer_factory.hpp:114] Creating layer relu4
I1226 15:04:03.930317 88631 net.cpp:178] Creating Layer relu4
I1226 15:04:03.930377 88631 net.cpp:612] relu4 <- conv4
I1226 15:04:03.934056 88631 net.cpp:573] relu4 -> conv4 (in-place)
I1226 15:04:03.934279 88631 net.cpp:228] Setting up relu4
I1226 15:04:03.934386 88631 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 15:04:03.934419 88631 net.cpp:243] Memory required for data: 250687232
I1226 15:04:03.934459 88631 layer_factory.hpp:114] Creating layer conv5
I1226 15:04:03.934741 88631 net.cpp:178] Creating Layer conv5
I1226 15:04:03.934859 88631 net.cpp:612] conv5 <- conv4
I1226 15:04:03.934973 88631 net.cpp:586] conv5 -> conv5
I1226 15:04:03.957257 92056 net.cpp:228] Setting up conv5
I1226 15:04:03.957370 92056 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 15:04:03.957399 92056 net.cpp:243] Memory required for data: 256225024
I1226 15:04:03.957494 92056 layer_factory.hpp:114] Creating layer relu5
I1226 15:04:03.957568 92056 net.cpp:178] Creating Layer relu5
I1226 15:04:03.957617 92056 net.cpp:612] relu5 <- conv5
I1226 15:04:03.957665 92056 net.cpp:573] relu5 -> conv5 (in-place)
I1226 15:04:03.957767 92056 net.cpp:228] Setting up relu5
I1226 15:04:03.957841 92056 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 15:04:03.957866 92056 net.cpp:243] Memory required for data: 261762816
I1226 15:04:03.957897 92056 layer_factory.hpp:114] Creating layer pool5
I1226 15:04:03.957967 92056 net.cpp:178] Creating Layer pool5
I1226 15:04:03.958009 92056 net.cpp:612] pool5 <- conv5
I1226 15:04:03.958051 92056 net.cpp:586] pool5 -> pool5
I1226 15:04:03.958142 92056 net.cpp:228] Setting up pool5
I1226 15:04:03.958204 92056 net.cpp:235] Top shape: 32 256 6 6 (294912)
I1226 15:04:03.958228 92056 net.cpp:243] Memory required for data: 262942464
I1226 15:04:03.958259 92056 layer_factory.hpp:114] Creating layer fc6
I1226 15:04:03.958335 92056 net.cpp:178] Creating Layer fc6
I1226 15:04:03.958372 92056 net.cpp:612] fc6 <- pool5
I1226 15:04:03.958415 92056 net.cpp:586] fc6 -> fc6
I1226 15:04:03.956435 118847 net.cpp:228] Setting up conv4
I1226 15:04:03.956571 118847 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 15:04:03.956607 118847 net.cpp:243] Memory required for data: 242380544
I1226 15:04:03.956712 118847 layer_factory.hpp:114] Creating layer relu4
I1226 15:04:03.956822 118847 net.cpp:178] Creating Layer relu4
I1226 15:04:03.956881 118847 net.cpp:612] relu4 <- conv4
I1226 15:04:03.956938 118847 net.cpp:573] relu4 -> conv4 (in-place)
I1226 15:04:03.957084 118847 net.cpp:228] Setting up relu4
I1226 15:04:03.957154 118847 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 15:04:03.957247 118847 net.cpp:243] Memory required for data: 250687232
I1226 15:04:03.957288 118847 layer_factory.hpp:114] Creating layer conv5
I1226 15:04:03.957408 118847 net.cpp:178] Creating Layer conv5
I1226 15:04:03.957468 118847 net.cpp:612] conv5 <- conv4
I1226 15:04:03.957525 118847 net.cpp:586] conv5 -> conv5
I1226 15:04:03.984822 92722 net.cpp:228] Setting up conv4
I1226 15:04:03.984933 92722 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 15:04:03.984961 92722 net.cpp:243] Memory required for data: 242380544
I1226 15:04:03.985045 92722 layer_factory.hpp:114] Creating layer relu4
I1226 15:04:03.985139 92722 net.cpp:178] Creating Layer relu4
I1226 15:04:03.985184 92722 net.cpp:612] relu4 <- conv4
I1226 15:04:03.985240 92722 net.cpp:573] relu4 -> conv4 (in-place)
I1226 15:04:03.985473 92722 net.cpp:228] Setting up relu4
I1226 15:04:03.985524 92722 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 15:04:03.985563 92722 net.cpp:243] Memory required for data: 250687232
I1226 15:04:03.985596 92722 layer_factory.hpp:114] Creating layer conv5
I1226 15:04:03.985688 92722 net.cpp:178] Creating Layer conv5
I1226 15:04:03.985728 92722 net.cpp:612] conv5 <- conv4
I1226 15:04:03.985774 92722 net.cpp:586] conv5 -> conv5
I1226 15:04:04.004269 93129 net.cpp:228] Setting up conv4
I1226 15:04:04.004381 93129 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 15:04:04.004407 93129 net.cpp:243] Memory required for data: 242380544
I1226 15:04:04.004493 93129 layer_factory.hpp:114] Creating layer relu4
I1226 15:04:04.004565 93129 net.cpp:178] Creating Layer relu4
I1226 15:04:04.004604 93129 net.cpp:612] relu4 <- conv4
I1226 15:04:04.004645 93129 net.cpp:573] relu4 -> conv4 (in-place)
I1226 15:04:04.004722 93129 net.cpp:228] Setting up relu4
I1226 15:04:04.004771 93129 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 15:04:04.004818 93129 net.cpp:243] Memory required for data: 250687232
I1226 15:04:04.004854 93129 layer_factory.hpp:114] Creating layer conv5
I1226 15:04:04.004927 93129 net.cpp:178] Creating Layer conv5
I1226 15:04:04.004957 93129 net.cpp:612] conv5 <- conv4
I1226 15:04:04.005012 93129 net.cpp:586] conv5 -> conv5
I1226 15:04:04.091501 92253 net.cpp:228] Setting up conv5
I1226 15:04:04.091629 92253 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 15:04:04.091680 92253 net.cpp:243] Memory required for data: 256225024
I1226 15:04:04.091758 92253 layer_factory.hpp:114] Creating layer relu5
I1226 15:04:04.091825 92253 net.cpp:178] Creating Layer relu5
I1226 15:04:04.091881 92253 net.cpp:612] relu5 <- conv5
I1226 15:04:04.092023 92253 net.cpp:573] relu5 -> conv5 (in-place)
I1226 15:04:04.092118 92253 net.cpp:228] Setting up relu5
I1226 15:04:04.092165 92253 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 15:04:04.092190 92253 net.cpp:243] Memory required for data: 261762816
I1226 15:04:04.092272 92253 layer_factory.hpp:114] Creating layer pool5
I1226 15:04:04.092339 92253 net.cpp:178] Creating Layer pool5
I1226 15:04:04.092466 92253 net.cpp:612] pool5 <- conv5
I1226 15:04:04.092506 92253 net.cpp:586] pool5 -> pool5
I1226 15:04:04.092607 92253 net.cpp:228] Setting up pool5
I1226 15:04:04.092658 92253 net.cpp:235] Top shape: 32 256 6 6 (294912)
I1226 15:04:04.092682 92253 net.cpp:243] Memory required for data: 262942464
I1226 15:04:04.092710 92253 layer_factory.hpp:114] Creating layer fc6
I1226 15:04:04.092811 92253 net.cpp:178] Creating Layer fc6
I1226 15:04:04.092846 92253 net.cpp:612] fc6 <- pool5
I1226 15:04:04.092898 92253 net.cpp:586] fc6 -> fc6
I1226 15:04:04.125772 88631 net.cpp:228] Setting up conv5
I1226 15:04:04.125912 88631 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 15:04:04.125999 88631 net.cpp:243] Memory required for data: 256225024
I1226 15:04:04.126754 88631 layer_factory.hpp:114] Creating layer relu5
I1226 15:04:04.126899 88631 net.cpp:178] Creating Layer relu5
I1226 15:04:04.127027 88631 net.cpp:612] relu5 <- conv5
I1226 15:04:04.127252 88631 net.cpp:573] relu5 -> conv5 (in-place)
I1226 15:04:04.127411 88631 net.cpp:228] Setting up relu5
I1226 15:04:04.127513 88631 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 15:04:04.127552 88631 net.cpp:243] Memory required for data: 261762816
I1226 15:04:04.127616 88631 layer_factory.hpp:114] Creating layer pool5
I1226 15:04:04.127861 88631 net.cpp:178] Creating Layer pool5
I1226 15:04:04.127912 88631 net.cpp:612] pool5 <- conv5
I1226 15:04:04.128340 88631 net.cpp:586] pool5 -> pool5
I1226 15:04:04.128538 88631 net.cpp:228] Setting up pool5
I1226 15:04:04.128623 88631 net.cpp:235] Top shape: 32 256 6 6 (294912)
I1226 15:04:04.128655 88631 net.cpp:243] Memory required for data: 262942464
I1226 15:04:04.128693 88631 layer_factory.hpp:114] Creating layer fc6
I1226 15:04:04.128782 88631 net.cpp:178] Creating Layer fc6
I1226 15:04:04.128823 88631 net.cpp:612] fc6 <- pool5
I1226 15:04:04.128872 88631 net.cpp:586] fc6 -> fc6
I1226 15:04:04.132241 118847 net.cpp:228] Setting up conv5
I1226 15:04:04.132369 118847 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 15:04:04.132397 118847 net.cpp:243] Memory required for data: 256225024
I1226 15:04:04.132517 118847 layer_factory.hpp:114] Creating layer relu5
I1226 15:04:04.132617 118847 net.cpp:178] Creating Layer relu5
I1226 15:04:04.132663 118847 net.cpp:612] relu5 <- conv5
I1226 15:04:04.132730 118847 net.cpp:573] relu5 -> conv5 (in-place)
I1226 15:04:04.132864 118847 net.cpp:228] Setting up relu5
I1226 15:04:04.132932 118847 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 15:04:04.132964 118847 net.cpp:243] Memory required for data: 261762816
I1226 15:04:04.133002 118847 layer_factory.hpp:114] Creating layer pool5
I1226 15:04:04.133066 118847 net.cpp:178] Creating Layer pool5
I1226 15:04:04.133102 118847 net.cpp:612] pool5 <- conv5
I1226 15:04:04.133188 118847 net.cpp:586] pool5 -> pool5
I1226 15:04:04.133327 118847 net.cpp:228] Setting up pool5
I1226 15:04:04.133394 118847 net.cpp:235] Top shape: 32 256 6 6 (294912)
I1226 15:04:04.133422 118847 net.cpp:243] Memory required for data: 262942464
I1226 15:04:04.133460 118847 layer_factory.hpp:114] Creating layer fc6
I1226 15:04:04.133580 118847 net.cpp:178] Creating Layer fc6
I1226 15:04:04.133630 118847 net.cpp:612] fc6 <- pool5
I1226 15:04:04.133684 118847 net.cpp:586] fc6 -> fc6
I1226 15:04:04.147305 92722 net.cpp:228] Setting up conv5
I1226 15:04:04.147464 92722 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 15:04:04.147500 92722 net.cpp:243] Memory required for data: 256225024
I1226 15:04:04.147574 92722 layer_factory.hpp:114] Creating layer relu5
I1226 15:04:04.147649 92722 net.cpp:178] Creating Layer relu5
I1226 15:04:04.147682 92722 net.cpp:612] relu5 <- conv5
I1226 15:04:04.147722 92722 net.cpp:573] relu5 -> conv5 (in-place)
I1226 15:04:04.147822 92722 net.cpp:228] Setting up relu5
I1226 15:04:04.147920 92722 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 15:04:04.148093 92722 net.cpp:243] Memory required for data: 261762816
I1226 15:04:04.148130 92722 layer_factory.hpp:114] Creating layer pool5
I1226 15:04:04.148196 92722 net.cpp:178] Creating Layer pool5
I1226 15:04:04.148252 92722 net.cpp:612] pool5 <- conv5
I1226 15:04:04.148357 92722 net.cpp:586] pool5 -> pool5
I1226 15:04:04.148802 92722 net.cpp:228] Setting up pool5
I1226 15:04:04.148902 92722 net.cpp:235] Top shape: 32 256 6 6 (294912)
I1226 15:04:04.148931 92722 net.cpp:243] Memory required for data: 262942464
I1226 15:04:04.148967 92722 layer_factory.hpp:114] Creating layer fc6
I1226 15:04:04.149104 92722 net.cpp:178] Creating Layer fc6
I1226 15:04:04.149153 92722 net.cpp:612] fc6 <- pool5
I1226 15:04:04.149202 92722 net.cpp:586] fc6 -> fc6
I1226 15:04:04.189869 93129 net.cpp:228] Setting up conv5
I1226 15:04:04.189990 93129 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 15:04:04.190021 93129 net.cpp:243] Memory required for data: 256225024
I1226 15:04:04.190119 93129 layer_factory.hpp:114] Creating layer relu5
I1226 15:04:04.190184 93129 net.cpp:178] Creating Layer relu5
I1226 15:04:04.190213 93129 net.cpp:612] relu5 <- conv5
I1226 15:04:04.190251 93129 net.cpp:573] relu5 -> conv5 (in-place)
I1226 15:04:04.190351 93129 net.cpp:228] Setting up relu5
I1226 15:04:04.190409 93129 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 15:04:04.190434 93129 net.cpp:243] Memory required for data: 261762816
I1226 15:04:04.190464 93129 layer_factory.hpp:114] Creating layer pool5
I1226 15:04:04.190515 93129 net.cpp:178] Creating Layer pool5
I1226 15:04:04.190548 93129 net.cpp:612] pool5 <- conv5
I1226 15:04:04.190596 93129 net.cpp:586] pool5 -> pool5
I1226 15:04:04.190682 93129 net.cpp:228] Setting up pool5
I1226 15:04:04.190719 93129 net.cpp:235] Top shape: 32 256 6 6 (294912)
I1226 15:04:04.190742 93129 net.cpp:243] Memory required for data: 262942464
I1226 15:04:04.190769 93129 layer_factory.hpp:114] Creating layer fc6
I1226 15:04:04.190873 93129 net.cpp:178] Creating Layer fc6
I1226 15:04:04.191058 93129 net.cpp:612] fc6 <- pool5
I1226 15:04:04.191103 93129 net.cpp:586] fc6 -> fc6
I1226 15:04:04.294950 98074 net.cpp:228] Setting up conv2
I1226 15:04:04.295071 98074 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 15:04:04.295104 98074 net.cpp:243] Memory required for data: 164146944
I1226 15:04:04.295182 98074 layer_factory.hpp:114] Creating layer relu2
I1226 15:04:04.295286 98074 net.cpp:178] Creating Layer relu2
I1226 15:04:04.295336 98074 net.cpp:612] relu2 <- conv2
I1226 15:04:04.295390 98074 net.cpp:573] relu2 -> conv2 (in-place)
I1226 15:04:04.295524 98074 net.cpp:228] Setting up relu2
I1226 15:04:04.295580 98074 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 15:04:04.295608 98074 net.cpp:243] Memory required for data: 188034816
I1226 15:04:04.295644 98074 layer_factory.hpp:114] Creating layer norm2
I1226 15:04:04.295701 98074 net.cpp:178] Creating Layer norm2
I1226 15:04:04.295768 98074 net.cpp:612] norm2 <- conv2
I1226 15:04:04.295824 98074 net.cpp:586] norm2 -> norm2
I1226 15:04:04.295943 98074 net.cpp:228] Setting up norm2
I1226 15:04:04.296003 98074 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 15:04:04.296030 98074 net.cpp:243] Memory required for data: 211922688
I1226 15:04:04.296062 98074 layer_factory.hpp:114] Creating layer pool2
I1226 15:04:04.296133 98074 net.cpp:178] Creating Layer pool2
I1226 15:04:04.296169 98074 net.cpp:612] pool2 <- norm2
I1226 15:04:04.296213 98074 net.cpp:586] pool2 -> pool2
I1226 15:04:04.296317 98074 net.cpp:228] Setting up pool2
I1226 15:04:04.296365 98074 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 15:04:04.296506 98074 net.cpp:243] Memory required for data: 217460480
I1226 15:04:04.296550 98074 layer_factory.hpp:114] Creating layer conv3
I1226 15:04:04.296656 98074 net.cpp:178] Creating Layer conv3
I1226 15:04:04.296708 98074 net.cpp:612] conv3 <- pool2
I1226 15:04:04.296787 98074 net.cpp:586] conv3 -> conv3
I1226 15:04:04.374034 95917 net.cpp:228] Setting up conv2
I1226 15:04:04.374691 95917 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 15:04:04.374812 95917 net.cpp:243] Memory required for data: 164146944
I1226 15:04:04.375048 95917 layer_factory.hpp:114] Creating layer relu2
I1226 15:04:04.375257 95917 net.cpp:178] Creating Layer relu2
I1226 15:04:04.375324 95917 net.cpp:612] relu2 <- conv2
I1226 15:04:04.375622 95917 net.cpp:573] relu2 -> conv2 (in-place)
I1226 15:04:04.376349 95917 net.cpp:228] Setting up relu2
I1226 15:04:04.376536 95917 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 15:04:04.376723 95917 net.cpp:243] Memory required for data: 188034816
I1226 15:04:04.376771 95917 layer_factory.hpp:114] Creating layer norm2
I1226 15:04:04.377089 95917 net.cpp:178] Creating Layer norm2
I1226 15:04:04.377141 95917 net.cpp:612] norm2 <- conv2
I1226 15:04:04.377195 95917 net.cpp:586] norm2 -> norm2
I1226 15:04:04.377646 95917 net.cpp:228] Setting up norm2
I1226 15:04:04.377815 95917 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 15:04:04.377888 95917 net.cpp:243] Memory required for data: 211922688
I1226 15:04:04.377934 95917 layer_factory.hpp:114] Creating layer pool2
I1226 15:04:04.378003 95917 net.cpp:178] Creating Layer pool2
I1226 15:04:04.378049 95917 net.cpp:612] pool2 <- norm2
I1226 15:04:04.378123 95917 net.cpp:586] pool2 -> pool2
I1226 15:04:04.378334 95917 net.cpp:228] Setting up pool2
I1226 15:04:04.378398 95917 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 15:04:04.379076 95917 net.cpp:243] Memory required for data: 217460480
I1226 15:04:04.379185 95917 layer_factory.hpp:114] Creating layer conv3
I1226 15:04:04.379350 95917 net.cpp:178] Creating Layer conv3
I1226 15:04:04.379398 95917 net.cpp:612] conv3 <- pool2
I1226 15:04:04.379458 95917 net.cpp:586] conv3 -> conv3
I1226 15:04:05.274485 95917 net.cpp:228] Setting up conv3
I1226 15:04:05.274616 95917 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 15:04:05.274659 95917 net.cpp:243] Memory required for data: 225767168
I1226 15:04:05.274796 95917 layer_factory.hpp:114] Creating layer relu3
I1226 15:04:05.274919 95917 net.cpp:178] Creating Layer relu3
I1226 15:04:05.274966 95917 net.cpp:612] relu3 <- conv3
I1226 15:04:05.275028 95917 net.cpp:573] relu3 -> conv3 (in-place)
I1226 15:04:05.275169 95917 net.cpp:228] Setting up relu3
I1226 15:04:05.275251 95917 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 15:04:05.275288 95917 net.cpp:243] Memory required for data: 234073856
I1226 15:04:05.275332 95917 layer_factory.hpp:114] Creating layer conv4
I1226 15:04:05.275442 95917 net.cpp:178] Creating Layer conv4
I1226 15:04:05.275488 95917 net.cpp:612] conv4 <- conv3
I1226 15:04:05.275558 95917 net.cpp:586] conv4 -> conv4
I1226 15:04:05.328624 98074 net.cpp:228] Setting up conv3
I1226 15:04:05.328778 98074 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 15:04:05.328815 98074 net.cpp:243] Memory required for data: 225767168
I1226 15:04:05.328902 98074 layer_factory.hpp:114] Creating layer relu3
I1226 15:04:05.328982 98074 net.cpp:178] Creating Layer relu3
I1226 15:04:05.329025 98074 net.cpp:612] relu3 <- conv3
I1226 15:04:05.329108 98074 net.cpp:573] relu3 -> conv3 (in-place)
I1226 15:04:05.329229 98074 net.cpp:228] Setting up relu3
I1226 15:04:05.329293 98074 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 15:04:05.329324 98074 net.cpp:243] Memory required for data: 234073856
I1226 15:04:05.329361 98074 layer_factory.hpp:114] Creating layer conv4
I1226 15:04:05.329455 98074 net.cpp:178] Creating Layer conv4
I1226 15:04:05.329520 98074 net.cpp:612] conv4 <- conv3
I1226 15:04:05.329581 98074 net.cpp:586] conv4 -> conv4
I1226 15:04:06.000859 95917 net.cpp:228] Setting up conv4
I1226 15:04:06.000985 95917 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 15:04:06.001030 95917 net.cpp:243] Memory required for data: 242380544
I1226 15:04:06.001123 95917 layer_factory.hpp:114] Creating layer relu4
I1226 15:04:06.001215 95917 net.cpp:178] Creating Layer relu4
I1226 15:04:06.001262 95917 net.cpp:612] relu4 <- conv4
I1226 15:04:06.001317 95917 net.cpp:573] relu4 -> conv4 (in-place)
I1226 15:04:06.001426 95917 net.cpp:228] Setting up relu4
I1226 15:04:06.001502 95917 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 15:04:06.001539 95917 net.cpp:243] Memory required for data: 250687232
I1226 15:04:06.001581 95917 layer_factory.hpp:114] Creating layer conv5
I1226 15:04:06.001684 95917 net.cpp:178] Creating Layer conv5
I1226 15:04:06.001730 95917 net.cpp:612] conv5 <- conv4
I1226 15:04:06.001788 95917 net.cpp:586] conv5 -> conv5
I1226 15:04:06.056283 98074 net.cpp:228] Setting up conv4
I1226 15:04:06.056403 98074 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 15:04:06.056437 98074 net.cpp:243] Memory required for data: 242380544
I1226 15:04:06.056502 98074 layer_factory.hpp:114] Creating layer relu4
I1226 15:04:06.056576 98074 net.cpp:178] Creating Layer relu4
I1226 15:04:06.056620 98074 net.cpp:612] relu4 <- conv4
I1226 15:04:06.056671 98074 net.cpp:573] relu4 -> conv4 (in-place)
I1226 15:04:06.056793 98074 net.cpp:228] Setting up relu4
I1226 15:04:06.056851 98074 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 15:04:06.056882 98074 net.cpp:243] Memory required for data: 250687232
I1226 15:04:06.056918 98074 layer_factory.hpp:114] Creating layer conv5
I1226 15:04:06.056999 98074 net.cpp:178] Creating Layer conv5
I1226 15:04:06.057035 98074 net.cpp:612] conv5 <- conv4
I1226 15:04:06.057081 98074 net.cpp:586] conv5 -> conv5
I1226 15:04:06.502411 95917 net.cpp:228] Setting up conv5
I1226 15:04:06.502580 95917 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 15:04:06.502632 95917 net.cpp:243] Memory required for data: 256225024
I1226 15:04:06.502735 95917 layer_factory.hpp:114] Creating layer relu5
I1226 15:04:06.502985 95917 net.cpp:178] Creating Layer relu5
I1226 15:04:06.503063 95917 net.cpp:612] relu5 <- conv5
I1226 15:04:06.503161 95917 net.cpp:573] relu5 -> conv5 (in-place)
I1226 15:04:06.503367 95917 net.cpp:228] Setting up relu5
I1226 15:04:06.503476 95917 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 15:04:06.503737 95917 net.cpp:243] Memory required for data: 261762816
I1226 15:04:06.504106 95917 layer_factory.hpp:114] Creating layer pool5
I1226 15:04:06.504276 95917 net.cpp:178] Creating Layer pool5
I1226 15:04:06.504556 95917 net.cpp:612] pool5 <- conv5
I1226 15:04:06.504642 95917 net.cpp:586] pool5 -> pool5
I1226 15:04:06.504782 95917 net.cpp:228] Setting up pool5
I1226 15:04:06.504883 95917 net.cpp:235] Top shape: 32 256 6 6 (294912)
I1226 15:04:06.504921 95917 net.cpp:243] Memory required for data: 262942464
I1226 15:04:06.504963 95917 layer_factory.hpp:114] Creating layer fc6
I1226 15:04:06.505081 95917 net.cpp:178] Creating Layer fc6
I1226 15:04:06.505165 95917 net.cpp:612] fc6 <- pool5
I1226 15:04:06.505234 95917 net.cpp:586] fc6 -> fc6
I1226 15:04:06.561475 98074 net.cpp:228] Setting up conv5
I1226 15:04:06.561591 98074 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 15:04:06.561624 98074 net.cpp:243] Memory required for data: 256225024
I1226 15:04:06.561704 98074 layer_factory.hpp:114] Creating layer relu5
I1226 15:04:06.561827 98074 net.cpp:178] Creating Layer relu5
I1226 15:04:06.561902 98074 net.cpp:612] relu5 <- conv5
I1226 15:04:06.561974 98074 net.cpp:573] relu5 -> conv5 (in-place)
I1226 15:04:06.562088 98074 net.cpp:228] Setting up relu5
I1226 15:04:06.562156 98074 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 15:04:06.562185 98074 net.cpp:243] Memory required for data: 261762816
I1226 15:04:06.562221 98074 layer_factory.hpp:114] Creating layer pool5
I1226 15:04:06.562279 98074 net.cpp:178] Creating Layer pool5
I1226 15:04:06.562310 98074 net.cpp:612] pool5 <- conv5
I1226 15:04:06.562373 98074 net.cpp:586] pool5 -> pool5
I1226 15:04:06.562481 98074 net.cpp:228] Setting up pool5
I1226 15:04:06.562541 98074 net.cpp:235] Top shape: 32 256 6 6 (294912)
I1226 15:04:06.562566 98074 net.cpp:243] Memory required for data: 262942464
I1226 15:04:06.562599 98074 layer_factory.hpp:114] Creating layer fc6
I1226 15:04:06.562693 98074 net.cpp:178] Creating Layer fc6
I1226 15:04:06.562752 98074 net.cpp:612] fc6 <- pool5
I1226 15:04:06.562803 98074 net.cpp:586] fc6 -> fc6
I1226 15:04:08.714218 52875 net.cpp:228] Setting up fc6
I1226 15:04:08.714334 52875 net.cpp:235] Top shape: 32 4096 (131072)
I1226 15:04:08.714365 52875 net.cpp:243] Memory required for data: 263466752
I1226 15:04:08.714419 52875 layer_factory.hpp:114] Creating layer relu6
I1226 15:04:08.714514 52875 net.cpp:178] Creating Layer relu6
I1226 15:04:08.714557 52875 net.cpp:612] relu6 <- fc6
I1226 15:04:08.714608 52875 net.cpp:573] relu6 -> fc6 (in-place)
I1226 15:04:08.714697 52875 net.cpp:228] Setting up relu6
I1226 15:04:08.714747 52875 net.cpp:235] Top shape: 32 4096 (131072)
I1226 15:04:08.714875 52875 net.cpp:243] Memory required for data: 263991040
I1226 15:04:08.714912 52875 layer_factory.hpp:114] Creating layer drop6
I1226 15:04:08.715010 52875 net.cpp:178] Creating Layer drop6
I1226 15:04:08.715112 52875 net.cpp:612] drop6 <- fc6
I1226 15:04:08.715188 52875 net.cpp:573] drop6 -> fc6 (in-place)
I1226 15:04:08.715252 52875 net.cpp:228] Setting up drop6
I1226 15:04:08.715299 52875 net.cpp:235] Top shape: 32 4096 (131072)
I1226 15:04:08.715323 52875 net.cpp:243] Memory required for data: 264515328
I1226 15:04:08.715353 52875 layer_factory.hpp:114] Creating layer fc7
I1226 15:04:08.715442 52875 net.cpp:178] Creating Layer fc7
I1226 15:04:08.715478 52875 net.cpp:612] fc7 <- fc6
I1226 15:04:08.715515 52875 net.cpp:586] fc7 -> fc7
I1226 15:04:09.050700 92056 net.cpp:228] Setting up fc6
I1226 15:04:09.050843 92056 net.cpp:235] Top shape: 32 4096 (131072)
I1226 15:04:09.050878 92056 net.cpp:243] Memory required for data: 263466752
I1226 15:04:09.050940 92056 layer_factory.hpp:114] Creating layer relu6
I1226 15:04:09.051021 92056 net.cpp:178] Creating Layer relu6
I1226 15:04:09.051144 92056 net.cpp:612] relu6 <- fc6
I1226 15:04:09.051209 92056 net.cpp:573] relu6 -> fc6 (in-place)
I1226 15:04:09.051317 92056 net.cpp:228] Setting up relu6
I1226 15:04:09.051475 92056 net.cpp:235] Top shape: 32 4096 (131072)
I1226 15:04:09.051523 92056 net.cpp:243] Memory required for data: 263991040
I1226 15:04:09.051558 92056 layer_factory.hpp:114] Creating layer drop6
I1226 15:04:09.051616 92056 net.cpp:178] Creating Layer drop6
I1226 15:04:09.051646 92056 net.cpp:612] drop6 <- fc6
I1226 15:04:09.051692 92056 net.cpp:573] drop6 -> fc6 (in-place)
I1226 15:04:09.051749 92056 net.cpp:228] Setting up drop6
I1226 15:04:09.051815 92056 net.cpp:235] Top shape: 32 4096 (131072)
I1226 15:04:09.051849 92056 net.cpp:243] Memory required for data: 264515328
I1226 15:04:09.051878 92056 layer_factory.hpp:114] Creating layer fc7
I1226 15:04:09.051959 92056 net.cpp:178] Creating Layer fc7
I1226 15:04:09.051998 92056 net.cpp:612] fc7 <- fc6
I1226 15:04:09.052043 92056 net.cpp:586] fc7 -> fc7
I1226 15:04:09.348696 88631 net.cpp:228] Setting up fc6
I1226 15:04:09.348814 88631 net.cpp:235] Top shape: 32 4096 (131072)
I1226 15:04:09.348852 88631 net.cpp:243] Memory required for data: 263466752
I1226 15:04:09.348994 88631 layer_factory.hpp:114] Creating layer relu6
I1226 15:04:09.349083 88631 net.cpp:178] Creating Layer relu6
I1226 15:04:09.349145 88631 net.cpp:612] relu6 <- fc6
I1226 15:04:09.349233 88631 net.cpp:573] relu6 -> fc6 (in-place)
I1226 15:04:09.349377 88631 net.cpp:228] Setting up relu6
I1226 15:04:09.349568 88631 net.cpp:235] Top shape: 32 4096 (131072)
I1226 15:04:09.349670 88631 net.cpp:243] Memory required for data: 263991040
I1226 15:04:09.349735 88631 layer_factory.hpp:114] Creating layer drop6
I1226 15:04:09.346889 92722 net.cpp:228] Setting up fc6
I1226 15:04:09.349810 88631 net.cpp:178] Creating Layer drop6
I1226 15:04:09.346998 92722 net.cpp:235] Top shape: 32 4096 (131072)
I1226 15:04:09.347056 92722 net.cpp:243] Memory required for data: 263466752
I1226 15:04:09.349864 88631 net.cpp:612] drop6 <- fc6
I1226 15:04:09.349915 88631 net.cpp:573] drop6 -> fc6 (in-place)
I1226 15:04:09.347121 92722 layer_factory.hpp:114] Creating layer relu6
I1226 15:04:09.350035 88631 net.cpp:228] Setting up drop6
I1226 15:04:09.347256 92722 net.cpp:178] Creating Layer relu6
I1226 15:04:09.350105 88631 net.cpp:235] Top shape: 32 4096 (131072)
I1226 15:04:09.350138 88631 net.cpp:243] Memory required for data: 264515328
I1226 15:04:09.347301 92722 net.cpp:612] relu6 <- fc6
I1226 15:04:09.350181 88631 layer_factory.hpp:114] Creating layer fc7
I1226 15:04:09.347393 92722 net.cpp:573] relu6 -> fc6 (in-place)
I1226 15:04:09.350304 88631 net.cpp:178] Creating Layer fc7
I1226 15:04:09.350354 88631 net.cpp:612] fc7 <- fc6
I1226 15:04:09.350414 88631 net.cpp:586] fc7 -> fc7
I1226 15:04:09.347478 92722 net.cpp:228] Setting up relu6
I1226 15:04:09.347637 92722 net.cpp:235] Top shape: 32 4096 (131072)
I1226 15:04:09.347666 92722 net.cpp:243] Memory required for data: 263991040
I1226 15:04:09.347702 92722 layer_factory.hpp:114] Creating layer drop6
I1226 15:04:09.347790 92722 net.cpp:178] Creating Layer drop6
I1226 15:04:09.347874 92722 net.cpp:612] drop6 <- fc6
I1226 15:04:09.347916 92722 net.cpp:573] drop6 -> fc6 (in-place)
I1226 15:04:09.348165 92722 net.cpp:228] Setting up drop6
I1226 15:04:09.348208 92722 net.cpp:235] Top shape: 32 4096 (131072)
I1226 15:04:09.348232 92722 net.cpp:243] Memory required for data: 264515328
I1226 15:04:09.348260 92722 layer_factory.hpp:114] Creating layer fc7
I1226 15:04:09.348387 92722 net.cpp:178] Creating Layer fc7
I1226 15:04:09.348434 92722 net.cpp:612] fc7 <- fc6
I1226 15:04:09.348479 92722 net.cpp:586] fc7 -> fc7
I1226 15:04:09.362697 92253 net.cpp:228] Setting up fc6
I1226 15:04:09.362810 92253 net.cpp:235] Top shape: 32 4096 (131072)
I1226 15:04:09.362840 92253 net.cpp:243] Memory required for data: 263466752
I1226 15:04:09.362922 92253 layer_factory.hpp:114] Creating layer relu6
I1226 15:04:09.363001 92253 net.cpp:178] Creating Layer relu6
I1226 15:04:09.363039 92253 net.cpp:612] relu6 <- fc6
I1226 15:04:09.363096 92253 net.cpp:573] relu6 -> fc6 (in-place)
I1226 15:04:09.363186 92253 net.cpp:228] Setting up relu6
I1226 15:04:09.363368 92253 net.cpp:235] Top shape: 32 4096 (131072)
I1226 15:04:09.363399 92253 net.cpp:243] Memory required for data: 263991040
I1226 15:04:09.363431 92253 layer_factory.hpp:114] Creating layer drop6
I1226 15:04:09.363498 92253 net.cpp:178] Creating Layer drop6
I1226 15:04:09.363528 92253 net.cpp:612] drop6 <- fc6
I1226 15:04:09.363565 92253 net.cpp:573] drop6 -> fc6 (in-place)
I1226 15:04:09.363620 92253 net.cpp:228] Setting up drop6
I1226 15:04:09.363661 92253 net.cpp:235] Top shape: 32 4096 (131072)
I1226 15:04:09.363684 92253 net.cpp:243] Memory required for data: 264515328
I1226 15:04:09.363713 92253 layer_factory.hpp:114] Creating layer fc7
I1226 15:04:09.363785 92253 net.cpp:178] Creating Layer fc7
I1226 15:04:09.363818 92253 net.cpp:612] fc7 <- fc6
I1226 15:04:09.363860 92253 net.cpp:586] fc7 -> fc7
I1226 15:04:09.401804 118847 net.cpp:228] Setting up fc6
I1226 15:04:09.401926 118847 net.cpp:235] Top shape: 32 4096 (131072)
I1226 15:04:09.401958 118847 net.cpp:243] Memory required for data: 263466752
I1226 15:04:09.402067 118847 layer_factory.hpp:114] Creating layer relu6
I1226 15:04:09.402154 118847 net.cpp:178] Creating Layer relu6
I1226 15:04:09.402200 118847 net.cpp:612] relu6 <- fc6
I1226 15:04:09.402392 118847 net.cpp:573] relu6 -> fc6 (in-place)
I1226 15:04:09.402509 118847 net.cpp:228] Setting up relu6
I1226 15:04:09.402608 118847 net.cpp:235] Top shape: 32 4096 (131072)
I1226 15:04:09.402642 118847 net.cpp:243] Memory required for data: 263991040
I1226 15:04:09.402681 118847 layer_factory.hpp:114] Creating layer drop6
I1226 15:04:09.402768 118847 net.cpp:178] Creating Layer drop6
I1226 15:04:09.402812 118847 net.cpp:612] drop6 <- fc6
I1226 15:04:09.402856 118847 net.cpp:573] drop6 -> fc6 (in-place)
I1226 15:04:09.402920 118847 net.cpp:228] Setting up drop6
I1226 15:04:09.402967 118847 net.cpp:235] Top shape: 32 4096 (131072)
I1226 15:04:09.402998 118847 net.cpp:243] Memory required for data: 264515328
I1226 15:04:09.403040 118847 layer_factory.hpp:114] Creating layer fc7
I1226 15:04:09.403132 118847 net.cpp:178] Creating Layer fc7
I1226 15:04:09.403168 118847 net.cpp:612] fc7 <- fc6
I1226 15:04:09.403218 118847 net.cpp:586] fc7 -> fc7
I1226 15:04:09.405058 93129 net.cpp:228] Setting up fc6
I1226 15:04:09.405172 93129 net.cpp:235] Top shape: 32 4096 (131072)
I1226 15:04:09.405201 93129 net.cpp:243] Memory required for data: 263466752
I1226 15:04:09.405284 93129 layer_factory.hpp:114] Creating layer relu6
I1226 15:04:09.405369 93129 net.cpp:178] Creating Layer relu6
I1226 15:04:09.405410 93129 net.cpp:612] relu6 <- fc6
I1226 15:04:09.405452 93129 net.cpp:573] relu6 -> fc6 (in-place)
I1226 15:04:09.405541 93129 net.cpp:228] Setting up relu6
I1226 15:04:09.405694 93129 net.cpp:235] Top shape: 32 4096 (131072)
I1226 15:04:09.405727 93129 net.cpp:243] Memory required for data: 263991040
I1226 15:04:09.405822 93129 layer_factory.hpp:114] Creating layer drop6
I1226 15:04:09.406024 93129 net.cpp:178] Creating Layer drop6
I1226 15:04:09.406121 93129 net.cpp:612] drop6 <- fc6
I1226 15:04:09.406164 93129 net.cpp:573] drop6 -> fc6 (in-place)
I1226 15:04:09.406224 93129 net.cpp:228] Setting up drop6
I1226 15:04:09.406271 93129 net.cpp:235] Top shape: 32 4096 (131072)
I1226 15:04:09.406319 93129 net.cpp:243] Memory required for data: 264515328
I1226 15:04:09.406357 93129 layer_factory.hpp:114] Creating layer fc7
I1226 15:04:09.406464 93129 net.cpp:178] Creating Layer fc7
I1226 15:04:09.406500 93129 net.cpp:612] fc7 <- fc6
I1226 15:04:09.406541 93129 net.cpp:586] fc7 -> fc7
I1226 15:04:10.977269 52875 net.cpp:228] Setting up fc7
I1226 15:04:10.977380 52875 net.cpp:235] Top shape: 32 4096 (131072)
I1226 15:04:10.977406 52875 net.cpp:243] Memory required for data: 265039616
I1226 15:04:10.977491 52875 layer_factory.hpp:114] Creating layer relu7
I1226 15:04:10.977571 52875 net.cpp:178] Creating Layer relu7
I1226 15:04:10.977604 52875 net.cpp:612] relu7 <- fc7
I1226 15:04:10.977654 52875 net.cpp:573] relu7 -> fc7 (in-place)
I1226 15:04:10.977757 52875 net.cpp:228] Setting up relu7
I1226 15:04:10.977809 52875 net.cpp:235] Top shape: 32 4096 (131072)
I1226 15:04:10.977833 52875 net.cpp:243] Memory required for data: 265563904
I1226 15:04:10.977864 52875 layer_factory.hpp:114] Creating layer drop7
I1226 15:04:10.977900 52875 net.cpp:178] Creating Layer drop7
I1226 15:04:10.977934 52875 net.cpp:612] drop7 <- fc7
I1226 15:04:10.977969 52875 net.cpp:573] drop7 -> fc7 (in-place)
I1226 15:04:10.978020 52875 net.cpp:228] Setting up drop7
I1226 15:04:10.978060 52875 net.cpp:235] Top shape: 32 4096 (131072)
I1226 15:04:10.978082 52875 net.cpp:243] Memory required for data: 266088192
I1226 15:04:10.978108 52875 layer_factory.hpp:114] Creating layer fc8
I1226 15:04:10.978214 52875 net.cpp:178] Creating Layer fc8
I1226 15:04:10.978255 52875 net.cpp:612] fc8 <- fc7
I1226 15:04:10.978307 52875 net.cpp:586] fc8 -> fc8
I1226 15:04:11.309299 92056 net.cpp:228] Setting up fc7
I1226 15:04:11.309433 92056 net.cpp:235] Top shape: 32 4096 (131072)
I1226 15:04:11.309463 92056 net.cpp:243] Memory required for data: 265039616
I1226 15:04:11.309521 92056 layer_factory.hpp:114] Creating layer relu7
I1226 15:04:11.309733 92056 net.cpp:178] Creating Layer relu7
I1226 15:04:11.309837 92056 net.cpp:612] relu7 <- fc7
I1226 15:04:11.309882 92056 net.cpp:573] relu7 -> fc7 (in-place)
I1226 15:04:11.310000 92056 net.cpp:228] Setting up relu7
I1226 15:04:11.310114 92056 net.cpp:235] Top shape: 32 4096 (131072)
I1226 15:04:11.310142 92056 net.cpp:243] Memory required for data: 265563904
I1226 15:04:11.310173 92056 layer_factory.hpp:114] Creating layer drop7
I1226 15:04:11.310233 92056 net.cpp:178] Creating Layer drop7
I1226 15:04:11.310272 92056 net.cpp:612] drop7 <- fc7
I1226 15:04:11.310310 92056 net.cpp:573] drop7 -> fc7 (in-place)
I1226 15:04:11.310358 92056 net.cpp:228] Setting up drop7
I1226 15:04:11.310391 92056 net.cpp:235] Top shape: 32 4096 (131072)
I1226 15:04:11.310420 92056 net.cpp:243] Memory required for data: 266088192
I1226 15:04:11.310446 92056 layer_factory.hpp:114] Creating layer fc8
I1226 15:04:11.310503 92056 net.cpp:178] Creating Layer fc8
I1226 15:04:11.310534 92056 net.cpp:612] fc8 <- fc7
I1226 15:04:11.310586 92056 net.cpp:586] fc8 -> fc8
I1226 15:04:11.551607 52875 net.cpp:228] Setting up fc8
I1226 15:04:11.551719 52875 net.cpp:235] Top shape: 32 1000 (32000)
I1226 15:04:11.551748 52875 net.cpp:243] Memory required for data: 266216192
I1226 15:04:11.551802 52875 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 15:04:11.551890 52875 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 15:04:11.551931 52875 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 15:04:11.551972 52875 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 15:04:11.552148 52875 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 15:04:11.552238 52875 net.cpp:228] Setting up fc8_fc8_0_split
I1226 15:04:11.552289 52875 net.cpp:235] Top shape: 32 1000 (32000)
I1226 15:04:11.552323 52875 net.cpp:235] Top shape: 32 1000 (32000)
I1226 15:04:11.552346 52875 net.cpp:243] Memory required for data: 266472192
I1226 15:04:11.552376 52875 layer_factory.hpp:114] Creating layer accuracy
I1226 15:04:11.552425 52875 net.cpp:178] Creating Layer accuracy
I1226 15:04:11.552458 52875 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 15:04:11.552500 52875 net.cpp:612] accuracy <- label_data_1_split_0
I1226 15:04:11.552547 52875 net.cpp:586] accuracy -> accuracy
I1226 15:04:11.552597 52875 net.cpp:228] Setting up accuracy
I1226 15:04:11.552630 52875 net.cpp:235] Top shape: (1)
I1226 15:04:11.552659 52875 net.cpp:243] Memory required for data: 266472196
I1226 15:04:11.552685 52875 layer_factory.hpp:114] Creating layer loss
I1226 15:04:11.552727 52875 net.cpp:178] Creating Layer loss
I1226 15:04:11.552757 52875 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 15:04:11.552785 52875 net.cpp:612] loss <- label_data_1_split_1
I1226 15:04:11.552819 52875 net.cpp:586] loss -> loss
I1226 15:04:11.552886 52875 layer_factory.hpp:114] Creating layer loss
I1226 15:04:11.578161 52875 net.cpp:228] Setting up loss
I1226 15:04:11.578266 52875 net.cpp:235] Top shape: (1)
I1226 15:04:11.578295 52875 net.cpp:238]     with loss weight 1
I1226 15:04:11.578514 52875 net.cpp:243] Memory required for data: 266472200
I1226 15:04:11.578557 52875 net.cpp:305] loss needs backward computation.
I1226 15:04:11.578594 52875 net.cpp:307] accuracy does not need backward computation.
I1226 15:04:11.578629 52875 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 15:04:11.578660 52875 net.cpp:305] fc8 needs backward computation.
I1226 15:04:11.578691 52875 net.cpp:305] drop7 needs backward computation.
I1226 15:04:11.578721 52875 net.cpp:305] relu7 needs backward computation.
I1226 15:04:11.578749 52875 net.cpp:305] fc7 needs backward computation.
I1226 15:04:11.578786 52875 net.cpp:305] drop6 needs backward computation.
I1226 15:04:11.578815 52875 net.cpp:305] relu6 needs backward computation.
I1226 15:04:11.578843 52875 net.cpp:305] fc6 needs backward computation.
I1226 15:04:11.578874 52875 net.cpp:305] pool5 needs backward computation.
I1226 15:04:11.578907 52875 net.cpp:305] relu5 needs backward computation.
I1226 15:04:11.578941 52875 net.cpp:305] conv5 needs backward computation.
I1226 15:04:11.578972 52875 net.cpp:305] relu4 needs backward computation.
I1226 15:04:11.579004 52875 net.cpp:305] conv4 needs backward computation.
I1226 15:04:11.579040 52875 net.cpp:305] relu3 needs backward computation.
I1226 15:04:11.579095 52875 net.cpp:305] conv3 needs backward computation.
I1226 15:04:11.579151 52875 net.cpp:305] pool2 needs backward computation.
I1226 15:04:11.579185 52875 net.cpp:305] norm2 needs backward computation.
I1226 15:04:11.579216 52875 net.cpp:305] relu2 needs backward computation.
I1226 15:04:11.579247 52875 net.cpp:305] conv2 needs backward computation.
I1226 15:04:11.579279 52875 net.cpp:305] pool1 needs backward computation.
I1226 15:04:11.579316 52875 net.cpp:305] norm1 needs backward computation.
I1226 15:04:11.579355 52875 net.cpp:305] relu1 needs backward computation.
I1226 15:04:11.579383 52875 net.cpp:305] conv1 needs backward computation.
I1226 15:04:11.579422 52875 net.cpp:307] label_data_1_split does not need backward computation.
I1226 15:04:11.579462 52875 net.cpp:307] data does not need backward computation.
I1226 15:04:11.579489 52875 net.cpp:349] This network produces output accuracy
I1226 15:04:11.579525 52875 net.cpp:349] This network produces output loss
I1226 15:04:11.579610 52875 net.cpp:363] Network initialization done.
I1226 15:04:11.580034 52875 solver.cpp:107] Solver scaffolding done.
I1226 15:04:11.580237 52875 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 15:04:11.600906 92722 net.cpp:228] Setting up fc7
I1226 15:04:11.601018 92722 net.cpp:235] Top shape: 32 4096 (131072)
I1226 15:04:11.601045 92722 net.cpp:243] Memory required for data: 265039616
I1226 15:04:11.601101 92722 layer_factory.hpp:114] Creating layer relu7
I1226 15:04:11.601193 92722 net.cpp:178] Creating Layer relu7
I1226 15:04:11.601229 92722 net.cpp:612] relu7 <- fc7
I1226 15:04:11.601282 92722 net.cpp:573] relu7 -> fc7 (in-place)
I1226 15:04:11.601395 92722 net.cpp:228] Setting up relu7
I1226 15:04:11.601451 92722 net.cpp:235] Top shape: 32 4096 (131072)
I1226 15:04:11.601481 92722 net.cpp:243] Memory required for data: 265563904
I1226 15:04:11.601511 92722 layer_factory.hpp:114] Creating layer drop7
I1226 15:04:11.601562 92722 net.cpp:178] Creating Layer drop7
I1226 15:04:11.601596 92722 net.cpp:612] drop7 <- fc7
I1226 15:04:11.601632 92722 net.cpp:573] drop7 -> fc7 (in-place)
I1226 15:04:11.601676 92722 net.cpp:228] Setting up drop7
I1226 15:04:11.601709 92722 net.cpp:235] Top shape: 32 4096 (131072)
I1226 15:04:11.601732 92722 net.cpp:243] Memory required for data: 266088192
I1226 15:04:11.601766 92722 layer_factory.hpp:114] Creating layer fc8
I1226 15:04:11.601822 92722 net.cpp:178] Creating Layer fc8
I1226 15:04:11.601855 92722 net.cpp:612] fc8 <- fc7
I1226 15:04:11.601891 92722 net.cpp:586] fc8 -> fc8
I1226 15:04:11.644593 92253 net.cpp:228] Setting up fc7
I1226 15:04:11.644703 92253 net.cpp:235] Top shape: 32 4096 (131072)
I1226 15:04:11.644733 92253 net.cpp:243] Memory required for data: 265039616
I1226 15:04:11.644791 92253 layer_factory.hpp:114] Creating layer relu7
I1226 15:04:11.644882 92253 net.cpp:178] Creating Layer relu7
I1226 15:04:11.644927 92253 net.cpp:612] relu7 <- fc7
I1226 15:04:11.644973 92253 net.cpp:573] relu7 -> fc7 (in-place)
I1226 15:04:11.645061 92253 net.cpp:228] Setting up relu7
I1226 15:04:11.645114 92253 net.cpp:235] Top shape: 32 4096 (131072)
I1226 15:04:11.645145 92253 net.cpp:243] Memory required for data: 265563904
I1226 15:04:11.645176 92253 layer_factory.hpp:114] Creating layer drop7
I1226 15:04:11.645259 92253 net.cpp:178] Creating Layer drop7
I1226 15:04:11.645304 92253 net.cpp:612] drop7 <- fc7
I1226 15:04:11.645342 92253 net.cpp:573] drop7 -> fc7 (in-place)
I1226 15:04:11.645391 92253 net.cpp:228] Setting up drop7
I1226 15:04:11.645436 92253 net.cpp:235] Top shape: 32 4096 (131072)
I1226 15:04:11.645463 92253 net.cpp:243] Memory required for data: 266088192
I1226 15:04:11.645498 92253 layer_factory.hpp:114] Creating layer fc8
I1226 15:04:11.645576 92253 net.cpp:178] Creating Layer fc8
I1226 15:04:11.645619 92253 net.cpp:612] fc8 <- fc7
I1226 15:04:11.645701 92253 net.cpp:586] fc8 -> fc8
I1226 15:04:11.662351 88631 net.cpp:228] Setting up fc7
I1226 15:04:11.662469 88631 net.cpp:235] Top shape: 32 4096 (131072)
I1226 15:04:11.662504 88631 net.cpp:243] Memory required for data: 265039616
I1226 15:04:11.662582 88631 layer_factory.hpp:114] Creating layer relu7
I1226 15:04:11.662700 88631 net.cpp:178] Creating Layer relu7
I1226 15:04:11.662753 88631 net.cpp:612] relu7 <- fc7
I1226 15:04:11.662801 88631 net.cpp:573] relu7 -> fc7 (in-place)
I1226 15:04:11.662906 88631 net.cpp:228] Setting up relu7
I1226 15:04:11.662998 88631 net.cpp:235] Top shape: 32 4096 (131072)
I1226 15:04:11.663030 88631 net.cpp:243] Memory required for data: 265563904
I1226 15:04:11.663069 88631 layer_factory.hpp:114] Creating layer drop7
I1226 15:04:11.663131 88631 net.cpp:178] Creating Layer drop7
I1226 15:04:11.663180 88631 net.cpp:612] drop7 <- fc7
I1226 15:04:11.663229 88631 net.cpp:573] drop7 -> fc7 (in-place)
I1226 15:04:11.663290 88631 net.cpp:228] Setting up drop7
I1226 15:04:11.663341 88631 net.cpp:235] Top shape: 32 4096 (131072)
I1226 15:04:11.663372 88631 net.cpp:243] Memory required for data: 266088192
I1226 15:04:11.663403 88631 layer_factory.hpp:114] Creating layer fc8
I1226 15:04:11.663473 88631 net.cpp:178] Creating Layer fc8
I1226 15:04:11.663516 88631 net.cpp:612] fc8 <- fc7
I1226 15:04:11.663578 88631 net.cpp:586] fc8 -> fc8
I1226 15:04:11.676559 93129 net.cpp:228] Setting up fc7
I1226 15:04:11.676671 93129 net.cpp:235] Top shape: 32 4096 (131072)
I1226 15:04:11.676702 93129 net.cpp:243] Memory required for data: 265039616
I1226 15:04:11.676761 93129 layer_factory.hpp:114] Creating layer relu7
I1226 15:04:11.676872 93129 net.cpp:178] Creating Layer relu7
I1226 15:04:11.676926 93129 net.cpp:612] relu7 <- fc7
I1226 15:04:11.676982 93129 net.cpp:573] relu7 -> fc7 (in-place)
I1226 15:04:11.677069 93129 net.cpp:228] Setting up relu7
I1226 15:04:11.677121 93129 net.cpp:235] Top shape: 32 4096 (131072)
I1226 15:04:11.677146 93129 net.cpp:243] Memory required for data: 265563904
I1226 15:04:11.677178 93129 layer_factory.hpp:114] Creating layer drop7
I1226 15:04:11.677250 93129 net.cpp:178] Creating Layer drop7
I1226 15:04:11.677287 93129 net.cpp:612] drop7 <- fc7
I1226 15:04:11.677323 93129 net.cpp:573] drop7 -> fc7 (in-place)
I1226 15:04:11.677369 93129 net.cpp:228] Setting up drop7
I1226 15:04:11.677417 93129 net.cpp:235] Top shape: 32 4096 (131072)
I1226 15:04:11.677440 93129 net.cpp:243] Memory required for data: 266088192
I1226 15:04:11.677466 93129 layer_factory.hpp:114] Creating layer fc8
I1226 15:04:11.677525 93129 net.cpp:178] Creating Layer fc8
I1226 15:04:11.677559 93129 net.cpp:612] fc8 <- fc7
I1226 15:04:11.677597 93129 net.cpp:586] fc8 -> fc8
I1226 15:04:11.683769 118847 net.cpp:228] Setting up fc7
I1226 15:04:11.683888 118847 net.cpp:235] Top shape: 32 4096 (131072)
I1226 15:04:11.683923 118847 net.cpp:243] Memory required for data: 265039616
I1226 15:04:11.684001 118847 layer_factory.hpp:114] Creating layer relu7
I1226 15:04:11.684108 118847 net.cpp:178] Creating Layer relu7
I1226 15:04:11.684159 118847 net.cpp:612] relu7 <- fc7
I1226 15:04:11.684206 118847 net.cpp:573] relu7 -> fc7 (in-place)
I1226 15:04:11.684311 118847 net.cpp:228] Setting up relu7
I1226 15:04:11.684365 118847 net.cpp:235] Top shape: 32 4096 (131072)
I1226 15:04:11.684406 118847 net.cpp:243] Memory required for data: 265563904
I1226 15:04:11.684445 118847 layer_factory.hpp:114] Creating layer drop7
I1226 15:04:11.684517 118847 net.cpp:178] Creating Layer drop7
I1226 15:04:11.684579 118847 net.cpp:612] drop7 <- fc7
I1226 15:04:11.684636 118847 net.cpp:573] drop7 -> fc7 (in-place)
I1226 15:04:11.684695 118847 net.cpp:228] Setting up drop7
I1226 15:04:11.684739 118847 net.cpp:235] Top shape: 32 4096 (131072)
I1226 15:04:11.684768 118847 net.cpp:243] Memory required for data: 266088192
I1226 15:04:11.684805 118847 layer_factory.hpp:114] Creating layer fc8
I1226 15:04:11.684876 118847 net.cpp:178] Creating Layer fc8
I1226 15:04:11.684909 118847 net.cpp:612] fc8 <- fc7
I1226 15:04:11.684958 118847 net.cpp:586] fc8 -> fc8
I1226 15:04:11.855854 92056 net.cpp:228] Setting up fc8
I1226 15:04:11.855973 92056 net.cpp:235] Top shape: 32 1000 (32000)
I1226 15:04:11.856006 92056 net.cpp:243] Memory required for data: 266216192
I1226 15:04:11.856065 92056 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 15:04:11.856138 92056 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 15:04:11.856302 92056 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 15:04:11.856350 92056 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 15:04:11.856453 92056 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 15:04:11.856628 92056 net.cpp:228] Setting up fc8_fc8_0_split
I1226 15:04:11.856686 92056 net.cpp:235] Top shape: 32 1000 (32000)
I1226 15:04:11.856717 92056 net.cpp:235] Top shape: 32 1000 (32000)
I1226 15:04:11.856741 92056 net.cpp:243] Memory required for data: 266472192
I1226 15:04:11.856779 92056 layer_factory.hpp:114] Creating layer accuracy
I1226 15:04:11.856863 92056 net.cpp:178] Creating Layer accuracy
I1226 15:04:11.856894 92056 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 15:04:11.856957 92056 net.cpp:612] accuracy <- label_data_1_split_0
I1226 15:04:11.856995 92056 net.cpp:586] accuracy -> accuracy
I1226 15:04:11.857050 92056 net.cpp:228] Setting up accuracy
I1226 15:04:11.857111 92056 net.cpp:235] Top shape: (1)
I1226 15:04:11.857146 92056 net.cpp:243] Memory required for data: 266472196
I1226 15:04:11.857182 92056 layer_factory.hpp:114] Creating layer loss
I1226 15:04:11.857241 92056 net.cpp:178] Creating Layer loss
I1226 15:04:11.857271 92056 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 15:04:11.857300 92056 net.cpp:612] loss <- label_data_1_split_1
I1226 15:04:11.857347 92056 net.cpp:586] loss -> loss
I1226 15:04:11.857412 92056 layer_factory.hpp:114] Creating layer loss
I1226 15:04:11.886361 92056 net.cpp:228] Setting up loss
I1226 15:04:11.886478 92056 net.cpp:235] Top shape: (1)
I1226 15:04:11.886648 92056 net.cpp:238]     with loss weight 1
I1226 15:04:11.886837 92056 net.cpp:243] Memory required for data: 266472200
I1226 15:04:11.886886 92056 net.cpp:305] loss needs backward computation.
I1226 15:04:11.886926 92056 net.cpp:307] accuracy does not need backward computation.
I1226 15:04:11.886960 92056 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 15:04:11.886991 92056 net.cpp:305] fc8 needs backward computation.
I1226 15:04:11.887022 92056 net.cpp:305] drop7 needs backward computation.
I1226 15:04:11.887049 92056 net.cpp:305] relu7 needs backward computation.
I1226 15:04:11.887078 92056 net.cpp:305] fc7 needs backward computation.
I1226 15:04:11.887107 92056 net.cpp:305] drop6 needs backward computation.
I1226 15:04:11.887135 92056 net.cpp:305] relu6 needs backward computation.
I1226 15:04:11.887164 92056 net.cpp:305] fc6 needs backward computation.
I1226 15:04:11.887193 92056 net.cpp:305] pool5 needs backward computation.
I1226 15:04:11.887223 92056 net.cpp:305] relu5 needs backward computation.
I1226 15:04:11.887251 92056 net.cpp:305] conv5 needs backward computation.
I1226 15:04:11.887281 92056 net.cpp:305] relu4 needs backward computation.
I1226 15:04:11.887311 92056 net.cpp:305] conv4 needs backward computation.
I1226 15:04:11.887346 92056 net.cpp:305] relu3 needs backward computation.
I1226 15:04:11.887378 92056 net.cpp:305] conv3 needs backward computation.
I1226 15:04:11.887416 92056 net.cpp:305] pool2 needs backward computation.
I1226 15:04:11.887624 92056 net.cpp:305] norm2 needs backward computation.
I1226 15:04:11.887657 92056 net.cpp:305] relu2 needs backward computation.
I1226 15:04:11.887689 92056 net.cpp:305] conv2 needs backward computation.
I1226 15:04:11.887750 92056 net.cpp:305] pool1 needs backward computation.
I1226 15:04:11.887814 92056 net.cpp:305] norm1 needs backward computation.
I1226 15:04:11.887850 92056 net.cpp:305] relu1 needs backward computation.
I1226 15:04:11.887881 92056 net.cpp:305] conv1 needs backward computation.
I1226 15:04:11.887917 92056 net.cpp:307] label_data_1_split does not need backward computation.
I1226 15:04:11.887950 92056 net.cpp:307] data does not need backward computation.
I1226 15:04:11.887987 92056 net.cpp:349] This network produces output accuracy
I1226 15:04:11.888023 92056 net.cpp:349] This network produces output loss
I1226 15:04:11.888124 92056 net.cpp:363] Network initialization done.
I1226 15:04:11.888576 92056 solver.cpp:107] Solver scaffolding done.
I1226 15:04:11.888833 92056 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 15:04:12.169834 92722 net.cpp:228] Setting up fc8
I1226 15:04:12.169961 92722 net.cpp:235] Top shape: 32 1000 (32000)
I1226 15:04:12.169993 92722 net.cpp:243] Memory required for data: 266216192
I1226 15:04:12.170053 92722 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 15:04:12.170130 92722 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 15:04:12.170172 92722 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 15:04:12.170220 92722 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 15:04:12.170286 92722 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 15:04:12.170413 92722 net.cpp:228] Setting up fc8_fc8_0_split
I1226 15:04:12.170472 92722 net.cpp:235] Top shape: 32 1000 (32000)
I1226 15:04:12.170511 92722 net.cpp:235] Top shape: 32 1000 (32000)
I1226 15:04:12.170534 92722 net.cpp:243] Memory required for data: 266472192
I1226 15:04:12.170563 92722 layer_factory.hpp:114] Creating layer accuracy
I1226 15:04:12.170617 92722 net.cpp:178] Creating Layer accuracy
I1226 15:04:12.170645 92722 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 15:04:12.170688 92722 net.cpp:612] accuracy <- label_data_1_split_0
I1226 15:04:12.170730 92722 net.cpp:586] accuracy -> accuracy
I1226 15:04:12.170785 92722 net.cpp:228] Setting up accuracy
I1226 15:04:12.170827 92722 net.cpp:235] Top shape: (1)
I1226 15:04:12.170850 92722 net.cpp:243] Memory required for data: 266472196
I1226 15:04:12.170878 92722 layer_factory.hpp:114] Creating layer loss
I1226 15:04:12.170928 92722 net.cpp:178] Creating Layer loss
I1226 15:04:12.170958 92722 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 15:04:12.171001 92722 net.cpp:612] loss <- label_data_1_split_1
I1226 15:04:12.171042 92722 net.cpp:586] loss -> loss
I1226 15:04:12.171105 92722 layer_factory.hpp:114] Creating layer loss
I1226 15:04:12.194545 92722 net.cpp:228] Setting up loss
I1226 15:04:12.194655 92722 net.cpp:235] Top shape: (1)
I1226 15:04:12.194823 92722 net.cpp:238]     with loss weight 1
I1226 15:04:12.194973 92722 net.cpp:243] Memory required for data: 266472200
I1226 15:04:12.195025 92722 net.cpp:305] loss needs backward computation.
I1226 15:04:12.195068 92722 net.cpp:307] accuracy does not need backward computation.
I1226 15:04:12.195104 92722 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 15:04:12.195135 92722 net.cpp:305] fc8 needs backward computation.
I1226 15:04:12.195183 92722 net.cpp:305] drop7 needs backward computation.
I1226 15:04:12.195214 92722 net.cpp:305] relu7 needs backward computation.
I1226 15:04:12.195243 92722 net.cpp:305] fc7 needs backward computation.
I1226 15:04:12.195273 92722 net.cpp:305] drop6 needs backward computation.
I1226 15:04:12.195302 92722 net.cpp:305] relu6 needs backward computation.
I1226 15:04:12.195360 92722 net.cpp:305] fc6 needs backward computation.
I1226 15:04:12.195394 92722 net.cpp:305] pool5 needs backward computation.
I1226 15:04:12.195426 92722 net.cpp:305] relu5 needs backward computation.
I1226 15:04:12.195457 92722 net.cpp:305] conv5 needs backward computation.
I1226 15:04:12.195492 92722 net.cpp:305] relu4 needs backward computation.
I1226 15:04:12.195521 92722 net.cpp:305] conv4 needs backward computation.
I1226 15:04:12.195554 92722 net.cpp:305] relu3 needs backward computation.
I1226 15:04:12.195600 92722 net.cpp:305] conv3 needs backward computation.
I1226 15:04:12.195643 92722 net.cpp:305] pool2 needs backward computation.
I1226 15:04:12.195680 92722 net.cpp:305] norm2 needs backward computation.
I1226 15:04:12.195716 92722 net.cpp:305] relu2 needs backward computation.
I1226 15:04:12.195752 92722 net.cpp:305] conv2 needs backward computation.
I1226 15:04:12.195802 92722 net.cpp:305] pool1 needs backward computation.
I1226 15:04:12.195837 92722 net.cpp:305] norm1 needs backward computation.
I1226 15:04:12.195868 92722 net.cpp:305] relu1 needs backward computation.
I1226 15:04:12.195904 92722 net.cpp:305] conv1 needs backward computation.
I1226 15:04:12.195969 92722 net.cpp:307] label_data_1_split does not need backward computation.
I1226 15:04:12.196012 92722 net.cpp:307] data does not need backward computation.
I1226 15:04:12.196048 92722 net.cpp:349] This network produces output accuracy
I1226 15:04:12.196089 92722 net.cpp:349] This network produces output loss
I1226 15:04:12.196179 92722 net.cpp:363] Network initialization done.
I1226 15:04:12.196666 92722 solver.cpp:107] Solver scaffolding done.
I1226 15:04:12.196900 92722 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 15:04:12.213155 92253 net.cpp:228] Setting up fc8
I1226 15:04:12.213331 92253 net.cpp:235] Top shape: 32 1000 (32000)
I1226 15:04:12.213376 92253 net.cpp:243] Memory required for data: 266216192
I1226 15:04:12.213459 92253 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 15:04:12.213544 92253 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 15:04:12.213587 92253 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 15:04:12.213639 92253 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 15:04:12.213722 92253 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 15:04:12.213845 92253 net.cpp:228] Setting up fc8_fc8_0_split
I1226 15:04:12.213907 92253 net.cpp:235] Top shape: 32 1000 (32000)
I1226 15:04:12.213951 92253 net.cpp:235] Top shape: 32 1000 (32000)
I1226 15:04:12.213984 92253 net.cpp:243] Memory required for data: 266472192
I1226 15:04:12.214025 92253 layer_factory.hpp:114] Creating layer accuracy
I1226 15:04:12.214092 92253 net.cpp:178] Creating Layer accuracy
I1226 15:04:12.214126 92253 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 15:04:12.214166 92253 net.cpp:612] accuracy <- label_data_1_split_0
I1226 15:04:12.214242 92253 net.cpp:586] accuracy -> accuracy
I1226 15:04:12.214339 92253 net.cpp:228] Setting up accuracy
I1226 15:04:12.214401 92253 net.cpp:235] Top shape: (1)
I1226 15:04:12.214447 92253 net.cpp:243] Memory required for data: 266472196
I1226 15:04:12.214491 92253 layer_factory.hpp:114] Creating layer loss
I1226 15:04:12.214589 92253 net.cpp:178] Creating Layer loss
I1226 15:04:12.214639 92253 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 15:04:12.214689 92253 net.cpp:612] loss <- label_data_1_split_1
I1226 15:04:12.214742 92253 net.cpp:586] loss -> loss
I1226 15:04:12.214843 92253 layer_factory.hpp:114] Creating layer loss
I1226 15:04:12.226114 88631 net.cpp:228] Setting up fc8
I1226 15:04:12.226244 88631 net.cpp:235] Top shape: 32 1000 (32000)
I1226 15:04:12.226281 88631 net.cpp:243] Memory required for data: 266216192
I1226 15:04:12.226357 88631 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 15:04:12.226438 88631 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 15:04:12.226488 88631 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 15:04:12.226534 88631 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 15:04:12.226615 88631 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 15:04:12.226732 88631 net.cpp:228] Setting up fc8_fc8_0_split
I1226 15:04:12.226795 88631 net.cpp:235] Top shape: 32 1000 (32000)
I1226 15:04:12.226833 88631 net.cpp:235] Top shape: 32 1000 (32000)
I1226 15:04:12.226862 88631 net.cpp:243] Memory required for data: 266472192
I1226 15:04:12.226897 88631 layer_factory.hpp:114] Creating layer accuracy
I1226 15:04:12.226986 88631 net.cpp:178] Creating Layer accuracy
I1226 15:04:12.227027 88631 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 15:04:12.227072 88631 net.cpp:612] accuracy <- label_data_1_split_0
I1226 15:04:12.227139 88631 net.cpp:586] accuracy -> accuracy
I1226 15:04:12.227211 88631 net.cpp:228] Setting up accuracy
I1226 15:04:12.227262 88631 net.cpp:235] Top shape: (1)
I1226 15:04:12.227294 88631 net.cpp:243] Memory required for data: 266472196
I1226 15:04:12.227334 88631 layer_factory.hpp:114] Creating layer loss
I1226 15:04:12.227411 88631 net.cpp:178] Creating Layer loss
I1226 15:04:12.227460 88631 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 15:04:12.227504 88631 net.cpp:612] loss <- label_data_1_split_1
I1226 15:04:12.227555 88631 net.cpp:586] loss -> loss
I1226 15:04:12.227654 88631 layer_factory.hpp:114] Creating layer loss
I1226 15:04:12.232066 93129 net.cpp:228] Setting up fc8
I1226 15:04:12.232187 93129 net.cpp:235] Top shape: 32 1000 (32000)
I1226 15:04:12.232218 93129 net.cpp:243] Memory required for data: 266216192
I1226 15:04:12.232300 93129 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 15:04:12.232373 93129 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 15:04:12.232405 93129 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 15:04:12.232445 93129 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 15:04:12.232491 93129 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 15:04:12.232591 93129 net.cpp:228] Setting up fc8_fc8_0_split
I1226 15:04:12.232640 93129 net.cpp:235] Top shape: 32 1000 (32000)
I1226 15:04:12.232671 93129 net.cpp:235] Top shape: 32 1000 (32000)
I1226 15:04:12.232693 93129 net.cpp:243] Memory required for data: 266472192
I1226 15:04:12.232723 93129 layer_factory.hpp:114] Creating layer accuracy
I1226 15:04:12.232787 93129 net.cpp:178] Creating Layer accuracy
I1226 15:04:12.232842 93129 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 15:04:12.232873 93129 net.cpp:612] accuracy <- label_data_1_split_0
I1226 15:04:12.232910 93129 net.cpp:586] accuracy -> accuracy
I1226 15:04:12.232972 93129 net.cpp:228] Setting up accuracy
I1226 15:04:12.233017 93129 net.cpp:235] Top shape: (1)
I1226 15:04:12.233047 93129 net.cpp:243] Memory required for data: 266472196
I1226 15:04:12.233074 93129 layer_factory.hpp:114] Creating layer loss
I1226 15:04:12.233117 93129 net.cpp:178] Creating Layer loss
I1226 15:04:12.233142 93129 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 15:04:12.233191 93129 net.cpp:612] loss <- label_data_1_split_1
I1226 15:04:12.233230 93129 net.cpp:586] loss -> loss
I1226 15:04:12.233290 93129 layer_factory.hpp:114] Creating layer loss
I1226 15:04:12.257752 118847 net.cpp:228] Setting up fc8
I1226 15:04:12.257880 118847 net.cpp:235] Top shape: 32 1000 (32000)
I1226 15:04:12.257954 118847 net.cpp:243] Memory required for data: 266216192
I1226 15:04:12.258193 118847 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 15:04:12.258270 118847 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 15:04:12.258316 118847 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 15:04:12.258361 118847 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 15:04:12.258433 118847 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 15:04:12.258540 118847 net.cpp:228] Setting up fc8_fc8_0_split
I1226 15:04:12.258636 118847 net.cpp:235] Top shape: 32 1000 (32000)
I1226 15:04:12.258677 118847 net.cpp:235] Top shape: 32 1000 (32000)
I1226 15:04:12.258705 118847 net.cpp:243] Memory required for data: 266472192
I1226 15:04:12.258744 118847 layer_factory.hpp:114] Creating layer accuracy
I1226 15:04:12.258821 118847 net.cpp:178] Creating Layer accuracy
I1226 15:04:12.250325 92253 net.cpp:228] Setting up loss
I1226 15:04:12.258860 118847 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 15:04:12.258898 118847 net.cpp:612] accuracy <- label_data_1_split_0
I1226 15:04:12.250424 92253 net.cpp:235] Top shape: (1)
I1226 15:04:12.258942 118847 net.cpp:586] accuracy -> accuracy
I1226 15:04:12.250560 92253 net.cpp:238]     with loss weight 1
I1226 15:04:12.259006 118847 net.cpp:228] Setting up accuracy
I1226 15:04:12.250679 92253 net.cpp:243] Memory required for data: 266472200
I1226 15:04:12.259047 118847 net.cpp:235] Top shape: (1)
I1226 15:04:12.250717 92253 net.cpp:305] loss needs backward computation.
I1226 15:04:12.259085 118847 net.cpp:243] Memory required for data: 266472196
I1226 15:04:12.259119 118847 layer_factory.hpp:114] Creating layer loss
I1226 15:04:12.250757 92253 net.cpp:307] accuracy does not need backward computation.
I1226 15:04:12.259172 118847 net.cpp:178] Creating Layer loss
I1226 15:04:12.250802 92253 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 15:04:12.259203 118847 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 15:04:12.250838 92253 net.cpp:305] fc8 needs backward computation.
I1226 15:04:12.259263 118847 net.cpp:612] loss <- label_data_1_split_1
I1226 15:04:12.250869 92253 net.cpp:305] drop7 needs backward computation.
I1226 15:04:12.259315 118847 net.cpp:586] loss -> loss
I1226 15:04:12.250898 92253 net.cpp:305] relu7 needs backward computation.
I1226 15:04:12.259498 118847 layer_factory.hpp:114] Creating layer loss
I1226 15:04:12.250928 92253 net.cpp:305] fc7 needs backward computation.
I1226 15:04:12.250959 92253 net.cpp:305] drop6 needs backward computation.
I1226 15:04:12.250990 92253 net.cpp:305] relu6 needs backward computation.
I1226 15:04:12.251025 92253 net.cpp:305] fc6 needs backward computation.
I1226 15:04:12.251056 92253 net.cpp:305] pool5 needs backward computation.
I1226 15:04:12.251092 92253 net.cpp:305] relu5 needs backward computation.
I1226 15:04:12.251123 92253 net.cpp:305] conv5 needs backward computation.
I1226 15:04:12.251159 92253 net.cpp:305] relu4 needs backward computation.
I1226 15:04:12.251194 92253 net.cpp:305] conv4 needs backward computation.
I1226 15:04:12.251256 92253 net.cpp:305] relu3 needs backward computation.
I1226 15:04:12.251286 92253 net.cpp:305] conv3 needs backward computation.
I1226 15:04:12.251327 92253 net.cpp:305] pool2 needs backward computation.
I1226 15:04:12.251365 92253 net.cpp:305] norm2 needs backward computation.
I1226 15:04:12.251412 92253 net.cpp:305] relu2 needs backward computation.
I1226 15:04:12.251443 92253 net.cpp:305] conv2 needs backward computation.
I1226 15:04:12.251483 92253 net.cpp:305] pool1 needs backward computation.
I1226 15:04:12.251523 92253 net.cpp:305] norm1 needs backward computation.
I1226 15:04:12.251554 92253 net.cpp:305] relu1 needs backward computation.
I1226 15:04:12.251588 92253 net.cpp:305] conv1 needs backward computation.
I1226 15:04:12.251628 92253 net.cpp:307] label_data_1_split does not need backward computation.
I1226 15:04:12.251667 92253 net.cpp:307] data does not need backward computation.
I1226 15:04:12.251694 92253 net.cpp:349] This network produces output accuracy
I1226 15:04:12.251734 92253 net.cpp:349] This network produces output loss
I1226 15:04:12.251824 92253 net.cpp:363] Network initialization done.
I1226 15:04:12.252251 92253 solver.cpp:107] Solver scaffolding done.
I1226 15:04:12.252439 92253 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 15:04:12.261682 88631 net.cpp:228] Setting up loss
I1226 15:04:12.261795 88631 net.cpp:235] Top shape: (1)
I1226 15:04:12.261976 88631 net.cpp:238]     with loss weight 1
I1226 15:04:12.262120 88631 net.cpp:243] Memory required for data: 266472200
I1226 15:04:12.262168 88631 net.cpp:305] loss needs backward computation.
I1226 15:04:12.262214 88631 net.cpp:307] accuracy does not need backward computation.
I1226 15:04:12.262250 88631 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 15:04:12.262290 88631 net.cpp:305] fc8 needs backward computation.
I1226 15:04:12.262331 88631 net.cpp:305] drop7 needs backward computation.
I1226 15:04:12.262361 88631 net.cpp:305] relu7 needs backward computation.
I1226 15:04:12.262401 88631 net.cpp:305] fc7 needs backward computation.
I1226 15:04:12.262433 88631 net.cpp:305] drop6 needs backward computation.
I1226 15:04:12.262461 88631 net.cpp:305] relu6 needs backward computation.
I1226 15:04:12.262492 88631 net.cpp:305] fc6 needs backward computation.
I1226 15:04:12.262531 88631 net.cpp:305] pool5 needs backward computation.
I1226 15:04:12.262569 88631 net.cpp:305] relu5 needs backward computation.
I1226 15:04:12.262608 88631 net.cpp:305] conv5 needs backward computation.
I1226 15:04:12.262645 88631 net.cpp:305] relu4 needs backward computation.
I1226 15:04:12.262675 88631 net.cpp:305] conv4 needs backward computation.
I1226 15:04:12.262712 88631 net.cpp:305] relu3 needs backward computation.
I1226 15:04:12.262747 88631 net.cpp:305] conv3 needs backward computation.
I1226 15:04:12.262778 88631 net.cpp:305] pool2 needs backward computation.
I1226 15:04:12.262807 88631 net.cpp:305] norm2 needs backward computation.
I1226 15:04:12.262840 88631 net.cpp:305] relu2 needs backward computation.
I1226 15:04:12.262876 88631 net.cpp:305] conv2 needs backward computation.
I1226 15:04:12.262962 88631 net.cpp:305] pool1 needs backward computation.
I1226 15:04:12.263008 88631 net.cpp:305] norm1 needs backward computation.
I1226 15:04:12.263039 88631 net.cpp:305] relu1 needs backward computation.
I1226 15:04:12.263069 88631 net.cpp:305] conv1 needs backward computation.
I1226 15:04:12.263105 88631 net.cpp:307] label_data_1_split does not need backward computation.
I1226 15:04:12.263139 88631 net.cpp:307] data does not need backward computation.
I1226 15:04:12.263169 88631 net.cpp:349] This network produces output accuracy
I1226 15:04:12.263205 88631 net.cpp:349] This network produces output loss
I1226 15:04:12.263299 88631 net.cpp:363] Network initialization done.
I1226 15:04:12.263738 88631 solver.cpp:107] Solver scaffolding done.
I1226 15:04:12.261273 93129 net.cpp:228] Setting up loss
I1226 15:04:12.263973 88631 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 15:04:12.261385 93129 net.cpp:235] Top shape: (1)
I1226 15:04:12.261549 93129 net.cpp:238]     with loss weight 1
I1226 15:04:12.261699 93129 net.cpp:243] Memory required for data: 266472200
I1226 15:04:12.261751 93129 net.cpp:305] loss needs backward computation.
I1226 15:04:12.261819 93129 net.cpp:307] accuracy does not need backward computation.
I1226 15:04:12.261862 93129 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 15:04:12.261898 93129 net.cpp:305] fc8 needs backward computation.
I1226 15:04:12.261931 93129 net.cpp:305] drop7 needs backward computation.
I1226 15:04:12.261962 93129 net.cpp:305] relu7 needs backward computation.
I1226 15:04:12.261992 93129 net.cpp:305] fc7 needs backward computation.
I1226 15:04:12.262022 93129 net.cpp:305] drop6 needs backward computation.
I1226 15:04:12.262053 93129 net.cpp:305] relu6 needs backward computation.
I1226 15:04:12.262081 93129 net.cpp:305] fc6 needs backward computation.
I1226 15:04:12.262115 93129 net.cpp:305] pool5 needs backward computation.
I1226 15:04:12.262161 93129 net.cpp:305] relu5 needs backward computation.
I1226 15:04:12.262192 93129 net.cpp:305] conv5 needs backward computation.
I1226 15:04:12.262223 93129 net.cpp:305] relu4 needs backward computation.
I1226 15:04:12.262253 93129 net.cpp:305] conv4 needs backward computation.
I1226 15:04:12.262293 93129 net.cpp:305] relu3 needs backward computation.
I1226 15:04:12.262332 93129 net.cpp:305] conv3 needs backward computation.
I1226 15:04:12.262367 93129 net.cpp:305] pool2 needs backward computation.
I1226 15:04:12.262403 93129 net.cpp:305] norm2 needs backward computation.
I1226 15:04:12.262435 93129 net.cpp:305] relu2 needs backward computation.
I1226 15:04:12.262465 93129 net.cpp:305] conv2 needs backward computation.
I1226 15:04:12.262502 93129 net.cpp:305] pool1 needs backward computation.
I1226 15:04:12.262542 93129 net.cpp:305] norm1 needs backward computation.
I1226 15:04:12.262579 93129 net.cpp:305] relu1 needs backward computation.
I1226 15:04:12.262619 93129 net.cpp:305] conv1 needs backward computation.
I1226 15:04:12.262652 93129 net.cpp:307] label_data_1_split does not need backward computation.
I1226 15:04:12.262713 93129 net.cpp:307] data does not need backward computation.
I1226 15:04:12.262748 93129 net.cpp:349] This network produces output accuracy
I1226 15:04:12.262783 93129 net.cpp:349] This network produces output loss
I1226 15:04:12.262900 93129 net.cpp:363] Network initialization done.
I1226 15:04:12.263345 93129 solver.cpp:107] Solver scaffolding done.
I1226 15:04:12.263568 93129 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 15:04:12.292493 118847 net.cpp:228] Setting up loss
I1226 15:04:12.292637 118847 net.cpp:235] Top shape: (1)
I1226 15:04:12.292683 118847 net.cpp:238]     with loss weight 1
I1226 15:04:12.292829 118847 net.cpp:243] Memory required for data: 266472200
I1226 15:04:12.292876 118847 net.cpp:305] loss needs backward computation.
I1226 15:04:12.292917 118847 net.cpp:307] accuracy does not need backward computation.
I1226 15:04:12.292953 118847 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 15:04:12.292984 118847 net.cpp:305] fc8 needs backward computation.
I1226 15:04:12.293028 118847 net.cpp:305] drop7 needs backward computation.
I1226 15:04:12.293059 118847 net.cpp:305] relu7 needs backward computation.
I1226 15:04:12.293089 118847 net.cpp:305] fc7 needs backward computation.
I1226 15:04:12.293125 118847 net.cpp:305] drop6 needs backward computation.
I1226 15:04:12.293154 118847 net.cpp:305] relu6 needs backward computation.
I1226 15:04:12.293184 118847 net.cpp:305] fc6 needs backward computation.
I1226 15:04:12.293215 118847 net.cpp:305] pool5 needs backward computation.
I1226 15:04:12.293247 118847 net.cpp:305] relu5 needs backward computation.
I1226 15:04:12.293288 118847 net.cpp:305] conv5 needs backward computation.
I1226 15:04:12.293320 118847 net.cpp:305] relu4 needs backward computation.
I1226 15:04:12.293356 118847 net.cpp:305] conv4 needs backward computation.
I1226 15:04:12.293387 118847 net.cpp:305] relu3 needs backward computation.
I1226 15:04:12.293426 118847 net.cpp:305] conv3 needs backward computation.
I1226 15:04:12.293467 118847 net.cpp:305] pool2 needs backward computation.
I1226 15:04:12.293504 118847 net.cpp:305] norm2 needs backward computation.
I1226 15:04:12.293545 118847 net.cpp:305] relu2 needs backward computation.
I1226 15:04:12.293603 118847 net.cpp:305] conv2 needs backward computation.
I1226 15:04:12.293637 118847 net.cpp:305] pool1 needs backward computation.
I1226 15:04:12.293679 118847 net.cpp:305] norm1 needs backward computation.
I1226 15:04:12.293711 118847 net.cpp:305] relu1 needs backward computation.
I1226 15:04:12.293745 118847 net.cpp:305] conv1 needs backward computation.
I1226 15:04:12.293800 118847 net.cpp:307] label_data_1_split does not need backward computation.
I1226 15:04:12.293838 118847 net.cpp:307] data does not need backward computation.
I1226 15:04:12.293867 118847 net.cpp:349] This network produces output accuracy
I1226 15:04:12.293903 118847 net.cpp:349] This network produces output loss
I1226 15:04:12.294003 118847 net.cpp:363] Network initialization done.
I1226 15:04:12.294432 118847 solver.cpp:107] Solver scaffolding done.
I1226 15:04:12.294672 118847 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 15:04:15.117452 92056 caffe.cpp:376] Configuring multinode setup
I1226 15:04:15.118978 92056 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 15:04:15.470074 92253 caffe.cpp:376] Configuring multinode setup
I1226 15:04:15.471611 92253 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 15:04:15.515630 88631 caffe.cpp:376] Configuring multinode setup
I1226 15:04:15.517153 88631 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 15:04:15.527390 93129 caffe.cpp:376] Configuring multinode setup
I1226 15:04:15.528921 93129 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 15:04:15.546131 118847 caffe.cpp:376] Configuring multinode setup
I1226 15:04:15.547588 118847 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 15:04:15.736526 52875 caffe.cpp:376] Configuring multinode setup
I1226 15:04:15.738780 52875 caffe.cpp:386] Starting parameter server in mpi environment
I1226 15:04:16.248780 92722 caffe.cpp:376] Configuring multinode setup
I1226 15:04:16.250201 92722 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 15:04:35.922868 95917 net.cpp:228] Setting up fc6
I1226 15:04:35.923136 95917 net.cpp:235] Top shape: 32 4096 (131072)
I1226 15:04:35.923194 95917 net.cpp:243] Memory required for data: 263466752
I1226 15:04:35.923285 95917 layer_factory.hpp:114] Creating layer relu6
I1226 15:04:35.923544 95917 net.cpp:178] Creating Layer relu6
I1226 15:04:35.923612 95917 net.cpp:612] relu6 <- fc6
I1226 15:04:35.923669 95917 net.cpp:573] relu6 -> fc6 (in-place)
I1226 15:04:35.923797 95917 net.cpp:228] Setting up relu6
I1226 15:04:35.923902 95917 net.cpp:235] Top shape: 32 4096 (131072)
I1226 15:04:35.923938 95917 net.cpp:243] Memory required for data: 263991040
I1226 15:04:35.923979 95917 layer_factory.hpp:114] Creating layer drop6
I1226 15:04:35.924047 95917 net.cpp:178] Creating Layer drop6
I1226 15:04:35.924094 95917 net.cpp:612] drop6 <- fc6
I1226 15:04:35.924144 95917 net.cpp:573] drop6 -> fc6 (in-place)
I1226 15:04:35.924221 95917 net.cpp:228] Setting up drop6
I1226 15:04:35.924278 95917 net.cpp:235] Top shape: 32 4096 (131072)
I1226 15:04:35.924311 95917 net.cpp:243] Memory required for data: 264515328
I1226 15:04:35.924350 95917 layer_factory.hpp:114] Creating layer fc7
I1226 15:04:35.924445 95917 net.cpp:178] Creating Layer fc7
I1226 15:04:35.924489 95917 net.cpp:612] fc7 <- fc6
I1226 15:04:35.924538 95917 net.cpp:586] fc7 -> fc7
I1226 15:04:36.102414 98074 net.cpp:228] Setting up fc6
I1226 15:04:36.102679 98074 net.cpp:235] Top shape: 32 4096 (131072)
I1226 15:04:36.102756 98074 net.cpp:243] Memory required for data: 263466752
I1226 15:04:36.102847 98074 layer_factory.hpp:114] Creating layer relu6
I1226 15:04:36.102936 98074 net.cpp:178] Creating Layer relu6
I1226 15:04:36.102982 98074 net.cpp:612] relu6 <- fc6
I1226 15:04:36.103034 98074 net.cpp:573] relu6 -> fc6 (in-place)
I1226 15:04:36.103162 98074 net.cpp:228] Setting up relu6
I1226 15:04:36.103241 98074 net.cpp:235] Top shape: 32 4096 (131072)
I1226 15:04:36.103274 98074 net.cpp:243] Memory required for data: 263991040
I1226 15:04:36.103314 98074 layer_factory.hpp:114] Creating layer drop6
I1226 15:04:36.103377 98074 net.cpp:178] Creating Layer drop6
I1226 15:04:36.103412 98074 net.cpp:612] drop6 <- fc6
I1226 15:04:36.103456 98074 net.cpp:573] drop6 -> fc6 (in-place)
I1226 15:04:36.103518 98074 net.cpp:228] Setting up drop6
I1226 15:04:36.103559 98074 net.cpp:235] Top shape: 32 4096 (131072)
I1226 15:04:36.103585 98074 net.cpp:243] Memory required for data: 264515328
I1226 15:04:36.103621 98074 layer_factory.hpp:114] Creating layer fc7
I1226 15:04:36.103705 98074 net.cpp:178] Creating Layer fc7
I1226 15:04:36.103773 98074 net.cpp:612] fc7 <- fc6
I1226 15:04:36.103826 98074 net.cpp:586] fc7 -> fc7
I1226 15:04:49.003536 95917 net.cpp:228] Setting up fc7
I1226 15:04:49.003655 95917 net.cpp:235] Top shape: 32 4096 (131072)
I1226 15:04:49.003702 95917 net.cpp:243] Memory required for data: 265039616
I1226 15:04:49.003794 95917 layer_factory.hpp:114] Creating layer relu7
I1226 15:04:49.004077 95917 net.cpp:178] Creating Layer relu7
I1226 15:04:49.004166 95917 net.cpp:612] relu7 <- fc7
I1226 15:04:49.004221 95917 net.cpp:573] relu7 -> fc7 (in-place)
I1226 15:04:49.004343 95917 net.cpp:228] Setting up relu7
I1226 15:04:49.004446 95917 net.cpp:235] Top shape: 32 4096 (131072)
I1226 15:04:49.004487 95917 net.cpp:243] Memory required for data: 265563904
I1226 15:04:49.004560 95917 layer_factory.hpp:114] Creating layer drop7
I1226 15:04:49.004619 95917 net.cpp:178] Creating Layer drop7
I1226 15:04:49.004658 95917 net.cpp:612] drop7 <- fc7
I1226 15:04:49.004730 95917 net.cpp:573] drop7 -> fc7 (in-place)
I1226 15:04:49.004859 95917 net.cpp:228] Setting up drop7
I1226 15:04:49.004943 95917 net.cpp:235] Top shape: 32 4096 (131072)
I1226 15:04:49.004989 95917 net.cpp:243] Memory required for data: 266088192
I1226 15:04:49.005235 95917 layer_factory.hpp:114] Creating layer fc8
I1226 15:04:49.005378 95917 net.cpp:178] Creating Layer fc8
I1226 15:04:49.005517 95917 net.cpp:612] fc8 <- fc7
I1226 15:04:49.005633 95917 net.cpp:586] fc8 -> fc8
I1226 15:04:49.226126 98074 net.cpp:228] Setting up fc7
I1226 15:04:49.226243 98074 net.cpp:235] Top shape: 32 4096 (131072)
I1226 15:04:49.226280 98074 net.cpp:243] Memory required for data: 265039616
I1226 15:04:49.226351 98074 layer_factory.hpp:114] Creating layer relu7
I1226 15:04:49.226471 98074 net.cpp:178] Creating Layer relu7
I1226 15:04:49.226548 98074 net.cpp:612] relu7 <- fc7
I1226 15:04:49.226613 98074 net.cpp:573] relu7 -> fc7 (in-place)
I1226 15:04:49.226763 98074 net.cpp:228] Setting up relu7
I1226 15:04:49.226831 98074 net.cpp:235] Top shape: 32 4096 (131072)
I1226 15:04:49.226864 98074 net.cpp:243] Memory required for data: 265563904
I1226 15:04:49.226902 98074 layer_factory.hpp:114] Creating layer drop7
I1226 15:04:49.226953 98074 net.cpp:178] Creating Layer drop7
I1226 15:04:49.226987 98074 net.cpp:612] drop7 <- fc7
I1226 15:04:49.227051 98074 net.cpp:573] drop7 -> fc7 (in-place)
I1226 15:04:49.227113 98074 net.cpp:228] Setting up drop7
I1226 15:04:49.227152 98074 net.cpp:235] Top shape: 32 4096 (131072)
I1226 15:04:49.227179 98074 net.cpp:243] Memory required for data: 266088192
I1226 15:04:49.227210 98074 layer_factory.hpp:114] Creating layer fc8
I1226 15:04:49.227274 98074 net.cpp:178] Creating Layer fc8
I1226 15:04:49.227308 98074 net.cpp:612] fc8 <- fc7
I1226 15:04:49.227354 98074 net.cpp:586] fc8 -> fc8
I1226 15:04:52.198523 95917 net.cpp:228] Setting up fc8
I1226 15:04:52.198653 95917 net.cpp:235] Top shape: 32 1000 (32000)
I1226 15:04:52.198701 95917 net.cpp:243] Memory required for data: 266216192
I1226 15:04:52.198787 95917 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 15:04:52.199050 95917 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 15:04:52.199111 95917 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 15:04:52.199177 95917 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 15:04:52.199271 95917 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 15:04:52.199414 95917 net.cpp:228] Setting up fc8_fc8_0_split
I1226 15:04:52.199493 95917 net.cpp:235] Top shape: 32 1000 (32000)
I1226 15:04:52.199544 95917 net.cpp:235] Top shape: 32 1000 (32000)
I1226 15:04:52.199576 95917 net.cpp:243] Memory required for data: 266472192
I1226 15:04:52.199620 95917 layer_factory.hpp:114] Creating layer accuracy
I1226 15:04:52.199692 95917 net.cpp:178] Creating Layer accuracy
I1226 15:04:52.199733 95917 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 15:04:52.199775 95917 net.cpp:612] accuracy <- label_data_1_split_0
I1226 15:04:52.199849 95917 net.cpp:586] accuracy -> accuracy
I1226 15:04:52.199951 95917 net.cpp:228] Setting up accuracy
I1226 15:04:52.200012 95917 net.cpp:235] Top shape: (1)
I1226 15:04:52.200045 95917 net.cpp:243] Memory required for data: 266472196
I1226 15:04:52.200084 95917 layer_factory.hpp:114] Creating layer loss
I1226 15:04:52.200261 95917 net.cpp:178] Creating Layer loss
I1226 15:04:52.200325 95917 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 15:04:52.200381 95917 net.cpp:612] loss <- label_data_1_split_1
I1226 15:04:52.200459 95917 net.cpp:586] loss -> loss
I1226 15:04:52.200556 95917 layer_factory.hpp:114] Creating layer loss
I1226 15:04:52.236367 95917 net.cpp:228] Setting up loss
I1226 15:04:52.236479 95917 net.cpp:235] Top shape: (1)
I1226 15:04:52.236516 95917 net.cpp:238]     with loss weight 1
I1226 15:04:52.236670 95917 net.cpp:243] Memory required for data: 266472200
I1226 15:04:52.236721 95917 net.cpp:305] loss needs backward computation.
I1226 15:04:52.236763 95917 net.cpp:307] accuracy does not need backward computation.
I1226 15:04:52.236798 95917 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 15:04:52.236852 95917 net.cpp:305] fc8 needs backward computation.
I1226 15:04:52.236899 95917 net.cpp:305] drop7 needs backward computation.
I1226 15:04:52.236932 95917 net.cpp:305] relu7 needs backward computation.
I1226 15:04:52.236960 95917 net.cpp:305] fc7 needs backward computation.
I1226 15:04:52.236992 95917 net.cpp:305] drop6 needs backward computation.
I1226 15:04:52.237033 95917 net.cpp:305] relu6 needs backward computation.
I1226 15:04:52.237063 95917 net.cpp:305] fc6 needs backward computation.
I1226 15:04:52.237259 95917 net.cpp:305] pool5 needs backward computation.
I1226 15:04:52.237293 95917 net.cpp:305] relu5 needs backward computation.
I1226 15:04:52.237324 95917 net.cpp:305] conv5 needs backward computation.
I1226 15:04:52.237355 95917 net.cpp:305] relu4 needs backward computation.
I1226 15:04:52.237386 95917 net.cpp:305] conv4 needs backward computation.
I1226 15:04:52.237427 95917 net.cpp:305] relu3 needs backward computation.
I1226 15:04:52.237458 95917 net.cpp:305] conv3 needs backward computation.
I1226 15:04:52.237491 95917 net.cpp:305] pool2 needs backward computation.
I1226 15:04:52.237524 95917 net.cpp:305] norm2 needs backward computation.
I1226 15:04:52.237562 95917 net.cpp:305] relu2 needs backward computation.
I1226 15:04:52.237593 95917 net.cpp:305] conv2 needs backward computation.
I1226 15:04:52.237633 95917 net.cpp:305] pool1 needs backward computation.
I1226 15:04:52.237664 95917 net.cpp:305] norm1 needs backward computation.
I1226 15:04:52.237697 95917 net.cpp:305] relu1 needs backward computation.
I1226 15:04:52.237727 95917 net.cpp:305] conv1 needs backward computation.
I1226 15:04:52.237767 95917 net.cpp:307] label_data_1_split does not need backward computation.
I1226 15:04:52.237810 95917 net.cpp:307] data does not need backward computation.
I1226 15:04:52.237870 95917 net.cpp:349] This network produces output accuracy
I1226 15:04:52.237907 95917 net.cpp:349] This network produces output loss
I1226 15:04:52.238018 95917 net.cpp:363] Network initialization done.
I1226 15:04:52.238450 95917 solver.cpp:107] Solver scaffolding done.
I1226 15:04:52.238672 95917 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 15:04:52.432273 98074 net.cpp:228] Setting up fc8
I1226 15:04:52.432438 98074 net.cpp:235] Top shape: 32 1000 (32000)
I1226 15:04:52.432548 98074 net.cpp:243] Memory required for data: 266216192
I1226 15:04:52.432639 98074 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 15:04:52.432852 98074 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 15:04:52.432921 98074 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 15:04:52.432981 98074 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 15:04:52.433082 98074 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 15:04:52.433233 98074 net.cpp:228] Setting up fc8_fc8_0_split
I1226 15:04:52.433336 98074 net.cpp:235] Top shape: 32 1000 (32000)
I1226 15:04:52.433377 98074 net.cpp:235] Top shape: 32 1000 (32000)
I1226 15:04:52.433400 98074 net.cpp:243] Memory required for data: 266472192
I1226 15:04:52.433436 98074 layer_factory.hpp:114] Creating layer accuracy
I1226 15:04:52.433526 98074 net.cpp:178] Creating Layer accuracy
I1226 15:04:52.433567 98074 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 15:04:52.433635 98074 net.cpp:612] accuracy <- label_data_1_split_0
I1226 15:04:52.433709 98074 net.cpp:586] accuracy -> accuracy
I1226 15:04:52.433811 98074 net.cpp:228] Setting up accuracy
I1226 15:04:52.433857 98074 net.cpp:235] Top shape: (1)
I1226 15:04:52.433884 98074 net.cpp:243] Memory required for data: 266472196
I1226 15:04:52.433920 98074 layer_factory.hpp:114] Creating layer loss
I1226 15:04:52.434085 98074 net.cpp:178] Creating Layer loss
I1226 15:04:52.434128 98074 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 15:04:52.434162 98074 net.cpp:612] loss <- label_data_1_split_1
I1226 15:04:52.434222 98074 net.cpp:586] loss -> loss
I1226 15:04:52.434298 98074 layer_factory.hpp:114] Creating layer loss
I1226 15:04:52.464345 98074 net.cpp:228] Setting up loss
I1226 15:04:52.464459 98074 net.cpp:235] Top shape: (1)
I1226 15:04:52.464498 98074 net.cpp:238]     with loss weight 1
I1226 15:04:52.464642 98074 net.cpp:243] Memory required for data: 266472200
I1226 15:04:52.464699 98074 net.cpp:305] loss needs backward computation.
I1226 15:04:52.464773 98074 net.cpp:307] accuracy does not need backward computation.
I1226 15:04:52.464817 98074 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 15:04:52.464848 98074 net.cpp:305] fc8 needs backward computation.
I1226 15:04:52.464877 98074 net.cpp:305] drop7 needs backward computation.
I1226 15:04:52.464905 98074 net.cpp:305] relu7 needs backward computation.
I1226 15:04:52.464932 98074 net.cpp:305] fc7 needs backward computation.
I1226 15:04:52.464962 98074 net.cpp:305] drop6 needs backward computation.
I1226 15:04:52.464989 98074 net.cpp:305] relu6 needs backward computation.
I1226 15:04:52.465016 98074 net.cpp:305] fc6 needs backward computation.
I1226 15:04:52.465046 98074 net.cpp:305] pool5 needs backward computation.
I1226 15:04:52.465076 98074 net.cpp:305] relu5 needs backward computation.
I1226 15:04:52.465104 98074 net.cpp:305] conv5 needs backward computation.
I1226 15:04:52.465132 98074 net.cpp:305] relu4 needs backward computation.
I1226 15:04:52.465160 98074 net.cpp:305] conv4 needs backward computation.
I1226 15:04:52.465189 98074 net.cpp:305] relu3 needs backward computation.
I1226 15:04:52.465216 98074 net.cpp:305] conv3 needs backward computation.
I1226 15:04:52.465245 98074 net.cpp:305] pool2 needs backward computation.
I1226 15:04:52.465275 98074 net.cpp:305] norm2 needs backward computation.
I1226 15:04:52.465303 98074 net.cpp:305] relu2 needs backward computation.
I1226 15:04:52.465330 98074 net.cpp:305] conv2 needs backward computation.
I1226 15:04:52.465359 98074 net.cpp:305] pool1 needs backward computation.
I1226 15:04:52.465389 98074 net.cpp:305] norm1 needs backward computation.
I1226 15:04:52.465416 98074 net.cpp:305] relu1 needs backward computation.
I1226 15:04:52.465445 98074 net.cpp:305] conv1 needs backward computation.
I1226 15:04:52.465476 98074 net.cpp:307] label_data_1_split does not need backward computation.
I1226 15:04:52.465505 98074 net.cpp:307] data does not need backward computation.
I1226 15:04:52.465531 98074 net.cpp:349] This network produces output accuracy
I1226 15:04:52.465564 98074 net.cpp:349] This network produces output loss
I1226 15:04:52.465661 98074 net.cpp:363] Network initialization done.
I1226 15:04:52.466290 98074 solver.cpp:107] Solver scaffolding done.
I1226 15:04:52.466548 98074 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 15:04:56.791606 95917 caffe.cpp:376] Configuring multinode setup
I1226 15:04:56.793596 95917 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 15:04:57.063983 98074 caffe.cpp:376] Configuring multinode setup
I1226 15:04:57.065853 98074 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 15:04:57.066076 98074 SynchronousNode.cpp:675] [0] [proc 0] solving
I1226 15:04:57.066174 98074 solver.cpp:354] Solving AlexNet
I1226 15:04:57.066220 98074 solver.cpp:355] Learning Rate Policy: step
I1226 15:04:57.074604 95917 SynchronousNode.cpp:675] [2] [proc 2] solving
I1226 15:04:57.074736 95917 solver.cpp:354] Solving AlexNet
I1226 15:04:57.074782 95917 solver.cpp:355] Learning Rate Policy: step
I1226 15:04:57.088800 92056 SynchronousNode.cpp:675] [7] [proc 7] solving
I1226 15:04:57.084298 88631 SynchronousNode.cpp:675] [3] [proc 3] solving
I1226 15:04:57.081723 93129 SynchronousNode.cpp:675] [1] [proc 1] solving
I1226 15:04:57.075335 92253 SynchronousNode.cpp:675] [6] [proc 6] solving
I1226 15:04:57.083961 118847 SynchronousNode.cpp:675] [4] [proc 4] solving
I1226 15:04:57.081502 92722 SynchronousNode.cpp:675] [5] [proc 5] solving
I1226 15:04:57.089087 92056 solver.cpp:354] Solving AlexNet
I1226 15:04:57.084571 88631 solver.cpp:354] Solving AlexNet
I1226 15:04:57.084620 88631 solver.cpp:355] Learning Rate Policy: step
I1226 15:04:57.082022 93129 solver.cpp:354] Solving AlexNet
I1226 15:04:57.075609 92253 solver.cpp:354] Solving AlexNet
I1226 15:04:57.075659 92253 solver.cpp:355] Learning Rate Policy: step
I1226 15:04:57.084280 118847 solver.cpp:354] Solving AlexNet
I1226 15:04:57.081782 92722 solver.cpp:354] Solving AlexNet
I1226 15:04:57.081827 92722 solver.cpp:355] Learning Rate Policy: step
I1226 15:04:57.089138 92056 solver.cpp:355] Learning Rate Policy: step
I1226 15:04:57.082105 93129 solver.cpp:355] Learning Rate Policy: step
I1226 15:04:57.084327 118847 solver.cpp:355] Learning Rate Policy: step
I1226 15:04:57.099298 92129 SynchronousNode.cpp:293] [7] Comm thread started 1 0
I1226 15:04:57.093238 93203 SynchronousNode.cpp:293] [1] Comm thread started 0 0
I1226 15:04:57.096814 118919 SynchronousNode.cpp:293] [4] Comm thread started 1 0
I1226 15:04:57.083971 98147 SynchronousNode.cpp:293] [0] Comm thread started 0 1
I1226 15:04:57.092548 95987 SynchronousNode.cpp:293] [2] Comm thread started 0 0
I1226 15:04:57.105532 88701 SynchronousNode.cpp:293] [3] Comm thread started 0 0
I1226 15:04:57.105146 92795 SynchronousNode.cpp:293] [5] Comm thread started 1 0
I1226 15:04:57.110414 92323 SynchronousNode.cpp:293] [6] Comm thread started 1 0
I1226 15:04:57.138237 98147 SynchronousNode.cpp:479] [0] initialized root of cluster with nodes: 9 and the total iter size is: 8
I1226 15:04:57.892277 98074 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 15:04:57.892369 98074 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 15:04:59.267509 98074 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 15:04:59.267606 98074 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 15:04:59.267652 98074 solver.cpp:291] [0] Iteration 1, loss = 2.56255
I1226 15:04:59.267750 98074 solver.cpp:317]     Train net output #0: accuracy = 0.40625
I1226 15:04:59.267931 98074 solver.cpp:317]     Train net output #1: loss = 2.56255 (* 1 = 2.56255 loss)
I1226 15:05:00.573794 95987 SynchronousNode.cpp:246] [2] PROFILING END[WaitingToSync]
I1226 15:05:00.587949 92129 SynchronousNode.cpp:246] [7] PROFILING END[WaitingToSync]
I1226 15:05:00.583488 88701 SynchronousNode.cpp:246] [3] PROFILING END[WaitingToSync]
I1226 15:05:00.580893 93203 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 15:05:00.583137 118919 SynchronousNode.cpp:246] [4] PROFILING END[WaitingToSync]
I1226 15:05:00.583217 118919 SynchronousNode.cpp:247] [4] PROFILING BEGIN[Forward]
I1226 15:05:00.573894 95987 SynchronousNode.cpp:247] [2] PROFILING BEGIN[Forward]
I1226 15:05:00.588029 92129 SynchronousNode.cpp:247] [7] PROFILING BEGIN[Forward]
I1226 15:05:00.583582 88701 SynchronousNode.cpp:247] [3] PROFILING BEGIN[Forward]
I1226 15:05:00.580983 93203 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 15:05:00.585777 92795 SynchronousNode.cpp:246] [5] PROFILING END[WaitingToSync]
I1226 15:05:00.585856 92795 SynchronousNode.cpp:247] [5] PROFILING BEGIN[Forward]
I1226 15:05:00.579673 92323 SynchronousNode.cpp:246] [6] PROFILING END[WaitingToSync]
I1226 15:05:00.579761 92323 SynchronousNode.cpp:247] [6] PROFILING BEGIN[Forward]
I1226 15:05:00.673187 92056 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 15:05:00.673270 92056 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 15:05:00.666414 92722 MultiSolver.cpp:92] [5] PROFILING END[Forward]
I1226 15:05:00.666498 92722 MultiSolver.cpp:94] [5] PROFILING BEGIN[Backward]
I1226 15:05:00.661252 92253 MultiSolver.cpp:92] [6] PROFILING END[Forward]
I1226 15:05:00.661365 92253 MultiSolver.cpp:94] [6] PROFILING BEGIN[Backward]
I1226 15:05:00.668947 93129 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 15:05:00.669057 93129 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 15:05:00.671814 118847 MultiSolver.cpp:92] [4] PROFILING END[Forward]
I1226 15:05:00.671923 118847 MultiSolver.cpp:94] [4] PROFILING BEGIN[Backward]
I1226 15:05:00.738008 88631 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 15:05:00.738174 88631 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 15:05:00.748042 93129 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 15:05:00.748127 93129 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 15:05:00.748172 93129 solver.cpp:291] [1] Iteration 1, loss = 2.91393
I1226 15:05:00.748239 93129 solver.cpp:317]     Train net output #0: accuracy = 0.28125
I1226 15:05:00.748306 93129 solver.cpp:317]     Train net output #1: loss = 2.91393 (* 1 = 2.91393 loss)
I1226 15:05:00.826323 92056 MultiSolver.cpp:108] [7] PROFILING END[Backward]
I1226 15:05:00.826409 92056 MultiSolver.cpp:109] [7] PROFILING BEGIN[WaitingToSync]
I1226 15:05:00.826462 92056 solver.cpp:291] [7] Iteration 1, loss = 3.41493
I1226 15:05:00.826539 92056 solver.cpp:317]     Train net output #0: accuracy = 0.34375
I1226 15:05:00.826627 92056 solver.cpp:317]     Train net output #1: loss = 3.41493 (* 1 = 3.41493 loss)
I1226 15:05:00.834707 118847 MultiSolver.cpp:108] [4] PROFILING END[Backward]
I1226 15:05:00.834800 118847 MultiSolver.cpp:109] [4] PROFILING BEGIN[WaitingToSync]
I1226 15:05:00.834856 118847 solver.cpp:291] [4] Iteration 1, loss = 3.01305
I1226 15:05:00.834928 118847 solver.cpp:317]     Train net output #0: accuracy = 0.28125
I1226 15:05:00.835005 118847 solver.cpp:317]     Train net output #1: loss = 3.01305 (* 1 = 3.01305 loss)
I1226 15:05:00.846173 92722 MultiSolver.cpp:108] [5] PROFILING END[Backward]
I1226 15:05:00.846259 92722 MultiSolver.cpp:109] [5] PROFILING BEGIN[WaitingToSync]
I1226 15:05:00.846349 92722 solver.cpp:291] [5] Iteration 1, loss = 3.48327
I1226 15:05:00.846423 92722 solver.cpp:317]     Train net output #0: accuracy = 0.25
I1226 15:05:00.846496 92722 solver.cpp:317]     Train net output #1: loss = 3.48327 (* 1 = 3.48327 loss)
I1226 15:05:00.851793 92253 MultiSolver.cpp:108] [6] PROFILING END[Backward]
I1226 15:05:00.851881 92253 MultiSolver.cpp:109] [6] PROFILING BEGIN[WaitingToSync]
I1226 15:05:00.851934 92253 solver.cpp:291] [6] Iteration 1, loss = 3.10256
I1226 15:05:00.852010 92253 solver.cpp:317]     Train net output #0: accuracy = 0.40625
I1226 15:05:00.852085 92253 solver.cpp:317]     Train net output #1: loss = 3.10256 (* 1 = 3.10256 loss)
I1226 15:05:00.930346 88631 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 15:05:00.930434 88631 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 15:05:00.930487 88631 solver.cpp:291] [3] Iteration 1, loss = 2.72247
I1226 15:05:00.930577 88631 solver.cpp:317]     Train net output #0: accuracy = 0.34375
I1226 15:05:00.921031 95917 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 15:05:00.930654 88631 solver.cpp:317]     Train net output #1: loss = 2.72247 (* 1 = 2.72247 loss)
I1226 15:05:00.921149 95917 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 15:05:01.464684 95917 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 15:05:01.464779 95917 MultiSolver.cpp:109] [2] PROFILING BEGIN[WaitingToSync]
I1226 15:05:01.464862 95917 solver.cpp:291] [2] Iteration 1, loss = 2.57481
I1226 15:05:01.465035 95917 solver.cpp:317]     Train net output #0: accuracy = 0.5
I1226 15:05:01.465116 95917 solver.cpp:317]     Train net output #1: loss = 2.57481 (* 1 = 2.57481 loss)
I1226 15:05:07.348294 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:07.349930 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:07.350023 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:07.414086 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:07.414206 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:07.508494 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:07.508576 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:07.522066 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:08.093960 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:08.095021 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:08.095109 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:08.095273 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:08.119657 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:08.119763 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:08.120764 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:08.125073 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:08.212599 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:08.212797 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:08.212851 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:08.219727 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:08.270303 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:08.270503 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:08.270790 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:08.277794 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:08.289319 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:08.289531 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:08.289636 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:08.358806 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:08.358898 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:08.359215 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:08.360828 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:08.569433 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:08.676276 98074 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 15:05:08.677891 98074 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 15:05:09.173574 98074 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 15:05:09.173658 98074 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 15:05:09.173701 98074 solver.cpp:291] [0] Iteration 2, loss = 3.91184
I1226 15:05:09.173806 98074 solver.cpp:317]     Train net output #0: accuracy = 0.15625
I1226 15:05:09.173892 98074 solver.cpp:317]     Train net output #1: loss = 3.91184 (* 1 = 3.91184 loss)
I1226 15:05:10.091771 92129 SynchronousNode.cpp:246] [7] PROFILING END[WaitingToSync]
I1226 15:05:10.091869 92129 SynchronousNode.cpp:247] [7] PROFILING BEGIN[Forward]
I1226 15:05:10.087335 88701 SynchronousNode.cpp:246] [3] PROFILING END[WaitingToSync]
I1226 15:05:10.084730 93203 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 15:05:10.086987 118919 SynchronousNode.cpp:246] [4] PROFILING END[WaitingToSync]
I1226 15:05:10.087072 118919 SynchronousNode.cpp:247] [4] PROFILING BEGIN[Forward]
I1226 15:05:10.087416 88701 SynchronousNode.cpp:247] [3] PROFILING BEGIN[Forward]
I1226 15:05:10.085019 93203 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 15:05:10.077749 95987 SynchronousNode.cpp:246] [2] PROFILING END[WaitingToSync]
I1226 15:05:10.078881 95987 SynchronousNode.cpp:247] [2] PROFILING BEGIN[Forward]
I1226 15:05:10.090121 88631 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 15:05:10.087510 93129 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 15:05:10.090226 88631 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 15:05:10.087616 93129 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 15:05:10.089844 118847 MultiSolver.cpp:92] [4] PROFILING END[Forward]
I1226 15:05:10.094749 92056 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 15:05:10.089969 118847 MultiSolver.cpp:94] [4] PROFILING BEGIN[Backward]
I1226 15:05:10.094892 92056 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 15:05:10.086331 95917 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 15:05:10.086457 95917 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 15:05:10.096002 92323 SynchronousNode.cpp:246] [6] PROFILING END[WaitingToSync]
I1226 15:05:10.102192 92795 SynchronousNode.cpp:246] [5] PROFILING END[WaitingToSync]
I1226 15:05:10.096088 92323 SynchronousNode.cpp:247] [6] PROFILING BEGIN[Forward]
I1226 15:05:10.102279 92795 SynchronousNode.cpp:247] [5] PROFILING BEGIN[Forward]
I1226 15:05:10.104377 92253 MultiSolver.cpp:92] [6] PROFILING END[Forward]
I1226 15:05:10.104491 92253 MultiSolver.cpp:94] [6] PROFILING BEGIN[Backward]
I1226 15:05:10.112229 92722 MultiSolver.cpp:92] [5] PROFILING END[Forward]
I1226 15:05:10.112378 92722 MultiSolver.cpp:94] [5] PROFILING BEGIN[Backward]
I1226 15:05:10.139597 88631 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 15:05:10.139680 88631 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 15:05:10.139726 88631 solver.cpp:291] [3] Iteration 2, loss = 3.60068
I1226 15:05:10.139792 88631 solver.cpp:317]     Train net output #0: accuracy = 0.25
I1226 15:05:10.139858 88631 solver.cpp:317]     Train net output #1: loss = 3.60068 (* 1 = 3.60068 loss)
I1226 15:05:10.142078 93129 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 15:05:10.142189 93129 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 15:05:10.142236 93129 solver.cpp:291] [1] Iteration 2, loss = 2.60615
I1226 15:05:10.142307 93129 solver.cpp:317]     Train net output #0: accuracy = 0.375
I1226 15:05:10.142377 93129 solver.cpp:317]     Train net output #1: loss = 2.60615 (* 1 = 2.60615 loss)
I1226 15:05:10.197945 92056 MultiSolver.cpp:108] [7] PROFILING END[Backward]
I1226 15:05:10.198036 92056 MultiSolver.cpp:109] [7] PROFILING BEGIN[WaitingToSync]
I1226 15:05:10.198087 92056 solver.cpp:291] [7] Iteration 2, loss = 3.01567
I1226 15:05:10.198161 92056 solver.cpp:317]     Train net output #0: accuracy = 0.40625
I1226 15:05:10.198236 92056 solver.cpp:317]     Train net output #1: loss = 3.01567 (* 1 = 3.01567 loss)
I1226 15:05:10.198081 118847 MultiSolver.cpp:108] [4] PROFILING END[Backward]
I1226 15:05:10.198168 118847 MultiSolver.cpp:109] [4] PROFILING BEGIN[WaitingToSync]
I1226 15:05:10.198218 118847 solver.cpp:291] [4] Iteration 2, loss = 3.16113
I1226 15:05:10.198292 118847 solver.cpp:317]     Train net output #0: accuracy = 0.21875
I1226 15:05:10.198366 118847 solver.cpp:317]     Train net output #1: loss = 3.16113 (* 1 = 3.16113 loss)
I1226 15:05:10.236239 92722 MultiSolver.cpp:108] [5] PROFILING END[Backward]
I1226 15:05:10.236361 92722 MultiSolver.cpp:109] [5] PROFILING BEGIN[WaitingToSync]
I1226 15:05:10.236413 92722 solver.cpp:291] [5] Iteration 2, loss = 3.17805
I1226 15:05:10.236488 92722 solver.cpp:317]     Train net output #0: accuracy = 0.25
I1226 15:05:10.236577 92722 solver.cpp:317]     Train net output #1: loss = 3.17805 (* 1 = 3.17805 loss)
I1226 15:05:10.230836 92253 MultiSolver.cpp:108] [6] PROFILING END[Backward]
I1226 15:05:10.230931 92253 MultiSolver.cpp:109] [6] PROFILING BEGIN[WaitingToSync]
I1226 15:05:10.230979 92253 solver.cpp:291] [6] Iteration 2, loss = 3.88728
I1226 15:05:10.231052 92253 solver.cpp:317]     Train net output #0: accuracy = 0.15625
I1226 15:05:10.231127 92253 solver.cpp:317]     Train net output #1: loss = 3.88728 (* 1 = 3.88728 loss)
I1226 15:05:10.572501 95917 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 15:05:10.572599 95917 MultiSolver.cpp:109] [2] PROFILING BEGIN[WaitingToSync]
I1226 15:05:10.572644 95917 solver.cpp:291] [2] Iteration 2, loss = 3.40387
I1226 15:05:10.572711 95917 solver.cpp:317]     Train net output #0: accuracy = 0.21875
I1226 15:05:10.572777 95917 solver.cpp:317]     Train net output #1: loss = 3.40387 (* 1 = 3.40387 loss)
I1226 15:05:16.750322 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:16.750608 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:16.750708 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:16.763681 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:16.774190 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:16.774335 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:16.776939 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:16.822819 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:17.540735 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:17.541517 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:17.541594 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:17.541726 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:17.546692 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:17.546783 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:17.566576 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:17.682878 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:17.682966 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:17.711338 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:17.711462 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:17.711534 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:17.711856 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:17.713488 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:17.713574 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:17.715473 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:17.715564 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:17.715695 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:17.715741 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:17.718420 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:17.718510 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:17.718639 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:17.718696 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:17.722375 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:17.952482 98074 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 15:05:17.952636 98074 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 15:05:18.443472 98074 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 15:05:18.443562 98074 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 15:05:18.443606 98074 solver.cpp:291] [0] Iteration 3, loss = 3.37
I1226 15:05:18.443672 98074 solver.cpp:317]     Train net output #0: accuracy = 0.34375
I1226 15:05:18.443785 98074 solver.cpp:317]     Train net output #1: loss = 3.37 (* 1 = 3.37 loss)
I1226 15:05:19.448014 88701 SynchronousNode.cpp:246] [3] PROFILING END[WaitingToSync]
I1226 15:05:19.445381 93203 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 15:05:19.445462 93203 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 15:05:19.447656 118919 SynchronousNode.cpp:246] [4] PROFILING END[WaitingToSync]
I1226 15:05:19.438410 95987 SynchronousNode.cpp:246] [2] PROFILING END[WaitingToSync]
I1226 15:05:19.452406 92129 SynchronousNode.cpp:246] [7] PROFILING END[WaitingToSync]
I1226 15:05:19.448096 88701 SynchronousNode.cpp:247] [3] PROFILING BEGIN[Forward]
I1226 15:05:19.447729 118919 SynchronousNode.cpp:247] [4] PROFILING BEGIN[Forward]
I1226 15:05:19.438484 95987 SynchronousNode.cpp:247] [2] PROFILING BEGIN[Forward]
I1226 15:05:19.452486 92129 SynchronousNode.cpp:247] [7] PROFILING BEGIN[Forward]
I1226 15:05:19.446646 92795 SynchronousNode.cpp:246] [5] PROFILING END[WaitingToSync]
I1226 15:05:19.440445 92323 SynchronousNode.cpp:246] [6] PROFILING END[WaitingToSync]
I1226 15:05:19.446728 92795 SynchronousNode.cpp:247] [5] PROFILING BEGIN[Forward]
I1226 15:05:19.440534 92323 SynchronousNode.cpp:247] [6] PROFILING BEGIN[Forward]
I1226 15:05:19.450738 88631 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 15:05:19.450379 118847 MultiSolver.cpp:92] [4] PROFILING END[Forward]
I1226 15:05:19.455240 92056 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 15:05:19.448216 93129 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 15:05:19.455350 92056 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 15:05:19.448328 93129 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 15:05:19.450503 118847 MultiSolver.cpp:94] [4] PROFILING BEGIN[Backward]
I1226 15:05:19.450844 88631 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 15:05:19.446748 95917 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 15:05:19.446897 95917 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 15:05:19.448842 92253 MultiSolver.cpp:92] [6] PROFILING END[Forward]
I1226 15:05:19.455032 92722 MultiSolver.cpp:92] [5] PROFILING END[Forward]
I1226 15:05:19.448953 92253 MultiSolver.cpp:94] [6] PROFILING BEGIN[Backward]
I1226 15:05:19.455148 92722 MultiSolver.cpp:94] [5] PROFILING BEGIN[Backward]
I1226 15:05:19.501271 93129 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 15:05:19.501394 93129 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 15:05:19.501668 93129 solver.cpp:291] [1] Iteration 3, loss = 3.25318
I1226 15:05:19.501758 93129 solver.cpp:317]     Train net output #0: accuracy = 0.375
I1226 15:05:19.504595 88631 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 15:05:19.501881 93129 solver.cpp:317]     Train net output #1: loss = 3.25318 (* 1 = 3.25318 loss)
I1226 15:05:19.504681 88631 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 15:05:19.504726 88631 solver.cpp:291] [3] Iteration 3, loss = 2.52708
I1226 15:05:19.504792 88631 solver.cpp:317]     Train net output #0: accuracy = 0.46875
I1226 15:05:19.504858 88631 solver.cpp:317]     Train net output #1: loss = 2.52708 (* 1 = 2.52708 loss)
I1226 15:05:19.560092 92056 MultiSolver.cpp:108] [7] PROFILING END[Backward]
I1226 15:05:19.560179 92056 MultiSolver.cpp:109] [7] PROFILING BEGIN[WaitingToSync]
I1226 15:05:19.560230 92056 solver.cpp:291] [7] Iteration 3, loss = 3.59715
I1226 15:05:19.560305 92056 solver.cpp:317]     Train net output #0: accuracy = 0.15625
I1226 15:05:19.560380 92056 solver.cpp:317]     Train net output #1: loss = 3.59715 (* 1 = 3.59715 loss)
I1226 15:05:19.561187 118847 MultiSolver.cpp:108] [4] PROFILING END[Backward]
I1226 15:05:19.561276 118847 MultiSolver.cpp:109] [4] PROFILING BEGIN[WaitingToSync]
I1226 15:05:19.561332 118847 solver.cpp:291] [4] Iteration 3, loss = 3.4736
I1226 15:05:19.561406 118847 solver.cpp:317]     Train net output #0: accuracy = 0.21875
I1226 15:05:19.561482 118847 solver.cpp:317]     Train net output #1: loss = 3.4736 (* 1 = 3.4736 loss)
I1226 15:05:19.573103 92722 MultiSolver.cpp:108] [5] PROFILING END[Backward]
I1226 15:05:19.573191 92722 MultiSolver.cpp:109] [5] PROFILING BEGIN[WaitingToSync]
I1226 15:05:19.573243 92722 solver.cpp:291] [5] Iteration 3, loss = 3.48562
I1226 15:05:19.573348 92722 solver.cpp:317]     Train net output #0: accuracy = 0.21875
I1226 15:05:19.573426 92722 solver.cpp:317]     Train net output #1: loss = 3.48562 (* 1 = 3.48562 loss)
I1226 15:05:19.572846 92253 MultiSolver.cpp:108] [6] PROFILING END[Backward]
I1226 15:05:19.572933 92253 MultiSolver.cpp:109] [6] PROFILING BEGIN[WaitingToSync]
I1226 15:05:19.572983 92253 solver.cpp:291] [6] Iteration 3, loss = 2.69755
I1226 15:05:19.573055 92253 solver.cpp:317]     Train net output #0: accuracy = 0.46875
I1226 15:05:19.573132 92253 solver.cpp:317]     Train net output #1: loss = 2.69755 (* 1 = 2.69755 loss)
I1226 15:05:19.930435 95917 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 15:05:19.930534 95917 MultiSolver.cpp:109] [2] PROFILING BEGIN[WaitingToSync]
I1226 15:05:19.930583 95917 solver.cpp:291] [2] Iteration 3, loss = 3.51162
I1226 15:05:19.930656 95917 solver.cpp:317]     Train net output #0: accuracy = 0.3125
I1226 15:05:19.930733 95917 solver.cpp:317]     Train net output #1: loss = 3.51162 (* 1 = 3.51162 loss)
I1226 15:05:26.125501 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:26.125819 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:26.125881 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:26.141453 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:26.141541 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:26.141690 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:26.142324 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:26.201222 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:26.900202 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:26.900913 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:26.900988 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:26.901082 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:26.928979 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:26.929065 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:26.929463 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:26.953725 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:26.953820 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:26.953963 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:26.960980 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:27.061292 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:27.061547 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:27.079886 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:27.079932 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:27.089247 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:27.089329 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:27.089460 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:27.089507 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:27.092188 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:27.092273 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:27.092399 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:27.092447 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:27.095603 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:27.335422 98074 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 15:05:27.335535 98074 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 15:05:27.829947 98074 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 15:05:27.830029 98074 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 15:05:27.830068 98074 solver.cpp:291] [0] Iteration 4, loss = 4.26954
I1226 15:05:27.830137 98074 solver.cpp:317]     Train net output #0: accuracy = 0.1875
I1226 15:05:27.830221 98074 solver.cpp:317]     Train net output #1: loss = 4.26954 (* 1 = 4.26954 loss)
I1226 15:05:28.690852 95987 SynchronousNode.cpp:246] [2] PROFILING END[WaitingToSync]
I1226 15:05:28.690932 95987 SynchronousNode.cpp:247] [2] PROFILING BEGIN[Forward]
I1226 15:05:28.704813 92129 SynchronousNode.cpp:246] [7] PROFILING END[WaitingToSync]
I1226 15:05:28.700449 88701 SynchronousNode.cpp:246] [3] PROFILING END[WaitingToSync]
I1226 15:05:28.700088 118919 SynchronousNode.cpp:246] [4] PROFILING END[WaitingToSync]
I1226 15:05:28.701268 118919 SynchronousNode.cpp:247] [4] PROFILING BEGIN[Forward]
I1226 15:05:28.705931 92129 SynchronousNode.cpp:247] [7] PROFILING BEGIN[Forward]
I1226 15:05:28.701656 88701 SynchronousNode.cpp:247] [3] PROFILING BEGIN[Forward]
I1226 15:05:28.697845 93203 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 15:05:28.699666 93203 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 15:05:28.703037 118847 MultiSolver.cpp:92] [4] PROFILING END[Forward]
I1226 15:05:28.707808 92056 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 15:05:28.703474 88631 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 15:05:28.703156 118847 MultiSolver.cpp:94] [4] PROFILING BEGIN[Backward]
I1226 15:05:28.707926 92056 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 15:05:28.703593 88631 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 15:05:28.700974 93129 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 15:05:28.701086 93129 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 15:05:28.699946 92795 SynchronousNode.cpp:246] [5] PROFILING END[WaitingToSync]
I1226 15:05:28.693749 92323 SynchronousNode.cpp:246] [6] PROFILING END[WaitingToSync]
I1226 15:05:28.701145 92795 SynchronousNode.cpp:247] [5] PROFILING BEGIN[Forward]
I1226 15:05:28.694967 92323 SynchronousNode.cpp:247] [6] PROFILING BEGIN[Forward]
I1226 15:05:28.699297 95917 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 15:05:28.699429 95917 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 15:05:28.702502 92253 MultiSolver.cpp:92] [6] PROFILING END[Forward]
I1226 15:05:28.702613 92253 MultiSolver.cpp:94] [6] PROFILING BEGIN[Backward]
I1226 15:05:28.708920 92722 MultiSolver.cpp:92] [5] PROFILING END[Forward]
I1226 15:05:28.709035 92722 MultiSolver.cpp:94] [5] PROFILING BEGIN[Backward]
I1226 15:05:28.754387 88631 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 15:05:28.754468 88631 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 15:05:28.754513 88631 solver.cpp:291] [3] Iteration 4, loss = 2.74153
I1226 15:05:28.754578 88631 solver.cpp:317]     Train net output #0: accuracy = 0.34375
I1226 15:05:28.754642 88631 solver.cpp:317]     Train net output #1: loss = 2.74153 (* 1 = 2.74153 loss)
I1226 15:05:28.756286 93129 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 15:05:28.756372 93129 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 15:05:28.756418 93129 solver.cpp:291] [1] Iteration 4, loss = 3.86727
I1226 15:05:28.756484 93129 solver.cpp:317]     Train net output #0: accuracy = 0.1875
I1226 15:05:28.756551 93129 solver.cpp:317]     Train net output #1: loss = 3.86727 (* 1 = 3.86727 loss)
I1226 15:05:28.811872 92056 MultiSolver.cpp:108] [7] PROFILING END[Backward]
I1226 15:05:28.811961 92056 MultiSolver.cpp:109] [7] PROFILING BEGIN[WaitingToSync]
I1226 15:05:28.812012 92056 solver.cpp:291] [7] Iteration 4, loss = 3.03262
I1226 15:05:28.812088 92056 solver.cpp:317]     Train net output #0: accuracy = 0.4375
I1226 15:05:28.812161 92056 solver.cpp:317]     Train net output #1: loss = 3.03262 (* 1 = 3.03262 loss)
I1226 15:05:28.811370 118847 MultiSolver.cpp:108] [4] PROFILING END[Backward]
I1226 15:05:28.811460 118847 MultiSolver.cpp:109] [4] PROFILING BEGIN[WaitingToSync]
I1226 15:05:28.811513 118847 solver.cpp:291] [4] Iteration 4, loss = 3.94701
I1226 15:05:28.811632 118847 solver.cpp:317]     Train net output #0: accuracy = 0.09375
I1226 15:05:28.811709 118847 solver.cpp:317]     Train net output #1: loss = 3.94701 (* 1 = 3.94701 loss)
I1226 15:05:28.816632 92253 MultiSolver.cpp:108] [6] PROFILING END[Backward]
I1226 15:05:28.816726 92253 MultiSolver.cpp:109] [6] PROFILING BEGIN[WaitingToSync]
I1226 15:05:28.816776 92253 solver.cpp:291] [6] Iteration 4, loss = 3.03566
I1226 15:05:28.816848 92253 solver.cpp:317]     Train net output #0: accuracy = 0.3125
I1226 15:05:28.816925 92253 solver.cpp:317]     Train net output #1: loss = 3.03566 (* 1 = 3.03566 loss)
I1226 15:05:28.826392 92722 MultiSolver.cpp:108] [5] PROFILING END[Backward]
I1226 15:05:28.826481 92722 MultiSolver.cpp:109] [5] PROFILING BEGIN[WaitingToSync]
I1226 15:05:28.826532 92722 solver.cpp:291] [5] Iteration 4, loss = 3.00255
I1226 15:05:28.826608 92722 solver.cpp:317]     Train net output #0: accuracy = 0.34375
I1226 15:05:28.826684 92722 solver.cpp:317]     Train net output #1: loss = 3.00255 (* 1 = 3.00255 loss)
I1226 15:05:29.183043 95917 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 15:05:29.183130 95917 MultiSolver.cpp:109] [2] PROFILING BEGIN[WaitingToSync]
I1226 15:05:29.183177 95917 solver.cpp:291] [2] Iteration 4, loss = 3.4958
I1226 15:05:29.183254 95917 solver.cpp:317]     Train net output #0: accuracy = 0.375
I1226 15:05:29.183368 95917 solver.cpp:317]     Train net output #1: loss = 3.4958 (* 1 = 3.4958 loss)
I1226 15:05:35.365901 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:35.366101 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:35.366647 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:35.386593 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:35.386693 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:35.432854 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:35.445585 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:35.445763 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:36.142024 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:36.142724 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:36.142802 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:36.142899 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:36.206799 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:36.206955 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:36.207002 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:36.209173 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:36.209267 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:36.209422 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:36.209465 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:36.209547 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:36.213246 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:36.314162 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:36.332669 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:36.342670 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:36.342751 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:36.342896 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:36.342941 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:36.345599 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:36.345685 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:36.345847 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:36.345888 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:36.349234 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:36.582278 98074 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 15:05:36.582396 98074 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 15:05:37.073493 98074 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 15:05:37.073580 98074 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 15:05:37.073624 98074 solver.cpp:291] [0] Iteration 5, loss = 2.89635
I1226 15:05:37.073696 98074 solver.cpp:317]     Train net output #0: accuracy = 0.375
I1226 15:05:37.073808 98074 solver.cpp:317]     Train net output #1: loss = 2.89635 (* 1 = 2.89635 loss)
I1226 15:05:37.999673 88701 SynchronousNode.cpp:246] [3] PROFILING END[WaitingToSync]
I1226 15:05:37.997046 93203 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 15:05:37.999302 118919 SynchronousNode.cpp:246] [4] PROFILING END[WaitingToSync]
I1226 15:05:37.999382 118919 SynchronousNode.cpp:247] [4] PROFILING BEGIN[Forward]
I1226 15:05:37.990072 95987 SynchronousNode.cpp:246] [2] PROFILING END[WaitingToSync]
I1226 15:05:38.003983 92129 SynchronousNode.cpp:246] [7] PROFILING END[WaitingToSync]
I1226 15:05:38.004060 92129 SynchronousNode.cpp:247] [7] PROFILING BEGIN[Forward]
I1226 15:05:37.999758 88701 SynchronousNode.cpp:247] [3] PROFILING BEGIN[Forward]
I1226 15:05:37.997126 93203 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 15:05:37.990152 95987 SynchronousNode.cpp:247] [2] PROFILING BEGIN[Forward]
I1226 15:05:37.997524 92795 SynchronousNode.cpp:246] [5] PROFILING END[WaitingToSync]
I1226 15:05:37.991293 92323 SynchronousNode.cpp:246] [6] PROFILING END[WaitingToSync]
I1226 15:05:37.997603 92795 SynchronousNode.cpp:247] [5] PROFILING BEGIN[Forward]
I1226 15:05:37.991369 92323 SynchronousNode.cpp:247] [6] PROFILING BEGIN[Forward]
I1226 15:05:38.002459 88631 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 15:05:37.999900 93129 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 15:05:38.006805 92056 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 15:05:38.002578 88631 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 15:05:38.000018 93129 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 15:05:38.002166 118847 MultiSolver.cpp:92] [4] PROFILING END[Forward]
I1226 15:05:38.006932 92056 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 15:05:38.002279 118847 MultiSolver.cpp:94] [4] PROFILING BEGIN[Backward]
I1226 15:05:38.000347 92722 MultiSolver.cpp:92] [5] PROFILING END[Forward]
I1226 15:05:38.000471 92722 MultiSolver.cpp:94] [5] PROFILING BEGIN[Backward]
I1226 15:05:37.994072 92253 MultiSolver.cpp:92] [6] PROFILING END[Forward]
I1226 15:05:37.994189 92253 MultiSolver.cpp:94] [6] PROFILING BEGIN[Backward]
I1226 15:05:37.998558 95917 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 15:05:37.998674 95917 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 15:05:38.053884 88631 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 15:05:38.054013 88631 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 15:05:38.054069 88631 solver.cpp:291] [3] Iteration 5, loss = 2.91622
I1226 15:05:38.054136 88631 solver.cpp:317]     Train net output #0: accuracy = 0.4375
I1226 15:05:38.054199 88631 solver.cpp:317]     Train net output #1: loss = 2.91622 (* 1 = 2.91622 loss)
I1226 15:05:38.053330 93129 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 15:05:38.053409 93129 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 15:05:38.053452 93129 solver.cpp:291] [1] Iteration 5, loss = 3.89601
I1226 15:05:38.053516 93129 solver.cpp:317]     Train net output #0: accuracy = 0.1875
I1226 15:05:38.053581 93129 solver.cpp:317]     Train net output #1: loss = 3.89601 (* 1 = 3.89601 loss)
I1226 15:05:38.113586 92056 MultiSolver.cpp:108] [7] PROFILING END[Backward]
I1226 15:05:38.113677 92056 MultiSolver.cpp:109] [7] PROFILING BEGIN[WaitingToSync]
I1226 15:05:38.113728 92056 solver.cpp:291] [7] Iteration 5, loss = 3.38951
I1226 15:05:38.113833 92056 solver.cpp:317]     Train net output #0: accuracy = 0.34375
I1226 15:05:38.113911 92056 solver.cpp:317]     Train net output #1: loss = 3.38951 (* 1 = 3.38951 loss)
I1226 15:05:38.112321 118847 MultiSolver.cpp:108] [4] PROFILING END[Backward]
I1226 15:05:38.112408 118847 MultiSolver.cpp:109] [4] PROFILING BEGIN[WaitingToSync]
I1226 15:05:38.112468 118847 solver.cpp:291] [4] Iteration 5, loss = 2.6586
I1226 15:05:38.112541 118847 solver.cpp:317]     Train net output #0: accuracy = 0.34375
I1226 15:05:38.112651 118847 solver.cpp:317]     Train net output #1: loss = 2.6586 (* 1 = 2.6586 loss)
I1226 15:05:38.108839 92253 MultiSolver.cpp:108] [6] PROFILING END[Backward]
I1226 15:05:38.108924 92253 MultiSolver.cpp:109] [6] PROFILING BEGIN[WaitingToSync]
I1226 15:05:38.108968 92253 solver.cpp:291] [6] Iteration 5, loss = 3.37004
I1226 15:05:38.109033 92253 solver.cpp:317]     Train net output #0: accuracy = 0.28125
I1226 15:05:38.109100 92253 solver.cpp:317]     Train net output #1: loss = 3.37004 (* 1 = 3.37004 loss)
I1226 15:05:38.125874 92722 MultiSolver.cpp:108] [5] PROFILING END[Backward]
I1226 15:05:38.125963 92722 MultiSolver.cpp:109] [5] PROFILING BEGIN[WaitingToSync]
I1226 15:05:38.126013 92722 solver.cpp:291] [5] Iteration 5, loss = 3.15488
I1226 15:05:38.126088 92722 solver.cpp:317]     Train net output #0: accuracy = 0.28125
I1226 15:05:38.126163 92722 solver.cpp:317]     Train net output #1: loss = 3.15488 (* 1 = 3.15488 loss)
I1226 15:05:38.486114 95917 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 15:05:38.486209 95917 MultiSolver.cpp:109] [2] PROFILING BEGIN[WaitingToSync]
I1226 15:05:38.486256 95917 solver.cpp:291] [2] Iteration 5, loss = 3.1851
I1226 15:05:38.486330 95917 solver.cpp:317]     Train net output #0: accuracy = 0.34375
I1226 15:05:38.486407 95917 solver.cpp:317]     Train net output #1: loss = 3.1851 (* 1 = 3.1851 loss)
I1226 15:05:44.695842 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:44.696926 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:44.696997 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:44.715440 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:44.715543 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:44.761989 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:44.774231 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:44.774377 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:45.446713 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:45.447460 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:45.447537 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:45.447638 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:45.512048 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:45.514279 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:45.514379 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:45.514502 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:45.514547 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:45.514617 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:45.514663 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:45.514753 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:45.518427 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:45.619084 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:45.637689 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:45.647171 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:45.647264 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:45.647385 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:45.647428 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:45.650051 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:45.650166 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:45.650293 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:45.650341 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:45.653559 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:45.887089 98074 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 15:05:45.887675 98074 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 15:05:46.373329 98074 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 15:05:46.373491 98074 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 15:05:46.373541 98074 solver.cpp:291] [0] Iteration 6, loss = 3.86922
I1226 15:05:46.373621 98074 solver.cpp:317]     Train net output #0: accuracy = 0.15625
I1226 15:05:46.373751 98074 solver.cpp:317]     Train net output #1: loss = 3.86922 (* 1 = 3.86922 loss)
I1226 15:05:47.238566 88701 SynchronousNode.cpp:246] [3] PROFILING END[WaitingToSync]
I1226 15:05:47.238646 88701 SynchronousNode.cpp:247] [3] PROFILING BEGIN[Forward]
I1226 15:05:47.235932 93203 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 15:05:47.238188 118919 SynchronousNode.cpp:246] [4] PROFILING END[WaitingToSync]
I1226 15:05:47.238261 118919 SynchronousNode.cpp:247] [4] PROFILING BEGIN[Forward]
I1226 15:05:47.242848 92129 SynchronousNode.cpp:246] [7] PROFILING END[WaitingToSync]
I1226 15:05:47.236009 93203 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 15:05:47.242928 92129 SynchronousNode.cpp:247] [7] PROFILING BEGIN[Forward]
I1226 15:05:47.228960 95987 SynchronousNode.cpp:246] [2] PROFILING END[WaitingToSync]
I1226 15:05:47.229416 95987 SynchronousNode.cpp:247] [2] PROFILING BEGIN[Forward]
I1226 15:05:47.237423 92795 SynchronousNode.cpp:246] [5] PROFILING END[WaitingToSync]
I1226 15:05:47.231137 92323 SynchronousNode.cpp:246] [6] PROFILING END[WaitingToSync]
I1226 15:05:47.231257 92323 SynchronousNode.cpp:247] [6] PROFILING BEGIN[Forward]
I1226 15:05:47.237504 92795 SynchronousNode.cpp:247] [5] PROFILING BEGIN[Forward]
I1226 15:05:47.241317 88631 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 15:05:47.238744 93129 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 15:05:47.241433 88631 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 15:05:47.238894 93129 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 15:05:47.241030 118847 MultiSolver.cpp:92] [4] PROFILING END[Forward]
I1226 15:05:47.241154 118847 MultiSolver.cpp:94] [4] PROFILING BEGIN[Backward]
I1226 15:05:47.246371 92056 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 15:05:47.246506 92056 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 15:05:47.237339 95917 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 15:05:47.237457 95917 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 15:05:47.239537 92253 MultiSolver.cpp:92] [6] PROFILING END[Forward]
I1226 15:05:47.239650 92253 MultiSolver.cpp:94] [6] PROFILING BEGIN[Backward]
I1226 15:05:47.247092 92722 MultiSolver.cpp:92] [5] PROFILING END[Forward]
I1226 15:05:47.247206 92722 MultiSolver.cpp:94] [5] PROFILING BEGIN[Backward]
I1226 15:05:47.291925 93129 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 15:05:47.292006 93129 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 15:05:47.292088 93129 solver.cpp:291] [1] Iteration 6, loss = 3.46089
I1226 15:05:47.292160 93129 solver.cpp:317]     Train net output #0: accuracy = 0.1875
I1226 15:05:47.292230 93129 solver.cpp:317]     Train net output #1: loss = 3.46089 (* 1 = 3.46089 loss)
I1226 15:05:47.297286 88631 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 15:05:47.297372 88631 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 15:05:47.297417 88631 solver.cpp:291] [3] Iteration 6, loss = 2.39242
I1226 15:05:47.297498 88631 solver.cpp:317]     Train net output #0: accuracy = 0.4375
I1226 15:05:47.297566 88631 solver.cpp:317]     Train net output #1: loss = 2.39242 (* 1 = 2.39242 loss)
I1226 15:05:47.351542 92056 MultiSolver.cpp:108] [7] PROFILING END[Backward]
I1226 15:05:47.351629 92056 MultiSolver.cpp:109] [7] PROFILING BEGIN[WaitingToSync]
I1226 15:05:47.351677 92056 solver.cpp:291] [7] Iteration 6, loss = 2.92666
I1226 15:05:47.351749 92056 solver.cpp:317]     Train net output #0: accuracy = 0.34375
I1226 15:05:47.352083 92056 solver.cpp:317]     Train net output #1: loss = 2.92666 (* 1 = 2.92666 loss)
I1226 15:05:47.353214 118847 MultiSolver.cpp:108] [4] PROFILING END[Backward]
I1226 15:05:47.353304 118847 MultiSolver.cpp:109] [4] PROFILING BEGIN[WaitingToSync]
I1226 15:05:47.353354 118847 solver.cpp:291] [4] Iteration 6, loss = 3.11326
I1226 15:05:47.353426 118847 solver.cpp:317]     Train net output #0: accuracy = 0.28125
I1226 15:05:47.353502 118847 solver.cpp:317]     Train net output #1: loss = 3.11326 (* 1 = 3.11326 loss)
I1226 15:05:47.359608 92253 MultiSolver.cpp:108] [6] PROFILING END[Backward]
I1226 15:05:47.359702 92253 MultiSolver.cpp:109] [6] PROFILING BEGIN[WaitingToSync]
I1226 15:05:47.359750 92253 solver.cpp:291] [6] Iteration 6, loss = 2.8624
I1226 15:05:47.359824 92253 solver.cpp:317]     Train net output #0: accuracy = 0.25
I1226 15:05:47.359899 92253 solver.cpp:317]     Train net output #1: loss = 2.8624 (* 1 = 2.8624 loss)
I1226 15:05:47.370121 92722 MultiSolver.cpp:108] [5] PROFILING END[Backward]
I1226 15:05:47.370213 92722 MultiSolver.cpp:109] [5] PROFILING BEGIN[WaitingToSync]
I1226 15:05:47.370263 92722 solver.cpp:291] [5] Iteration 6, loss = 3.44974
I1226 15:05:47.370368 92722 solver.cpp:317]     Train net output #0: accuracy = 0.1875
I1226 15:05:47.370436 92722 solver.cpp:317]     Train net output #1: loss = 3.44974 (* 1 = 3.44974 loss)
I1226 15:05:47.721170 95917 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 15:05:47.721263 95917 MultiSolver.cpp:109] [2] PROFILING BEGIN[WaitingToSync]
I1226 15:05:47.721312 95917 solver.cpp:291] [2] Iteration 6, loss = 2.47697
I1226 15:05:47.721426 95917 solver.cpp:317]     Train net output #0: accuracy = 0.4375
I1226 15:05:47.721691 95917 solver.cpp:317]     Train net output #1: loss = 2.47697 (* 1 = 2.47697 loss)
I1226 15:05:53.953361 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:53.953577 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:53.954023 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:53.976796 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:53.976898 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:54.023025 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:54.035475 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:54.035615 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:54.712898 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:54.713668 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:54.713752 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:54.713850 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:54.780287 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:54.782464 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:54.782564 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:54.782690 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:54.782739 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:54.782814 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:54.782982 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:54.783094 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:54.786610 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:54.886899 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:54.905630 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:54.915053 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:54.915160 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:54.915287 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:54.915336 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:54.917982 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:54.918069 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:54.918220 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
I1226 15:05:54.918268 52943 async_param_server.cpp:87] [8] PROFILING BEGIN[PSUpdate]
I1226 15:05:54.921473 52943 async_param_server.cpp:120] [8] PROFILING END[PSUpdate]
User defined signal 2
