Dec 26 13:57:49 2016 92026 3 10.1 NIOS_DEBUG: stdin_fd set to -1
Dec 26 13:57:49 2016 92026 3 10.1 NIOS_DEBUG: fds[0] has a value of -1
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 13:57:52.932615 94427 mpiutil.cpp:166] Process rank 3 from number of 9 processes running on knl-node015
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 13:57:52.936398 97230 mpiutil.cpp:166] Process rank 2 from number of 9 processes running on knl-node078
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 13:57:52.936887 96716 mpiutil.cpp:166] Process rank 6 from number of 9 processes running on knl-node079
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 13:57:52.945855 93154 mpiutil.cpp:166] Process rank 8 from number of 9 processes running on knl-node051
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 13:57:52.945428 92037 mpiutil.cpp:166] Process rank 0 from number of 9 processes running on knl-node019
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 13:57:52.948163 95085 mpiutil.cpp:166] Process rank 1 from number of 9 processes running on knl-node084
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 13:57:52.955935 91478 mpiutil.cpp:166] Process rank 5 from number of 9 processes running on knl-node047
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 13:57:52.947700 92257 mpiutil.cpp:166] Process rank 7 from number of 9 processes running on knl-node048
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 13:57:52.943478 91481 mpiutil.cpp:166] Process rank 4 from number of 9 processes running on knl-node014
I1226 13:57:52.968956 91478 caffe.cpp:316] Use CPU.
I1226 13:57:52.946013 94427 caffe.cpp:316] Use CPU.
I1226 13:57:52.959002 92037 caffe.cpp:316] Use CPU.
I1226 13:57:52.959599 93154 caffe.cpp:316] Use CPU.
I1226 13:57:52.949792 97230 caffe.cpp:316] Use CPU.
I1226 13:57:52.961431 95085 caffe.cpp:316] Use CPU.
I1226 13:57:52.950584 96716 caffe.cpp:316] Use CPU.
I1226 13:57:52.970022 91478 solver.cpp:93] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_6.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 13:57:52.970955 91478 solver.cpp:128] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_6.prototxt
I1226 13:57:52.947307 94427 solver.cpp:93] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_4.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 13:57:52.960346 92037 solver.cpp:93] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_1.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 13:57:52.948341 94427 solver.cpp:128] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_4.prototxt
I1226 13:57:52.961223 92037 solver.cpp:128] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_1.prototxt
I1226 13:57:52.951092 97230 solver.cpp:93] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_3.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 13:57:52.960857 93154 solver.cpp:93] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_dummy.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 13:57:52.952195 97230 solver.cpp:128] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_3.prototxt
I1226 13:57:52.962028 93154 solver.cpp:128] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_dummy.prototxt
I1226 13:57:52.962914 95085 solver.cpp:93] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_2.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 13:57:52.964049 95085 solver.cpp:128] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_2.prototxt
I1226 13:57:52.951920 96716 solver.cpp:93] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_7.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 13:57:52.952893 96716 solver.cpp:128] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_7.prototxt
I1226 13:57:52.963500 92257 caffe.cpp:316] Use CPU.
I1226 13:57:52.960511 91481 caffe.cpp:316] Use CPU.
I1226 13:57:52.964659 92257 solver.cpp:93] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_8.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 13:57:52.965663 92257 solver.cpp:128] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_8.prototxt
I1226 13:57:52.961899 91481 solver.cpp:93] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_5.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 13:57:52.963030 91481 solver.cpp:128] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_5.prototxt
I1226 13:57:52.991849 93154 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 13:57:52.991925 93154 cpu_info.cpp:455] Total number of sockets: 1
I1226 13:57:52.991953 93154 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 13:57:52.991976 93154 cpu_info.cpp:461] Total number of processors: 272
I1226 13:57:52.992004 93154 cpu_info.cpp:464] GPU is used: no
I1226 13:57:52.992059 93154 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 13:57:52.992100 93154 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 13:57:52.979882 94427 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 13:57:52.979957 94427 cpu_info.cpp:455] Total number of sockets: 1
I1226 13:57:52.992787 92037 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 13:57:52.992866 92037 cpu_info.cpp:455] Total number of sockets: 1
I1226 13:57:52.979980 94427 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 13:57:52.980000 94427 cpu_info.cpp:461] Total number of processors: 272
I1226 13:57:52.980020 94427 cpu_info.cpp:464] GPU is used: no
I1226 13:57:52.980039 94427 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 13:57:52.992895 92037 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 13:57:52.992921 92037 cpu_info.cpp:461] Total number of processors: 272
I1226 13:57:52.992949 92037 cpu_info.cpp:464] GPU is used: no
I1226 13:57:52.980059 94427 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 13:57:52.992976 92037 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 13:57:52.993003 92037 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 13:57:52.996212 92257 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 13:57:52.996289 92257 cpu_info.cpp:455] Total number of sockets: 1
I1226 13:57:52.996331 92257 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 13:57:52.996357 92257 cpu_info.cpp:461] Total number of processors: 272
I1226 13:57:52.996382 92257 cpu_info.cpp:464] GPU is used: no
I1226 13:57:52.996412 92257 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 13:57:52.996438 92257 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 13:57:52.985751 97230 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 13:57:52.986342 96716 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 13:57:52.986429 96716 cpu_info.cpp:455] Total number of sockets: 1
I1226 13:57:52.985841 97230 cpu_info.cpp:455] Total number of sockets: 1
I1226 13:57:52.985877 97230 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 13:57:52.985913 97230 cpu_info.cpp:461] Total number of processors: 272
I1226 13:57:52.986457 96716 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 13:57:52.986479 96716 cpu_info.cpp:461] Total number of processors: 272
I1226 13:57:52.986501 96716 cpu_info.cpp:464] GPU is used: no
I1226 13:57:52.985945 97230 cpu_info.cpp:464] GPU is used: no
I1226 13:57:52.985975 97230 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 13:57:52.986001 97230 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 13:57:52.986523 96716 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 13:57:52.997876 95085 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 13:57:52.986558 96716 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 13:57:52.997968 95085 cpu_info.cpp:455] Total number of sockets: 1
I1226 13:57:52.997995 95085 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 13:57:52.998016 95085 cpu_info.cpp:461] Total number of processors: 272
I1226 13:57:52.998039 95085 cpu_info.cpp:464] GPU is used: no
I1226 13:57:52.998062 95085 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 13:57:52.998085 95085 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 13:57:52.997205 91481 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 13:57:52.997280 91481 cpu_info.cpp:455] Total number of sockets: 1
I1226 13:57:52.997303 91481 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 13:57:52.997323 91481 cpu_info.cpp:461] Total number of processors: 272
I1226 13:57:52.997342 91481 cpu_info.cpp:464] GPU is used: no
I1226 13:57:52.997362 91481 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 13:57:52.997382 91481 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 13:57:53.010072 91478 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 13:57:53.010156 91478 cpu_info.cpp:455] Total number of sockets: 1
I1226 13:57:53.010179 91478 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 13:57:53.010200 91478 cpu_info.cpp:461] Total number of processors: 272
I1226 13:57:53.010238 91478 cpu_info.cpp:464] GPU is used: no
I1226 13:57:53.010259 91478 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 13:57:53.010279 91478 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 13:57:53.018607 95085 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 13:57:53.019044 95085 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 13:57:53.021147 95085 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_1"
    batch_size: 32
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 13:57:53.010846 96716 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 13:57:53.011230 96716 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 13:57:53.024179 97230 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 13:57:53.013365 96716 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_6"
    batch_size: 32
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 13:57:53.037111 93154 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 13:57:53.017226 94427 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 13:57:53.036422 95085 layer_factory.hpp:114] Creating layer data
I1226 13:57:53.047134 91478 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 13:57:53.036367 91481 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 13:57:53.024556 97230 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 13:57:53.032385 96716 layer_factory.hpp:114] Creating layer data
I1226 13:57:53.037466 93154 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 13:57:53.017614 94427 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 13:57:53.044888 95085 net.cpp:178] Creating Layer data
I1226 13:57:53.047471 91478 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 13:57:53.036721 91481 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 13:57:53.026653 97230 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_2"
    batch_size: 32
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 13:57:53.039244 93154 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "DummyData"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  dummy_data_param {
    data_filler {
      type: "gaussian"
      std: 1
    }
    data_filler {
      type: "constant"
      value: 0
    }
    shape {
      dim: 32
      dim: 3
      dim: 227
      dim: 227
    }
    shape {
      dim: 32
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 13:57:53.019279 94427 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_3"
    batch_size: 32
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 13:57:53.045022 95085 net.cpp:586] data -> data
I1226 13:57:53.049302 91478 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_5"
    batch_size: 32
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 13:57:53.038460 91481 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_4"
    batch_size: 32
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 13:57:53.045133 95085 net.cpp:586] data -> label
I1226 13:57:53.053248 92257 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 13:57:53.053714 92257 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 13:57:53.055860 95087 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_1
I1226 13:57:53.056004 92257 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_7"
    batch_size: 32
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 13:57:53.057744 92037 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 13:57:53.058059 92037 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 13:57:53.060405 95085 data_layer.cpp:80] output data size: 32,3,227,227
I1226 13:57:53.059804 92037 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_0"
    batch_size: 32
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 13:57:53.052279 96716 net.cpp:178] Creating Layer data
I1226 13:57:53.052412 96716 net.cpp:586] data -> data
I1226 13:57:53.052538 96716 net.cpp:586] data -> label
I1226 13:57:53.065001 93154 layer_factory.hpp:114] Creating layer data
I1226 13:57:53.065172 93154 net.cpp:178] Creating Layer data
I1226 13:57:53.065235 93154 net.cpp:586] data -> data
I1226 13:57:53.065364 93154 net.cpp:586] data -> label
I1226 13:57:53.084743 91478 layer_factory.hpp:114] Creating layer data
I1226 13:57:53.077354 91481 layer_factory.hpp:114] Creating layer data
I1226 13:57:53.073309 97230 layer_factory.hpp:114] Creating layer data
I1226 13:57:53.076743 96718 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_6
I1226 13:57:53.099201 91478 net.cpp:178] Creating Layer data
I1226 13:57:53.099318 91478 net.cpp:586] data -> data
I1226 13:57:53.099395 91478 net.cpp:586] data -> label
I1226 13:57:53.089491 91481 net.cpp:178] Creating Layer data
I1226 13:57:53.089577 91481 net.cpp:586] data -> data
I1226 13:57:53.089651 91481 net.cpp:586] data -> label
I1226 13:57:53.084949 96716 data_layer.cpp:80] output data size: 32,3,227,227
I1226 13:57:53.088376 97230 net.cpp:178] Creating Layer data
I1226 13:57:53.088505 97230 net.cpp:586] data -> data
I1226 13:57:53.088627 97230 net.cpp:586] data -> label
I1226 13:57:53.103586 92257 layer_factory.hpp:114] Creating layer data
I1226 13:57:53.114169 92257 net.cpp:178] Creating Layer data
I1226 13:57:53.114262 92257 net.cpp:586] data -> data
I1226 13:57:53.114370 92257 net.cpp:586] data -> label
I1226 13:57:53.113127 92037 layer_factory.hpp:114] Creating layer data
I1226 13:57:53.124258 91480 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_5
I1226 13:57:53.114234 93154 net.cpp:228] Setting up data
I1226 13:57:53.114346 93154 net.cpp:235] Top shape: 32 3 227 227 (4946784)
I1226 13:57:53.114404 93154 net.cpp:235] Top shape: 32 1 1 1 (32)
I1226 13:57:53.114428 93154 net.cpp:243] Memory required for data: 19787264
I1226 13:57:53.114464 93154 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 13:57:53.114554 93154 net.cpp:178] Creating Layer label_data_1_split
I1226 13:57:53.114588 93154 net.cpp:612] label_data_1_split <- label
I1226 13:57:53.114709 93154 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 13:57:53.114754 93154 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 13:57:53.118569 92037 net.cpp:178] Creating Layer data
I1226 13:57:53.118654 92037 net.cpp:586] data -> data
I1226 13:57:53.118731 92037 net.cpp:586] data -> label
I1226 13:57:53.111549 97232 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_2
I1226 13:57:53.125738 93154 net.cpp:228] Setting up label_data_1_split
I1226 13:57:53.125840 93154 net.cpp:235] Top shape: 32 1 1 1 (32)
I1226 13:57:53.125871 93154 net.cpp:235] Top shape: 32 1 1 1 (32)
I1226 13:57:53.125892 93154 net.cpp:243] Memory required for data: 19787520
I1226 13:57:53.125927 93154 layer_factory.hpp:114] Creating layer conv1
I1226 13:57:53.126024 93154 net.cpp:178] Creating Layer conv1
I1226 13:57:53.112962 94427 layer_factory.hpp:114] Creating layer data
I1226 13:57:53.123608 91484 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_4
I1226 13:57:53.126056 93154 net.cpp:612] conv1 <- data
I1226 13:57:53.126101 93154 net.cpp:586] conv1 -> conv1
I1226 13:57:53.119210 97230 data_layer.cpp:80] output data size: 32,3,227,227
I1226 13:57:53.126617 94427 net.cpp:178] Creating Layer data
I1226 13:57:53.126708 94427 net.cpp:586] data -> data
I1226 13:57:53.126785 94427 net.cpp:586] data -> label
I1226 13:57:53.148331 92259 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_7
I1226 13:57:53.157660 91478 data_layer.cpp:80] output data size: 32,3,227,227
I1226 13:57:53.140336 94429 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_3
I1226 13:57:53.157158 92257 data_layer.cpp:80] output data size: 32,3,227,227
I1226 13:57:53.153661 91481 data_layer.cpp:80] output data size: 32,3,227,227
I1226 13:57:53.144431 94427 data_layer.cpp:80] output data size: 32,3,227,227
I1226 13:57:53.159585 92040 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_0
I1226 13:57:53.168094 92037 data_layer.cpp:80] output data size: 32,3,227,227
I1226 13:57:53.219099 92257 net.cpp:228] Setting up data
I1226 13:57:53.219231 92257 net.cpp:235] Top shape: 32 3 227 227 (4946784)
I1226 13:57:53.231240 91478 net.cpp:228] Setting up data
I1226 13:57:53.231355 91478 net.cpp:235] Top shape: 32 3 227 227 (4946784)
I1226 13:57:53.231390 91478 net.cpp:235] Top shape: 32 (32)
I1226 13:57:53.231413 91478 net.cpp:243] Memory required for data: 19787264
I1226 13:57:53.231448 91478 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 13:57:53.231528 91478 net.cpp:178] Creating Layer label_data_1_split
I1226 13:57:53.231668 91478 net.cpp:612] label_data_1_split <- label
I1226 13:57:53.231705 91478 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 13:57:53.231779 91478 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 13:57:53.227468 93154 net.cpp:228] Setting up conv1
I1226 13:57:53.227576 93154 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 13:57:53.227598 93154 net.cpp:243] Memory required for data: 56958720
I1226 13:57:53.227744 93154 layer_factory.hpp:114] Creating layer relu1
I1226 13:57:53.227815 93154 net.cpp:178] Creating Layer relu1
I1226 13:57:53.227859 93154 net.cpp:612] relu1 <- conv1
I1226 13:57:53.227900 93154 net.cpp:573] relu1 -> conv1 (in-place)
I1226 13:57:53.227993 93154 net.cpp:228] Setting up relu1
I1226 13:57:53.228031 93154 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 13:57:53.228060 93154 net.cpp:243] Memory required for data: 94129920
I1226 13:57:53.228091 93154 layer_factory.hpp:114] Creating layer norm1
I1226 13:57:53.228171 93154 net.cpp:178] Creating Layer norm1
I1226 13:57:53.228196 93154 net.cpp:612] norm1 <- conv1
I1226 13:57:53.228231 93154 net.cpp:586] norm1 -> norm1
I1226 13:57:53.228308 93154 net.cpp:228] Setting up norm1
I1226 13:57:53.228353 93154 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 13:57:53.228397 93154 net.cpp:243] Memory required for data: 131301120
I1226 13:57:53.228427 93154 layer_factory.hpp:114] Creating layer pool1
I1226 13:57:53.228484 93154 net.cpp:178] Creating Layer pool1
I1226 13:57:53.228514 93154 net.cpp:612] pool1 <- norm1
I1226 13:57:53.228548 93154 net.cpp:586] pool1 -> pool1
I1226 13:57:53.228628 93154 net.cpp:228] Setting up pool1
I1226 13:57:53.228667 93154 net.cpp:235] Top shape: 32 96 27 27 (2239488)
I1226 13:57:53.228690 93154 net.cpp:243] Memory required for data: 140259072
I1226 13:57:53.228714 93154 layer_factory.hpp:114] Creating layer conv2
I1226 13:57:53.228786 93154 net.cpp:178] Creating Layer conv2
I1226 13:57:53.228816 93154 net.cpp:612] conv2 <- pool1
I1226 13:57:53.228865 93154 net.cpp:586] conv2 -> conv2
I1226 13:57:53.219279 92257 net.cpp:235] Top shape: 32 (32)
I1226 13:57:53.231456 92257 net.cpp:243] Memory required for data: 19787264
I1226 13:57:53.231528 92257 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 13:57:53.231639 92257 net.cpp:178] Creating Layer label_data_1_split
I1226 13:57:53.231762 92257 net.cpp:612] label_data_1_split <- label
I1226 13:57:53.231807 92257 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 13:57:53.231863 92257 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 13:57:53.218268 94427 net.cpp:228] Setting up data
I1226 13:57:53.218403 94427 net.cpp:235] Top shape: 32 3 227 227 (4946784)
I1226 13:57:53.218442 94427 net.cpp:235] Top shape: 32 (32)
I1226 13:57:53.218466 94427 net.cpp:243] Memory required for data: 19787264
I1226 13:57:53.218503 94427 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 13:57:53.218595 94427 net.cpp:178] Creating Layer label_data_1_split
I1226 13:57:53.218720 94427 net.cpp:612] label_data_1_split <- label
I1226 13:57:53.218765 94427 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 13:57:53.218811 94427 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 13:57:53.231883 91481 net.cpp:228] Setting up data
I1226 13:57:53.231998 91481 net.cpp:235] Top shape: 32 3 227 227 (4946784)
I1226 13:57:53.232033 91481 net.cpp:235] Top shape: 32 (32)
I1226 13:57:53.232084 91481 net.cpp:243] Memory required for data: 19787264
I1226 13:57:53.232121 91481 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 13:57:53.232208 91481 net.cpp:178] Creating Layer label_data_1_split
I1226 13:57:53.232328 91481 net.cpp:612] label_data_1_split <- label
I1226 13:57:53.232367 91481 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 13:57:53.232419 91481 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 13:57:53.246796 91478 net.cpp:228] Setting up label_data_1_split
I1226 13:57:53.246896 91478 net.cpp:235] Top shape: 32 (32)
I1226 13:57:53.246927 91478 net.cpp:235] Top shape: 32 (32)
I1226 13:57:53.246976 91478 net.cpp:243] Memory required for data: 19787520
I1226 13:57:53.247023 91478 layer_factory.hpp:114] Creating layer conv1
I1226 13:57:53.247113 91478 net.cpp:178] Creating Layer conv1
I1226 13:57:53.247165 91478 net.cpp:612] conv1 <- data
I1226 13:57:53.247229 91478 net.cpp:586] conv1 -> conv1
I1226 13:57:53.239286 95085 net.cpp:228] Setting up data
I1226 13:57:53.239420 95085 net.cpp:235] Top shape: 32 3 227 227 (4946784)
I1226 13:57:53.239470 95085 net.cpp:235] Top shape: 32 (32)
I1226 13:57:53.239498 95085 net.cpp:243] Memory required for data: 19787264
I1226 13:57:53.239540 95085 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 13:57:53.239660 95085 net.cpp:178] Creating Layer label_data_1_split
I1226 13:57:53.239817 95085 net.cpp:612] label_data_1_split <- label
I1226 13:57:53.239912 95085 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 13:57:53.239972 95085 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 13:57:53.243826 92037 net.cpp:228] Setting up data
I1226 13:57:53.243945 92037 net.cpp:235] Top shape: 32 3 227 227 (4946784)
I1226 13:57:53.243985 92037 net.cpp:235] Top shape: 32 (32)
I1226 13:57:53.244011 92037 net.cpp:243] Memory required for data: 19787264
I1226 13:57:53.244053 92037 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 13:57:53.244153 92037 net.cpp:178] Creating Layer label_data_1_split
I1226 13:57:53.244302 92037 net.cpp:612] label_data_1_split <- label
I1226 13:57:53.244393 92037 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 13:57:53.244458 92037 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 13:57:53.248724 92257 net.cpp:228] Setting up label_data_1_split
I1226 13:57:53.244423 91481 net.cpp:228] Setting up label_data_1_split
I1226 13:57:53.248836 92257 net.cpp:235] Top shape: 32 (32)
I1226 13:57:53.248874 92257 net.cpp:235] Top shape: 32 (32)
I1226 13:57:53.244525 91481 net.cpp:235] Top shape: 32 (32)
I1226 13:57:53.244556 91481 net.cpp:235] Top shape: 32 (32)
I1226 13:57:53.244578 91481 net.cpp:243] Memory required for data: 19787520
I1226 13:57:53.248903 92257 net.cpp:243] Memory required for data: 19787520
I1226 13:57:53.244640 91481 layer_factory.hpp:114] Creating layer conv1
I1226 13:57:53.248944 92257 layer_factory.hpp:114] Creating layer conv1
I1226 13:57:53.244743 91481 net.cpp:178] Creating Layer conv1
I1226 13:57:53.249060 92257 net.cpp:178] Creating Layer conv1
I1226 13:57:53.244777 91481 net.cpp:612] conv1 <- data
I1226 13:57:53.249097 92257 net.cpp:612] conv1 <- data
I1226 13:57:53.249145 92257 net.cpp:586] conv1 -> conv1
I1226 13:57:53.244817 91481 net.cpp:586] conv1 -> conv1
I1226 13:57:53.238005 96716 net.cpp:228] Setting up data
I1226 13:57:53.238131 96716 net.cpp:235] Top shape: 32 3 227 227 (4946784)
I1226 13:57:53.238180 96716 net.cpp:235] Top shape: 32 (32)
I1226 13:57:53.238209 96716 net.cpp:243] Memory required for data: 19787264
I1226 13:57:53.238250 96716 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 13:57:53.238346 96716 net.cpp:178] Creating Layer label_data_1_split
I1226 13:57:53.238497 96716 net.cpp:612] label_data_1_split <- label
I1226 13:57:53.238554 96716 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 13:57:53.238616 96716 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 13:57:53.243886 94427 net.cpp:228] Setting up label_data_1_split
I1226 13:57:53.243994 94427 net.cpp:235] Top shape: 32 (32)
I1226 13:57:53.244029 94427 net.cpp:235] Top shape: 32 (32)
I1226 13:57:53.244052 94427 net.cpp:243] Memory required for data: 19787520
I1226 13:57:53.244087 94427 layer_factory.hpp:114] Creating layer conv1
I1226 13:57:53.244192 94427 net.cpp:178] Creating Layer conv1
I1226 13:57:53.244230 94427 net.cpp:612] conv1 <- data
I1226 13:57:53.244278 94427 net.cpp:586] conv1 -> conv1
I1226 13:57:53.259869 92037 net.cpp:228] Setting up label_data_1_split
I1226 13:57:53.259979 92037 net.cpp:235] Top shape: 32 (32)
I1226 13:57:53.260015 92037 net.cpp:235] Top shape: 32 (32)
I1226 13:57:53.260037 92037 net.cpp:243] Memory required for data: 19787520
I1226 13:57:53.260074 92037 layer_factory.hpp:114] Creating layer conv1
I1226 13:57:53.260179 92037 net.cpp:178] Creating Layer conv1
I1226 13:57:53.260236 92037 net.cpp:612] conv1 <- data
I1226 13:57:53.260293 92037 net.cpp:586] conv1 -> conv1
I1226 13:57:53.266170 95085 net.cpp:228] Setting up label_data_1_split
I1226 13:57:53.266284 95085 net.cpp:235] Top shape: 32 (32)
I1226 13:57:53.266319 95085 net.cpp:235] Top shape: 32 (32)
I1226 13:57:53.266345 95085 net.cpp:243] Memory required for data: 19787520
I1226 13:57:53.266382 95085 layer_factory.hpp:114] Creating layer conv1
I1226 13:57:53.266499 95085 net.cpp:178] Creating Layer conv1
I1226 13:57:53.266542 95085 net.cpp:612] conv1 <- data
I1226 13:57:53.266590 95085 net.cpp:586] conv1 -> conv1
I1226 13:57:53.278470 97230 net.cpp:228] Setting up data
I1226 13:57:53.278600 97230 net.cpp:235] Top shape: 32 3 227 227 (4946784)
I1226 13:57:53.278648 97230 net.cpp:235] Top shape: 32 (32)
I1226 13:57:53.278676 97230 net.cpp:243] Memory required for data: 19787264
I1226 13:57:53.278717 97230 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 13:57:53.278856 97230 net.cpp:178] Creating Layer label_data_1_split
I1226 13:57:53.279011 97230 net.cpp:612] label_data_1_split <- label
I1226 13:57:53.279068 97230 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 13:57:53.279139 97230 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 13:57:53.280167 96716 net.cpp:228] Setting up label_data_1_split
I1226 13:57:53.280285 96716 net.cpp:235] Top shape: 32 (32)
I1226 13:57:53.280325 96716 net.cpp:235] Top shape: 32 (32)
I1226 13:57:53.280354 96716 net.cpp:243] Memory required for data: 19787520
I1226 13:57:53.280513 96716 layer_factory.hpp:114] Creating layer conv1
I1226 13:57:53.280658 96716 net.cpp:178] Creating Layer conv1
I1226 13:57:53.280704 96716 net.cpp:612] conv1 <- data
I1226 13:57:53.280755 96716 net.cpp:586] conv1 -> conv1
I1226 13:57:53.313251 97230 net.cpp:228] Setting up label_data_1_split
I1226 13:57:53.313362 97230 net.cpp:235] Top shape: 32 (32)
I1226 13:57:53.313400 97230 net.cpp:235] Top shape: 32 (32)
I1226 13:57:53.313426 97230 net.cpp:243] Memory required for data: 19787520
I1226 13:57:53.313462 97230 layer_factory.hpp:114] Creating layer conv1
I1226 13:57:53.313563 97230 net.cpp:178] Creating Layer conv1
I1226 13:57:53.313673 97230 net.cpp:612] conv1 <- data
I1226 13:57:53.313745 97230 net.cpp:586] conv1 -> conv1
I1226 13:57:53.358667 93154 net.cpp:228] Setting up conv2
I1226 13:57:53.358778 93154 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 13:57:53.358803 93154 net.cpp:243] Memory required for data: 164146944
I1226 13:57:53.358875 93154 layer_factory.hpp:114] Creating layer relu2
I1226 13:57:53.358953 93154 net.cpp:178] Creating Layer relu2
I1226 13:57:53.358991 93154 net.cpp:612] relu2 <- conv2
I1226 13:57:53.359031 93154 net.cpp:573] relu2 -> conv2 (in-place)
I1226 13:57:53.359125 93154 net.cpp:228] Setting up relu2
I1226 13:57:53.359166 93154 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 13:57:53.359189 93154 net.cpp:243] Memory required for data: 188034816
I1226 13:57:53.359216 93154 layer_factory.hpp:114] Creating layer norm2
I1226 13:57:53.359266 93154 net.cpp:178] Creating Layer norm2
I1226 13:57:53.359297 93154 net.cpp:612] norm2 <- conv2
I1226 13:57:53.359333 93154 net.cpp:586] norm2 -> norm2
I1226 13:57:53.359447 93154 net.cpp:228] Setting up norm2
I1226 13:57:53.359498 93154 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 13:57:53.359520 93154 net.cpp:243] Memory required for data: 211922688
I1226 13:57:53.359547 93154 layer_factory.hpp:114] Creating layer pool2
I1226 13:57:53.359606 93154 net.cpp:178] Creating Layer pool2
I1226 13:57:53.359637 93154 net.cpp:612] pool2 <- norm2
I1226 13:57:53.359681 93154 net.cpp:586] pool2 -> pool2
I1226 13:57:53.359760 93154 net.cpp:228] Setting up pool2
I1226 13:57:53.359805 93154 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 13:57:53.359925 93154 net.cpp:243] Memory required for data: 217460480
I1226 13:57:53.359961 93154 layer_factory.hpp:114] Creating layer conv3
I1226 13:57:53.360040 93154 net.cpp:178] Creating Layer conv3
I1226 13:57:53.360074 93154 net.cpp:612] conv3 <- pool2
I1226 13:57:53.360115 93154 net.cpp:586] conv3 -> conv3
I1226 13:57:53.374645 92257 net.cpp:228] Setting up conv1
I1226 13:57:53.374809 92257 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 13:57:53.374857 92257 net.cpp:243] Memory required for data: 56958720
I1226 13:57:53.375051 92257 layer_factory.hpp:114] Creating layer relu1
I1226 13:57:53.375162 92257 net.cpp:178] Creating Layer relu1
I1226 13:57:53.375222 92257 net.cpp:612] relu1 <- conv1
I1226 13:57:53.375284 92257 net.cpp:573] relu1 -> conv1 (in-place)
I1226 13:57:53.375450 92257 net.cpp:228] Setting up relu1
I1226 13:57:53.375511 92257 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 13:57:53.375546 92257 net.cpp:243] Memory required for data: 94129920
I1226 13:57:53.375583 92257 layer_factory.hpp:114] Creating layer norm1
I1226 13:57:53.375671 92257 net.cpp:178] Creating Layer norm1
I1226 13:57:53.375708 92257 net.cpp:612] norm1 <- conv1
I1226 13:57:53.375771 92257 net.cpp:586] norm1 -> norm1
I1226 13:57:53.375882 92257 net.cpp:228] Setting up norm1
I1226 13:57:53.375936 92257 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 13:57:53.375967 92257 net.cpp:243] Memory required for data: 131301120
I1226 13:57:53.376001 92257 layer_factory.hpp:114] Creating layer pool1
I1226 13:57:53.376082 92257 net.cpp:178] Creating Layer pool1
I1226 13:57:53.376117 92257 net.cpp:612] pool1 <- norm1
I1226 13:57:53.376160 92257 net.cpp:586] pool1 -> pool1
I1226 13:57:53.376272 92257 net.cpp:228] Setting up pool1
I1226 13:57:53.376343 92257 net.cpp:235] Top shape: 32 96 27 27 (2239488)
I1226 13:57:53.376372 92257 net.cpp:243] Memory required for data: 140259072
I1226 13:57:53.386087 92257 layer_factory.hpp:114] Creating layer conv2
I1226 13:57:53.386759 92257 net.cpp:178] Creating Layer conv2
I1226 13:57:53.386862 92257 net.cpp:612] conv2 <- pool1
I1226 13:57:53.386936 92257 net.cpp:586] conv2 -> conv2
I1226 13:57:53.391321 91481 net.cpp:228] Setting up conv1
I1226 13:57:53.391432 91481 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 13:57:53.391458 91481 net.cpp:243] Memory required for data: 56958720
I1226 13:57:53.391569 91481 layer_factory.hpp:114] Creating layer relu1
I1226 13:57:53.391652 91481 net.cpp:178] Creating Layer relu1
I1226 13:57:53.391688 91481 net.cpp:612] relu1 <- conv1
I1226 13:57:53.391747 91481 net.cpp:573] relu1 -> conv1 (in-place)
I1226 13:57:53.391965 91481 net.cpp:228] Setting up relu1
I1226 13:57:53.392017 91481 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 13:57:53.392040 91481 net.cpp:243] Memory required for data: 94129920
I1226 13:57:53.392071 91481 layer_factory.hpp:114] Creating layer norm1
I1226 13:57:53.392132 91481 net.cpp:178] Creating Layer norm1
I1226 13:57:53.392166 91481 net.cpp:612] norm1 <- conv1
I1226 13:57:53.392222 91481 net.cpp:586] norm1 -> norm1
I1226 13:57:53.392303 91481 net.cpp:228] Setting up norm1
I1226 13:57:53.392345 91481 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 13:57:53.392369 91481 net.cpp:243] Memory required for data: 131301120
I1226 13:57:53.392395 91481 layer_factory.hpp:114] Creating layer pool1
I1226 13:57:53.392457 91481 net.cpp:178] Creating Layer pool1
I1226 13:57:53.392488 91481 net.cpp:612] pool1 <- norm1
I1226 13:57:53.392519 91481 net.cpp:586] pool1 -> pool1
I1226 13:57:53.392606 91481 net.cpp:228] Setting up pool1
I1226 13:57:53.392647 91481 net.cpp:235] Top shape: 32 96 27 27 (2239488)
I1226 13:57:53.392668 91481 net.cpp:243] Memory required for data: 140259072
I1226 13:57:53.392694 91481 layer_factory.hpp:114] Creating layer conv2
I1226 13:57:53.392770 91481 net.cpp:178] Creating Layer conv2
I1226 13:57:53.392802 91481 net.cpp:612] conv2 <- pool1
I1226 13:57:53.392838 91481 net.cpp:586] conv2 -> conv2
I1226 13:57:53.412044 91478 net.cpp:228] Setting up conv1
I1226 13:57:53.412154 91478 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 13:57:53.412183 91478 net.cpp:243] Memory required for data: 56958720
I1226 13:57:53.412358 91478 layer_factory.hpp:114] Creating layer relu1
I1226 13:57:53.412511 91478 net.cpp:178] Creating Layer relu1
I1226 13:57:53.412556 91478 net.cpp:612] relu1 <- conv1
I1226 13:57:53.412619 91478 net.cpp:573] relu1 -> conv1 (in-place)
I1226 13:57:53.412853 91478 net.cpp:228] Setting up relu1
I1226 13:57:53.412931 91478 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 13:57:53.412956 91478 net.cpp:243] Memory required for data: 94129920
I1226 13:57:53.412986 91478 layer_factory.hpp:114] Creating layer norm1
I1226 13:57:53.413074 91478 net.cpp:178] Creating Layer norm1
I1226 13:57:53.413110 91478 net.cpp:612] norm1 <- conv1
I1226 13:57:53.413156 91478 net.cpp:586] norm1 -> norm1
I1226 13:57:53.404115 92037 net.cpp:228] Setting up conv1
I1226 13:57:53.404232 92037 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 13:57:53.404259 92037 net.cpp:243] Memory required for data: 56958720
I1226 13:57:53.414780 91478 net.cpp:228] Setting up norm1
I1226 13:57:53.404413 92037 layer_factory.hpp:114] Creating layer relu1
I1226 13:57:53.414911 91478 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 13:57:53.414939 91478 net.cpp:243] Memory required for data: 131301120
I1226 13:57:53.404518 92037 net.cpp:178] Creating Layer relu1
I1226 13:57:53.404561 92037 net.cpp:612] relu1 <- conv1
I1226 13:57:53.414975 91478 layer_factory.hpp:114] Creating layer pool1
I1226 13:57:53.404616 92037 net.cpp:573] relu1 -> conv1 (in-place)
I1226 13:57:53.415097 91478 net.cpp:178] Creating Layer pool1
I1226 13:57:53.415140 91478 net.cpp:612] pool1 <- norm1
I1226 13:57:53.415190 91478 net.cpp:586] pool1 -> pool1
I1226 13:57:53.404726 92037 net.cpp:228] Setting up relu1
I1226 13:57:53.415339 91478 net.cpp:228] Setting up pool1
I1226 13:57:53.404795 92037 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 13:57:53.404824 92037 net.cpp:243] Memory required for data: 94129920
I1226 13:57:53.404860 92037 layer_factory.hpp:114] Creating layer norm1
I1226 13:57:53.415391 91478 net.cpp:235] Top shape: 32 96 27 27 (2239488)
I1226 13:57:53.404938 92037 net.cpp:178] Creating Layer norm1
I1226 13:57:53.415413 91478 net.cpp:243] Memory required for data: 140259072
I1226 13:57:53.404971 92037 net.cpp:612] norm1 <- conv1
I1226 13:57:53.405019 92037 net.cpp:586] norm1 -> norm1
I1226 13:57:53.415441 91478 layer_factory.hpp:114] Creating layer conv2
I1226 13:57:53.415526 91478 net.cpp:178] Creating Layer conv2
I1226 13:57:53.405122 92037 net.cpp:228] Setting up norm1
I1226 13:57:53.415560 91478 net.cpp:612] conv2 <- pool1
I1226 13:57:53.415604 91478 net.cpp:586] conv2 -> conv2
I1226 13:57:53.405180 92037 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 13:57:53.405205 92037 net.cpp:243] Memory required for data: 131301120
I1226 13:57:53.405236 92037 layer_factory.hpp:114] Creating layer pool1
I1226 13:57:53.405330 92037 net.cpp:178] Creating Layer pool1
I1226 13:57:53.405366 92037 net.cpp:612] pool1 <- norm1
I1226 13:57:53.405408 92037 net.cpp:586] pool1 -> pool1
I1226 13:57:53.405510 92037 net.cpp:228] Setting up pool1
I1226 13:57:53.405560 92037 net.cpp:235] Top shape: 32 96 27 27 (2239488)
I1226 13:57:53.405585 92037 net.cpp:243] Memory required for data: 140259072
I1226 13:57:53.405614 92037 layer_factory.hpp:114] Creating layer conv2
I1226 13:57:53.405694 92037 net.cpp:178] Creating Layer conv2
I1226 13:57:53.405725 92037 net.cpp:612] conv2 <- pool1
I1226 13:57:53.405766 92037 net.cpp:586] conv2 -> conv2
I1226 13:57:53.404423 94427 net.cpp:228] Setting up conv1
I1226 13:57:53.404536 94427 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 13:57:53.404566 94427 net.cpp:243] Memory required for data: 56958720
I1226 13:57:53.404680 94427 layer_factory.hpp:114] Creating layer relu1
I1226 13:57:53.404836 94427 net.cpp:178] Creating Layer relu1
I1226 13:57:53.404947 94427 net.cpp:612] relu1 <- conv1
I1226 13:57:53.404991 94427 net.cpp:573] relu1 -> conv1 (in-place)
I1226 13:57:53.405174 94427 net.cpp:228] Setting up relu1
I1226 13:57:53.405269 94427 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 13:57:53.405293 94427 net.cpp:243] Memory required for data: 94129920
I1226 13:57:53.405323 94427 layer_factory.hpp:114] Creating layer norm1
I1226 13:57:53.405428 94427 net.cpp:178] Creating Layer norm1
I1226 13:57:53.405472 94427 net.cpp:612] norm1 <- conv1
I1226 13:57:53.405511 94427 net.cpp:586] norm1 -> norm1
I1226 13:57:53.405611 94427 net.cpp:228] Setting up norm1
I1226 13:57:53.405706 94427 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 13:57:53.405731 94427 net.cpp:243] Memory required for data: 131301120
I1226 13:57:53.405761 94427 layer_factory.hpp:114] Creating layer pool1
I1226 13:57:53.405819 94427 net.cpp:178] Creating Layer pool1
I1226 13:57:53.405910 94427 net.cpp:612] pool1 <- norm1
I1226 13:57:53.405956 94427 net.cpp:586] pool1 -> pool1
I1226 13:57:53.406049 94427 net.cpp:228] Setting up pool1
I1226 13:57:53.406095 94427 net.cpp:235] Top shape: 32 96 27 27 (2239488)
I1226 13:57:53.406126 94427 net.cpp:243] Memory required for data: 140259072
I1226 13:57:53.406152 94427 layer_factory.hpp:114] Creating layer conv2
I1226 13:57:53.406222 94427 net.cpp:178] Creating Layer conv2
I1226 13:57:53.406250 94427 net.cpp:612] conv2 <- pool1
I1226 13:57:53.406299 94427 net.cpp:586] conv2 -> conv2
I1226 13:57:53.546221 93154 net.cpp:228] Setting up conv3
I1226 13:57:53.546335 93154 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:57:53.546365 93154 net.cpp:243] Memory required for data: 225767168
I1226 13:57:53.546473 93154 layer_factory.hpp:114] Creating layer relu3
I1226 13:57:53.546566 93154 net.cpp:178] Creating Layer relu3
I1226 13:57:53.546617 93154 net.cpp:612] relu3 <- conv3
I1226 13:57:53.546663 93154 net.cpp:573] relu3 -> conv3 (in-place)
I1226 13:57:53.546756 93154 net.cpp:228] Setting up relu3
I1226 13:57:53.546804 93154 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:57:53.546833 93154 net.cpp:243] Memory required for data: 234073856
I1226 13:57:53.546864 93154 layer_factory.hpp:114] Creating layer conv4
I1226 13:57:53.546946 93154 net.cpp:178] Creating Layer conv4
I1226 13:57:53.546977 93154 net.cpp:612] conv4 <- conv3
I1226 13:57:53.547034 93154 net.cpp:586] conv4 -> conv4
I1226 13:57:53.588713 92257 net.cpp:228] Setting up conv2
I1226 13:57:53.588835 92257 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 13:57:53.588874 92257 net.cpp:243] Memory required for data: 164146944
I1226 13:57:53.588981 92257 layer_factory.hpp:114] Creating layer relu2
I1226 13:57:53.589061 92257 net.cpp:178] Creating Layer relu2
I1226 13:57:53.589103 92257 net.cpp:612] relu2 <- conv2
I1226 13:57:53.589169 92257 net.cpp:573] relu2 -> conv2 (in-place)
I1226 13:57:53.589290 92257 net.cpp:228] Setting up relu2
I1226 13:57:53.589418 92257 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 13:57:53.589457 92257 net.cpp:243] Memory required for data: 188034816
I1226 13:57:53.589498 92257 layer_factory.hpp:114] Creating layer norm2
I1226 13:57:53.589576 92257 net.cpp:178] Creating Layer norm2
I1226 13:57:53.589624 92257 net.cpp:612] norm2 <- conv2
I1226 13:57:53.589696 92257 net.cpp:586] norm2 -> norm2
I1226 13:57:53.589833 92257 net.cpp:228] Setting up norm2
I1226 13:57:53.589903 92257 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 13:57:53.589936 92257 net.cpp:243] Memory required for data: 211922688
I1226 13:57:53.589974 92257 layer_factory.hpp:114] Creating layer pool2
I1226 13:57:53.590052 92257 net.cpp:178] Creating Layer pool2
I1226 13:57:53.590096 92257 net.cpp:612] pool2 <- norm2
I1226 13:57:53.590143 92257 net.cpp:586] pool2 -> pool2
I1226 13:57:53.590257 92257 net.cpp:228] Setting up pool2
I1226 13:57:53.590338 92257 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 13:57:53.590478 92257 net.cpp:243] Memory required for data: 217460480
I1226 13:57:53.590524 92257 layer_factory.hpp:114] Creating layer conv3
I1226 13:57:53.590636 92257 net.cpp:178] Creating Layer conv3
I1226 13:57:53.590677 92257 net.cpp:612] conv3 <- pool2
I1226 13:57:53.590746 92257 net.cpp:586] conv3 -> conv3
I1226 13:57:53.632504 92037 net.cpp:228] Setting up conv2
I1226 13:57:53.632609 92037 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 13:57:53.632638 92037 net.cpp:243] Memory required for data: 164146944
I1226 13:57:53.632714 92037 layer_factory.hpp:114] Creating layer relu2
I1226 13:57:53.632784 92037 net.cpp:178] Creating Layer relu2
I1226 13:57:53.632822 92037 net.cpp:612] relu2 <- conv2
I1226 13:57:53.632865 92037 net.cpp:573] relu2 -> conv2 (in-place)
I1226 13:57:53.633733 92037 net.cpp:228] Setting up relu2
I1226 13:57:53.633831 92037 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 13:57:53.633857 92037 net.cpp:243] Memory required for data: 188034816
I1226 13:57:53.633895 92037 layer_factory.hpp:114] Creating layer norm2
I1226 13:57:53.633986 92037 net.cpp:178] Creating Layer norm2
I1226 13:57:53.634042 92037 net.cpp:612] norm2 <- conv2
I1226 13:57:53.634093 92037 net.cpp:586] norm2 -> norm2
I1226 13:57:53.634210 92037 net.cpp:228] Setting up norm2
I1226 13:57:53.634276 92037 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 13:57:53.634304 92037 net.cpp:243] Memory required for data: 211922688
I1226 13:57:53.634358 92037 layer_factory.hpp:114] Creating layer pool2
I1226 13:57:53.634412 92037 net.cpp:178] Creating Layer pool2
I1226 13:57:53.634440 92037 net.cpp:612] pool2 <- norm2
I1226 13:57:53.635143 92037 net.cpp:586] pool2 -> pool2
I1226 13:57:53.635296 92037 net.cpp:228] Setting up pool2
I1226 13:57:53.635607 92037 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 13:57:53.635743 92037 net.cpp:243] Memory required for data: 217460480
I1226 13:57:53.635784 92037 layer_factory.hpp:114] Creating layer conv3
I1226 13:57:53.635882 92037 net.cpp:178] Creating Layer conv3
I1226 13:57:53.635929 92037 net.cpp:612] conv3 <- pool2
I1226 13:57:53.635989 92037 net.cpp:586] conv3 -> conv3
I1226 13:57:53.641964 91481 net.cpp:228] Setting up conv2
I1226 13:57:53.642079 91481 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 13:57:53.642110 91481 net.cpp:243] Memory required for data: 164146944
I1226 13:57:53.642189 91481 layer_factory.hpp:114] Creating layer relu2
I1226 13:57:53.642252 91481 net.cpp:178] Creating Layer relu2
I1226 13:57:53.642292 91481 net.cpp:612] relu2 <- conv2
I1226 13:57:53.642333 91481 net.cpp:573] relu2 -> conv2 (in-place)
I1226 13:57:53.642436 91481 net.cpp:228] Setting up relu2
I1226 13:57:53.642487 91481 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 13:57:53.642513 91481 net.cpp:243] Memory required for data: 188034816
I1226 13:57:53.642541 91481 layer_factory.hpp:114] Creating layer norm2
I1226 13:57:53.642612 91481 net.cpp:178] Creating Layer norm2
I1226 13:57:53.642645 91481 net.cpp:612] norm2 <- conv2
I1226 13:57:53.642683 91481 net.cpp:586] norm2 -> norm2
I1226 13:57:53.642771 91481 net.cpp:228] Setting up norm2
I1226 13:57:53.642819 91481 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 13:57:53.642853 91481 net.cpp:243] Memory required for data: 211922688
I1226 13:57:53.642911 91481 layer_factory.hpp:114] Creating layer pool2
I1226 13:57:53.642966 91481 net.cpp:178] Creating Layer pool2
I1226 13:57:53.642997 91481 net.cpp:612] pool2 <- norm2
I1226 13:57:53.643049 91481 net.cpp:586] pool2 -> pool2
I1226 13:57:53.643137 91481 net.cpp:228] Setting up pool2
I1226 13:57:53.643180 91481 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 13:57:53.643308 91481 net.cpp:243] Memory required for data: 217460480
I1226 13:57:53.643344 91481 layer_factory.hpp:114] Creating layer conv3
I1226 13:57:53.643429 91481 net.cpp:178] Creating Layer conv3
I1226 13:57:53.643463 91481 net.cpp:612] conv3 <- pool2
I1226 13:57:53.643507 91481 net.cpp:586] conv3 -> conv3
I1226 13:57:53.663254 91478 net.cpp:228] Setting up conv2
I1226 13:57:53.663359 91478 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 13:57:53.663384 91478 net.cpp:243] Memory required for data: 164146944
I1226 13:57:53.663481 91478 layer_factory.hpp:114] Creating layer relu2
I1226 13:57:53.663564 91478 net.cpp:178] Creating Layer relu2
I1226 13:57:53.663602 91478 net.cpp:612] relu2 <- conv2
I1226 13:57:53.663640 91478 net.cpp:573] relu2 -> conv2 (in-place)
I1226 13:57:53.663725 91478 net.cpp:228] Setting up relu2
I1226 13:57:53.663765 91478 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 13:57:53.663787 91478 net.cpp:243] Memory required for data: 188034816
I1226 13:57:53.663815 91478 layer_factory.hpp:114] Creating layer norm2
I1226 13:57:53.663871 91478 net.cpp:178] Creating Layer norm2
I1226 13:57:53.663900 91478 net.cpp:612] norm2 <- conv2
I1226 13:57:53.663935 91478 net.cpp:586] norm2 -> norm2
I1226 13:57:53.664011 91478 net.cpp:228] Setting up norm2
I1226 13:57:53.664057 91478 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 13:57:53.664079 91478 net.cpp:243] Memory required for data: 211922688
I1226 13:57:53.664106 91478 layer_factory.hpp:114] Creating layer pool2
I1226 13:57:53.664149 91478 net.cpp:178] Creating Layer pool2
I1226 13:57:53.664182 91478 net.cpp:612] pool2 <- norm2
I1226 13:57:53.664249 91478 net.cpp:586] pool2 -> pool2
I1226 13:57:53.664340 91478 net.cpp:228] Setting up pool2
I1226 13:57:53.664384 91478 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 13:57:53.664505 91478 net.cpp:243] Memory required for data: 217460480
I1226 13:57:53.664536 91478 layer_factory.hpp:114] Creating layer conv3
I1226 13:57:53.664616 91478 net.cpp:178] Creating Layer conv3
I1226 13:57:53.664649 91478 net.cpp:612] conv3 <- pool2
I1226 13:57:53.664691 91478 net.cpp:586] conv3 -> conv3
I1226 13:57:53.674937 96716 net.cpp:228] Setting up conv1
I1226 13:57:53.675058 96716 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 13:57:53.675094 96716 net.cpp:243] Memory required for data: 56958720
I1226 13:57:53.675215 96716 layer_factory.hpp:114] Creating layer relu1
I1226 13:57:53.675314 96716 net.cpp:178] Creating Layer relu1
I1226 13:57:53.675359 96716 net.cpp:612] relu1 <- conv1
I1226 13:57:53.675405 96716 net.cpp:573] relu1 -> conv1 (in-place)
I1226 13:57:53.675525 96716 net.cpp:228] Setting up relu1
I1226 13:57:53.675586 96716 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 13:57:53.675627 96716 net.cpp:243] Memory required for data: 94129920
I1226 13:57:53.675662 96716 layer_factory.hpp:114] Creating layer norm1
I1226 13:57:53.675737 96716 net.cpp:178] Creating Layer norm1
I1226 13:57:53.675775 96716 net.cpp:612] norm1 <- conv1
I1226 13:57:53.675844 96716 net.cpp:586] norm1 -> norm1
I1226 13:57:53.675956 96716 net.cpp:228] Setting up norm1
I1226 13:57:53.672178 94427 net.cpp:228] Setting up conv2
I1226 13:57:53.676014 96716 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 13:57:53.672291 94427 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 13:57:53.672317 94427 net.cpp:243] Memory required for data: 164146944
I1226 13:57:53.676039 96716 net.cpp:243] Memory required for data: 131301120
I1226 13:57:53.676071 96716 layer_factory.hpp:114] Creating layer pool1
I1226 13:57:53.672413 94427 layer_factory.hpp:114] Creating layer relu2
I1226 13:57:53.676126 96716 net.cpp:178] Creating Layer pool1
I1226 13:57:53.672489 94427 net.cpp:178] Creating Layer relu2
I1226 13:57:53.676161 96716 net.cpp:612] pool1 <- norm1
I1226 13:57:53.672533 94427 net.cpp:612] relu2 <- conv2
I1226 13:57:53.676208 96716 net.cpp:586] pool1 -> pool1
I1226 13:57:53.672577 94427 net.cpp:573] relu2 -> conv2 (in-place)
I1226 13:57:53.672663 94427 net.cpp:228] Setting up relu2
I1226 13:57:53.672703 94427 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 13:57:53.672734 94427 net.cpp:243] Memory required for data: 188034816
I1226 13:57:53.672762 94427 layer_factory.hpp:114] Creating layer norm2
I1226 13:57:53.672809 94427 net.cpp:178] Creating Layer norm2
I1226 13:57:53.676702 96716 net.cpp:228] Setting up pool1
I1226 13:57:53.672839 94427 net.cpp:612] norm2 <- conv2
I1226 13:57:53.676867 96716 net.cpp:235] Top shape: 32 96 27 27 (2239488)
I1226 13:57:53.672883 94427 net.cpp:586] norm2 -> norm2
I1226 13:57:53.676947 96716 net.cpp:243] Memory required for data: 140259072
I1226 13:57:53.672963 94427 net.cpp:228] Setting up norm2
I1226 13:57:53.676987 96716 layer_factory.hpp:114] Creating layer conv2
I1226 13:57:53.673009 94427 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 13:57:53.677099 96716 net.cpp:178] Creating Layer conv2
I1226 13:57:53.673032 94427 net.cpp:243] Memory required for data: 211922688
I1226 13:57:53.677141 96716 net.cpp:612] conv2 <- pool1
I1226 13:57:53.673058 94427 layer_factory.hpp:114] Creating layer pool2
I1226 13:57:53.677191 96716 net.cpp:586] conv2 -> conv2
I1226 13:57:53.673118 94427 net.cpp:178] Creating Layer pool2
I1226 13:57:53.673144 94427 net.cpp:612] pool2 <- norm2
I1226 13:57:53.673177 94427 net.cpp:586] pool2 -> pool2
I1226 13:57:53.673254 94427 net.cpp:228] Setting up pool2
I1226 13:57:53.673293 94427 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 13:57:53.686501 94427 net.cpp:243] Memory required for data: 217460480
I1226 13:57:53.686561 94427 layer_factory.hpp:114] Creating layer conv3
I1226 13:57:53.686688 94427 net.cpp:178] Creating Layer conv3
I1226 13:57:53.686731 94427 net.cpp:612] conv3 <- pool2
I1226 13:57:53.686780 94427 net.cpp:586] conv3 -> conv3
I1226 13:57:53.702957 95085 net.cpp:228] Setting up conv1
I1226 13:57:53.703089 95085 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 13:57:53.703121 95085 net.cpp:243] Memory required for data: 56958720
I1226 13:57:53.703238 95085 layer_factory.hpp:114] Creating layer relu1
I1226 13:57:53.703323 95085 net.cpp:178] Creating Layer relu1
I1226 13:57:53.703358 95085 net.cpp:612] relu1 <- conv1
I1226 13:57:53.703394 95085 net.cpp:573] relu1 -> conv1 (in-place)
I1226 13:57:53.703516 95085 net.cpp:228] Setting up relu1
I1226 13:57:53.703568 95085 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 13:57:53.703594 95085 net.cpp:243] Memory required for data: 94129920
I1226 13:57:53.703624 95085 layer_factory.hpp:114] Creating layer norm1
I1226 13:57:53.703706 95085 net.cpp:178] Creating Layer norm1
I1226 13:57:53.703747 95085 net.cpp:612] norm1 <- conv1
I1226 13:57:53.703784 95085 net.cpp:586] norm1 -> norm1
I1226 13:57:53.703927 95085 net.cpp:228] Setting up norm1
I1226 13:57:53.703982 95085 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 13:57:53.704007 95085 net.cpp:243] Memory required for data: 131301120
I1226 13:57:53.704035 95085 layer_factory.hpp:114] Creating layer pool1
I1226 13:57:53.704107 95085 net.cpp:178] Creating Layer pool1
I1226 13:57:53.704143 95085 net.cpp:612] pool1 <- norm1
I1226 13:57:53.704180 95085 net.cpp:586] pool1 -> pool1
I1226 13:57:53.704285 95085 net.cpp:228] Setting up pool1
I1226 13:57:53.704331 95085 net.cpp:235] Top shape: 32 96 27 27 (2239488)
I1226 13:57:53.704356 95085 net.cpp:243] Memory required for data: 140259072
I1226 13:57:53.704385 95085 layer_factory.hpp:114] Creating layer conv2
I1226 13:57:53.704466 95085 net.cpp:178] Creating Layer conv2
I1226 13:57:53.704501 95085 net.cpp:612] conv2 <- pool1
I1226 13:57:53.704542 95085 net.cpp:586] conv2 -> conv2
I1226 13:57:53.706632 93154 net.cpp:228] Setting up conv4
I1226 13:57:53.706740 93154 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:57:53.706765 93154 net.cpp:243] Memory required for data: 242380544
I1226 13:57:53.706851 93154 layer_factory.hpp:114] Creating layer relu4
I1226 13:57:53.706938 93154 net.cpp:178] Creating Layer relu4
I1226 13:57:53.706976 93154 net.cpp:612] relu4 <- conv4
I1226 13:57:53.707046 93154 net.cpp:573] relu4 -> conv4 (in-place)
I1226 13:57:53.707177 93154 net.cpp:228] Setting up relu4
I1226 13:57:53.707226 93154 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:57:53.707252 93154 net.cpp:243] Memory required for data: 250687232
I1226 13:57:53.707283 93154 layer_factory.hpp:114] Creating layer conv5
I1226 13:57:53.707402 93154 net.cpp:178] Creating Layer conv5
I1226 13:57:53.707450 93154 net.cpp:612] conv5 <- conv4
I1226 13:57:53.707500 93154 net.cpp:586] conv5 -> conv5
I1226 13:57:53.713793 97230 net.cpp:228] Setting up conv1
I1226 13:57:53.713939 97230 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 13:57:53.714000 97230 net.cpp:243] Memory required for data: 56958720
I1226 13:57:53.714170 97230 layer_factory.hpp:114] Creating layer relu1
I1226 13:57:53.714303 97230 net.cpp:178] Creating Layer relu1
I1226 13:57:53.714365 97230 net.cpp:612] relu1 <- conv1
I1226 13:57:53.714429 97230 net.cpp:573] relu1 -> conv1 (in-place)
I1226 13:57:53.714622 97230 net.cpp:228] Setting up relu1
I1226 13:57:53.714756 97230 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 13:57:53.714817 97230 net.cpp:243] Memory required for data: 94129920
I1226 13:57:53.714869 97230 layer_factory.hpp:114] Creating layer norm1
I1226 13:57:53.714989 97230 net.cpp:178] Creating Layer norm1
I1226 13:57:53.715153 97230 net.cpp:612] norm1 <- conv1
I1226 13:57:53.715229 97230 net.cpp:586] norm1 -> norm1
I1226 13:57:53.715406 97230 net.cpp:228] Setting up norm1
I1226 13:57:53.715984 97230 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 13:57:53.716140 97230 net.cpp:243] Memory required for data: 131301120
I1226 13:57:53.716251 97230 layer_factory.hpp:114] Creating layer pool1
I1226 13:57:53.716572 97230 net.cpp:178] Creating Layer pool1
I1226 13:57:53.716756 97230 net.cpp:612] pool1 <- norm1
I1226 13:57:53.716889 97230 net.cpp:586] pool1 -> pool1
I1226 13:57:53.717139 97230 net.cpp:228] Setting up pool1
I1226 13:57:53.717630 97230 net.cpp:235] Top shape: 32 96 27 27 (2239488)
I1226 13:57:53.717815 97230 net.cpp:243] Memory required for data: 140259072
I1226 13:57:53.718024 97230 layer_factory.hpp:114] Creating layer conv2
I1226 13:57:53.718400 97230 net.cpp:178] Creating Layer conv2
I1226 13:57:53.718477 97230 net.cpp:612] conv2 <- pool1
I1226 13:57:53.718555 97230 net.cpp:586] conv2 -> conv2
I1226 13:57:53.814365 93154 net.cpp:228] Setting up conv5
I1226 13:57:53.814504 93154 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 13:57:53.814532 93154 net.cpp:243] Memory required for data: 256225024
I1226 13:57:53.814631 93154 layer_factory.hpp:114] Creating layer relu5
I1226 13:57:53.814702 93154 net.cpp:178] Creating Layer relu5
I1226 13:57:53.814734 93154 net.cpp:612] relu5 <- conv5
I1226 13:57:53.814800 93154 net.cpp:573] relu5 -> conv5 (in-place)
I1226 13:57:53.814906 93154 net.cpp:228] Setting up relu5
I1226 13:57:53.814954 93154 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 13:57:53.814986 93154 net.cpp:243] Memory required for data: 261762816
I1226 13:57:53.815021 93154 layer_factory.hpp:114] Creating layer pool5
I1226 13:57:53.815089 93154 net.cpp:178] Creating Layer pool5
I1226 13:57:53.815125 93154 net.cpp:612] pool5 <- conv5
I1226 13:57:53.815163 93154 net.cpp:586] pool5 -> pool5
I1226 13:57:53.815263 93154 net.cpp:228] Setting up pool5
I1226 13:57:53.815307 93154 net.cpp:235] Top shape: 32 256 6 6 (294912)
I1226 13:57:53.815330 93154 net.cpp:243] Memory required for data: 262942464
I1226 13:57:53.815356 93154 layer_factory.hpp:114] Creating layer fc6
I1226 13:57:53.815455 93154 net.cpp:178] Creating Layer fc6
I1226 13:57:53.815497 93154 net.cpp:612] fc6 <- pool5
I1226 13:57:53.815551 93154 net.cpp:586] fc6 -> fc6
I1226 13:57:53.889019 92257 net.cpp:228] Setting up conv3
I1226 13:57:53.889134 92257 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:57:53.889171 92257 net.cpp:243] Memory required for data: 225767168
I1226 13:57:53.889262 92257 layer_factory.hpp:114] Creating layer relu3
I1226 13:57:53.889396 92257 net.cpp:178] Creating Layer relu3
I1226 13:57:53.889703 92257 net.cpp:612] relu3 <- conv3
I1226 13:57:53.889987 92257 net.cpp:573] relu3 -> conv3 (in-place)
I1226 13:57:53.890549 92257 net.cpp:228] Setting up relu3
I1226 13:57:53.890662 92257 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:57:53.890696 92257 net.cpp:243] Memory required for data: 234073856
I1226 13:57:53.890734 92257 layer_factory.hpp:114] Creating layer conv4
I1226 13:57:53.890847 92257 net.cpp:178] Creating Layer conv4
I1226 13:57:53.890892 92257 net.cpp:612] conv4 <- conv3
I1226 13:57:53.890959 92257 net.cpp:586] conv4 -> conv4
I1226 13:57:53.907444 92037 net.cpp:228] Setting up conv3
I1226 13:57:53.907585 92037 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:57:53.907614 92037 net.cpp:243] Memory required for data: 225767168
I1226 13:57:53.907690 92037 layer_factory.hpp:114] Creating layer relu3
I1226 13:57:53.907760 92037 net.cpp:178] Creating Layer relu3
I1226 13:57:53.907799 92037 net.cpp:612] relu3 <- conv3
I1226 13:57:53.907866 92037 net.cpp:573] relu3 -> conv3 (in-place)
I1226 13:57:53.907976 92037 net.cpp:228] Setting up relu3
I1226 13:57:53.908049 92037 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:57:53.908078 92037 net.cpp:243] Memory required for data: 234073856
I1226 13:57:53.908113 92037 layer_factory.hpp:114] Creating layer conv4
I1226 13:57:53.908211 92037 net.cpp:178] Creating Layer conv4
I1226 13:57:53.908247 92037 net.cpp:612] conv4 <- conv3
I1226 13:57:53.908300 92037 net.cpp:586] conv4 -> conv4
I1226 13:57:53.952486 91481 net.cpp:228] Setting up conv3
I1226 13:57:53.952607 91481 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:57:53.952638 91481 net.cpp:243] Memory required for data: 225767168
I1226 13:57:53.953042 91481 layer_factory.hpp:114] Creating layer relu3
I1226 13:57:53.953147 91481 net.cpp:178] Creating Layer relu3
I1226 13:57:53.953241 91481 net.cpp:612] relu3 <- conv3
I1226 13:57:53.953341 91481 net.cpp:573] relu3 -> conv3 (in-place)
I1226 13:57:53.953677 91481 net.cpp:228] Setting up relu3
I1226 13:57:53.953766 91481 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:57:53.953800 91481 net.cpp:243] Memory required for data: 234073856
I1226 13:57:53.953840 91481 layer_factory.hpp:114] Creating layer conv4
I1226 13:57:53.954306 91481 net.cpp:178] Creating Layer conv4
I1226 13:57:53.954413 91481 net.cpp:612] conv4 <- conv3
I1226 13:57:53.954552 91481 net.cpp:586] conv4 -> conv4
I1226 13:57:53.980415 91478 net.cpp:228] Setting up conv3
I1226 13:57:53.980525 91478 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:57:53.980553 91478 net.cpp:243] Memory required for data: 225767168
I1226 13:57:53.980648 91478 layer_factory.hpp:114] Creating layer relu3
I1226 13:57:53.980716 91478 net.cpp:178] Creating Layer relu3
I1226 13:57:53.980746 91478 net.cpp:612] relu3 <- conv3
I1226 13:57:53.980794 91478 net.cpp:573] relu3 -> conv3 (in-place)
I1226 13:57:53.980901 91478 net.cpp:228] Setting up relu3
I1226 13:57:53.980947 91478 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:57:53.980972 91478 net.cpp:243] Memory required for data: 234073856
I1226 13:57:53.980998 91478 layer_factory.hpp:114] Creating layer conv4
I1226 13:57:53.981089 91478 net.cpp:178] Creating Layer conv4
I1226 13:57:53.981125 91478 net.cpp:612] conv4 <- conv3
I1226 13:57:53.981166 91478 net.cpp:586] conv4 -> conv4
I1226 13:57:53.983839 94427 net.cpp:228] Setting up conv3
I1226 13:57:53.983952 94427 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:57:53.983978 94427 net.cpp:243] Memory required for data: 225767168
I1226 13:57:53.984046 94427 layer_factory.hpp:114] Creating layer relu3
I1226 13:57:53.984128 94427 net.cpp:178] Creating Layer relu3
I1226 13:57:53.984163 94427 net.cpp:612] relu3 <- conv3
I1226 13:57:53.984200 94427 net.cpp:573] relu3 -> conv3 (in-place)
I1226 13:57:53.984290 94427 net.cpp:228] Setting up relu3
I1226 13:57:53.984336 94427 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:57:53.984359 94427 net.cpp:243] Memory required for data: 234073856
I1226 13:57:53.984419 94427 layer_factory.hpp:114] Creating layer conv4
I1226 13:57:53.984499 94427 net.cpp:178] Creating Layer conv4
I1226 13:57:53.984532 94427 net.cpp:612] conv4 <- conv3
I1226 13:57:53.984578 94427 net.cpp:586] conv4 -> conv4
I1226 13:57:54.147742 92037 net.cpp:228] Setting up conv4
I1226 13:57:54.147850 92037 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:57:54.147903 92037 net.cpp:243] Memory required for data: 242380544
I1226 13:57:54.147972 92037 layer_factory.hpp:114] Creating layer relu4
I1226 13:57:54.148037 92037 net.cpp:178] Creating Layer relu4
I1226 13:57:54.148073 92037 net.cpp:612] relu4 <- conv4
I1226 13:57:54.148501 92037 net.cpp:573] relu4 -> conv4 (in-place)
I1226 13:57:54.148664 92037 net.cpp:228] Setting up relu4
I1226 13:57:54.148736 92037 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:57:54.148802 92037 net.cpp:243] Memory required for data: 250687232
I1226 13:57:54.148840 92037 layer_factory.hpp:114] Creating layer conv5
I1226 13:57:54.148916 92037 net.cpp:178] Creating Layer conv5
I1226 13:57:54.148952 92037 net.cpp:612] conv5 <- conv4
I1226 13:57:54.149024 92037 net.cpp:586] conv5 -> conv5
I1226 13:57:54.140877 94427 net.cpp:228] Setting up conv4
I1226 13:57:54.140985 94427 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:57:54.141012 94427 net.cpp:243] Memory required for data: 242380544
I1226 13:57:54.141096 94427 layer_factory.hpp:114] Creating layer relu4
I1226 13:57:54.141180 94427 net.cpp:178] Creating Layer relu4
I1226 13:57:54.141219 94427 net.cpp:612] relu4 <- conv4
I1226 13:57:54.141258 94427 net.cpp:573] relu4 -> conv4 (in-place)
I1226 13:57:54.141360 94427 net.cpp:228] Setting up relu4
I1226 13:57:54.141443 94427 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:57:54.141471 94427 net.cpp:243] Memory required for data: 250687232
I1226 13:57:54.141500 94427 layer_factory.hpp:114] Creating layer conv5
I1226 13:57:54.141573 94427 net.cpp:178] Creating Layer conv5
I1226 13:57:54.141686 94427 net.cpp:612] conv5 <- conv4
I1226 13:57:54.141737 94427 net.cpp:586] conv5 -> conv5
I1226 13:57:54.157737 92257 net.cpp:228] Setting up conv4
I1226 13:57:54.157855 92257 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:57:54.157887 92257 net.cpp:243] Memory required for data: 242380544
I1226 13:57:54.157996 92257 layer_factory.hpp:114] Creating layer relu4
I1226 13:57:54.158098 92257 net.cpp:178] Creating Layer relu4
I1226 13:57:54.158149 92257 net.cpp:612] relu4 <- conv4
I1226 13:57:54.158212 92257 net.cpp:573] relu4 -> conv4 (in-place)
I1226 13:57:54.158366 92257 net.cpp:228] Setting up relu4
I1226 13:57:54.158445 92257 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:57:54.158516 92257 net.cpp:243] Memory required for data: 250687232
I1226 13:57:54.158584 92257 layer_factory.hpp:114] Creating layer conv5
I1226 13:57:54.158715 92257 net.cpp:178] Creating Layer conv5
I1226 13:57:54.158773 92257 net.cpp:612] conv5 <- conv4
I1226 13:57:54.158845 92257 net.cpp:586] conv5 -> conv5
I1226 13:57:54.265672 91478 net.cpp:228] Setting up conv4
I1226 13:57:54.265784 91478 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:57:54.265815 91478 net.cpp:243] Memory required for data: 242380544
I1226 13:57:54.265902 91478 layer_factory.hpp:114] Creating layer relu4
I1226 13:57:54.266008 91478 net.cpp:178] Creating Layer relu4
I1226 13:57:54.266119 91478 net.cpp:612] relu4 <- conv4
I1226 13:57:54.266196 91478 net.cpp:573] relu4 -> conv4 (in-place)
I1226 13:57:54.266412 91478 net.cpp:228] Setting up relu4
I1226 13:57:54.266834 91478 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:57:54.266880 91478 net.cpp:243] Memory required for data: 250687232
I1226 13:57:54.266953 91478 layer_factory.hpp:114] Creating layer conv5
I1226 13:57:54.267134 91478 net.cpp:178] Creating Layer conv5
I1226 13:57:54.267233 91478 net.cpp:612] conv5 <- conv4
I1226 13:57:54.267323 91478 net.cpp:586] conv5 -> conv5
I1226 13:57:54.256490 91481 net.cpp:228] Setting up conv4
I1226 13:57:54.256635 91481 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:57:54.256708 91481 net.cpp:243] Memory required for data: 242380544
I1226 13:57:54.256834 91481 layer_factory.hpp:114] Creating layer relu4
I1226 13:57:54.257094 91481 net.cpp:178] Creating Layer relu4
I1226 13:57:54.257170 91481 net.cpp:612] relu4 <- conv4
I1226 13:57:54.257616 91481 net.cpp:573] relu4 -> conv4 (in-place)
I1226 13:57:54.258041 91481 net.cpp:228] Setting up relu4
I1226 13:57:54.258148 91481 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:57:54.258205 91481 net.cpp:243] Memory required for data: 250687232
I1226 13:57:54.258263 91481 layer_factory.hpp:114] Creating layer conv5
I1226 13:57:54.258522 91481 net.cpp:178] Creating Layer conv5
I1226 13:57:54.258900 91481 net.cpp:612] conv5 <- conv4
I1226 13:57:54.259030 91481 net.cpp:586] conv5 -> conv5
I1226 13:57:54.253089 94427 net.cpp:228] Setting up conv5
I1226 13:57:54.253198 94427 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 13:57:54.253226 94427 net.cpp:243] Memory required for data: 256225024
I1226 13:57:54.253319 94427 layer_factory.hpp:114] Creating layer relu5
I1226 13:57:54.253407 94427 net.cpp:178] Creating Layer relu5
I1226 13:57:54.253442 94427 net.cpp:612] relu5 <- conv5
I1226 13:57:54.253496 94427 net.cpp:573] relu5 -> conv5 (in-place)
I1226 13:57:54.253587 94427 net.cpp:228] Setting up relu5
I1226 13:57:54.253630 94427 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 13:57:54.253654 94427 net.cpp:243] Memory required for data: 261762816
I1226 13:57:54.253682 94427 layer_factory.hpp:114] Creating layer pool5
I1226 13:57:54.253739 94427 net.cpp:178] Creating Layer pool5
I1226 13:57:54.253765 94427 net.cpp:612] pool5 <- conv5
I1226 13:57:54.253813 94427 net.cpp:586] pool5 -> pool5
I1226 13:57:54.253896 94427 net.cpp:228] Setting up pool5
I1226 13:57:54.253942 94427 net.cpp:235] Top shape: 32 256 6 6 (294912)
I1226 13:57:54.253970 94427 net.cpp:243] Memory required for data: 262942464
I1226 13:57:54.253998 94427 layer_factory.hpp:114] Creating layer fc6
I1226 13:57:54.254051 94427 net.cpp:178] Creating Layer fc6
I1226 13:57:54.254076 94427 net.cpp:612] fc6 <- pool5
I1226 13:57:54.254109 94427 net.cpp:586] fc6 -> fc6
I1226 13:57:54.309792 95085 net.cpp:228] Setting up conv2
I1226 13:57:54.309932 95085 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 13:57:54.309967 95085 net.cpp:243] Memory required for data: 164146944
I1226 13:57:54.310048 95085 layer_factory.hpp:114] Creating layer relu2
I1226 13:57:54.310145 95085 net.cpp:178] Creating Layer relu2
I1226 13:57:54.310194 95085 net.cpp:612] relu2 <- conv2
I1226 13:57:54.310238 95085 net.cpp:573] relu2 -> conv2 (in-place)
I1226 13:57:54.310340 95085 net.cpp:228] Setting up relu2
I1226 13:57:54.310395 95085 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 13:57:54.310423 95085 net.cpp:243] Memory required for data: 188034816
I1226 13:57:54.310456 95085 layer_factory.hpp:114] Creating layer norm2
I1226 13:57:54.310516 95085 net.cpp:178] Creating Layer norm2
I1226 13:57:54.310566 95085 net.cpp:612] norm2 <- conv2
I1226 13:57:54.310616 95085 net.cpp:586] norm2 -> norm2
I1226 13:57:54.310717 95085 net.cpp:228] Setting up norm2
I1226 13:57:54.310770 95085 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 13:57:54.310796 95085 net.cpp:243] Memory required for data: 211922688
I1226 13:57:54.310849 95085 layer_factory.hpp:114] Creating layer pool2
I1226 13:57:54.310922 95085 net.cpp:178] Creating Layer pool2
I1226 13:57:54.310962 95085 net.cpp:612] pool2 <- norm2
I1226 13:57:54.311000 95085 net.cpp:586] pool2 -> pool2
I1226 13:57:54.311094 95085 net.cpp:228] Setting up pool2
I1226 13:57:54.311141 95085 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 13:57:54.311267 95085 net.cpp:243] Memory required for data: 217460480
I1226 13:57:54.311305 95085 layer_factory.hpp:114] Creating layer conv3
I1226 13:57:54.311391 95085 net.cpp:178] Creating Layer conv3
I1226 13:57:54.311432 95085 net.cpp:612] conv3 <- pool2
I1226 13:57:54.311486 95085 net.cpp:586] conv3 -> conv3
I1226 13:57:54.325392 92037 net.cpp:228] Setting up conv5
I1226 13:57:54.325498 92037 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 13:57:54.325526 92037 net.cpp:243] Memory required for data: 256225024
I1226 13:57:54.325605 92037 layer_factory.hpp:114] Creating layer relu5
I1226 13:57:54.325671 92037 net.cpp:178] Creating Layer relu5
I1226 13:57:54.325707 92037 net.cpp:612] relu5 <- conv5
I1226 13:57:54.325767 92037 net.cpp:573] relu5 -> conv5 (in-place)
I1226 13:57:54.325863 92037 net.cpp:228] Setting up relu5
I1226 13:57:54.325907 92037 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 13:57:54.325930 92037 net.cpp:243] Memory required for data: 261762816
I1226 13:57:54.325961 92037 layer_factory.hpp:114] Creating layer pool5
I1226 13:57:54.326026 92037 net.cpp:178] Creating Layer pool5
I1226 13:57:54.326067 92037 net.cpp:612] pool5 <- conv5
I1226 13:57:54.326112 92037 net.cpp:586] pool5 -> pool5
I1226 13:57:54.326212 92037 net.cpp:228] Setting up pool5
I1226 13:57:54.326267 92037 net.cpp:235] Top shape: 32 256 6 6 (294912)
I1226 13:57:54.326289 92037 net.cpp:243] Memory required for data: 262942464
I1226 13:57:54.326344 92037 layer_factory.hpp:114] Creating layer fc6
I1226 13:57:54.326409 92037 net.cpp:178] Creating Layer fc6
I1226 13:57:54.326437 92037 net.cpp:612] fc6 <- pool5
I1226 13:57:54.326491 92037 net.cpp:586] fc6 -> fc6
I1226 13:57:54.371770 92257 net.cpp:228] Setting up conv5
I1226 13:57:54.371899 92257 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 13:57:54.371930 92257 net.cpp:243] Memory required for data: 256225024
I1226 13:57:54.372025 92257 layer_factory.hpp:114] Creating layer relu5
I1226 13:57:54.372095 92257 net.cpp:178] Creating Layer relu5
I1226 13:57:54.372131 92257 net.cpp:612] relu5 <- conv5
I1226 13:57:54.372205 92257 net.cpp:573] relu5 -> conv5 (in-place)
I1226 13:57:54.375174 92257 net.cpp:228] Setting up relu5
I1226 13:57:54.375355 92257 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 13:57:54.375422 92257 net.cpp:243] Memory required for data: 261762816
I1226 13:57:54.375468 92257 layer_factory.hpp:114] Creating layer pool5
I1226 13:57:54.375576 92257 net.cpp:178] Creating Layer pool5
I1226 13:57:54.375757 92257 net.cpp:612] pool5 <- conv5
I1226 13:57:54.375828 92257 net.cpp:586] pool5 -> pool5
I1226 13:57:54.375957 92257 net.cpp:228] Setting up pool5
I1226 13:57:54.376018 92257 net.cpp:235] Top shape: 32 256 6 6 (294912)
I1226 13:57:54.376046 92257 net.cpp:243] Memory required for data: 262942464
I1226 13:57:54.376080 92257 layer_factory.hpp:114] Creating layer fc6
I1226 13:57:54.376150 92257 net.cpp:178] Creating Layer fc6
I1226 13:57:54.376186 92257 net.cpp:612] fc6 <- pool5
I1226 13:57:54.376232 92257 net.cpp:586] fc6 -> fc6
I1226 13:57:54.461521 91481 net.cpp:228] Setting up conv5
I1226 13:57:54.461671 91481 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 13:57:54.461710 91481 net.cpp:243] Memory required for data: 256225024
I1226 13:57:54.461841 91481 layer_factory.hpp:114] Creating layer relu5
I1226 13:57:54.462085 91481 net.cpp:178] Creating Layer relu5
I1226 13:57:54.462594 91481 net.cpp:612] relu5 <- conv5
I1226 13:57:54.462733 91481 net.cpp:573] relu5 -> conv5 (in-place)
I1226 13:57:54.462929 91481 net.cpp:228] Setting up relu5
I1226 13:57:54.463039 91481 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 13:57:54.463266 91481 net.cpp:243] Memory required for data: 261762816
I1226 13:57:54.463310 91481 layer_factory.hpp:114] Creating layer pool5
I1226 13:57:54.463387 91481 net.cpp:178] Creating Layer pool5
I1226 13:57:54.463448 91481 net.cpp:612] pool5 <- conv5
I1226 13:57:54.463515 91481 net.cpp:586] pool5 -> pool5
I1226 13:57:54.463665 91481 net.cpp:228] Setting up pool5
I1226 13:57:54.479789 91478 net.cpp:228] Setting up conv5
I1226 13:57:54.479910 91478 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 13:57:54.479945 91478 net.cpp:243] Memory required for data: 256225024
I1226 13:57:54.480046 91478 layer_factory.hpp:114] Creating layer relu5
I1226 13:57:54.480145 91478 net.cpp:178] Creating Layer relu5
I1226 13:57:54.480331 91478 net.cpp:612] relu5 <- conv5
I1226 13:57:54.480386 91478 net.cpp:573] relu5 -> conv5 (in-place)
I1226 13:57:54.480492 91478 net.cpp:228] Setting up relu5
I1226 13:57:54.480552 91478 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 13:57:54.480583 91478 net.cpp:243] Memory required for data: 261762816
I1226 13:57:54.480621 91478 layer_factory.hpp:114] Creating layer pool5
I1226 13:57:54.480890 91478 net.cpp:178] Creating Layer pool5
I1226 13:57:54.480928 91478 net.cpp:612] pool5 <- conv5
I1226 13:57:54.480975 91478 net.cpp:586] pool5 -> pool5
I1226 13:57:54.481091 91478 net.cpp:228] Setting up pool5
I1226 13:57:54.481261 91478 net.cpp:235] Top shape: 32 256 6 6 (294912)
I1226 13:57:54.481324 91478 net.cpp:243] Memory required for data: 262942464
I1226 13:57:54.481529 91478 layer_factory.hpp:114] Creating layer fc6
I1226 13:57:54.481600 91478 net.cpp:178] Creating Layer fc6
I1226 13:57:54.481751 91478 net.cpp:612] fc6 <- pool5
I1226 13:57:54.481798 91478 net.cpp:586] fc6 -> fc6
I1226 13:57:54.476274 91481 net.cpp:235] Top shape: 32 256 6 6 (294912)
I1226 13:57:54.476364 91481 net.cpp:243] Memory required for data: 262942464
I1226 13:57:54.476446 91481 layer_factory.hpp:114] Creating layer fc6
I1226 13:57:54.476550 91481 net.cpp:178] Creating Layer fc6
I1226 13:57:54.476599 91481 net.cpp:612] fc6 <- pool5
I1226 13:57:54.476656 91481 net.cpp:586] fc6 -> fc6
I1226 13:57:54.582486 96716 net.cpp:228] Setting up conv2
I1226 13:57:54.582604 96716 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 13:57:54.582634 96716 net.cpp:243] Memory required for data: 164146944
I1226 13:57:54.582710 96716 layer_factory.hpp:114] Creating layer relu2
I1226 13:57:54.582809 96716 net.cpp:178] Creating Layer relu2
I1226 13:57:54.582846 96716 net.cpp:612] relu2 <- conv2
I1226 13:57:54.582908 96716 net.cpp:573] relu2 -> conv2 (in-place)
I1226 13:57:54.583032 96716 net.cpp:228] Setting up relu2
I1226 13:57:54.583096 96716 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 13:57:54.583122 96716 net.cpp:243] Memory required for data: 188034816
I1226 13:57:54.583154 96716 layer_factory.hpp:114] Creating layer norm2
I1226 13:57:54.583210 96716 net.cpp:178] Creating Layer norm2
I1226 13:57:54.583250 96716 net.cpp:612] norm2 <- conv2
I1226 13:57:54.583302 96716 net.cpp:586] norm2 -> norm2
I1226 13:57:54.583405 96716 net.cpp:228] Setting up norm2
I1226 13:57:54.583454 96716 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 13:57:54.583478 96716 net.cpp:243] Memory required for data: 211922688
I1226 13:57:54.583508 96716 layer_factory.hpp:114] Creating layer pool2
I1226 13:57:54.583554 96716 net.cpp:178] Creating Layer pool2
I1226 13:57:54.583590 96716 net.cpp:612] pool2 <- norm2
I1226 13:57:54.583640 96716 net.cpp:586] pool2 -> pool2
I1226 13:57:54.583732 96716 net.cpp:228] Setting up pool2
I1226 13:57:54.583777 96716 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 13:57:54.583927 96716 net.cpp:243] Memory required for data: 217460480
I1226 13:57:54.583963 96716 layer_factory.hpp:114] Creating layer conv3
I1226 13:57:54.584049 96716 net.cpp:178] Creating Layer conv3
I1226 13:57:54.584087 96716 net.cpp:612] conv3 <- pool2
I1226 13:57:54.584141 96716 net.cpp:586] conv3 -> conv3
I1226 13:57:54.617935 97230 net.cpp:228] Setting up conv2
I1226 13:57:54.618062 97230 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 13:57:54.618103 97230 net.cpp:243] Memory required for data: 164146944
I1226 13:57:54.618207 97230 layer_factory.hpp:114] Creating layer relu2
I1226 13:57:54.618306 97230 net.cpp:178] Creating Layer relu2
I1226 13:57:54.618367 97230 net.cpp:612] relu2 <- conv2
I1226 13:57:54.618456 97230 net.cpp:573] relu2 -> conv2 (in-place)
I1226 13:57:54.618580 97230 net.cpp:228] Setting up relu2
I1226 13:57:54.618656 97230 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 13:57:54.618690 97230 net.cpp:243] Memory required for data: 188034816
I1226 13:57:54.618767 97230 layer_factory.hpp:114] Creating layer norm2
I1226 13:57:54.618850 97230 net.cpp:178] Creating Layer norm2
I1226 13:57:54.618908 97230 net.cpp:612] norm2 <- conv2
I1226 13:57:54.618971 97230 net.cpp:586] norm2 -> norm2
I1226 13:57:54.619137 97230 net.cpp:228] Setting up norm2
I1226 13:57:54.619220 97230 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 13:57:54.619256 97230 net.cpp:243] Memory required for data: 211922688
I1226 13:57:54.619302 97230 layer_factory.hpp:114] Creating layer pool2
I1226 13:57:54.619371 97230 net.cpp:178] Creating Layer pool2
I1226 13:57:54.619424 97230 net.cpp:612] pool2 <- norm2
I1226 13:57:54.619503 97230 net.cpp:586] pool2 -> pool2
I1226 13:57:54.619648 97230 net.cpp:228] Setting up pool2
I1226 13:57:54.619745 97230 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 13:57:54.619913 97230 net.cpp:243] Memory required for data: 217460480
I1226 13:57:54.619966 97230 layer_factory.hpp:114] Creating layer conv3
I1226 13:57:54.620074 97230 net.cpp:178] Creating Layer conv3
I1226 13:57:54.620121 97230 net.cpp:612] conv3 <- pool2
I1226 13:57:54.620190 97230 net.cpp:586] conv3 -> conv3
I1226 13:57:55.185334 95085 net.cpp:228] Setting up conv3
I1226 13:57:55.185453 95085 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:57:55.185484 95085 net.cpp:243] Memory required for data: 225767168
I1226 13:57:55.185585 95085 layer_factory.hpp:114] Creating layer relu3
I1226 13:57:55.185736 95085 net.cpp:178] Creating Layer relu3
I1226 13:57:55.185780 95085 net.cpp:612] relu3 <- conv3
I1226 13:57:55.185842 95085 net.cpp:573] relu3 -> conv3 (in-place)
I1226 13:57:55.185953 95085 net.cpp:228] Setting up relu3
I1226 13:57:55.186020 95085 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:57:55.186048 95085 net.cpp:243] Memory required for data: 234073856
I1226 13:57:55.186087 95085 layer_factory.hpp:114] Creating layer conv4
I1226 13:57:55.186184 95085 net.cpp:178] Creating Layer conv4
I1226 13:57:55.186223 95085 net.cpp:612] conv4 <- conv3
I1226 13:57:55.186269 95085 net.cpp:586] conv4 -> conv4
I1226 13:57:55.460089 96716 net.cpp:228] Setting up conv3
I1226 13:57:55.460211 96716 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:57:55.460247 96716 net.cpp:243] Memory required for data: 225767168
I1226 13:57:55.460351 96716 layer_factory.hpp:114] Creating layer relu3
I1226 13:57:55.460433 96716 net.cpp:178] Creating Layer relu3
I1226 13:57:55.460471 96716 net.cpp:612] relu3 <- conv3
I1226 13:57:55.460521 96716 net.cpp:573] relu3 -> conv3 (in-place)
I1226 13:57:55.460626 96716 net.cpp:228] Setting up relu3
I1226 13:57:55.460697 96716 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:57:55.460728 96716 net.cpp:243] Memory required for data: 234073856
I1226 13:57:55.460763 96716 layer_factory.hpp:114] Creating layer conv4
I1226 13:57:55.460896 96716 net.cpp:178] Creating Layer conv4
I1226 13:57:55.461071 96716 net.cpp:612] conv4 <- conv3
I1226 13:57:55.461143 96716 net.cpp:586] conv4 -> conv4
I1226 13:57:55.496577 97230 net.cpp:228] Setting up conv3
I1226 13:57:55.496704 97230 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:57:55.496776 97230 net.cpp:243] Memory required for data: 225767168
I1226 13:57:55.496902 97230 layer_factory.hpp:114] Creating layer relu3
I1226 13:57:55.497032 97230 net.cpp:178] Creating Layer relu3
I1226 13:57:55.497095 97230 net.cpp:612] relu3 <- conv3
I1226 13:57:55.497158 97230 net.cpp:573] relu3 -> conv3 (in-place)
I1226 13:57:55.497282 97230 net.cpp:228] Setting up relu3
I1226 13:57:55.497371 97230 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:57:55.497411 97230 net.cpp:243] Memory required for data: 234073856
I1226 13:57:55.497457 97230 layer_factory.hpp:114] Creating layer conv4
I1226 13:57:55.497602 97230 net.cpp:178] Creating Layer conv4
I1226 13:57:55.497663 97230 net.cpp:612] conv4 <- conv3
I1226 13:57:55.497762 97230 net.cpp:586] conv4 -> conv4
I1226 13:57:55.910886 95085 net.cpp:228] Setting up conv4
I1226 13:57:55.911003 95085 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:57:55.911034 95085 net.cpp:243] Memory required for data: 242380544
I1226 13:57:55.911097 95085 layer_factory.hpp:114] Creating layer relu4
I1226 13:57:55.911188 95085 net.cpp:178] Creating Layer relu4
I1226 13:57:55.911236 95085 net.cpp:612] relu4 <- conv4
I1226 13:57:55.911288 95085 net.cpp:573] relu4 -> conv4 (in-place)
I1226 13:57:55.911392 95085 net.cpp:228] Setting up relu4
I1226 13:57:55.911442 95085 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:57:55.911468 95085 net.cpp:243] Memory required for data: 250687232
I1226 13:57:55.911500 95085 layer_factory.hpp:114] Creating layer conv5
I1226 13:57:55.911597 95085 net.cpp:178] Creating Layer conv5
I1226 13:57:55.911638 95085 net.cpp:612] conv5 <- conv4
I1226 13:57:55.911701 95085 net.cpp:586] conv5 -> conv5
I1226 13:57:56.186482 96716 net.cpp:228] Setting up conv4
I1226 13:57:56.186621 96716 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:57:56.186666 96716 net.cpp:243] Memory required for data: 242380544
I1226 13:57:56.186730 96716 layer_factory.hpp:114] Creating layer relu4
I1226 13:57:56.186858 96716 net.cpp:178] Creating Layer relu4
I1226 13:57:56.186920 96716 net.cpp:612] relu4 <- conv4
I1226 13:57:56.186969 96716 net.cpp:573] relu4 -> conv4 (in-place)
I1226 13:57:56.187067 96716 net.cpp:228] Setting up relu4
I1226 13:57:56.187127 96716 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:57:56.187153 96716 net.cpp:243] Memory required for data: 250687232
I1226 13:57:56.187186 96716 layer_factory.hpp:114] Creating layer conv5
I1226 13:57:56.187286 96716 net.cpp:178] Creating Layer conv5
I1226 13:57:56.187333 96716 net.cpp:612] conv5 <- conv4
I1226 13:57:56.187383 96716 net.cpp:586] conv5 -> conv5
I1226 13:57:56.222374 97230 net.cpp:228] Setting up conv4
I1226 13:57:56.222501 97230 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:57:56.222553 97230 net.cpp:243] Memory required for data: 242380544
I1226 13:57:56.222647 97230 layer_factory.hpp:114] Creating layer relu4
I1226 13:57:56.222903 97230 net.cpp:178] Creating Layer relu4
I1226 13:57:56.222959 97230 net.cpp:612] relu4 <- conv4
I1226 13:57:56.223021 97230 net.cpp:573] relu4 -> conv4 (in-place)
I1226 13:57:56.223150 97230 net.cpp:228] Setting up relu4
I1226 13:57:56.223219 97230 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:57:56.223254 97230 net.cpp:243] Memory required for data: 250687232
I1226 13:57:56.223294 97230 layer_factory.hpp:114] Creating layer conv5
I1226 13:57:56.223402 97230 net.cpp:178] Creating Layer conv5
I1226 13:57:56.223459 97230 net.cpp:612] conv5 <- conv4
I1226 13:57:56.223513 97230 net.cpp:586] conv5 -> conv5
I1226 13:57:56.412423 95085 net.cpp:228] Setting up conv5
I1226 13:57:56.412567 95085 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 13:57:56.412608 95085 net.cpp:243] Memory required for data: 256225024
I1226 13:57:56.412689 95085 layer_factory.hpp:114] Creating layer relu5
I1226 13:57:56.412891 95085 net.cpp:178] Creating Layer relu5
I1226 13:57:56.412978 95085 net.cpp:612] relu5 <- conv5
I1226 13:57:56.413023 95085 net.cpp:573] relu5 -> conv5 (in-place)
I1226 13:57:56.413141 95085 net.cpp:228] Setting up relu5
I1226 13:57:56.413224 95085 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 13:57:56.413342 95085 net.cpp:243] Memory required for data: 261762816
I1226 13:57:56.413377 95085 layer_factory.hpp:114] Creating layer pool5
I1226 13:57:56.413432 95085 net.cpp:178] Creating Layer pool5
I1226 13:57:56.413494 95085 net.cpp:612] pool5 <- conv5
I1226 13:57:56.413560 95085 net.cpp:586] pool5 -> pool5
I1226 13:57:56.413662 95085 net.cpp:228] Setting up pool5
I1226 13:57:56.413732 95085 net.cpp:235] Top shape: 32 256 6 6 (294912)
I1226 13:57:56.413766 95085 net.cpp:243] Memory required for data: 262942464
I1226 13:57:56.413854 95085 layer_factory.hpp:114] Creating layer fc6
I1226 13:57:56.413934 95085 net.cpp:178] Creating Layer fc6
I1226 13:57:56.413972 95085 net.cpp:612] fc6 <- pool5
I1226 13:57:56.414017 95085 net.cpp:586] fc6 -> fc6
I1226 13:57:56.686738 96716 net.cpp:228] Setting up conv5
I1226 13:57:56.686894 96716 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 13:57:56.686931 96716 net.cpp:243] Memory required for data: 256225024
I1226 13:57:56.687014 96716 layer_factory.hpp:114] Creating layer relu5
I1226 13:57:56.687181 96716 net.cpp:178] Creating Layer relu5
I1226 13:57:56.687233 96716 net.cpp:612] relu5 <- conv5
I1226 13:57:56.687294 96716 net.cpp:573] relu5 -> conv5 (in-place)
I1226 13:57:56.687404 96716 net.cpp:228] Setting up relu5
I1226 13:57:56.687463 96716 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 13:57:56.687489 96716 net.cpp:243] Memory required for data: 261762816
I1226 13:57:56.687521 96716 layer_factory.hpp:114] Creating layer pool5
I1226 13:57:56.687608 96716 net.cpp:178] Creating Layer pool5
I1226 13:57:56.687646 96716 net.cpp:612] pool5 <- conv5
I1226 13:57:56.687690 96716 net.cpp:586] pool5 -> pool5
I1226 13:57:56.687809 96716 net.cpp:228] Setting up pool5
I1226 13:57:56.687868 96716 net.cpp:235] Top shape: 32 256 6 6 (294912)
I1226 13:57:56.687893 96716 net.cpp:243] Memory required for data: 262942464
I1226 13:57:56.687923 96716 layer_factory.hpp:114] Creating layer fc6
I1226 13:57:56.688007 96716 net.cpp:178] Creating Layer fc6
I1226 13:57:56.688046 96716 net.cpp:612] fc6 <- pool5
I1226 13:57:56.688091 96716 net.cpp:586] fc6 -> fc6
I1226 13:57:56.723044 97230 net.cpp:228] Setting up conv5
I1226 13:57:56.723166 97230 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 13:57:56.723196 97230 net.cpp:243] Memory required for data: 256225024
I1226 13:57:56.723276 97230 layer_factory.hpp:114] Creating layer relu5
I1226 13:57:56.723359 97230 net.cpp:178] Creating Layer relu5
I1226 13:57:56.723412 97230 net.cpp:612] relu5 <- conv5
I1226 13:57:56.723474 97230 net.cpp:573] relu5 -> conv5 (in-place)
I1226 13:57:56.723587 97230 net.cpp:228] Setting up relu5
I1226 13:57:56.723640 97230 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 13:57:56.723667 97230 net.cpp:243] Memory required for data: 261762816
I1226 13:57:56.723698 97230 layer_factory.hpp:114] Creating layer pool5
I1226 13:57:56.723779 97230 net.cpp:178] Creating Layer pool5
I1226 13:57:56.723831 97230 net.cpp:612] pool5 <- conv5
I1226 13:57:56.723884 97230 net.cpp:586] pool5 -> pool5
I1226 13:57:56.723986 97230 net.cpp:228] Setting up pool5
I1226 13:57:56.724040 97230 net.cpp:235] Top shape: 32 256 6 6 (294912)
I1226 13:57:56.724071 97230 net.cpp:243] Memory required for data: 262942464
I1226 13:57:56.724102 97230 layer_factory.hpp:114] Creating layer fc6
I1226 13:57:56.724179 97230 net.cpp:178] Creating Layer fc6
I1226 13:57:56.724216 97230 net.cpp:612] fc6 <- pool5
I1226 13:57:56.724258 97230 net.cpp:586] fc6 -> fc6
I1226 13:57:58.918782 93154 net.cpp:228] Setting up fc6
I1226 13:57:58.918895 93154 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:57:58.918923 93154 net.cpp:243] Memory required for data: 263466752
I1226 13:57:58.918979 93154 layer_factory.hpp:114] Creating layer relu6
I1226 13:57:58.919067 93154 net.cpp:178] Creating Layer relu6
I1226 13:57:58.919116 93154 net.cpp:612] relu6 <- fc6
I1226 13:57:58.919165 93154 net.cpp:573] relu6 -> fc6 (in-place)
I1226 13:57:58.919250 93154 net.cpp:228] Setting up relu6
I1226 13:57:58.919293 93154 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:57:58.919450 93154 net.cpp:243] Memory required for data: 263991040
I1226 13:57:58.919493 93154 layer_factory.hpp:114] Creating layer drop6
I1226 13:57:58.919564 93154 net.cpp:178] Creating Layer drop6
I1226 13:57:58.919603 93154 net.cpp:612] drop6 <- fc6
I1226 13:57:58.919642 93154 net.cpp:573] drop6 -> fc6 (in-place)
I1226 13:57:58.919701 93154 net.cpp:228] Setting up drop6
I1226 13:57:58.919741 93154 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:57:58.919764 93154 net.cpp:243] Memory required for data: 264515328
I1226 13:57:58.919790 93154 layer_factory.hpp:114] Creating layer fc7
I1226 13:57:58.919848 93154 net.cpp:178] Creating Layer fc7
I1226 13:57:58.919880 93154 net.cpp:612] fc7 <- fc6
I1226 13:57:58.919917 93154 net.cpp:586] fc7 -> fc7
I1226 13:57:59.395665 94427 net.cpp:228] Setting up fc6
I1226 13:57:59.395786 94427 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:57:59.395823 94427 net.cpp:243] Memory required for data: 263466752
I1226 13:57:59.395886 94427 layer_factory.hpp:114] Creating layer relu6
I1226 13:57:59.395972 94427 net.cpp:178] Creating Layer relu6
I1226 13:57:59.396126 94427 net.cpp:612] relu6 <- fc6
I1226 13:57:59.396193 94427 net.cpp:573] relu6 -> fc6 (in-place)
I1226 13:57:59.396334 94427 net.cpp:228] Setting up relu6
I1226 13:57:59.396514 94427 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:57:59.396548 94427 net.cpp:243] Memory required for data: 263991040
I1226 13:57:59.396595 94427 layer_factory.hpp:114] Creating layer drop6
I1226 13:57:59.396656 94427 net.cpp:178] Creating Layer drop6
I1226 13:57:59.396684 94427 net.cpp:612] drop6 <- fc6
I1226 13:57:59.396721 94427 net.cpp:573] drop6 -> fc6 (in-place)
I1226 13:57:59.396786 94427 net.cpp:228] Setting up drop6
I1226 13:57:59.396826 94427 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:57:59.396848 94427 net.cpp:243] Memory required for data: 264515328
I1226 13:57:59.396874 94427 layer_factory.hpp:114] Creating layer fc7
I1226 13:57:59.396955 94427 net.cpp:178] Creating Layer fc7
I1226 13:57:59.396992 94427 net.cpp:612] fc7 <- fc6
I1226 13:57:59.397029 94427 net.cpp:586] fc7 -> fc7
I1226 13:57:59.630736 91478 net.cpp:228] Setting up fc6
I1226 13:57:59.630853 91478 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:57:59.630892 91478 net.cpp:243] Memory required for data: 263466752
I1226 13:57:59.630971 91478 layer_factory.hpp:114] Creating layer relu6
I1226 13:57:59.631052 91478 net.cpp:178] Creating Layer relu6
I1226 13:57:59.631103 91478 net.cpp:612] relu6 <- fc6
I1226 13:57:59.631173 91478 net.cpp:573] relu6 -> fc6 (in-place)
I1226 13:57:59.631327 91478 net.cpp:228] Setting up relu6
I1226 13:57:59.631508 91478 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:57:59.631548 91478 net.cpp:243] Memory required for data: 263991040
I1226 13:57:59.631592 91478 layer_factory.hpp:114] Creating layer drop6
I1226 13:57:59.631660 91478 net.cpp:178] Creating Layer drop6
I1226 13:57:59.631700 91478 net.cpp:612] drop6 <- fc6
I1226 13:57:59.631744 91478 net.cpp:573] drop6 -> fc6 (in-place)
I1226 13:57:59.631811 91478 net.cpp:228] Setting up drop6
I1226 13:57:59.631858 91478 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:57:59.631885 91478 net.cpp:243] Memory required for data: 264515328
I1226 13:57:59.631911 91478 layer_factory.hpp:114] Creating layer fc7
I1226 13:57:59.631986 91478 net.cpp:178] Creating Layer fc7
I1226 13:57:59.632033 91478 net.cpp:612] fc7 <- fc6
I1226 13:57:59.632074 91478 net.cpp:586] fc7 -> fc7
I1226 13:57:59.621351 91481 net.cpp:228] Setting up fc6
I1226 13:57:59.621470 91481 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:57:59.621508 91481 net.cpp:243] Memory required for data: 263466752
I1226 13:57:59.621593 91481 layer_factory.hpp:114] Creating layer relu6
I1226 13:57:59.621830 91481 net.cpp:178] Creating Layer relu6
I1226 13:57:59.621912 91481 net.cpp:612] relu6 <- fc6
I1226 13:57:59.622004 91481 net.cpp:573] relu6 -> fc6 (in-place)
I1226 13:57:59.622120 91481 net.cpp:228] Setting up relu6
I1226 13:57:59.622275 91481 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:57:59.622313 91481 net.cpp:243] Memory required for data: 263991040
I1226 13:57:59.622356 91481 layer_factory.hpp:114] Creating layer drop6
I1226 13:57:59.622433 91481 net.cpp:178] Creating Layer drop6
I1226 13:57:59.622480 91481 net.cpp:612] drop6 <- fc6
I1226 13:57:59.622537 91481 net.cpp:573] drop6 -> fc6 (in-place)
I1226 13:57:59.622614 91481 net.cpp:228] Setting up drop6
I1226 13:57:59.622664 91481 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:57:59.622696 91481 net.cpp:243] Memory required for data: 264515328
I1226 13:57:59.622732 91481 layer_factory.hpp:114] Creating layer fc7
I1226 13:57:59.622824 91481 net.cpp:178] Creating Layer fc7
I1226 13:57:59.622885 91481 net.cpp:612] fc7 <- fc6
I1226 13:57:59.622948 91481 net.cpp:586] fc7 -> fc7
I1226 13:57:59.641651 92257 net.cpp:228] Setting up fc6
I1226 13:57:59.641772 92257 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:57:59.641806 92257 net.cpp:243] Memory required for data: 263466752
I1226 13:57:59.641885 92257 layer_factory.hpp:114] Creating layer relu6
I1226 13:57:59.641970 92257 net.cpp:178] Creating Layer relu6
I1226 13:57:59.642019 92257 net.cpp:612] relu6 <- fc6
I1226 13:57:59.642097 92257 net.cpp:573] relu6 -> fc6 (in-place)
I1226 13:57:59.642204 92257 net.cpp:228] Setting up relu6
I1226 13:57:59.642396 92257 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:57:59.642433 92257 net.cpp:243] Memory required for data: 263991040
I1226 13:57:59.642472 92257 layer_factory.hpp:114] Creating layer drop6
I1226 13:57:59.642551 92257 net.cpp:178] Creating Layer drop6
I1226 13:57:59.642590 92257 net.cpp:612] drop6 <- fc6
I1226 13:57:59.642647 92257 net.cpp:573] drop6 -> fc6 (in-place)
I1226 13:57:59.642717 92257 net.cpp:228] Setting up drop6
I1226 13:57:59.642774 92257 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:57:59.642804 92257 net.cpp:243] Memory required for data: 264515328
I1226 13:57:59.642839 92257 layer_factory.hpp:114] Creating layer fc7
I1226 13:57:59.642921 92257 net.cpp:178] Creating Layer fc7
I1226 13:57:59.642961 92257 net.cpp:612] fc7 <- fc6
I1226 13:57:59.643007 92257 net.cpp:586] fc7 -> fc7
I1226 13:57:59.720885 92037 net.cpp:228] Setting up fc6
I1226 13:57:59.721002 92037 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:57:59.721034 92037 net.cpp:243] Memory required for data: 263466752
I1226 13:57:59.721093 92037 layer_factory.hpp:114] Creating layer relu6
I1226 13:57:59.721170 92037 net.cpp:178] Creating Layer relu6
I1226 13:57:59.721213 92037 net.cpp:612] relu6 <- fc6
I1226 13:57:59.721278 92037 net.cpp:573] relu6 -> fc6 (in-place)
I1226 13:57:59.721408 92037 net.cpp:228] Setting up relu6
I1226 13:57:59.721585 92037 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:57:59.721626 92037 net.cpp:243] Memory required for data: 263991040
I1226 13:57:59.721664 92037 layer_factory.hpp:114] Creating layer drop6
I1226 13:57:59.721729 92037 net.cpp:178] Creating Layer drop6
I1226 13:57:59.721763 92037 net.cpp:612] drop6 <- fc6
I1226 13:57:59.721804 92037 net.cpp:573] drop6 -> fc6 (in-place)
I1226 13:57:59.721868 92037 net.cpp:228] Setting up drop6
I1226 13:57:59.721909 92037 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:57:59.721932 92037 net.cpp:243] Memory required for data: 264515328
I1226 13:57:59.721962 92037 layer_factory.hpp:114] Creating layer fc7
I1226 13:57:59.722039 92037 net.cpp:178] Creating Layer fc7
I1226 13:57:59.722079 92037 net.cpp:612] fc7 <- fc6
I1226 13:57:59.722126 92037 net.cpp:586] fc7 -> fc7
I1226 13:58:01.202606 93154 net.cpp:228] Setting up fc7
I1226 13:58:01.202718 93154 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:58:01.202744 93154 net.cpp:243] Memory required for data: 265039616
I1226 13:58:01.202827 93154 layer_factory.hpp:114] Creating layer relu7
I1226 13:58:01.202908 93154 net.cpp:178] Creating Layer relu7
I1226 13:58:01.202951 93154 net.cpp:612] relu7 <- fc7
I1226 13:58:01.202991 93154 net.cpp:573] relu7 -> fc7 (in-place)
I1226 13:58:01.203173 93154 net.cpp:228] Setting up relu7
I1226 13:58:01.203233 93154 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:58:01.203260 93154 net.cpp:243] Memory required for data: 265563904
I1226 13:58:01.203297 93154 layer_factory.hpp:114] Creating layer drop7
I1226 13:58:01.203337 93154 net.cpp:178] Creating Layer drop7
I1226 13:58:01.203364 93154 net.cpp:612] drop7 <- fc7
I1226 13:58:01.203433 93154 net.cpp:573] drop7 -> fc7 (in-place)
I1226 13:58:01.203490 93154 net.cpp:228] Setting up drop7
I1226 13:58:01.203524 93154 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:58:01.203549 93154 net.cpp:243] Memory required for data: 266088192
I1226 13:58:01.203577 93154 layer_factory.hpp:114] Creating layer fc8
I1226 13:58:01.203649 93154 net.cpp:178] Creating Layer fc8
I1226 13:58:01.203685 93154 net.cpp:612] fc8 <- fc7
I1226 13:58:01.203733 93154 net.cpp:586] fc8 -> fc8
I1226 13:58:01.683514 94427 net.cpp:228] Setting up fc7
I1226 13:58:01.683624 94427 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:58:01.683655 94427 net.cpp:243] Memory required for data: 265039616
I1226 13:58:01.683712 94427 layer_factory.hpp:114] Creating layer relu7
I1226 13:58:01.683816 94427 net.cpp:178] Creating Layer relu7
I1226 13:58:01.683917 94427 net.cpp:612] relu7 <- fc7
I1226 13:58:01.683960 94427 net.cpp:573] relu7 -> fc7 (in-place)
I1226 13:58:01.684056 94427 net.cpp:228] Setting up relu7
I1226 13:58:01.684108 94427 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:58:01.684139 94427 net.cpp:243] Memory required for data: 265563904
I1226 13:58:01.684170 94427 layer_factory.hpp:114] Creating layer drop7
I1226 13:58:01.684221 94427 net.cpp:178] Creating Layer drop7
I1226 13:58:01.684250 94427 net.cpp:612] drop7 <- fc7
I1226 13:58:01.684286 94427 net.cpp:573] drop7 -> fc7 (in-place)
I1226 13:58:01.684448 94427 net.cpp:228] Setting up drop7
I1226 13:58:01.684484 94427 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:58:01.684509 94427 net.cpp:243] Memory required for data: 266088192
I1226 13:58:01.684545 94427 layer_factory.hpp:114] Creating layer fc8
I1226 13:58:01.684600 94427 net.cpp:178] Creating Layer fc8
I1226 13:58:01.684635 94427 net.cpp:612] fc8 <- fc7
I1226 13:58:01.684691 94427 net.cpp:586] fc8 -> fc8
I1226 13:58:01.763886 93154 net.cpp:228] Setting up fc8
I1226 13:58:01.764003 93154 net.cpp:235] Top shape: 32 1000 (32000)
I1226 13:58:01.764030 93154 net.cpp:243] Memory required for data: 266216192
I1226 13:58:01.764086 93154 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 13:58:01.764175 93154 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 13:58:01.764220 93154 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 13:58:01.764266 93154 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 13:58:01.764334 93154 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 13:58:01.764439 93154 net.cpp:228] Setting up fc8_fc8_0_split
I1226 13:58:01.764497 93154 net.cpp:235] Top shape: 32 1000 (32000)
I1226 13:58:01.764528 93154 net.cpp:235] Top shape: 32 1000 (32000)
I1226 13:58:01.764551 93154 net.cpp:243] Memory required for data: 266472192
I1226 13:58:01.764587 93154 layer_factory.hpp:114] Creating layer accuracy
I1226 13:58:01.764642 93154 net.cpp:178] Creating Layer accuracy
I1226 13:58:01.764678 93154 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 13:58:01.764706 93154 net.cpp:612] accuracy <- label_data_1_split_0
I1226 13:58:01.764753 93154 net.cpp:586] accuracy -> accuracy
I1226 13:58:01.764809 93154 net.cpp:228] Setting up accuracy
I1226 13:58:01.764849 93154 net.cpp:235] Top shape: (1)
I1226 13:58:01.764878 93154 net.cpp:243] Memory required for data: 266472196
I1226 13:58:01.764912 93154 layer_factory.hpp:114] Creating layer loss
I1226 13:58:01.764955 93154 net.cpp:178] Creating Layer loss
I1226 13:58:01.764986 93154 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 13:58:01.765015 93154 net.cpp:612] loss <- label_data_1_split_1
I1226 13:58:01.765049 93154 net.cpp:586] loss -> loss
I1226 13:58:01.765110 93154 layer_factory.hpp:114] Creating layer loss
I1226 13:58:01.792064 93154 net.cpp:228] Setting up loss
I1226 13:58:01.792181 93154 net.cpp:235] Top shape: (1)
I1226 13:58:01.792222 93154 net.cpp:238]     with loss weight 1
I1226 13:58:01.792516 93154 net.cpp:243] Memory required for data: 266472200
I1226 13:58:01.792577 93154 net.cpp:305] loss needs backward computation.
I1226 13:58:01.792620 93154 net.cpp:307] accuracy does not need backward computation.
I1226 13:58:01.792659 93154 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 13:58:01.792707 93154 net.cpp:305] fc8 needs backward computation.
I1226 13:58:01.792743 93154 net.cpp:305] drop7 needs backward computation.
I1226 13:58:01.792780 93154 net.cpp:305] relu7 needs backward computation.
I1226 13:58:01.792821 93154 net.cpp:305] fc7 needs backward computation.
I1226 13:58:01.792861 93154 net.cpp:305] drop6 needs backward computation.
I1226 13:58:01.792896 93154 net.cpp:305] relu6 needs backward computation.
I1226 13:58:01.792927 93154 net.cpp:305] fc6 needs backward computation.
I1226 13:58:01.792959 93154 net.cpp:305] pool5 needs backward computation.
I1226 13:58:01.792992 93154 net.cpp:305] relu5 needs backward computation.
I1226 13:58:01.793022 93154 net.cpp:305] conv5 needs backward computation.
I1226 13:58:01.793053 93154 net.cpp:305] relu4 needs backward computation.
I1226 13:58:01.793083 93154 net.cpp:305] conv4 needs backward computation.
I1226 13:58:01.793121 93154 net.cpp:305] relu3 needs backward computation.
I1226 13:58:01.793160 93154 net.cpp:305] conv3 needs backward computation.
I1226 13:58:01.793195 93154 net.cpp:305] pool2 needs backward computation.
I1226 13:58:01.793249 93154 net.cpp:305] norm2 needs backward computation.
I1226 13:58:01.793282 93154 net.cpp:305] relu2 needs backward computation.
I1226 13:58:01.793314 93154 net.cpp:305] conv2 needs backward computation.
I1226 13:58:01.793359 93154 net.cpp:305] pool1 needs backward computation.
I1226 13:58:01.793409 93154 net.cpp:305] norm1 needs backward computation.
I1226 13:58:01.793439 93154 net.cpp:305] relu1 needs backward computation.
I1226 13:58:01.793467 93154 net.cpp:305] conv1 needs backward computation.
I1226 13:58:01.793498 93154 net.cpp:307] label_data_1_split does not need backward computation.
I1226 13:58:01.793529 93154 net.cpp:307] data does not need backward computation.
I1226 13:58:01.793555 93154 net.cpp:349] This network produces output accuracy
I1226 13:58:01.793587 93154 net.cpp:349] This network produces output loss
I1226 13:58:01.793681 93154 net.cpp:363] Network initialization done.
I1226 13:58:01.794149 93154 solver.cpp:107] Solver scaffolding done.
I1226 13:58:01.794358 93154 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 13:58:01.869482 91481 net.cpp:228] Setting up fc7
I1226 13:58:01.869601 91481 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:58:01.869637 91481 net.cpp:243] Memory required for data: 265039616
I1226 13:58:01.869715 91481 layer_factory.hpp:114] Creating layer relu7
I1226 13:58:01.869819 91481 net.cpp:178] Creating Layer relu7
I1226 13:58:01.869894 91481 net.cpp:612] relu7 <- fc7
I1226 13:58:01.869946 91481 net.cpp:573] relu7 -> fc7 (in-place)
I1226 13:58:01.870050 91481 net.cpp:228] Setting up relu7
I1226 13:58:01.870121 91481 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:58:01.870156 91481 net.cpp:243] Memory required for data: 265563904
I1226 13:58:01.870198 91481 layer_factory.hpp:114] Creating layer drop7
I1226 13:58:01.870265 91481 net.cpp:178] Creating Layer drop7
I1226 13:58:01.870312 91481 net.cpp:612] drop7 <- fc7
I1226 13:58:01.870362 91481 net.cpp:573] drop7 -> fc7 (in-place)
I1226 13:58:01.870424 91481 net.cpp:228] Setting up drop7
I1226 13:58:01.870482 91481 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:58:01.870514 91481 net.cpp:243] Memory required for data: 266088192
I1226 13:58:01.870550 91481 layer_factory.hpp:114] Creating layer fc8
I1226 13:58:01.870630 91481 net.cpp:178] Creating Layer fc8
I1226 13:58:01.870679 91481 net.cpp:612] fc8 <- fc7
I1226 13:58:01.870757 91481 net.cpp:586] fc8 -> fc8
I1226 13:58:01.892014 91478 net.cpp:228] Setting up fc7
I1226 13:58:01.892122 91478 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:58:01.892151 91478 net.cpp:243] Memory required for data: 265039616
I1226 13:58:01.892263 91478 layer_factory.hpp:114] Creating layer relu7
I1226 13:58:01.892355 91478 net.cpp:178] Creating Layer relu7
I1226 13:58:01.892408 91478 net.cpp:612] relu7 <- fc7
I1226 13:58:01.892453 91478 net.cpp:573] relu7 -> fc7 (in-place)
I1226 13:58:01.892556 91478 net.cpp:228] Setting up relu7
I1226 13:58:01.892608 91478 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:58:01.892633 91478 net.cpp:243] Memory required for data: 265563904
I1226 13:58:01.892663 91478 layer_factory.hpp:114] Creating layer drop7
I1226 13:58:01.892712 91478 net.cpp:178] Creating Layer drop7
I1226 13:58:01.892827 91478 net.cpp:612] drop7 <- fc7
I1226 13:58:01.892866 91478 net.cpp:573] drop7 -> fc7 (in-place)
I1226 13:58:01.892911 91478 net.cpp:228] Setting up drop7
I1226 13:58:01.892946 91478 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:58:01.892967 91478 net.cpp:243] Memory required for data: 266088192
I1226 13:58:01.892994 91478 layer_factory.hpp:114] Creating layer fc8
I1226 13:58:01.893050 91478 net.cpp:178] Creating Layer fc8
I1226 13:58:01.893085 91478 net.cpp:612] fc8 <- fc7
I1226 13:58:01.893122 91478 net.cpp:586] fc8 -> fc8
I1226 13:58:01.924331 92257 net.cpp:228] Setting up fc7
I1226 13:58:01.924444 92257 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:58:01.924475 92257 net.cpp:243] Memory required for data: 265039616
I1226 13:58:01.924552 92257 layer_factory.hpp:114] Creating layer relu7
I1226 13:58:01.924669 92257 net.cpp:178] Creating Layer relu7
I1226 13:58:01.924716 92257 net.cpp:612] relu7 <- fc7
I1226 13:58:01.924759 92257 net.cpp:573] relu7 -> fc7 (in-place)
I1226 13:58:01.924855 92257 net.cpp:228] Setting up relu7
I1226 13:58:01.924918 92257 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:58:01.924947 92257 net.cpp:243] Memory required for data: 265563904
I1226 13:58:01.924985 92257 layer_factory.hpp:114] Creating layer drop7
I1226 13:58:01.925053 92257 net.cpp:178] Creating Layer drop7
I1226 13:58:01.925087 92257 net.cpp:612] drop7 <- fc7
I1226 13:58:01.925134 92257 net.cpp:573] drop7 -> fc7 (in-place)
I1226 13:58:01.925194 92257 net.cpp:228] Setting up drop7
I1226 13:58:01.925240 92257 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:58:01.925274 92257 net.cpp:243] Memory required for data: 266088192
I1226 13:58:01.925341 92257 layer_factory.hpp:114] Creating layer fc8
I1226 13:58:01.925405 92257 net.cpp:178] Creating Layer fc8
I1226 13:58:01.925438 92257 net.cpp:612] fc8 <- fc7
I1226 13:58:01.925479 92257 net.cpp:586] fc8 -> fc8
I1226 13:58:02.036644 92037 net.cpp:228] Setting up fc7
I1226 13:58:02.036757 92037 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:58:02.036785 92037 net.cpp:243] Memory required for data: 265039616
I1226 13:58:02.036844 92037 layer_factory.hpp:114] Creating layer relu7
I1226 13:58:02.036936 92037 net.cpp:178] Creating Layer relu7
I1226 13:58:02.036983 92037 net.cpp:612] relu7 <- fc7
I1226 13:58:02.037029 92037 net.cpp:573] relu7 -> fc7 (in-place)
I1226 13:58:02.037122 92037 net.cpp:228] Setting up relu7
I1226 13:58:02.037186 92037 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:58:02.037214 92037 net.cpp:243] Memory required for data: 265563904
I1226 13:58:02.037245 92037 layer_factory.hpp:114] Creating layer drop7
I1226 13:58:02.037300 92037 net.cpp:178] Creating Layer drop7
I1226 13:58:02.037391 92037 net.cpp:612] drop7 <- fc7
I1226 13:58:02.037436 92037 net.cpp:573] drop7 -> fc7 (in-place)
I1226 13:58:02.037492 92037 net.cpp:228] Setting up drop7
I1226 13:58:02.037530 92037 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:58:02.037554 92037 net.cpp:243] Memory required for data: 266088192
I1226 13:58:02.037582 92037 layer_factory.hpp:114] Creating layer fc8
I1226 13:58:02.037642 92037 net.cpp:178] Creating Layer fc8
I1226 13:58:02.037669 92037 net.cpp:612] fc8 <- fc7
I1226 13:58:02.037710 92037 net.cpp:586] fc8 -> fc8
I1226 13:58:02.253417 94427 net.cpp:228] Setting up fc8
I1226 13:58:02.253542 94427 net.cpp:235] Top shape: 32 1000 (32000)
I1226 13:58:02.253577 94427 net.cpp:243] Memory required for data: 266216192
I1226 13:58:02.253634 94427 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 13:58:02.253710 94427 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 13:58:02.253752 94427 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 13:58:02.253798 94427 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 13:58:02.253862 94427 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 13:58:02.253948 94427 net.cpp:228] Setting up fc8_fc8_0_split
I1226 13:58:02.253999 94427 net.cpp:235] Top shape: 32 1000 (32000)
I1226 13:58:02.254029 94427 net.cpp:235] Top shape: 32 1000 (32000)
I1226 13:58:02.254051 94427 net.cpp:243] Memory required for data: 266472192
I1226 13:58:02.254081 94427 layer_factory.hpp:114] Creating layer accuracy
I1226 13:58:02.254128 94427 net.cpp:178] Creating Layer accuracy
I1226 13:58:02.254163 94427 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 13:58:02.254194 94427 net.cpp:612] accuracy <- label_data_1_split_0
I1226 13:58:02.254230 94427 net.cpp:586] accuracy -> accuracy
I1226 13:58:02.254277 94427 net.cpp:228] Setting up accuracy
I1226 13:58:02.254309 94427 net.cpp:235] Top shape: (1)
I1226 13:58:02.254338 94427 net.cpp:243] Memory required for data: 266472196
I1226 13:58:02.254364 94427 layer_factory.hpp:114] Creating layer loss
I1226 13:58:02.254458 94427 net.cpp:178] Creating Layer loss
I1226 13:58:02.254499 94427 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 13:58:02.254529 94427 net.cpp:612] loss <- label_data_1_split_1
I1226 13:58:02.254565 94427 net.cpp:586] loss -> loss
I1226 13:58:02.254624 94427 layer_factory.hpp:114] Creating layer loss
I1226 13:58:02.286064 94427 net.cpp:228] Setting up loss
I1226 13:58:02.286187 94427 net.cpp:235] Top shape: (1)
I1226 13:58:02.286360 94427 net.cpp:238]     with loss weight 1
I1226 13:58:02.286548 94427 net.cpp:243] Memory required for data: 266472200
I1226 13:58:02.286597 94427 net.cpp:305] loss needs backward computation.
I1226 13:58:02.286643 94427 net.cpp:307] accuracy does not need backward computation.
I1226 13:58:02.286687 94427 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 13:58:02.286723 94427 net.cpp:305] fc8 needs backward computation.
I1226 13:58:02.286777 94427 net.cpp:305] drop7 needs backward computation.
I1226 13:58:02.286811 94427 net.cpp:305] relu7 needs backward computation.
I1226 13:58:02.286856 94427 net.cpp:305] fc7 needs backward computation.
I1226 13:58:02.286896 94427 net.cpp:305] drop6 needs backward computation.
I1226 13:58:02.286936 94427 net.cpp:305] relu6 needs backward computation.
I1226 13:58:02.286968 94427 net.cpp:305] fc6 needs backward computation.
I1226 13:58:02.287008 94427 net.cpp:305] pool5 needs backward computation.
I1226 13:58:02.287047 94427 net.cpp:305] relu5 needs backward computation.
I1226 13:58:02.287080 94427 net.cpp:305] conv5 needs backward computation.
I1226 13:58:02.287111 94427 net.cpp:305] relu4 needs backward computation.
I1226 13:58:02.287142 94427 net.cpp:305] conv4 needs backward computation.
I1226 13:58:02.287180 94427 net.cpp:305] relu3 needs backward computation.
I1226 13:58:02.287210 94427 net.cpp:305] conv3 needs backward computation.
I1226 13:58:02.287248 94427 net.cpp:305] pool2 needs backward computation.
I1226 13:58:02.287287 94427 net.cpp:305] norm2 needs backward computation.
I1226 13:58:02.287325 94427 net.cpp:305] relu2 needs backward computation.
I1226 13:58:02.287356 94427 net.cpp:305] conv2 needs backward computation.
I1226 13:58:02.287413 94427 net.cpp:305] pool1 needs backward computation.
I1226 13:58:02.287474 94427 net.cpp:305] norm1 needs backward computation.
I1226 13:58:02.287510 94427 net.cpp:305] relu1 needs backward computation.
I1226 13:58:02.287542 94427 net.cpp:305] conv1 needs backward computation.
I1226 13:58:02.287577 94427 net.cpp:307] label_data_1_split does not need backward computation.
I1226 13:58:02.287611 94427 net.cpp:307] data does not need backward computation.
I1226 13:58:02.287648 94427 net.cpp:349] This network produces output accuracy
I1226 13:58:02.287684 94427 net.cpp:349] This network produces output loss
I1226 13:58:02.287771 94427 net.cpp:363] Network initialization done.
I1226 13:58:02.288213 94427 solver.cpp:107] Solver scaffolding done.
I1226 13:58:02.288449 94427 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 13:58:02.438464 91478 net.cpp:228] Setting up fc8
I1226 13:58:02.438593 91478 net.cpp:235] Top shape: 32 1000 (32000)
I1226 13:58:02.438627 91478 net.cpp:243] Memory required for data: 266216192
I1226 13:58:02.438709 91478 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 13:58:02.438786 91478 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 13:58:02.438920 91478 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 13:58:02.438961 91478 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 13:58:02.439007 91478 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 13:58:02.439097 91478 net.cpp:228] Setting up fc8_fc8_0_split
I1226 13:58:02.439153 91478 net.cpp:235] Top shape: 32 1000 (32000)
I1226 13:58:02.439184 91478 net.cpp:235] Top shape: 32 1000 (32000)
I1226 13:58:02.439231 91478 net.cpp:243] Memory required for data: 266472192
I1226 13:58:02.439270 91478 layer_factory.hpp:114] Creating layer accuracy
I1226 13:58:02.439327 91478 net.cpp:178] Creating Layer accuracy
I1226 13:58:02.439366 91478 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 13:58:02.439399 91478 net.cpp:612] accuracy <- label_data_1_split_0
I1226 13:58:02.439435 91478 net.cpp:586] accuracy -> accuracy
I1226 13:58:02.439484 91478 net.cpp:228] Setting up accuracy
I1226 13:58:02.439525 91478 net.cpp:235] Top shape: (1)
I1226 13:58:02.439548 91478 net.cpp:243] Memory required for data: 266472196
I1226 13:58:02.439575 91478 layer_factory.hpp:114] Creating layer loss
I1226 13:58:02.439625 91478 net.cpp:178] Creating Layer loss
I1226 13:58:02.439656 91478 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 13:58:02.439702 91478 net.cpp:612] loss <- label_data_1_split_1
I1226 13:58:02.439752 91478 net.cpp:586] loss -> loss
I1226 13:58:02.439813 91478 layer_factory.hpp:114] Creating layer loss
I1226 13:58:02.431481 91481 net.cpp:228] Setting up fc8
I1226 13:58:02.431622 91481 net.cpp:235] Top shape: 32 1000 (32000)
I1226 13:58:02.431661 91481 net.cpp:243] Memory required for data: 266216192
I1226 13:58:02.431741 91481 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 13:58:02.431820 91481 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 13:58:02.431896 91481 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 13:58:02.431958 91481 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 13:58:02.432047 91481 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 13:58:02.432183 91481 net.cpp:228] Setting up fc8_fc8_0_split
I1226 13:58:02.432250 91481 net.cpp:235] Top shape: 32 1000 (32000)
I1226 13:58:02.432301 91481 net.cpp:235] Top shape: 32 1000 (32000)
I1226 13:58:02.432333 91481 net.cpp:243] Memory required for data: 266472192
I1226 13:58:02.432374 91481 layer_factory.hpp:114] Creating layer accuracy
I1226 13:58:02.432445 91481 net.cpp:178] Creating Layer accuracy
I1226 13:58:02.432490 91481 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 13:58:02.432535 91481 net.cpp:612] accuracy <- label_data_1_split_0
I1226 13:58:02.432585 91481 net.cpp:586] accuracy -> accuracy
I1226 13:58:02.432646 91481 net.cpp:228] Setting up accuracy
I1226 13:58:02.432694 91481 net.cpp:235] Top shape: (1)
I1226 13:58:02.432739 91481 net.cpp:243] Memory required for data: 266472196
I1226 13:58:02.432777 91481 layer_factory.hpp:114] Creating layer loss
I1226 13:58:02.432858 91481 net.cpp:178] Creating Layer loss
I1226 13:58:02.432929 91481 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 13:58:02.432973 91481 net.cpp:612] loss <- label_data_1_split_1
I1226 13:58:02.433022 91481 net.cpp:586] loss -> loss
I1226 13:58:02.433101 91481 layer_factory.hpp:114] Creating layer loss
I1226 13:58:02.467164 91478 net.cpp:228] Setting up loss
I1226 13:58:02.467319 91478 net.cpp:235] Top shape: (1)
I1226 13:58:02.467494 91478 net.cpp:238]     with loss weight 1
I1226 13:58:02.467658 91478 net.cpp:243] Memory required for data: 266472200
I1226 13:58:02.467712 91478 net.cpp:305] loss needs backward computation.
I1226 13:58:02.467756 91478 net.cpp:307] accuracy does not need backward computation.
I1226 13:58:02.467792 91478 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 13:58:02.467823 91478 net.cpp:305] fc8 needs backward computation.
I1226 13:58:02.467874 91478 net.cpp:305] drop7 needs backward computation.
I1226 13:58:02.467905 91478 net.cpp:305] relu7 needs backward computation.
I1226 13:58:02.467933 91478 net.cpp:305] fc7 needs backward computation.
I1226 13:58:02.467964 91478 net.cpp:305] drop6 needs backward computation.
I1226 13:58:02.467994 91478 net.cpp:305] relu6 needs backward computation.
I1226 13:58:02.468024 91478 net.cpp:305] fc6 needs backward computation.
I1226 13:58:02.468055 91478 net.cpp:305] pool5 needs backward computation.
I1226 13:58:02.468092 91478 net.cpp:305] relu5 needs backward computation.
I1226 13:58:02.468122 91478 net.cpp:305] conv5 needs backward computation.
I1226 13:58:02.468152 91478 net.cpp:305] relu4 needs backward computation.
I1226 13:58:02.468190 91478 net.cpp:305] conv4 needs backward computation.
I1226 13:58:02.468247 91478 net.cpp:305] relu3 needs backward computation.
I1226 13:58:02.468278 91478 net.cpp:305] conv3 needs backward computation.
I1226 13:58:02.468310 91478 net.cpp:305] pool2 needs backward computation.
I1226 13:58:02.468340 91478 net.cpp:305] norm2 needs backward computation.
I1226 13:58:02.468372 91478 net.cpp:305] relu2 needs backward computation.
I1226 13:58:02.468401 91478 net.cpp:305] conv2 needs backward computation.
I1226 13:58:02.468433 91478 net.cpp:305] pool1 needs backward computation.
I1226 13:58:02.468463 91478 net.cpp:305] norm1 needs backward computation.
I1226 13:58:02.468493 91478 net.cpp:305] relu1 needs backward computation.
I1226 13:58:02.468524 91478 net.cpp:305] conv1 needs backward computation.
I1226 13:58:02.468556 91478 net.cpp:307] label_data_1_split does not need backward computation.
I1226 13:58:02.468588 91478 net.cpp:307] data does not need backward computation.
I1226 13:58:02.468631 91478 net.cpp:349] This network produces output accuracy
I1226 13:58:02.468667 91478 net.cpp:349] This network produces output loss
I1226 13:58:02.468755 91478 net.cpp:363] Network initialization done.
I1226 13:58:02.469238 91478 solver.cpp:107] Solver scaffolding done.
I1226 13:58:02.469465 91478 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 13:58:02.459683 91481 net.cpp:228] Setting up loss
I1226 13:58:02.459790 91481 net.cpp:235] Top shape: (1)
I1226 13:58:02.459992 91481 net.cpp:238]     with loss weight 1
I1226 13:58:02.460140 91481 net.cpp:243] Memory required for data: 266472200
I1226 13:58:02.460187 91481 net.cpp:305] loss needs backward computation.
I1226 13:58:02.460230 91481 net.cpp:307] accuracy does not need backward computation.
I1226 13:58:02.460263 91481 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 13:58:02.460296 91481 net.cpp:305] fc8 needs backward computation.
I1226 13:58:02.460342 91481 net.cpp:305] drop7 needs backward computation.
I1226 13:58:02.460372 91481 net.cpp:305] relu7 needs backward computation.
I1226 13:58:02.460402 91481 net.cpp:305] fc7 needs backward computation.
I1226 13:58:02.460433 91481 net.cpp:305] drop6 needs backward computation.
I1226 13:58:02.460464 91481 net.cpp:305] relu6 needs backward computation.
I1226 13:58:02.460494 91481 net.cpp:305] fc6 needs backward computation.
I1226 13:58:02.460525 91481 net.cpp:305] pool5 needs backward computation.
I1226 13:58:02.460557 91481 net.cpp:305] relu5 needs backward computation.
I1226 13:58:02.460595 91481 net.cpp:305] conv5 needs backward computation.
I1226 13:58:02.460626 91481 net.cpp:305] relu4 needs backward computation.
I1226 13:58:02.460661 91481 net.cpp:305] conv4 needs backward computation.
I1226 13:58:02.460695 91481 net.cpp:305] relu3 needs backward computation.
I1226 13:58:02.460723 91481 net.cpp:305] conv3 needs backward computation.
I1226 13:58:02.460757 91481 net.cpp:305] pool2 needs backward computation.
I1226 13:58:02.460788 91481 net.cpp:305] norm2 needs backward computation.
I1226 13:58:02.460819 91481 net.cpp:305] relu2 needs backward computation.
I1226 13:58:02.460850 91481 net.cpp:305] conv2 needs backward computation.
I1226 13:58:02.460908 91481 net.cpp:305] pool1 needs backward computation.
I1226 13:58:02.460963 91481 net.cpp:305] norm1 needs backward computation.
I1226 13:58:02.460997 91481 net.cpp:305] relu1 needs backward computation.
I1226 13:58:02.461040 91481 net.cpp:305] conv1 needs backward computation.
I1226 13:58:02.461073 91481 net.cpp:307] label_data_1_split does not need backward computation.
I1226 13:58:02.461107 91481 net.cpp:307] data does not need backward computation.
I1226 13:58:02.461134 91481 net.cpp:349] This network produces output accuracy
I1226 13:58:02.461177 91481 net.cpp:349] This network produces output loss
I1226 13:58:02.461267 91481 net.cpp:363] Network initialization done.
I1226 13:58:02.461710 91481 solver.cpp:107] Solver scaffolding done.
I1226 13:58:02.461943 91481 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 13:58:02.483366 92257 net.cpp:228] Setting up fc8
I1226 13:58:02.483497 92257 net.cpp:235] Top shape: 32 1000 (32000)
I1226 13:58:02.483536 92257 net.cpp:243] Memory required for data: 266216192
I1226 13:58:02.483639 92257 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 13:58:02.483716 92257 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 13:58:02.483757 92257 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 13:58:02.483803 92257 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 13:58:02.483875 92257 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 13:58:02.483980 92257 net.cpp:228] Setting up fc8_fc8_0_split
I1226 13:58:02.484038 92257 net.cpp:235] Top shape: 32 1000 (32000)
I1226 13:58:02.484076 92257 net.cpp:235] Top shape: 32 1000 (32000)
I1226 13:58:02.484104 92257 net.cpp:243] Memory required for data: 266472192
I1226 13:58:02.484140 92257 layer_factory.hpp:114] Creating layer accuracy
I1226 13:58:02.484211 92257 net.cpp:178] Creating Layer accuracy
I1226 13:58:02.484246 92257 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 13:58:02.484283 92257 net.cpp:612] accuracy <- label_data_1_split_0
I1226 13:58:02.484362 92257 net.cpp:586] accuracy -> accuracy
I1226 13:58:02.484422 92257 net.cpp:228] Setting up accuracy
I1226 13:58:02.484465 92257 net.cpp:235] Top shape: (1)
I1226 13:58:02.484493 92257 net.cpp:243] Memory required for data: 266472196
I1226 13:58:02.484527 92257 layer_factory.hpp:114] Creating layer loss
I1226 13:58:02.484580 92257 net.cpp:178] Creating Layer loss
I1226 13:58:02.484612 92257 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 13:58:02.484668 92257 net.cpp:612] loss <- label_data_1_split_1
I1226 13:58:02.484871 92257 net.cpp:586] loss -> loss
I1226 13:58:02.484949 92257 layer_factory.hpp:114] Creating layer loss
I1226 13:58:02.508545 92257 net.cpp:228] Setting up loss
I1226 13:58:02.508656 92257 net.cpp:235] Top shape: (1)
I1226 13:58:02.508816 92257 net.cpp:238]     with loss weight 1
I1226 13:58:02.508961 92257 net.cpp:243] Memory required for data: 266472200
I1226 13:58:02.509052 92257 net.cpp:305] loss needs backward computation.
I1226 13:58:02.509246 92257 net.cpp:307] accuracy does not need backward computation.
I1226 13:58:02.509289 92257 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 13:58:02.509346 92257 net.cpp:305] fc8 needs backward computation.
I1226 13:58:02.509553 92257 net.cpp:305] drop7 needs backward computation.
I1226 13:58:02.509587 92257 net.cpp:305] relu7 needs backward computation.
I1226 13:58:02.509814 92257 net.cpp:305] fc7 needs backward computation.
I1226 13:58:02.509847 92257 net.cpp:305] drop6 needs backward computation.
I1226 13:58:02.509876 92257 net.cpp:305] relu6 needs backward computation.
I1226 13:58:02.509907 92257 net.cpp:305] fc6 needs backward computation.
I1226 13:58:02.510121 92257 net.cpp:305] pool5 needs backward computation.
I1226 13:58:02.510154 92257 net.cpp:305] relu5 needs backward computation.
I1226 13:58:02.510185 92257 net.cpp:305] conv5 needs backward computation.
I1226 13:58:02.510215 92257 net.cpp:305] relu4 needs backward computation.
I1226 13:58:02.510447 92257 net.cpp:305] conv4 needs backward computation.
I1226 13:58:02.510478 92257 net.cpp:305] relu3 needs backward computation.
I1226 13:58:02.510509 92257 net.cpp:305] conv3 needs backward computation.
I1226 13:58:02.510550 92257 net.cpp:305] pool2 needs backward computation.
I1226 13:58:02.510582 92257 net.cpp:305] norm2 needs backward computation.
I1226 13:58:02.510620 92257 net.cpp:305] relu2 needs backward computation.
I1226 13:58:02.510651 92257 net.cpp:305] conv2 needs backward computation.
I1226 13:58:02.510684 92257 net.cpp:305] pool1 needs backward computation.
I1226 13:58:02.510718 92257 net.cpp:305] norm1 needs backward computation.
I1226 13:58:02.510771 92257 net.cpp:305] relu1 needs backward computation.
I1226 13:58:02.510804 92257 net.cpp:305] conv1 needs backward computation.
I1226 13:58:02.510869 92257 net.cpp:307] label_data_1_split does not need backward computation.
I1226 13:58:02.510908 92257 net.cpp:307] data does not need backward computation.
I1226 13:58:02.510959 92257 net.cpp:349] This network produces output accuracy
I1226 13:58:02.510994 92257 net.cpp:349] This network produces output loss
I1226 13:58:02.511085 92257 net.cpp:363] Network initialization done.
I1226 13:58:02.511580 92257 solver.cpp:107] Solver scaffolding done.
I1226 13:58:02.511813 92257 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 13:58:02.607422 92037 net.cpp:228] Setting up fc8
I1226 13:58:02.607552 92037 net.cpp:235] Top shape: 32 1000 (32000)
I1226 13:58:02.607583 92037 net.cpp:243] Memory required for data: 266216192
I1226 13:58:02.607645 92037 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 13:58:02.607717 92037 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 13:58:02.607755 92037 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 13:58:02.607800 92037 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 13:58:02.607869 92037 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 13:58:02.607969 92037 net.cpp:228] Setting up fc8_fc8_0_split
I1226 13:58:02.608034 92037 net.cpp:235] Top shape: 32 1000 (32000)
I1226 13:58:02.608072 92037 net.cpp:235] Top shape: 32 1000 (32000)
I1226 13:58:02.608095 92037 net.cpp:243] Memory required for data: 266472192
I1226 13:58:02.608129 92037 layer_factory.hpp:114] Creating layer accuracy
I1226 13:58:02.608186 92037 net.cpp:178] Creating Layer accuracy
I1226 13:58:02.608218 92037 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 13:58:02.608252 92037 net.cpp:612] accuracy <- label_data_1_split_0
I1226 13:58:02.608292 92037 net.cpp:586] accuracy -> accuracy
I1226 13:58:02.608369 92037 net.cpp:228] Setting up accuracy
I1226 13:58:02.608412 92037 net.cpp:235] Top shape: (1)
I1226 13:58:02.608435 92037 net.cpp:243] Memory required for data: 266472196
I1226 13:58:02.608466 92037 layer_factory.hpp:114] Creating layer loss
I1226 13:58:02.608516 92037 net.cpp:178] Creating Layer loss
I1226 13:58:02.608546 92037 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 13:58:02.608593 92037 net.cpp:612] loss <- label_data_1_split_1
I1226 13:58:02.608634 92037 net.cpp:586] loss -> loss
I1226 13:58:02.608695 92037 layer_factory.hpp:114] Creating layer loss
I1226 13:58:02.633183 92037 net.cpp:228] Setting up loss
I1226 13:58:02.633286 92037 net.cpp:235] Top shape: (1)
I1226 13:58:02.633421 92037 net.cpp:238]     with loss weight 1
I1226 13:58:02.633523 92037 net.cpp:243] Memory required for data: 266472200
I1226 13:58:02.633558 92037 net.cpp:305] loss needs backward computation.
I1226 13:58:02.633594 92037 net.cpp:307] accuracy does not need backward computation.
I1226 13:58:02.633626 92037 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 13:58:02.633656 92037 net.cpp:305] fc8 needs backward computation.
I1226 13:58:02.633688 92037 net.cpp:305] drop7 needs backward computation.
I1226 13:58:02.633718 92037 net.cpp:305] relu7 needs backward computation.
I1226 13:58:02.633749 92037 net.cpp:305] fc7 needs backward computation.
I1226 13:58:02.633781 92037 net.cpp:305] drop6 needs backward computation.
I1226 13:58:02.633813 92037 net.cpp:305] relu6 needs backward computation.
I1226 13:58:02.633844 92037 net.cpp:305] fc6 needs backward computation.
I1226 13:58:02.633877 92037 net.cpp:305] pool5 needs backward computation.
I1226 13:58:02.633909 92037 net.cpp:305] relu5 needs backward computation.
I1226 13:58:02.633939 92037 net.cpp:305] conv5 needs backward computation.
I1226 13:58:02.633971 92037 net.cpp:305] relu4 needs backward computation.
I1226 13:58:02.634001 92037 net.cpp:305] conv4 needs backward computation.
I1226 13:58:02.634034 92037 net.cpp:305] relu3 needs backward computation.
I1226 13:58:02.634063 92037 net.cpp:305] conv3 needs backward computation.
I1226 13:58:02.634096 92037 net.cpp:305] pool2 needs backward computation.
I1226 13:58:02.634127 92037 net.cpp:305] norm2 needs backward computation.
I1226 13:58:02.634160 92037 net.cpp:305] relu2 needs backward computation.
I1226 13:58:02.634189 92037 net.cpp:305] conv2 needs backward computation.
I1226 13:58:02.634222 92037 net.cpp:305] pool1 needs backward computation.
I1226 13:58:02.634253 92037 net.cpp:305] norm1 needs backward computation.
I1226 13:58:02.634286 92037 net.cpp:305] relu1 needs backward computation.
I1226 13:58:02.634335 92037 net.cpp:305] conv1 needs backward computation.
I1226 13:58:02.634387 92037 net.cpp:307] label_data_1_split does not need backward computation.
I1226 13:58:02.634420 92037 net.cpp:307] data does not need backward computation.
I1226 13:58:02.634449 92037 net.cpp:349] This network produces output accuracy
I1226 13:58:02.634485 92037 net.cpp:349] This network produces output loss
I1226 13:58:02.634570 92037 net.cpp:363] Network initialization done.
I1226 13:58:02.634997 92037 solver.cpp:107] Solver scaffolding done.
I1226 13:58:02.635188 92037 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 13:58:03.438002 93154 caffe.cpp:376] Configuring multinode setup
I1226 13:58:03.440498 93154 caffe.cpp:386] Starting parameter server in mpi environment
I1226 13:58:05.520861 94427 caffe.cpp:376] Configuring multinode setup
I1226 13:58:05.522297 94427 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 13:58:05.676563 91478 caffe.cpp:376] Configuring multinode setup
I1226 13:58:05.677922 91478 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 13:58:05.770656 92257 caffe.cpp:376] Configuring multinode setup
I1226 13:58:05.772099 92257 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 13:58:05.902395 92037 caffe.cpp:376] Configuring multinode setup
I1226 13:58:05.903831 92037 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 13:58:06.543006 91481 caffe.cpp:376] Configuring multinode setup
I1226 13:58:06.544497 91481 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 13:58:25.807294 95085 net.cpp:228] Setting up fc6
I1226 13:58:25.807565 95085 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:58:25.807610 95085 net.cpp:243] Memory required for data: 263466752
I1226 13:58:25.807677 95085 layer_factory.hpp:114] Creating layer relu6
I1226 13:58:25.807890 95085 net.cpp:178] Creating Layer relu6
I1226 13:58:25.807950 95085 net.cpp:612] relu6 <- fc6
I1226 13:58:25.808001 95085 net.cpp:573] relu6 -> fc6 (in-place)
I1226 13:58:25.808117 95085 net.cpp:228] Setting up relu6
I1226 13:58:25.808171 95085 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:58:25.808198 95085 net.cpp:243] Memory required for data: 263991040
I1226 13:58:25.808230 95085 layer_factory.hpp:114] Creating layer drop6
I1226 13:58:25.808295 95085 net.cpp:178] Creating Layer drop6
I1226 13:58:25.808336 95085 net.cpp:612] drop6 <- fc6
I1226 13:58:25.808375 95085 net.cpp:573] drop6 -> fc6 (in-place)
I1226 13:58:25.808437 95085 net.cpp:228] Setting up drop6
I1226 13:58:25.808482 95085 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:58:25.808508 95085 net.cpp:243] Memory required for data: 264515328
I1226 13:58:25.808545 95085 layer_factory.hpp:114] Creating layer fc7
I1226 13:58:25.808612 95085 net.cpp:178] Creating Layer fc7
I1226 13:58:25.808647 95085 net.cpp:612] fc7 <- fc6
I1226 13:58:25.808703 95085 net.cpp:586] fc7 -> fc7
I1226 13:58:26.096499 96716 net.cpp:228] Setting up fc6
I1226 13:58:26.096758 96716 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:58:26.096843 96716 net.cpp:243] Memory required for data: 263466752
I1226 13:58:26.096920 96716 layer_factory.hpp:114] Creating layer relu6
I1226 13:58:26.097003 96716 net.cpp:178] Creating Layer relu6
I1226 13:58:26.097050 96716 net.cpp:612] relu6 <- fc6
I1226 13:58:26.097093 96716 net.cpp:573] relu6 -> fc6 (in-place)
I1226 13:58:26.097198 96716 net.cpp:228] Setting up relu6
I1226 13:58:26.097249 96716 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:58:26.097277 96716 net.cpp:243] Memory required for data: 263991040
I1226 13:58:26.097311 96716 layer_factory.hpp:114] Creating layer drop6
I1226 13:58:26.097374 96716 net.cpp:178] Creating Layer drop6
I1226 13:58:26.097409 96716 net.cpp:612] drop6 <- fc6
I1226 13:58:26.097448 96716 net.cpp:573] drop6 -> fc6 (in-place)
I1226 13:58:26.097523 96716 net.cpp:228] Setting up drop6
I1226 13:58:26.097573 96716 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:58:26.097599 96716 net.cpp:243] Memory required for data: 264515328
I1226 13:58:26.097635 96716 layer_factory.hpp:114] Creating layer fc7
I1226 13:58:26.097718 96716 net.cpp:178] Creating Layer fc7
I1226 13:58:26.097757 96716 net.cpp:612] fc7 <- fc6
I1226 13:58:26.097837 96716 net.cpp:586] fc7 -> fc7
I1226 13:58:26.124691 97230 net.cpp:228] Setting up fc6
I1226 13:58:26.124984 97230 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:58:26.125030 97230 net.cpp:243] Memory required for data: 263466752
I1226 13:58:26.125092 97230 layer_factory.hpp:114] Creating layer relu6
I1226 13:58:26.125164 97230 net.cpp:178] Creating Layer relu6
I1226 13:58:26.125213 97230 net.cpp:612] relu6 <- fc6
I1226 13:58:26.125255 97230 net.cpp:573] relu6 -> fc6 (in-place)
I1226 13:58:26.125370 97230 net.cpp:228] Setting up relu6
I1226 13:58:26.125430 97230 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:58:26.125458 97230 net.cpp:243] Memory required for data: 263991040
I1226 13:58:26.125504 97230 layer_factory.hpp:114] Creating layer drop6
I1226 13:58:26.125568 97230 net.cpp:178] Creating Layer drop6
I1226 13:58:26.125638 97230 net.cpp:612] drop6 <- fc6
I1226 13:58:26.125689 97230 net.cpp:573] drop6 -> fc6 (in-place)
I1226 13:58:26.125779 97230 net.cpp:228] Setting up drop6
I1226 13:58:26.125831 97230 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:58:26.125891 97230 net.cpp:243] Memory required for data: 264515328
I1226 13:58:26.125923 97230 layer_factory.hpp:114] Creating layer fc7
I1226 13:58:26.126009 97230 net.cpp:178] Creating Layer fc7
I1226 13:58:26.126044 97230 net.cpp:612] fc7 <- fc6
I1226 13:58:26.126127 97230 net.cpp:586] fc7 -> fc7
I1226 13:58:38.870443 95085 net.cpp:228] Setting up fc7
I1226 13:58:38.870558 95085 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:58:38.870594 95085 net.cpp:243] Memory required for data: 265039616
I1226 13:58:38.870654 95085 layer_factory.hpp:114] Creating layer relu7
I1226 13:58:38.870734 95085 net.cpp:178] Creating Layer relu7
I1226 13:58:38.870780 95085 net.cpp:612] relu7 <- fc7
I1226 13:58:38.870872 95085 net.cpp:573] relu7 -> fc7 (in-place)
I1226 13:58:38.870985 95085 net.cpp:228] Setting up relu7
I1226 13:58:38.871042 95085 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:58:38.871073 95085 net.cpp:243] Memory required for data: 265563904
I1226 13:58:38.871114 95085 layer_factory.hpp:114] Creating layer drop7
I1226 13:58:38.871160 95085 net.cpp:178] Creating Layer drop7
I1226 13:58:38.871194 95085 net.cpp:612] drop7 <- fc7
I1226 13:58:38.871253 95085 net.cpp:573] drop7 -> fc7 (in-place)
I1226 13:58:38.871309 95085 net.cpp:228] Setting up drop7
I1226 13:58:38.871352 95085 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:58:38.871382 95085 net.cpp:243] Memory required for data: 266088192
I1226 13:58:38.871419 95085 layer_factory.hpp:114] Creating layer fc8
I1226 13:58:38.871482 95085 net.cpp:178] Creating Layer fc8
I1226 13:58:38.871516 95085 net.cpp:612] fc8 <- fc7
I1226 13:58:38.871567 95085 net.cpp:586] fc8 -> fc8
I1226 13:58:39.155215 96716 net.cpp:228] Setting up fc7
I1226 13:58:39.155326 96716 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:58:39.155364 96716 net.cpp:243] Memory required for data: 265039616
I1226 13:58:39.155427 96716 layer_factory.hpp:114] Creating layer relu7
I1226 13:58:39.155519 96716 net.cpp:178] Creating Layer relu7
I1226 13:58:39.155573 96716 net.cpp:612] relu7 <- fc7
I1226 13:58:39.155619 96716 net.cpp:573] relu7 -> fc7 (in-place)
I1226 13:58:39.155716 96716 net.cpp:228] Setting up relu7
I1226 13:58:39.155766 96716 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:58:39.155820 96716 net.cpp:243] Memory required for data: 265563904
I1226 13:58:39.155859 96716 layer_factory.hpp:114] Creating layer drop7
I1226 13:58:39.155900 96716 net.cpp:178] Creating Layer drop7
I1226 13:58:39.155928 96716 net.cpp:612] drop7 <- fc7
I1226 13:58:39.155982 96716 net.cpp:573] drop7 -> fc7 (in-place)
I1226 13:58:39.156033 96716 net.cpp:228] Setting up drop7
I1226 13:58:39.156071 96716 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:58:39.156097 96716 net.cpp:243] Memory required for data: 266088192
I1226 13:58:39.156126 96716 layer_factory.hpp:114] Creating layer fc8
I1226 13:58:39.156185 96716 net.cpp:178] Creating Layer fc8
I1226 13:58:39.156227 96716 net.cpp:612] fc8 <- fc7
I1226 13:58:39.156272 96716 net.cpp:586] fc8 -> fc8
I1226 13:58:39.194397 97230 net.cpp:228] Setting up fc7
I1226 13:58:39.194516 97230 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:58:39.194551 97230 net.cpp:243] Memory required for data: 265039616
I1226 13:58:39.194615 97230 layer_factory.hpp:114] Creating layer relu7
I1226 13:58:39.194828 97230 net.cpp:178] Creating Layer relu7
I1226 13:58:39.194874 97230 net.cpp:612] relu7 <- fc7
I1226 13:58:39.194914 97230 net.cpp:573] relu7 -> fc7 (in-place)
I1226 13:58:39.195008 97230 net.cpp:228] Setting up relu7
I1226 13:58:39.195051 97230 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:58:39.195076 97230 net.cpp:243] Memory required for data: 265563904
I1226 13:58:39.195106 97230 layer_factory.hpp:114] Creating layer drop7
I1226 13:58:39.195143 97230 net.cpp:178] Creating Layer drop7
I1226 13:58:39.195169 97230 net.cpp:612] drop7 <- fc7
I1226 13:58:39.195216 97230 net.cpp:573] drop7 -> fc7 (in-place)
I1226 13:58:39.195263 97230 net.cpp:228] Setting up drop7
I1226 13:58:39.195297 97230 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:58:39.195319 97230 net.cpp:243] Memory required for data: 266088192
I1226 13:58:39.195346 97230 layer_factory.hpp:114] Creating layer fc8
I1226 13:58:39.195400 97230 net.cpp:178] Creating Layer fc8
I1226 13:58:39.195427 97230 net.cpp:612] fc8 <- fc7
I1226 13:58:39.195463 97230 net.cpp:586] fc8 -> fc8
I1226 13:58:42.061367 95085 net.cpp:228] Setting up fc8
I1226 13:58:42.061483 95085 net.cpp:235] Top shape: 32 1000 (32000)
I1226 13:58:42.061522 95085 net.cpp:243] Memory required for data: 266216192
I1226 13:58:42.061586 95085 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 13:58:42.061764 95085 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 13:58:42.061841 95085 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 13:58:42.061897 95085 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 13:58:42.061951 95085 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 13:58:42.062063 95085 net.cpp:228] Setting up fc8_fc8_0_split
I1226 13:58:42.062124 95085 net.cpp:235] Top shape: 32 1000 (32000)
I1226 13:58:42.062157 95085 net.cpp:235] Top shape: 32 1000 (32000)
I1226 13:58:42.062183 95085 net.cpp:243] Memory required for data: 266472192
I1226 13:58:42.062216 95085 layer_factory.hpp:114] Creating layer accuracy
I1226 13:58:42.062296 95085 net.cpp:178] Creating Layer accuracy
I1226 13:58:42.062340 95085 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 13:58:42.062386 95085 net.cpp:612] accuracy <- label_data_1_split_0
I1226 13:58:42.062425 95085 net.cpp:586] accuracy -> accuracy
I1226 13:58:42.062475 95085 net.cpp:228] Setting up accuracy
I1226 13:58:42.062522 95085 net.cpp:235] Top shape: (1)
I1226 13:58:42.062553 95085 net.cpp:243] Memory required for data: 266472196
I1226 13:58:42.062584 95085 layer_factory.hpp:114] Creating layer loss
I1226 13:58:42.062744 95085 net.cpp:178] Creating Layer loss
I1226 13:58:42.062788 95085 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 13:58:42.062854 95085 net.cpp:612] loss <- label_data_1_split_1
I1226 13:58:42.062896 95085 net.cpp:586] loss -> loss
I1226 13:58:42.062975 95085 layer_factory.hpp:114] Creating layer loss
I1226 13:58:42.091771 95085 net.cpp:228] Setting up loss
I1226 13:58:42.091909 95085 net.cpp:235] Top shape: (1)
I1226 13:58:42.091948 95085 net.cpp:238]     with loss weight 1
I1226 13:58:42.092093 95085 net.cpp:243] Memory required for data: 266472200
I1226 13:58:42.092152 95085 net.cpp:305] loss needs backward computation.
I1226 13:58:42.092200 95085 net.cpp:307] accuracy does not need backward computation.
I1226 13:58:42.092247 95085 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 13:58:42.092279 95085 net.cpp:305] fc8 needs backward computation.
I1226 13:58:42.092313 95085 net.cpp:305] drop7 needs backward computation.
I1226 13:58:42.092344 95085 net.cpp:305] relu7 needs backward computation.
I1226 13:58:42.092381 95085 net.cpp:305] fc7 needs backward computation.
I1226 13:58:42.092411 95085 net.cpp:305] drop6 needs backward computation.
I1226 13:58:42.092442 95085 net.cpp:305] relu6 needs backward computation.
I1226 13:58:42.092473 95085 net.cpp:305] fc6 needs backward computation.
I1226 13:58:42.092504 95085 net.cpp:305] pool5 needs backward computation.
I1226 13:58:42.092535 95085 net.cpp:305] relu5 needs backward computation.
I1226 13:58:42.092572 95085 net.cpp:305] conv5 needs backward computation.
I1226 13:58:42.092603 95085 net.cpp:305] relu4 needs backward computation.
I1226 13:58:42.092634 95085 net.cpp:305] conv4 needs backward computation.
I1226 13:58:42.092672 95085 net.cpp:305] relu3 needs backward computation.
I1226 13:58:42.092712 95085 net.cpp:305] conv3 needs backward computation.
I1226 13:58:42.092746 95085 net.cpp:305] pool2 needs backward computation.
I1226 13:58:42.092777 95085 net.cpp:305] norm2 needs backward computation.
I1226 13:58:42.092839 95085 net.cpp:305] relu2 needs backward computation.
I1226 13:58:42.092875 95085 net.cpp:305] conv2 needs backward computation.
I1226 13:58:42.092907 95085 net.cpp:305] pool1 needs backward computation.
I1226 13:58:42.092938 95085 net.cpp:305] norm1 needs backward computation.
I1226 13:58:42.092980 95085 net.cpp:305] relu1 needs backward computation.
I1226 13:58:42.093022 95085 net.cpp:305] conv1 needs backward computation.
I1226 13:58:42.093063 95085 net.cpp:307] label_data_1_split does not need backward computation.
I1226 13:58:42.093106 95085 net.cpp:307] data does not need backward computation.
I1226 13:58:42.093134 95085 net.cpp:349] This network produces output accuracy
I1226 13:58:42.093170 95085 net.cpp:349] This network produces output loss
I1226 13:58:42.093286 95085 net.cpp:363] Network initialization done.
I1226 13:58:42.093740 95085 solver.cpp:107] Solver scaffolding done.
I1226 13:58:42.094000 95085 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 13:58:42.352947 96716 net.cpp:228] Setting up fc8
I1226 13:58:42.353070 96716 net.cpp:235] Top shape: 32 1000 (32000)
I1226 13:58:42.353106 96716 net.cpp:243] Memory required for data: 266216192
I1226 13:58:42.353173 96716 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 13:58:42.353364 96716 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 13:58:42.353410 96716 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 13:58:42.353453 96716 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 13:58:42.353523 96716 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 13:58:42.353631 96716 net.cpp:228] Setting up fc8_fc8_0_split
I1226 13:58:42.353693 96716 net.cpp:235] Top shape: 32 1000 (32000)
I1226 13:58:42.353725 96716 net.cpp:235] Top shape: 32 1000 (32000)
I1226 13:58:42.353749 96716 net.cpp:243] Memory required for data: 266472192
I1226 13:58:42.353780 96716 layer_factory.hpp:114] Creating layer accuracy
I1226 13:58:42.353865 96716 net.cpp:178] Creating Layer accuracy
I1226 13:58:42.353907 96716 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 13:58:42.353943 96716 net.cpp:612] accuracy <- label_data_1_split_0
I1226 13:58:42.353981 96716 net.cpp:586] accuracy -> accuracy
I1226 13:58:42.354038 96716 net.cpp:228] Setting up accuracy
I1226 13:58:42.354076 96716 net.cpp:235] Top shape: (1)
I1226 13:58:42.354100 96716 net.cpp:243] Memory required for data: 266472196
I1226 13:58:42.354140 96716 layer_factory.hpp:114] Creating layer loss
I1226 13:58:42.354297 96716 net.cpp:178] Creating Layer loss
I1226 13:58:42.354336 96716 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 13:58:42.354382 96716 net.cpp:612] loss <- label_data_1_split_1
I1226 13:58:42.354435 96716 net.cpp:586] loss -> loss
I1226 13:58:42.354516 96716 layer_factory.hpp:114] Creating layer loss
I1226 13:58:42.383690 96716 net.cpp:228] Setting up loss
I1226 13:58:42.383836 96716 net.cpp:235] Top shape: (1)
I1226 13:58:42.383889 96716 net.cpp:238]     with loss weight 1
I1226 13:58:42.384009 96716 net.cpp:243] Memory required for data: 266472200
I1226 13:58:42.384050 96716 net.cpp:305] loss needs backward computation.
I1226 13:58:42.384089 96716 net.cpp:307] accuracy does not need backward computation.
I1226 13:58:42.384124 96716 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 13:58:42.384153 96716 net.cpp:305] fc8 needs backward computation.
I1226 13:58:42.384182 96716 net.cpp:305] drop7 needs backward computation.
I1226 13:58:42.384210 96716 net.cpp:305] relu7 needs backward computation.
I1226 13:58:42.384238 96716 net.cpp:305] fc7 needs backward computation.
I1226 13:58:42.384266 96716 net.cpp:305] drop6 needs backward computation.
I1226 13:58:42.384294 96716 net.cpp:305] relu6 needs backward computation.
I1226 13:58:42.384321 96716 net.cpp:305] fc6 needs backward computation.
I1226 13:58:42.384349 96716 net.cpp:305] pool5 needs backward computation.
I1226 13:58:42.384378 96716 net.cpp:305] relu5 needs backward computation.
I1226 13:58:42.384407 96716 net.cpp:305] conv5 needs backward computation.
I1226 13:58:42.384436 96716 net.cpp:305] relu4 needs backward computation.
I1226 13:58:42.384464 96716 net.cpp:305] conv4 needs backward computation.
I1226 13:58:42.384495 96716 net.cpp:305] relu3 needs backward computation.
I1226 13:58:42.384533 96716 net.cpp:305] conv3 needs backward computation.
I1226 13:58:42.384575 96716 net.cpp:305] pool2 needs backward computation.
I1226 13:58:42.384608 96716 net.cpp:305] norm2 needs backward computation.
I1226 13:58:42.384637 96716 net.cpp:305] relu2 needs backward computation.
I1226 13:58:42.384704 96716 net.cpp:305] conv2 needs backward computation.
I1226 13:58:42.384737 96716 net.cpp:305] pool1 needs backward computation.
I1226 13:58:42.384768 96716 net.cpp:305] norm1 needs backward computation.
I1226 13:58:42.384817 96716 net.cpp:305] relu1 needs backward computation.
I1226 13:58:42.384850 96716 net.cpp:305] conv1 needs backward computation.
I1226 13:58:42.384882 96716 net.cpp:307] label_data_1_split does not need backward computation.
I1226 13:58:42.384913 96716 net.cpp:307] data does not need backward computation.
I1226 13:58:42.384940 96716 net.cpp:349] This network produces output accuracy
I1226 13:58:42.385004 96716 net.cpp:349] This network produces output loss
I1226 13:58:42.385104 96716 net.cpp:363] Network initialization done.
I1226 13:58:42.385875 96716 solver.cpp:107] Solver scaffolding done.
I1226 13:58:42.386153 96716 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 13:58:42.393235 97230 net.cpp:228] Setting up fc8
I1226 13:58:42.393357 97230 net.cpp:235] Top shape: 32 1000 (32000)
I1226 13:58:42.393394 97230 net.cpp:243] Memory required for data: 266216192
I1226 13:58:42.393458 97230 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 13:58:42.393551 97230 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 13:58:42.393610 97230 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 13:58:42.393656 97230 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 13:58:42.393741 97230 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 13:58:42.393858 97230 net.cpp:228] Setting up fc8_fc8_0_split
I1226 13:58:42.393929 97230 net.cpp:235] Top shape: 32 1000 (32000)
I1226 13:58:42.393967 97230 net.cpp:235] Top shape: 32 1000 (32000)
I1226 13:58:42.393991 97230 net.cpp:243] Memory required for data: 266472192
I1226 13:58:42.394035 97230 layer_factory.hpp:114] Creating layer accuracy
I1226 13:58:42.394099 97230 net.cpp:178] Creating Layer accuracy
I1226 13:58:42.394140 97230 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 13:58:42.394172 97230 net.cpp:612] accuracy <- label_data_1_split_0
I1226 13:58:42.394210 97230 net.cpp:586] accuracy -> accuracy
I1226 13:58:42.394269 97230 net.cpp:228] Setting up accuracy
I1226 13:58:42.394315 97230 net.cpp:235] Top shape: (1)
I1226 13:58:42.394340 97230 net.cpp:243] Memory required for data: 266472196
I1226 13:58:42.394371 97230 layer_factory.hpp:114] Creating layer loss
I1226 13:58:42.394528 97230 net.cpp:178] Creating Layer loss
I1226 13:58:42.394574 97230 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 13:58:42.394609 97230 net.cpp:612] loss <- label_data_1_split_1
I1226 13:58:42.394665 97230 net.cpp:586] loss -> loss
I1226 13:58:42.394765 97230 layer_factory.hpp:114] Creating layer loss
I1226 13:58:42.428115 97230 net.cpp:228] Setting up loss
I1226 13:58:42.428232 97230 net.cpp:235] Top shape: (1)
I1226 13:58:42.428272 97230 net.cpp:238]     with loss weight 1
I1226 13:58:42.428434 97230 net.cpp:243] Memory required for data: 266472200
I1226 13:58:42.428490 97230 net.cpp:305] loss needs backward computation.
I1226 13:58:42.428549 97230 net.cpp:307] accuracy does not need backward computation.
I1226 13:58:42.428601 97230 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 13:58:42.428645 97230 net.cpp:305] fc8 needs backward computation.
I1226 13:58:42.428690 97230 net.cpp:305] drop7 needs backward computation.
I1226 13:58:42.428756 97230 net.cpp:305] relu7 needs backward computation.
I1226 13:58:42.428791 97230 net.cpp:305] fc7 needs backward computation.
I1226 13:58:42.428823 97230 net.cpp:305] drop6 needs backward computation.
I1226 13:58:42.428855 97230 net.cpp:305] relu6 needs backward computation.
I1226 13:58:42.428884 97230 net.cpp:305] fc6 needs backward computation.
I1226 13:58:42.428915 97230 net.cpp:305] pool5 needs backward computation.
I1226 13:58:42.428946 97230 net.cpp:305] relu5 needs backward computation.
I1226 13:58:42.428989 97230 net.cpp:305] conv5 needs backward computation.
I1226 13:58:42.429019 97230 net.cpp:305] relu4 needs backward computation.
I1226 13:58:42.429050 97230 net.cpp:305] conv4 needs backward computation.
I1226 13:58:42.429081 97230 net.cpp:305] relu3 needs backward computation.
I1226 13:58:42.429111 97230 net.cpp:305] conv3 needs backward computation.
I1226 13:58:42.429143 97230 net.cpp:305] pool2 needs backward computation.
I1226 13:58:42.429174 97230 net.cpp:305] norm2 needs backward computation.
I1226 13:58:42.429217 97230 net.cpp:305] relu2 needs backward computation.
I1226 13:58:42.429247 97230 net.cpp:305] conv2 needs backward computation.
I1226 13:58:42.429278 97230 net.cpp:305] pool1 needs backward computation.
I1226 13:58:42.429309 97230 net.cpp:305] norm1 needs backward computation.
I1226 13:58:42.429339 97230 net.cpp:305] relu1 needs backward computation.
I1226 13:58:42.429368 97230 net.cpp:305] conv1 needs backward computation.
I1226 13:58:42.429401 97230 net.cpp:307] label_data_1_split does not need backward computation.
I1226 13:58:42.429441 97230 net.cpp:307] data does not need backward computation.
I1226 13:58:42.429469 97230 net.cpp:349] This network produces output accuracy
I1226 13:58:42.429502 97230 net.cpp:349] This network produces output loss
I1226 13:58:42.429610 97230 net.cpp:363] Network initialization done.
I1226 13:58:42.430088 97230 solver.cpp:107] Solver scaffolding done.
I1226 13:58:42.430315 97230 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 13:58:46.659648 95085 caffe.cpp:376] Configuring multinode setup
I1226 13:58:46.661557 95085 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 13:58:46.940835 96716 caffe.cpp:376] Configuring multinode setup
I1226 13:58:46.942657 96716 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 13:58:46.979001 97230 caffe.cpp:376] Configuring multinode setup
I1226 13:58:46.980888 97230 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 13:58:46.981578 96716 SynchronousNode.cpp:675] [6] [proc 6] solving
I1226 13:58:46.990814 92037 SynchronousNode.cpp:675] [0] [proc 0] solving
I1226 13:58:46.978160 94427 SynchronousNode.cpp:675] [3] [proc 3] solving
I1226 13:58:46.992918 95085 SynchronousNode.cpp:675] [1] [proc 1] solving
I1226 13:58:46.981104 97230 SynchronousNode.cpp:675] [2] [proc 2] solving
I1226 13:58:46.981708 96716 solver.cpp:354] Solving AlexNet
I1226 13:58:46.981750 96716 solver.cpp:355] Learning Rate Policy: step
I1226 13:58:46.991103 92037 solver.cpp:354] Solving AlexNet
I1226 13:58:46.991153 92037 solver.cpp:355] Learning Rate Policy: step
I1226 13:58:46.978478 94427 solver.cpp:354] Solving AlexNet
I1226 13:58:46.993090 95085 solver.cpp:354] Solving AlexNet
I1226 13:58:47.001054 91478 SynchronousNode.cpp:675] [5] [proc 5] solving
I1226 13:58:46.992789 92257 SynchronousNode.cpp:675] [7] [proc 7] solving
I1226 13:58:46.988579 91481 SynchronousNode.cpp:675] [4] [proc 4] solving
I1226 13:58:46.981212 97230 solver.cpp:354] Solving AlexNet
I1226 13:58:46.978551 94427 solver.cpp:355] Learning Rate Policy: step
I1226 13:58:46.993142 95085 solver.cpp:355] Learning Rate Policy: step
I1226 13:58:47.001392 91478 solver.cpp:354] Solving AlexNet
I1226 13:58:46.993098 92257 solver.cpp:354] Solving AlexNet
I1226 13:58:46.988883 91481 solver.cpp:354] Solving AlexNet
I1226 13:58:46.988936 91481 solver.cpp:355] Learning Rate Policy: step
I1226 13:58:46.981253 97230 solver.cpp:355] Learning Rate Policy: step
I1226 13:58:47.001495 91478 solver.cpp:355] Learning Rate Policy: step
I1226 13:58:46.993145 92257 solver.cpp:355] Learning Rate Policy: step
I1226 13:58:47.002272 95156 SynchronousNode.cpp:293] [1] Comm thread started 0 0
I1226 13:58:46.999438 91553 SynchronousNode.cpp:293] [4] Comm thread started 1 0
I1226 13:58:47.012495 91550 SynchronousNode.cpp:293] [5] Comm thread started 1 0
I1226 13:58:47.002768 92111 SynchronousNode.cpp:293] [0] Comm thread started 0 1
I1226 13:58:46.990123 94497 SynchronousNode.cpp:293] [3] Comm thread started 0 0
I1226 13:58:47.003484 97298 SynchronousNode.cpp:293] [2] Comm thread started 0 0
I1226 13:58:47.000046 96789 SynchronousNode.cpp:293] [6] Comm thread started 1 0
I1226 13:58:47.020658 92326 SynchronousNode.cpp:293] [7] Comm thread started 1 0
I1226 13:58:47.039186 92111 SynchronousNode.cpp:479] [0] initialized root of cluster with nodes: 9 and the total iter size is: 8
I1226 13:58:47.213968 92037 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 13:58:47.214049 92037 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 13:58:47.548681 92037 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 13:58:47.548759 92037 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 13:58:47.548797 92037 solver.cpp:291] [0] Iteration 1, loss = 2.70656
I1226 13:58:47.548851 92037 solver.cpp:317]     Train net output #0: accuracy = 0.3125
I1226 13:58:47.548902 92037 solver.cpp:317]     Train net output #1: loss = 2.70656 (* 1 = 2.70656 loss)
I1226 13:58:49.705857 95156 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 13:58:49.705942 95156 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 13:58:49.694267 97298 SynchronousNode.cpp:246] [2] PROFILING END[WaitingToSync]
I1226 13:58:49.694352 97298 SynchronousNode.cpp:247] [2] PROFILING BEGIN[Forward]
I1226 13:58:49.733726 94497 SynchronousNode.cpp:246] [3] PROFILING END[WaitingToSync]
I1226 13:58:49.744114 91553 SynchronousNode.cpp:246] [4] PROFILING END[WaitingToSync]
I1226 13:58:49.733816 94497 SynchronousNode.cpp:247] [3] PROFILING BEGIN[Forward]
I1226 13:58:49.744202 91553 SynchronousNode.cpp:247] [4] PROFILING BEGIN[Forward]
I1226 13:58:49.748692 92326 SynchronousNode.cpp:246] [7] PROFILING END[WaitingToSync]
I1226 13:58:49.748766 92326 SynchronousNode.cpp:247] [7] PROFILING BEGIN[Forward]
I1226 13:58:49.787462 91481 MultiSolver.cpp:92] [4] PROFILING END[Forward]
I1226 13:58:49.787541 91481 MultiSolver.cpp:94] [4] PROFILING BEGIN[Backward]
I1226 13:58:49.778195 94427 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 13:58:49.792881 92257 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 13:58:49.778276 94427 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 13:58:49.793002 92257 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 13:58:49.857846 94427 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 13:58:49.857926 94427 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 13:58:49.857985 94427 solver.cpp:291] [3] Iteration 1, loss = 2.49996
I1226 13:58:49.858065 94427 solver.cpp:317]     Train net output #0: accuracy = 0.4375
I1226 13:58:49.858140 94427 solver.cpp:317]     Train net output #1: loss = 2.49996 (* 1 = 2.49996 loss)
I1226 13:58:49.873769 96789 SynchronousNode.cpp:246] [6] PROFILING END[WaitingToSync]
I1226 13:58:49.873891 96789 SynchronousNode.cpp:247] [6] PROFILING BEGIN[Forward]
I1226 13:58:49.893470 91550 SynchronousNode.cpp:246] [5] PROFILING END[WaitingToSync]
I1226 13:58:49.893579 91550 SynchronousNode.cpp:247] [5] PROFILING BEGIN[Forward]
I1226 13:58:49.936235 91478 MultiSolver.cpp:92] [5] PROFILING END[Forward]
I1226 13:58:49.936352 91478 MultiSolver.cpp:94] [5] PROFILING BEGIN[Backward]
I1226 13:58:49.950381 92257 MultiSolver.cpp:108] [7] PROFILING END[Backward]
I1226 13:58:49.950469 92257 MultiSolver.cpp:109] [7] PROFILING BEGIN[WaitingToSync]
I1226 13:58:49.950520 92257 solver.cpp:291] [7] Iteration 1, loss = 3.40951
I1226 13:58:49.950610 92257 solver.cpp:317]     Train net output #0: accuracy = 0.34375
I1226 13:58:49.950695 92257 solver.cpp:317]     Train net output #1: loss = 3.40951 (* 1 = 3.40951 loss)
I1226 13:58:49.973747 91481 MultiSolver.cpp:108] [4] PROFILING END[Backward]
I1226 13:58:49.973845 91481 MultiSolver.cpp:109] [4] PROFILING BEGIN[WaitingToSync]
I1226 13:58:49.973930 91481 solver.cpp:291] [4] Iteration 1, loss = 2.92751
I1226 13:58:49.974021 91481 solver.cpp:317]     Train net output #0: accuracy = 0.4375
I1226 13:58:49.974104 91481 solver.cpp:317]     Train net output #1: loss = 2.92751 (* 1 = 2.92751 loss)
I1226 13:58:50.049149 95085 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 13:58:50.049283 95085 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 13:58:50.039487 97230 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 13:58:50.039618 97230 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 13:58:50.047374 96716 MultiSolver.cpp:92] [6] PROFILING END[Forward]
I1226 13:58:50.047490 96716 MultiSolver.cpp:94] [6] PROFILING BEGIN[Backward]
I1226 13:58:50.116303 91478 MultiSolver.cpp:108] [5] PROFILING END[Backward]
I1226 13:58:50.116389 91478 MultiSolver.cpp:109] [5] PROFILING BEGIN[WaitingToSync]
I1226 13:58:50.116441 91478 solver.cpp:291] [5] Iteration 1, loss = 3.37391
I1226 13:58:50.116530 91478 solver.cpp:317]     Train net output #0: accuracy = 0.28125
I1226 13:58:50.116616 91478 solver.cpp:317]     Train net output #1: loss = 3.37391 (* 1 = 3.37391 loss)
I1226 13:58:50.598111 95085 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 13:58:50.598201 95085 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 13:58:50.598271 95085 solver.cpp:291] [1] Iteration 1, loss = 2.65186
I1226 13:58:50.598448 95085 solver.cpp:317]     Train net output #0: accuracy = 0.375
I1226 13:58:50.598537 95085 solver.cpp:317]     Train net output #1: loss = 2.65186 (* 1 = 2.65186 loss)
I1226 13:58:50.645110 96716 MultiSolver.cpp:108] [6] PROFILING END[Backward]
I1226 13:58:50.645201 96716 MultiSolver.cpp:109] [6] PROFILING BEGIN[WaitingToSync]
I1226 13:58:50.645252 96716 solver.cpp:291] [6] Iteration 1, loss = 2.93044
I1226 13:58:50.645561 96716 solver.cpp:317]     Train net output #0: accuracy = 0.4375
I1226 13:58:50.645660 96716 solver.cpp:317]     Train net output #1: loss = 2.93044 (* 1 = 2.93044 loss)
I1226 13:58:50.886024 97230 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 13:58:50.886134 97230 MultiSolver.cpp:109] [2] PROFILING BEGIN[WaitingToSync]
I1226 13:58:50.886198 97230 solver.cpp:291] [2] Iteration 1, loss = 2.63974
I1226 13:58:50.886389 97230 solver.cpp:317]     Train net output #0: accuracy = 0.46875
I1226 13:58:50.886488 97230 solver.cpp:317]     Train net output #1: loss = 2.63974 (* 1 = 2.63974 loss)
I1226 13:58:57.586784 92037 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 13:58:57.586901 92037 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 13:58:57.660279 92037 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 13:58:57.660415 92037 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 13:58:57.660459 92037 solver.cpp:291] [0] Iteration 2, loss = 4.15963
I1226 13:58:57.660529 92037 solver.cpp:317]     Train net output #0: accuracy = 0.1875
I1226 13:58:57.660610 92037 solver.cpp:317]     Train net output #1: loss = 4.15963 (* 1 = 4.15963 loss)
I1226 13:58:59.497315 97298 SynchronousNode.cpp:246] [2] PROFILING END[WaitingToSync]
I1226 13:58:59.498524 97298 SynchronousNode.cpp:247] [2] PROFILING BEGIN[Forward]
I1226 13:58:59.509320 95156 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 13:58:59.510581 95156 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 13:58:59.505787 97230 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 13:58:59.506008 97230 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 13:58:59.517693 95085 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 13:58:59.517923 95085 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 13:58:59.622699 94497 SynchronousNode.cpp:246] [3] PROFILING END[WaitingToSync]
I1226 13:58:59.622783 94497 SynchronousNode.cpp:247] [3] PROFILING BEGIN[Forward]
I1226 13:58:59.633103 91553 SynchronousNode.cpp:246] [4] PROFILING END[WaitingToSync]
I1226 13:58:59.637557 92326 SynchronousNode.cpp:246] [7] PROFILING END[WaitingToSync]
I1226 13:58:59.633188 91553 SynchronousNode.cpp:247] [4] PROFILING BEGIN[Forward]
I1226 13:58:59.637639 92326 SynchronousNode.cpp:247] [7] PROFILING BEGIN[Forward]
I1226 13:58:59.647385 91550 SynchronousNode.cpp:246] [5] PROFILING END[WaitingToSync]
I1226 13:58:59.647469 91550 SynchronousNode.cpp:247] [5] PROFILING BEGIN[Forward]
I1226 13:58:59.629303 96789 SynchronousNode.cpp:246] [6] PROFILING END[WaitingToSync]
I1226 13:58:59.630478 96789 SynchronousNode.cpp:247] [6] PROFILING BEGIN[Forward]
I1226 13:58:59.643056 91481 MultiSolver.cpp:92] [4] PROFILING END[Forward]
I1226 13:58:59.643167 91481 MultiSolver.cpp:94] [4] PROFILING BEGIN[Backward]
I1226 13:58:59.633951 94427 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 13:58:59.634053 94427 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 13:58:59.657336 91478 MultiSolver.cpp:92] [5] PROFILING END[Forward]
I1226 13:58:59.649047 92257 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 13:58:59.657447 91478 MultiSolver.cpp:94] [5] PROFILING BEGIN[Backward]
I1226 13:58:59.649477 92257 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 13:58:59.684516 94427 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 13:58:59.684597 94427 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 13:58:59.684638 94427 solver.cpp:291] [3] Iteration 2, loss = 3.38526
I1226 13:58:59.684706 94427 solver.cpp:317]     Train net output #0: accuracy = 0.375
I1226 13:58:59.684770 94427 solver.cpp:317]     Train net output #1: loss = 3.38526 (* 1 = 3.38526 loss)
I1226 13:58:59.694525 96716 MultiSolver.cpp:92] [6] PROFILING END[Forward]
I1226 13:58:59.694650 96716 MultiSolver.cpp:94] [6] PROFILING BEGIN[Backward]
I1226 13:58:59.748935 92257 MultiSolver.cpp:108] [7] PROFILING END[Backward]
I1226 13:58:59.749053 92257 MultiSolver.cpp:109] [7] PROFILING BEGIN[WaitingToSync]
I1226 13:58:59.749107 92257 solver.cpp:291] [7] Iteration 2, loss = 2.69589
I1226 13:58:59.749178 92257 solver.cpp:317]     Train net output #0: accuracy = 0.4375
I1226 13:58:59.749435 92257 solver.cpp:317]     Train net output #1: loss = 2.69589 (* 1 = 2.69589 loss)
I1226 13:58:59.753218 91481 MultiSolver.cpp:108] [4] PROFILING END[Backward]
I1226 13:58:59.753295 91481 MultiSolver.cpp:109] [4] PROFILING BEGIN[WaitingToSync]
I1226 13:58:59.753336 91481 solver.cpp:291] [4] Iteration 2, loss = 3.05772
I1226 13:58:59.753401 91481 solver.cpp:317]     Train net output #0: accuracy = 0.28125
I1226 13:58:59.753468 91481 solver.cpp:317]     Train net output #1: loss = 3.05772 (* 1 = 3.05772 loss)
I1226 13:58:59.773638 91478 MultiSolver.cpp:108] [5] PROFILING END[Backward]
I1226 13:58:59.773726 91478 MultiSolver.cpp:109] [5] PROFILING BEGIN[WaitingToSync]
I1226 13:58:59.773775 91478 solver.cpp:291] [5] Iteration 2, loss = 3.04639
I1226 13:58:59.773851 91478 solver.cpp:317]     Train net output #0: accuracy = 0.25
I1226 13:58:59.773926 91478 solver.cpp:317]     Train net output #1: loss = 3.04639 (* 1 = 3.04639 loss)
I1226 13:59:00.004164 95085 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 13:59:00.004261 95085 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 13:59:00.004309 95085 solver.cpp:291] [1] Iteration 2, loss = 2.74281
I1226 13:59:00.004384 95085 solver.cpp:317]     Train net output #0: accuracy = 0.375
I1226 13:59:00.004459 95085 solver.cpp:317]     Train net output #1: loss = 2.74281 (* 1 = 2.74281 loss)
I1226 13:58:59.993597 97230 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 13:58:59.993696 97230 MultiSolver.cpp:109] [2] PROFILING BEGIN[WaitingToSync]
I1226 13:58:59.993782 97230 solver.cpp:291] [2] Iteration 2, loss = 3.81069
I1226 13:58:59.993858 97230 solver.cpp:317]     Train net output #0: accuracy = 0.15625
I1226 13:58:59.993935 97230 solver.cpp:317]     Train net output #1: loss = 3.81069 (* 1 = 3.81069 loss)
I1226 13:59:00.234567 96716 MultiSolver.cpp:108] [6] PROFILING END[Backward]
I1226 13:59:00.234659 96716 MultiSolver.cpp:109] [6] PROFILING BEGIN[WaitingToSync]
I1226 13:59:00.234709 96716 solver.cpp:291] [6] Iteration 2, loss = 3.43424
I1226 13:59:00.234814 96716 solver.cpp:317]     Train net output #0: accuracy = 0.25
I1226 13:59:00.234894 96716 solver.cpp:317]     Train net output #1: loss = 3.43424 (* 1 = 3.43424 loss)
I1226 13:59:07.096228 92037 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 13:59:07.096365 92037 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 13:59:07.164752 92037 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 13:59:07.164831 92037 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 13:59:07.164873 92037 solver.cpp:291] [0] Iteration 3, loss = 3.24803
I1226 13:59:07.164942 92037 solver.cpp:317]     Train net output #0: accuracy = 0.40625
I1226 13:59:07.165025 92037 solver.cpp:317]     Train net output #1: loss = 3.24803 (* 1 = 3.24803 loss)
I1226 13:59:09.031466 97298 SynchronousNode.cpp:246] [2] PROFILING END[WaitingToSync]
I1226 13:59:09.031544 97298 SynchronousNode.cpp:247] [2] PROFILING BEGIN[Forward]
I1226 13:59:09.043449 95156 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 13:59:09.043529 95156 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 13:59:09.029060 94497 SynchronousNode.cpp:246] [3] PROFILING END[WaitingToSync]
I1226 13:59:09.029139 94497 SynchronousNode.cpp:247] [3] PROFILING BEGIN[Forward]
I1226 13:59:09.043602 92326 SynchronousNode.cpp:246] [7] PROFILING END[WaitingToSync]
I1226 13:59:09.039322 91553 SynchronousNode.cpp:246] [4] PROFILING END[WaitingToSync]
I1226 13:59:09.043681 92326 SynchronousNode.cpp:247] [7] PROFILING BEGIN[Forward]
I1226 13:59:09.039403 91553 SynchronousNode.cpp:247] [4] PROFILING BEGIN[Forward]
I1226 13:59:09.031822 94427 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 13:59:09.046450 92257 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 13:59:09.042163 91481 MultiSolver.cpp:92] [4] PROFILING END[Forward]
I1226 13:59:09.031939 94427 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 13:59:09.046571 92257 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 13:59:09.042280 91481 MultiSolver.cpp:94] [4] PROFILING BEGIN[Backward]
I1226 13:59:09.039840 97230 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 13:59:09.051795 95085 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 13:59:09.040030 97230 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 13:59:09.051939 95085 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 13:59:09.082334 94427 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 13:59:09.082450 94427 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 13:59:09.082494 94427 solver.cpp:291] [3] Iteration 3, loss = 2.83949
I1226 13:59:09.082561 94427 solver.cpp:317]     Train net output #0: accuracy = 0.375
I1226 13:59:09.082626 94427 solver.cpp:317]     Train net output #1: loss = 2.83949 (* 1 = 2.83949 loss)
I1226 13:59:09.149315 91550 SynchronousNode.cpp:246] [5] PROFILING END[WaitingToSync]
I1226 13:59:09.149400 91550 SynchronousNode.cpp:247] [5] PROFILING BEGIN[Forward]
I1226 13:59:09.130137 96789 SynchronousNode.cpp:246] [6] PROFILING END[WaitingToSync]
I1226 13:59:09.130216 96789 SynchronousNode.cpp:247] [6] PROFILING BEGIN[Forward]
I1226 13:59:09.148676 92257 MultiSolver.cpp:108] [7] PROFILING END[Backward]
I1226 13:59:09.148818 92257 MultiSolver.cpp:109] [7] PROFILING BEGIN[WaitingToSync]
I1226 13:59:09.148870 92257 solver.cpp:291] [7] Iteration 3, loss = 3.36995
I1226 13:59:09.148954 92257 solver.cpp:317]     Train net output #0: accuracy = 0.1875
I1226 13:59:09.149036 92257 solver.cpp:317]     Train net output #1: loss = 3.36995 (* 1 = 3.36995 loss)
I1226 13:59:09.157377 91478 MultiSolver.cpp:92] [5] PROFILING END[Forward]
I1226 13:59:09.157490 91478 MultiSolver.cpp:94] [5] PROFILING BEGIN[Backward]
I1226 13:59:09.168089 91481 MultiSolver.cpp:108] [4] PROFILING END[Backward]
I1226 13:59:09.168177 91481 MultiSolver.cpp:109] [4] PROFILING BEGIN[WaitingToSync]
I1226 13:59:09.168229 91481 solver.cpp:291] [4] Iteration 3, loss = 3.80316
I1226 13:59:09.168305 91481 solver.cpp:317]     Train net output #0: accuracy = 0.1875
I1226 13:59:09.168387 91481 solver.cpp:317]     Train net output #1: loss = 3.80316 (* 1 = 3.80316 loss)
I1226 13:59:09.172336 96716 MultiSolver.cpp:92] [6] PROFILING END[Forward]
I1226 13:59:09.172463 96716 MultiSolver.cpp:94] [6] PROFILING BEGIN[Backward]
I1226 13:59:09.279261 91478 MultiSolver.cpp:108] [5] PROFILING END[Backward]
I1226 13:59:09.279350 91478 MultiSolver.cpp:109] [5] PROFILING BEGIN[WaitingToSync]
I1226 13:59:09.279398 91478 solver.cpp:291] [5] Iteration 3, loss = 3.55798
I1226 13:59:09.279474 91478 solver.cpp:317]     Train net output #0: accuracy = 0.25
I1226 13:59:09.279549 91478 solver.cpp:317]     Train net output #1: loss = 3.55798 (* 1 = 3.55798 loss)
I1226 13:59:09.526943 97230 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 13:59:09.527040 97230 MultiSolver.cpp:109] [2] PROFILING BEGIN[WaitingToSync]
I1226 13:59:09.527091 97230 solver.cpp:291] [2] Iteration 3, loss = 3.65057
I1226 13:59:09.527166 97230 solver.cpp:317]     Train net output #0: accuracy = 0.3125
I1226 13:59:09.527243 97230 solver.cpp:317]     Train net output #1: loss = 3.65057 (* 1 = 3.65057 loss)
I1226 13:59:09.540012 95085 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 13:59:09.540105 95085 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 13:59:09.540153 95085 solver.cpp:291] [1] Iteration 3, loss = 3.43405
I1226 13:59:09.540228 95085 solver.cpp:317]     Train net output #0: accuracy = 0.28125
I1226 13:59:09.540477 95085 solver.cpp:317]     Train net output #1: loss = 3.43405 (* 1 = 3.43405 loss)
I1226 13:59:09.728548 96716 MultiSolver.cpp:108] [6] PROFILING END[Backward]
I1226 13:59:09.728647 96716 MultiSolver.cpp:109] [6] PROFILING BEGIN[WaitingToSync]
I1226 13:59:09.728698 96716 solver.cpp:291] [6] Iteration 3, loss = 2.83836
I1226 13:59:09.728773 96716 solver.cpp:317]     Train net output #0: accuracy = 0.4375
I1226 13:59:09.728886 96716 solver.cpp:317]     Train net output #1: loss = 2.83836 (* 1 = 2.83836 loss)
I1226 13:59:16.549679 92037 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 13:59:16.549806 92037 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 13:59:16.616228 92037 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 13:59:16.616341 92037 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 13:59:16.616387 92037 solver.cpp:291] [0] Iteration 4, loss = 4.0586
I1226 13:59:16.616456 92037 solver.cpp:317]     Train net output #0: accuracy = 0.1875
I1226 13:59:16.616539 92037 solver.cpp:317]     Train net output #1: loss = 4.0586 (* 1 = 4.0586 loss)
I1226 13:59:18.430385 97298 SynchronousNode.cpp:246] [2] PROFILING END[WaitingToSync]
I1226 13:59:18.430464 97298 SynchronousNode.cpp:247] [2] PROFILING BEGIN[Forward]
I1226 13:59:18.430917 96789 SynchronousNode.cpp:246] [6] PROFILING END[WaitingToSync]
I1226 13:59:18.442348 95156 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 13:59:18.430994 96789 SynchronousNode.cpp:247] [6] PROFILING BEGIN[Forward]
I1226 13:59:18.442428 95156 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 13:59:18.450275 91550 SynchronousNode.cpp:246] [5] PROFILING END[WaitingToSync]
I1226 13:59:18.451298 91550 SynchronousNode.cpp:247] [5] PROFILING BEGIN[Forward]
I1226 13:59:18.455104 91478 MultiSolver.cpp:92] [5] PROFILING END[Forward]
I1226 13:59:18.455257 91478 MultiSolver.cpp:94] [5] PROFILING BEGIN[Backward]
I1226 13:59:18.449923 97230 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 13:59:18.450198 96716 MultiSolver.cpp:92] [6] PROFILING END[Forward]
I1226 13:59:18.461665 95085 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 13:59:18.450052 97230 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 13:59:18.450328 96716 MultiSolver.cpp:94] [6] PROFILING BEGIN[Backward]
I1226 13:59:18.461783 95085 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 13:59:18.460543 94497 SynchronousNode.cpp:246] [3] PROFILING END[WaitingToSync]
I1226 13:59:18.470701 91553 SynchronousNode.cpp:246] [4] PROFILING END[WaitingToSync]
I1226 13:59:18.461654 94497 SynchronousNode.cpp:247] [3] PROFILING BEGIN[Forward]
I1226 13:59:18.471751 91553 SynchronousNode.cpp:247] [4] PROFILING BEGIN[Forward]
I1226 13:59:18.475401 92326 SynchronousNode.cpp:246] [7] PROFILING END[WaitingToSync]
I1226 13:59:18.476447 92326 SynchronousNode.cpp:247] [7] PROFILING BEGIN[Forward]
I1226 13:59:18.482748 91481 MultiSolver.cpp:92] [4] PROFILING END[Forward]
I1226 13:59:18.482894 91481 MultiSolver.cpp:94] [4] PROFILING BEGIN[Backward]
I1226 13:59:18.487565 92257 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 13:59:18.487678 92257 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 13:59:18.473225 94427 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 13:59:18.473332 94427 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 13:59:18.523541 94427 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 13:59:18.523624 94427 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 13:59:18.523671 94427 solver.cpp:291] [3] Iteration 4, loss = 2.82423
I1226 13:59:18.523738 94427 solver.cpp:317]     Train net output #0: accuracy = 0.28125
I1226 13:59:18.523804 94427 solver.cpp:317]     Train net output #1: loss = 2.82423 (* 1 = 2.82423 loss)
I1226 13:59:18.579255 91478 MultiSolver.cpp:108] [5] PROFILING END[Backward]
I1226 13:59:18.579342 91478 MultiSolver.cpp:109] [5] PROFILING BEGIN[WaitingToSync]
I1226 13:59:18.579391 91478 solver.cpp:291] [5] Iteration 4, loss = 3.07738
I1226 13:59:18.579465 91478 solver.cpp:317]     Train net output #0: accuracy = 0.28125
I1226 13:59:18.579541 91478 solver.cpp:317]     Train net output #1: loss = 3.07738 (* 1 = 3.07738 loss)
I1226 13:59:18.587968 92257 MultiSolver.cpp:108] [7] PROFILING END[Backward]
I1226 13:59:18.588057 92257 MultiSolver.cpp:109] [7] PROFILING BEGIN[WaitingToSync]
I1226 13:59:18.588114 92257 solver.cpp:291] [7] Iteration 4, loss = 2.91205
I1226 13:59:18.588186 92257 solver.cpp:317]     Train net output #0: accuracy = 0.4375
I1226 13:59:18.588259 92257 solver.cpp:317]     Train net output #1: loss = 2.91205 (* 1 = 2.91205 loss)
I1226 13:59:18.599764 91481 MultiSolver.cpp:108] [4] PROFILING END[Backward]
I1226 13:59:18.599853 91481 MultiSolver.cpp:109] [4] PROFILING BEGIN[WaitingToSync]
I1226 13:59:18.599956 91481 solver.cpp:291] [4] Iteration 4, loss = 4.02601
I1226 13:59:18.600030 91481 solver.cpp:317]     Train net output #0: accuracy = 0.21875
I1226 13:59:18.600106 91481 solver.cpp:317]     Train net output #1: loss = 4.02601 (* 1 = 4.02601 loss)
I1226 13:59:18.949378 95085 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 13:59:18.949471 95085 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 13:59:18.949515 95085 solver.cpp:291] [1] Iteration 4, loss = 3.66612
I1226 13:59:18.949582 95085 solver.cpp:317]     Train net output #0: accuracy = 0.3125
I1226 13:59:18.949657 95085 solver.cpp:317]     Train net output #1: loss = 3.66612 (* 1 = 3.66612 loss)
I1226 13:59:18.938853 97230 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 13:59:18.938935 97230 MultiSolver.cpp:109] [2] PROFILING BEGIN[WaitingToSync]
I1226 13:59:18.938974 97230 solver.cpp:291] [2] Iteration 4, loss = 3.38226
I1226 13:59:18.939039 97230 solver.cpp:317]     Train net output #0: accuracy = 0.34375
I1226 13:59:18.939106 97230 solver.cpp:317]     Train net output #1: loss = 3.38226 (* 1 = 3.38226 loss)
I1226 13:59:18.994066 96716 MultiSolver.cpp:108] [6] PROFILING END[Backward]
I1226 13:59:18.994158 96716 MultiSolver.cpp:109] [6] PROFILING BEGIN[WaitingToSync]
I1226 13:59:18.994207 96716 solver.cpp:291] [6] Iteration 4, loss = 2.90315
I1226 13:59:18.994282 96716 solver.cpp:317]     Train net output #0: accuracy = 0.4375
I1226 13:59:18.994359 96716 solver.cpp:317]     Train net output #1: loss = 2.90315 (* 1 = 2.90315 loss)
I1226 13:59:25.802659 92037 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 13:59:25.803818 92037 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 13:59:25.874392 92037 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 13:59:25.874470 92037 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 13:59:25.874516 92037 solver.cpp:291] [0] Iteration 5, loss = 2.77801
I1226 13:59:25.874585 92037 solver.cpp:317]     Train net output #0: accuracy = 0.3125
I1226 13:59:25.874670 92037 solver.cpp:317]     Train net output #1: loss = 2.77801 (* 1 = 2.77801 loss)
I1226 13:59:27.722270 97298 SynchronousNode.cpp:246] [2] PROFILING END[WaitingToSync]
I1226 13:59:27.722800 96789 SynchronousNode.cpp:246] [6] PROFILING END[WaitingToSync]
I1226 13:59:27.734283 95156 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 13:59:27.742089 91550 SynchronousNode.cpp:246] [5] PROFILING END[WaitingToSync]
I1226 13:59:27.722354 97298 SynchronousNode.cpp:247] [2] PROFILING BEGIN[Forward]
I1226 13:59:27.722883 96789 SynchronousNode.cpp:247] [6] PROFILING BEGIN[Forward]
I1226 13:59:27.734366 95156 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 13:59:27.742172 91550 SynchronousNode.cpp:247] [5] PROFILING BEGIN[Forward]
I1226 13:59:27.746886 91478 MultiSolver.cpp:92] [5] PROFILING END[Forward]
I1226 13:59:27.747012 91478 MultiSolver.cpp:94] [5] PROFILING BEGIN[Backward]
I1226 13:59:27.741969 96716 MultiSolver.cpp:92] [6] PROFILING END[Forward]
I1226 13:59:27.753484 95085 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 13:59:27.741771 97230 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 13:59:27.742091 96716 MultiSolver.cpp:94] [6] PROFILING BEGIN[Backward]
I1226 13:59:27.741896 97230 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 13:59:27.753747 95085 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 13:59:27.770824 94497 SynchronousNode.cpp:246] [3] PROFILING END[WaitingToSync]
I1226 13:59:27.780845 91553 SynchronousNode.cpp:246] [4] PROFILING END[WaitingToSync]
I1226 13:59:27.770908 94497 SynchronousNode.cpp:247] [3] PROFILING BEGIN[Forward]
I1226 13:59:27.780957 91553 SynchronousNode.cpp:247] [4] PROFILING BEGIN[Forward]
I1226 13:59:27.785636 92326 SynchronousNode.cpp:246] [7] PROFILING END[WaitingToSync]
I1226 13:59:27.785719 92326 SynchronousNode.cpp:247] [7] PROFILING BEGIN[Forward]
I1226 13:59:27.784967 94427 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 13:59:27.795295 91481 MultiSolver.cpp:92] [4] PROFILING END[Forward]
I1226 13:59:27.785075 94427 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 13:59:27.795416 91481 MultiSolver.cpp:94] [4] PROFILING BEGIN[Backward]
I1226 13:59:27.800746 92257 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 13:59:27.800866 92257 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 13:59:27.836283 94427 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 13:59:27.836365 94427 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 13:59:27.836505 94427 solver.cpp:291] [3] Iteration 5, loss = 2.88112
I1226 13:59:27.836575 94427 solver.cpp:317]     Train net output #0: accuracy = 0.25
I1226 13:59:27.836644 94427 solver.cpp:317]     Train net output #1: loss = 2.88112 (* 1 = 2.88112 loss)
I1226 13:59:27.870741 91478 MultiSolver.cpp:108] [5] PROFILING END[Backward]
I1226 13:59:27.870829 91478 MultiSolver.cpp:109] [5] PROFILING BEGIN[WaitingToSync]
I1226 13:59:27.870877 91478 solver.cpp:291] [5] Iteration 5, loss = 2.88867
I1226 13:59:27.870950 91478 solver.cpp:317]     Train net output #0: accuracy = 0.375
I1226 13:59:27.871024 91478 solver.cpp:317]     Train net output #1: loss = 2.88867 (* 1 = 2.88867 loss)
I1226 13:59:27.904860 92257 MultiSolver.cpp:108] [7] PROFILING END[Backward]
I1226 13:59:27.904949 92257 MultiSolver.cpp:109] [7] PROFILING BEGIN[WaitingToSync]
I1226 13:59:27.904999 92257 solver.cpp:291] [7] Iteration 5, loss = 3.57785
I1226 13:59:27.905073 92257 solver.cpp:317]     Train net output #0: accuracy = 0.25
I1226 13:59:27.905148 92257 solver.cpp:317]     Train net output #1: loss = 3.57785 (* 1 = 3.57785 loss)
I1226 13:59:27.904295 91481 MultiSolver.cpp:108] [4] PROFILING END[Backward]
I1226 13:59:27.904372 91481 MultiSolver.cpp:109] [4] PROFILING BEGIN[WaitingToSync]
I1226 13:59:27.904413 91481 solver.cpp:291] [4] Iteration 5, loss = 2.71781
I1226 13:59:27.904480 91481 solver.cpp:317]     Train net output #0: accuracy = 0.4375
I1226 13:59:27.904546 91481 solver.cpp:317]     Train net output #1: loss = 2.71781 (* 1 = 2.71781 loss)
I1226 13:59:28.224416 97230 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 13:59:28.224509 97230 MultiSolver.cpp:109] [2] PROFILING BEGIN[WaitingToSync]
I1226 13:59:28.224552 97230 solver.cpp:291] [2] Iteration 5, loss = 3.37227
I1226 13:59:28.224618 97230 solver.cpp:317]     Train net output #0: accuracy = 0.40625
I1226 13:59:28.224684 97230 solver.cpp:317]     Train net output #1: loss = 3.37227 (* 1 = 3.37227 loss)
I1226 13:59:28.237735 95085 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 13:59:28.237860 95085 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 13:59:28.237905 95085 solver.cpp:291] [1] Iteration 5, loss = 4.17698
I1226 13:59:28.237972 95085 solver.cpp:317]     Train net output #0: accuracy = 0.15625
I1226 13:59:28.238037 95085 solver.cpp:317]     Train net output #1: loss = 4.17698 (* 1 = 4.17698 loss)
I1226 13:59:28.286459 96716 MultiSolver.cpp:108] [6] PROFILING END[Backward]
I1226 13:59:28.286551 96716 MultiSolver.cpp:109] [6] PROFILING BEGIN[WaitingToSync]
I1226 13:59:28.286600 96716 solver.cpp:291] [6] Iteration 5, loss = 3.36337
I1226 13:59:28.286675 96716 solver.cpp:317]     Train net output #0: accuracy = 0.28125
I1226 13:59:28.286757 96716 solver.cpp:317]     Train net output #1: loss = 3.36337 (* 1 = 3.36337 loss)
I1226 13:59:34.902850 92037 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 13:59:34.902961 92037 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 13:59:34.976001 92037 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 13:59:34.976081 92037 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 13:59:34.976122 92037 solver.cpp:291] [0] Iteration 6, loss = 3.72162
I1226 13:59:34.976191 92037 solver.cpp:317]     Train net output #0: accuracy = 0.21875
I1226 13:59:34.976275 92037 solver.cpp:317]     Train net output #1: loss = 3.72162 (* 1 = 3.72162 loss)
I1226 13:59:37.064803 91550 SynchronousNode.cpp:246] [5] PROFILING END[WaitingToSync]
I1226 13:59:37.064883 91550 SynchronousNode.cpp:247] [5] PROFILING BEGIN[Forward]
I1226 13:59:37.044994 97298 SynchronousNode.cpp:246] [2] PROFILING END[WaitingToSync]
I1226 13:59:37.045462 96789 SynchronousNode.cpp:246] [6] PROFILING END[WaitingToSync]
I1226 13:59:37.056972 95156 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 13:59:37.045440 97298 SynchronousNode.cpp:247] [2] PROFILING BEGIN[Forward]
I1226 13:59:37.046046 96789 SynchronousNode.cpp:247] [6] PROFILING BEGIN[Forward]
I1226 13:59:37.057482 95156 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 13:59:37.067617 91478 MultiSolver.cpp:92] [5] PROFILING END[Forward]
I1226 13:59:37.067729 91478 MultiSolver.cpp:94] [5] PROFILING BEGIN[Backward]
I1226 13:59:37.053509 97230 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 13:59:37.054185 96716 MultiSolver.cpp:92] [6] PROFILING END[Forward]
I1226 13:59:37.065457 95085 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 13:59:37.053637 97230 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 13:59:37.054302 96716 MultiSolver.cpp:94] [6] PROFILING BEGIN[Backward]
I1226 13:59:37.065572 95085 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 13:59:37.079690 94497 SynchronousNode.cpp:246] [3] PROFILING END[WaitingToSync]
I1226 13:59:37.089768 91553 SynchronousNode.cpp:246] [4] PROFILING END[WaitingToSync]
I1226 13:59:37.089850 91553 SynchronousNode.cpp:247] [4] PROFILING BEGIN[Forward]
I1226 13:59:37.079777 94497 SynchronousNode.cpp:247] [3] PROFILING BEGIN[Forward]
I1226 13:59:37.094544 92326 SynchronousNode.cpp:246] [7] PROFILING END[WaitingToSync]
I1226 13:59:37.094630 92326 SynchronousNode.cpp:247] [7] PROFILING BEGIN[Forward]
I1226 13:59:37.093896 94427 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 13:59:37.094000 94427 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 13:59:37.104200 91481 MultiSolver.cpp:92] [4] PROFILING END[Forward]
I1226 13:59:37.104313 91481 MultiSolver.cpp:94] [4] PROFILING BEGIN[Backward]
I1226 13:59:37.108633 92257 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 13:59:37.108747 92257 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 13:59:37.144520 94427 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 13:59:37.144603 94427 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 13:59:37.144647 94427 solver.cpp:291] [3] Iteration 6, loss = 2.6641
I1226 13:59:37.144712 94427 solver.cpp:317]     Train net output #0: accuracy = 0.25
I1226 13:59:37.144778 94427 solver.cpp:317]     Train net output #1: loss = 2.6641 (* 1 = 2.6641 loss)
I1226 13:59:37.188411 91478 MultiSolver.cpp:108] [5] PROFILING END[Backward]
I1226 13:59:37.188499 91478 MultiSolver.cpp:109] [5] PROFILING BEGIN[WaitingToSync]
I1226 13:59:37.188550 91478 solver.cpp:291] [5] Iteration 6, loss = 3.58634
I1226 13:59:37.188627 91478 solver.cpp:317]     Train net output #0: accuracy = 0.25
I1226 13:59:37.188704 91478 solver.cpp:317]     Train net output #1: loss = 3.58634 (* 1 = 3.58634 loss)
I1226 13:59:37.213066 92257 MultiSolver.cpp:108] [7] PROFILING END[Backward]
I1226 13:59:37.213160 92257 MultiSolver.cpp:109] [7] PROFILING BEGIN[WaitingToSync]
I1226 13:59:37.213209 92257 solver.cpp:291] [7] Iteration 6, loss = 3.0676
I1226 13:59:37.213281 92257 solver.cpp:317]     Train net output #0: accuracy = 0.3125
I1226 13:59:37.213382 92257 solver.cpp:317]     Train net output #1: loss = 3.0676 (* 1 = 3.0676 loss)
I1226 13:59:37.213449 91481 MultiSolver.cpp:108] [4] PROFILING END[Backward]
I1226 13:59:37.213527 91481 MultiSolver.cpp:109] [4] PROFILING BEGIN[WaitingToSync]
I1226 13:59:37.213569 91481 solver.cpp:291] [4] Iteration 6, loss = 3.3711
I1226 13:59:37.213636 91481 solver.cpp:317]     Train net output #0: accuracy = 0.21875
I1226 13:59:37.213704 91481 solver.cpp:317]     Train net output #1: loss = 3.3711 (* 1 = 3.3711 loss)
I1226 13:59:37.536403 97230 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 13:59:37.536574 97230 MultiSolver.cpp:109] [2] PROFILING BEGIN[WaitingToSync]
I1226 13:59:37.536628 97230 solver.cpp:291] [2] Iteration 6, loss = 2.9305
I1226 13:59:37.536698 97230 solver.cpp:317]     Train net output #0: accuracy = 0.3125
I1226 13:59:37.536798 97230 solver.cpp:317]     Train net output #1: loss = 2.9305 (* 1 = 2.9305 loss)
I1226 13:59:37.549139 95085 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 13:59:37.549227 95085 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 13:59:37.549270 95085 solver.cpp:291] [1] Iteration 6, loss = 3.68471
I1226 13:59:37.549335 95085 solver.cpp:317]     Train net output #0: accuracy = 0.1875
I1226 13:59:37.549401 95085 solver.cpp:317]     Train net output #1: loss = 3.68471 (* 1 = 3.68471 loss)
I1226 13:59:37.592757 96716 MultiSolver.cpp:108] [6] PROFILING END[Backward]
I1226 13:59:37.592875 96716 MultiSolver.cpp:109] [6] PROFILING BEGIN[WaitingToSync]
I1226 13:59:37.592923 96716 solver.cpp:291] [6] Iteration 6, loss = 3.16595
I1226 13:59:37.592998 96716 solver.cpp:317]     Train net output #0: accuracy = 0.21875
I1226 13:59:37.593075 96716 solver.cpp:317]     Train net output #1: loss = 3.16595 (* 1 = 3.16595 loss)
I1226 13:59:44.249475 92037 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 13:59:44.249578 92037 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 13:59:44.320538 92037 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 13:59:44.320621 92037 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 13:59:44.320662 92037 solver.cpp:291] [0] Iteration 7, loss = 3.0412
I1226 13:59:44.320731 92037 solver.cpp:317]     Train net output #0: accuracy = 0.3125
I1226 13:59:44.320814 92037 solver.cpp:317]     Train net output #1: loss = 3.0412 (* 1 = 3.0412 loss)
I1226 13:59:46.440062 97298 SynchronousNode.cpp:246] [2] PROFILING END[WaitingToSync]
I1226 13:59:46.440517 96789 SynchronousNode.cpp:246] [6] PROFILING END[WaitingToSync]
I1226 13:59:46.452041 95156 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 13:59:46.459847 91550 SynchronousNode.cpp:246] [5] PROFILING END[WaitingToSync]
I1226 13:59:46.459925 91550 SynchronousNode.cpp:247] [5] PROFILING BEGIN[Forward]
I1226 13:59:46.440140 97298 SynchronousNode.cpp:247] [2] PROFILING BEGIN[Forward]
I1226 13:59:46.440592 96789 SynchronousNode.cpp:247] [6] PROFILING BEGIN[Forward]
I1226 13:59:46.452122 95156 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 13:59:46.462617 91478 MultiSolver.cpp:92] [5] PROFILING END[Forward]
I1226 13:59:46.462725 91478 MultiSolver.cpp:94] [5] PROFILING BEGIN[Backward]
I1226 13:59:46.448515 97230 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 13:59:46.448954 96716 MultiSolver.cpp:92] [6] PROFILING END[Forward]
I1226 13:59:46.448637 97230 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 13:59:46.449076 96716 MultiSolver.cpp:94] [6] PROFILING BEGIN[Backward]
I1226 13:59:46.461092 95085 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 13:59:46.461204 95085 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 13:59:46.464561 94497 SynchronousNode.cpp:246] [3] PROFILING END[WaitingToSync]
I1226 13:59:46.474578 91553 SynchronousNode.cpp:246] [4] PROFILING END[WaitingToSync]
I1226 13:59:46.464648 94497 SynchronousNode.cpp:247] [3] PROFILING BEGIN[Forward]
I1226 13:59:46.474665 91553 SynchronousNode.cpp:247] [4] PROFILING BEGIN[Forward]
I1226 13:59:46.479403 92326 SynchronousNode.cpp:246] [7] PROFILING END[WaitingToSync]
I1226 13:59:46.479486 92326 SynchronousNode.cpp:247] [7] PROFILING BEGIN[Forward]
I1226 13:59:46.488855 91481 MultiSolver.cpp:92] [4] PROFILING END[Forward]
I1226 13:59:46.488999 91481 MultiSolver.cpp:94] [4] PROFILING BEGIN[Backward]
I1226 13:59:46.482131 94427 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 13:59:46.482240 94427 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 13:59:46.497859 92257 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 13:59:46.497977 92257 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 13:59:46.545302 94427 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 13:59:46.545413 94427 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 13:59:46.545457 94427 solver.cpp:291] [3] Iteration 7, loss = 3.42561
I1226 13:59:46.545526 94427 solver.cpp:317]     Train net output #0: accuracy = 0.28125
I1226 13:59:46.545598 94427 solver.cpp:317]     Train net output #1: loss = 3.42561 (* 1 = 3.42561 loss)
I1226 13:59:46.587366 91478 MultiSolver.cpp:108] [5] PROFILING END[Backward]
I1226 13:59:46.587455 91478 MultiSolver.cpp:109] [5] PROFILING BEGIN[WaitingToSync]
I1226 13:59:46.587532 91478 solver.cpp:291] [5] Iteration 7, loss = 2.79778
I1226 13:59:46.587607 91478 solver.cpp:317]     Train net output #0: accuracy = 0.375
I1226 13:59:46.587867 91478 solver.cpp:317]     Train net output #1: loss = 2.79778 (* 1 = 2.79778 loss)
I1226 13:59:46.598525 92257 MultiSolver.cpp:108] [7] PROFILING END[Backward]
I1226 13:59:46.598636 92257 MultiSolver.cpp:109] [7] PROFILING BEGIN[WaitingToSync]
I1226 13:59:46.598690 92257 solver.cpp:291] [7] Iteration 7, loss = 3.37758
I1226 13:59:46.598762 92257 solver.cpp:317]     Train net output #0: accuracy = 0.25
I1226 13:59:46.598997 92257 solver.cpp:317]     Train net output #1: loss = 3.37758 (* 1 = 3.37758 loss)
I1226 13:59:46.608242 91481 MultiSolver.cpp:108] [4] PROFILING END[Backward]
I1226 13:59:46.608330 91481 MultiSolver.cpp:109] [4] PROFILING BEGIN[WaitingToSync]
I1226 13:59:46.608386 91481 solver.cpp:291] [4] Iteration 7, loss = 3.61684
I1226 13:59:46.608458 91481 solver.cpp:317]     Train net output #0: accuracy = 0.28125
I1226 13:59:46.608533 91481 solver.cpp:317]     Train net output #1: loss = 3.61684 (* 1 = 3.61684 loss)
I1226 13:59:46.933923 97230 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 13:59:46.934008 97230 MultiSolver.cpp:109] [2] PROFILING BEGIN[WaitingToSync]
I1226 13:59:46.934049 97230 solver.cpp:291] [2] Iteration 7, loss = 3.29311
I1226 13:59:46.934114 97230 solver.cpp:317]     Train net output #0: accuracy = 0.25
I1226 13:59:46.934187 97230 solver.cpp:317]     Train net output #1: loss = 3.29311 (* 1 = 3.29311 loss)
I1226 13:59:46.953675 95085 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 13:59:46.953757 95085 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 13:59:46.953799 95085 solver.cpp:291] [1] Iteration 7, loss = 3.79833
I1226 13:59:46.953896 95085 solver.cpp:317]     Train net output #0: accuracy = 0.34375
I1226 13:59:46.953956 95085 solver.cpp:317]     Train net output #1: loss = 3.79833 (* 1 = 3.79833 loss)
I1226 13:59:46.985218 96716 MultiSolver.cpp:108] [6] PROFILING END[Backward]
I1226 13:59:46.985308 96716 MultiSolver.cpp:109] [6] PROFILING BEGIN[WaitingToSync]
I1226 13:59:46.985397 96716 solver.cpp:291] [6] Iteration 7, loss = 2.98659
I1226 13:59:46.985471 96716 solver.cpp:317]     Train net output #0: accuracy = 0.34375
I1226 13:59:46.985716 96716 solver.cpp:317]     Train net output #1: loss = 2.98659 (* 1 = 2.98659 loss)
/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/8_node_run.sh: line 57: 92020 User defined signal 2   mpirun -configfile $nodeconfig

real	2m0.679s
user	0m0.011s
sys	0m0.028s
