Dec 26 13:02:47 2016 93731 3 10.1 NIOS_DEBUG: stdin_fd set to -1
Dec 26 13:02:47 2016 93731 3 10.1 NIOS_DEBUG: fds[0] has a value of -1
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 13:02:51.057387 85539 mpiutil.cpp:166] Process rank 6 from number of 9 processes running on knl-node022
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 13:02:51.060850 91895 mpiutil.cpp:166] Process rank 3 from number of 9 processes running on knl-node048
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 13:02:51.060925 92018 mpiutil.cpp:166] Process rank 5 from number of 9 processes running on knl-node060
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 13:02:51.046933 93741 mpiutil.cpp:166] Process rank 0 from number of 9 processes running on knl-node015
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 13:02:51.052136 96103 mpiutil.cpp:166] Process rank 4 from number of 9 processes running on knl-node079
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 13:02:51.059784 94300 mpiutil.cpp:166] Process rank 1 from number of 9 processes running on knl-node084
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 13:02:51.054860 154317 mpiutil.cpp:166] Process rank 8 from number of 9 processes running on knl-node001
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 13:02:51.068814 90983 mpiutil.cpp:166] Process rank 7 from number of 9 processes running on knl-node047
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 13:02:51.045778 122158 mpiutil.cpp:166] Process rank 2 from number of 9 processes running on knl-node010
I1226 13:02:51.066020 96103 caffe.cpp:316] Use CPU.
I1226 13:02:51.075342 91895 caffe.cpp:316] Use CPU.
I1226 13:02:51.069484 154317 caffe.cpp:316] Use CPU.
I1226 13:02:51.072753 85539 caffe.cpp:316] Use CPU.
I1226 13:02:51.067318 96103 solver.cpp:93] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_5.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 13:02:51.076499 91895 solver.cpp:93] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_4.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 13:02:51.068310 96103 solver.cpp:128] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_5.prototxt
I1226 13:02:51.075006 94300 caffe.cpp:316] Use CPU.
I1226 13:02:51.077452 91895 solver.cpp:128] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_4.prototxt
I1226 13:02:51.070703 154317 solver.cpp:93] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_dummy.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 13:02:51.076920 92018 caffe.cpp:316] Use CPU.
I1226 13:02:51.071545 154317 solver.cpp:128] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_dummy.prototxt
I1226 13:02:51.073912 85539 solver.cpp:93] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_7.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 13:02:51.074771 85539 solver.cpp:128] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_7.prototxt
I1226 13:02:51.085765 90983 caffe.cpp:316] Use CPU.
I1226 13:02:51.078038 92018 solver.cpp:93] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_6.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 13:02:51.078857 92018 solver.cpp:128] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_6.prototxt
I1226 13:02:51.063724 93741 caffe.cpp:316] Use CPU.
I1226 13:02:51.076457 94300 solver.cpp:93] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_2.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 13:02:51.077694 94300 solver.cpp:128] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_2.prototxt
I1226 13:02:51.087036 90983 solver.cpp:93] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_8.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 13:02:51.087913 90983 solver.cpp:128] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_8.prototxt
I1226 13:02:51.064558 122158 caffe.cpp:316] Use CPU.
I1226 13:02:51.065706 93741 solver.cpp:93] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_1.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 13:02:51.067070 93741 solver.cpp:128] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_1.prototxt
I1226 13:02:51.066141 122158 solver.cpp:93] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_3.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 13:02:51.067543 122158 solver.cpp:128] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_3.prototxt
I1226 13:02:51.102866 154317 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 13:02:51.102939 154317 cpu_info.cpp:455] Total number of sockets: 1
I1226 13:02:51.102962 154317 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 13:02:51.103000 154317 cpu_info.cpp:461] Total number of processors: 272
I1226 13:02:51.103021 154317 cpu_info.cpp:464] GPU is used: no
I1226 13:02:51.103041 154317 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 13:02:51.103061 154317 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 13:02:51.109895 91895 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 13:02:51.109974 91895 cpu_info.cpp:455] Total number of sockets: 1
I1226 13:02:51.109997 91895 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 13:02:51.110015 91895 cpu_info.cpp:461] Total number of processors: 272
I1226 13:02:51.110035 91895 cpu_info.cpp:464] GPU is used: no
I1226 13:02:51.110054 91895 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 13:02:51.110074 91895 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 13:02:51.106947 85539 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 13:02:51.107023 85539 cpu_info.cpp:455] Total number of sockets: 1
I1226 13:02:51.107045 85539 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 13:02:51.107065 85539 cpu_info.cpp:461] Total number of processors: 272
I1226 13:02:51.107085 85539 cpu_info.cpp:464] GPU is used: no
I1226 13:02:51.107105 85539 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 13:02:51.107125 85539 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 13:02:51.111146 92018 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 13:02:51.111223 92018 cpu_info.cpp:455] Total number of sockets: 1
I1226 13:02:51.111248 92018 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 13:02:51.111295 92018 cpu_info.cpp:461] Total number of processors: 272
I1226 13:02:51.111313 92018 cpu_info.cpp:464] GPU is used: no
I1226 13:02:51.111332 92018 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 13:02:51.111379 92018 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 13:02:51.119707 90983 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 13:02:51.119787 90983 cpu_info.cpp:455] Total number of sockets: 1
I1226 13:02:51.119818 90983 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 13:02:51.119843 90983 cpu_info.cpp:461] Total number of processors: 272
I1226 13:02:51.119871 90983 cpu_info.cpp:464] GPU is used: no
I1226 13:02:51.119899 90983 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 13:02:51.119925 90983 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 13:02:51.112277 94300 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 13:02:51.104687 96103 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 13:02:51.104776 96103 cpu_info.cpp:455] Total number of sockets: 1
I1226 13:02:51.112370 94300 cpu_info.cpp:455] Total number of sockets: 1
I1226 13:02:51.112407 94300 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 13:02:51.112448 94300 cpu_info.cpp:461] Total number of processors: 272
I1226 13:02:51.104832 96103 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 13:02:51.104856 96103 cpu_info.cpp:461] Total number of processors: 272
I1226 13:02:51.104878 96103 cpu_info.cpp:464] GPU is used: no
I1226 13:02:51.112488 94300 cpu_info.cpp:464] GPU is used: no
I1226 13:02:51.099612 93741 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 13:02:51.104902 96103 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 13:02:51.099869 93741 cpu_info.cpp:455] Total number of sockets: 1
I1226 13:02:51.104923 96103 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 13:02:51.112519 94300 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 13:02:51.099933 93741 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 13:02:51.099962 93741 cpu_info.cpp:461] Total number of processors: 272
I1226 13:02:51.112550 94300 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 13:02:51.099992 93741 cpu_info.cpp:464] GPU is used: no
I1226 13:02:51.100018 93741 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 13:02:51.100102 93741 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 13:02:51.133293 94300 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 13:02:51.133643 94300 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 13:02:51.120371 122158 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 13:02:51.120477 122158 cpu_info.cpp:455] Total number of sockets: 1
I1226 13:02:51.120522 122158 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 13:02:51.120560 122158 cpu_info.cpp:461] Total number of processors: 272
I1226 13:02:51.120604 122158 cpu_info.cpp:464] GPU is used: no
I1226 13:02:51.120671 122158 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 13:02:51.120712 122158 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 13:02:51.135725 94300 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_1"
    batch_size: 32
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 13:02:51.141098 154317 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 13:02:51.141526 154317 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 13:02:51.144335 85539 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 13:02:51.144637 85539 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 13:02:51.146231 85539 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_6"
    batch_size: 32
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 13:02:51.143575 154317 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "DummyData"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  dummy_data_param {
    data_filler {
      type: "gaussian"
      std: 1
    }
    data_filler {
      type: "constant"
      value: 0
    }
    shape {
      dim: 32
      dim: 3
      dim: 227
      dim: 227
    }
    shape {
      dim: 32
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 13:02:51.139240 93741 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 13:02:51.139685 93741 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 13:02:51.154675 91895 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 13:02:51.155058 91895 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 13:02:51.146142 96103 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 13:02:51.146494 96103 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 13:02:51.141856 93741 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_0"
    batch_size: 32
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 13:02:51.156967 91895 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_3"
    batch_size: 32
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 13:02:51.148574 96103 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_4"
    batch_size: 32
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 13:02:51.157927 92018 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 13:02:51.158288 92018 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 13:02:51.160419 92018 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_5"
    batch_size: 32
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 13:02:51.180344 90983 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 13:02:51.180732 90983 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 13:02:51.183027 90983 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_7"
    batch_size: 32
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 13:02:51.183758 122158 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 13:02:51.184146 122158 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 13:02:51.186929 122158 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_2"
    batch_size: 32
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 13:02:51.219835 91895 layer_factory.hpp:114] Creating layer data
I1226 13:02:51.221593 154317 layer_factory.hpp:114] Creating layer data
I1226 13:02:51.221729 154317 net.cpp:178] Creating Layer data
I1226 13:02:51.221773 154317 net.cpp:586] data -> data
I1226 13:02:51.221860 154317 net.cpp:586] data -> label
I1226 13:02:51.229185 85539 layer_factory.hpp:114] Creating layer data
I1226 13:02:51.226510 96103 layer_factory.hpp:114] Creating layer data
I1226 13:02:51.239581 92018 layer_factory.hpp:114] Creating layer data
I1226 13:02:51.259976 90983 layer_factory.hpp:114] Creating layer data
I1226 13:02:51.255105 91895 net.cpp:178] Creating Layer data
I1226 13:02:51.255225 91895 net.cpp:586] data -> data
I1226 13:02:51.255319 91895 net.cpp:586] data -> label
I1226 13:02:51.262069 94300 layer_factory.hpp:114] Creating layer data
I1226 13:02:51.260771 85539 net.cpp:178] Creating Layer data
I1226 13:02:51.260857 85539 net.cpp:586] data -> data
I1226 13:02:51.260931 85539 net.cpp:586] data -> label
I1226 13:02:51.288928 90983 net.cpp:178] Creating Layer data
I1226 13:02:51.289016 90983 net.cpp:586] data -> data
I1226 13:02:51.289089 90983 net.cpp:586] data -> label
I1226 13:02:51.272657 96103 net.cpp:178] Creating Layer data
I1226 13:02:51.272845 96103 net.cpp:586] data -> data
I1226 13:02:51.273006 96103 net.cpp:586] data -> label
I1226 13:02:51.289213 92018 net.cpp:178] Creating Layer data
I1226 13:02:51.287889 94300 net.cpp:178] Creating Layer data
I1226 13:02:51.289311 92018 net.cpp:586] data -> data
I1226 13:02:51.288023 94300 net.cpp:586] data -> data
I1226 13:02:51.289388 92018 net.cpp:586] data -> label
I1226 13:02:51.288148 94300 net.cpp:586] data -> label
I1226 13:02:51.280043 122158 layer_factory.hpp:114] Creating layer data
I1226 13:02:51.283449 93741 layer_factory.hpp:114] Creating layer data
I1226 13:02:51.294106 154317 net.cpp:228] Setting up data
I1226 13:02:51.294234 154317 net.cpp:235] Top shape: 32 3 227 227 (4946784)
I1226 13:02:51.294277 154317 net.cpp:235] Top shape: 32 1 1 1 (32)
I1226 13:02:51.294306 154317 net.cpp:243] Memory required for data: 19787264
I1226 13:02:51.294348 154317 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 13:02:51.294455 154317 net.cpp:178] Creating Layer label_data_1_split
I1226 13:02:51.294582 154317 net.cpp:612] label_data_1_split <- label
I1226 13:02:51.294626 154317 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 13:02:51.294680 154317 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 13:02:51.304541 85541 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_6
I1226 13:02:51.308502 91897 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_3
I1226 13:02:51.295302 122158 net.cpp:178] Creating Layer data
I1226 13:02:51.295413 122158 net.cpp:586] data -> data
I1226 13:02:51.295513 122158 net.cpp:586] data -> label
I1226 13:02:51.308763 154317 net.cpp:228] Setting up label_data_1_split
I1226 13:02:51.308871 154317 net.cpp:235] Top shape: 32 1 1 1 (32)
I1226 13:02:51.308908 154317 net.cpp:235] Top shape: 32 1 1 1 (32)
I1226 13:02:51.308935 154317 net.cpp:243] Memory required for data: 19787520
I1226 13:02:51.309003 154317 layer_factory.hpp:114] Creating layer conv1
I1226 13:02:51.309130 154317 net.cpp:178] Creating Layer conv1
I1226 13:02:51.309164 154317 net.cpp:612] conv1 <- data
I1226 13:02:51.309209 154317 net.cpp:586] conv1 -> conv1
I1226 13:02:51.310534 93741 net.cpp:178] Creating Layer data
I1226 13:02:51.310632 93741 net.cpp:586] data -> data
I1226 13:02:51.310710 93741 net.cpp:586] data -> label
I1226 13:02:51.322000 85539 data_layer.cpp:80] output data size: 32,3,227,227
I1226 13:02:51.330099 91895 data_layer.cpp:80] output data size: 32,3,227,227
I1226 13:02:51.321050 96105 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_4
I1226 13:02:51.331497 92020 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_5
I1226 13:02:51.340448 94302 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_1
I1226 13:02:51.339736 90985 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_7
I1226 13:02:51.350040 92018 data_layer.cpp:80] output data size: 32,3,227,227
I1226 13:02:51.348727 90983 data_layer.cpp:80] output data size: 32,3,227,227
I1226 13:02:51.354836 96103 data_layer.cpp:80] output data size: 32,3,227,227
I1226 13:02:51.364377 94300 data_layer.cpp:80] output data size: 32,3,227,227
I1226 13:02:51.356084 122160 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_2
I1226 13:02:51.366649 93745 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_0
I1226 13:02:51.377243 122158 data_layer.cpp:80] output data size: 32,3,227,227
I1226 13:02:51.384531 93741 data_layer.cpp:80] output data size: 32,3,227,227
I1226 13:02:51.397653 85539 net.cpp:228] Setting up data
I1226 13:02:51.397788 85539 net.cpp:235] Top shape: 32 3 227 227 (4946784)
I1226 13:02:51.397822 85539 net.cpp:235] Top shape: 32 (32)
I1226 13:02:51.397846 85539 net.cpp:243] Memory required for data: 19787264
I1226 13:02:51.397883 85539 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 13:02:51.397969 85539 net.cpp:178] Creating Layer label_data_1_split
I1226 13:02:51.398083 85539 net.cpp:612] label_data_1_split <- label
I1226 13:02:51.398121 85539 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 13:02:51.398169 85539 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 13:02:51.408344 91895 net.cpp:228] Setting up data
I1226 13:02:51.408453 91895 net.cpp:235] Top shape: 32 3 227 227 (4946784)
I1226 13:02:51.408488 91895 net.cpp:235] Top shape: 32 (32)
I1226 13:02:51.408512 91895 net.cpp:243] Memory required for data: 19787264
I1226 13:02:51.408576 91895 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 13:02:51.408665 91895 net.cpp:178] Creating Layer label_data_1_split
I1226 13:02:51.408802 91895 net.cpp:612] label_data_1_split <- label
I1226 13:02:51.408843 91895 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 13:02:51.408892 91895 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 13:02:51.412715 85539 net.cpp:228] Setting up label_data_1_split
I1226 13:02:51.412818 85539 net.cpp:235] Top shape: 32 (32)
I1226 13:02:51.412850 85539 net.cpp:235] Top shape: 32 (32)
I1226 13:02:51.412874 85539 net.cpp:243] Memory required for data: 19787520
I1226 13:02:51.412909 85539 layer_factory.hpp:114] Creating layer conv1
I1226 13:02:51.413008 85539 net.cpp:178] Creating Layer conv1
I1226 13:02:51.413043 85539 net.cpp:612] conv1 <- data
I1226 13:02:51.413089 85539 net.cpp:586] conv1 -> conv1
I1226 13:02:51.421905 91895 net.cpp:228] Setting up label_data_1_split
I1226 13:02:51.422010 91895 net.cpp:235] Top shape: 32 (32)
I1226 13:02:51.422041 91895 net.cpp:235] Top shape: 32 (32)
I1226 13:02:51.422065 91895 net.cpp:243] Memory required for data: 19787520
I1226 13:02:51.422098 91895 layer_factory.hpp:114] Creating layer conv1
I1226 13:02:51.422191 91895 net.cpp:178] Creating Layer conv1
I1226 13:02:51.422226 91895 net.cpp:612] conv1 <- data
I1226 13:02:51.422266 91895 net.cpp:586] conv1 -> conv1
I1226 13:02:51.431476 90983 net.cpp:228] Setting up data
I1226 13:02:51.431586 90983 net.cpp:235] Top shape: 32 3 227 227 (4946784)
I1226 13:02:51.431618 90983 net.cpp:235] Top shape: 32 (32)
I1226 13:02:51.431668 90983 net.cpp:243] Memory required for data: 19787264
I1226 13:02:51.431704 90983 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 13:02:51.431784 90983 net.cpp:178] Creating Layer label_data_1_split
I1226 13:02:51.431900 90983 net.cpp:612] label_data_1_split <- label
I1226 13:02:51.431937 90983 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 13:02:51.431984 90983 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 13:02:51.432881 92018 net.cpp:228] Setting up data
I1226 13:02:51.432996 92018 net.cpp:235] Top shape: 32 3 227 227 (4946784)
I1226 13:02:51.433032 92018 net.cpp:235] Top shape: 32 (32)
I1226 13:02:51.433055 92018 net.cpp:243] Memory required for data: 19787264
I1226 13:02:51.433094 92018 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 13:02:51.433179 92018 net.cpp:178] Creating Layer label_data_1_split
I1226 13:02:51.433298 92018 net.cpp:612] label_data_1_split <- label
I1226 13:02:51.433341 92018 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 13:02:51.433390 92018 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 13:02:51.433058 154317 net.cpp:228] Setting up conv1
I1226 13:02:51.433171 154317 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 13:02:51.433199 154317 net.cpp:243] Memory required for data: 56958720
I1226 13:02:51.433323 154317 layer_factory.hpp:114] Creating layer relu1
I1226 13:02:51.433396 154317 net.cpp:178] Creating Layer relu1
I1226 13:02:51.433447 154317 net.cpp:612] relu1 <- conv1
I1226 13:02:51.433491 154317 net.cpp:573] relu1 -> conv1 (in-place)
I1226 13:02:51.433590 154317 net.cpp:228] Setting up relu1
I1226 13:02:51.433634 154317 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 13:02:51.433660 154317 net.cpp:243] Memory required for data: 94129920
I1226 13:02:51.433687 154317 layer_factory.hpp:114] Creating layer norm1
I1226 13:02:51.433763 154317 net.cpp:178] Creating Layer norm1
I1226 13:02:51.433796 154317 net.cpp:612] norm1 <- conv1
I1226 13:02:51.433831 154317 net.cpp:586] norm1 -> norm1
I1226 13:02:51.433921 154317 net.cpp:228] Setting up norm1
I1226 13:02:51.433995 154317 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 13:02:51.434023 154317 net.cpp:243] Memory required for data: 131301120
I1226 13:02:51.434052 154317 layer_factory.hpp:114] Creating layer pool1
I1226 13:02:51.434123 154317 net.cpp:178] Creating Layer pool1
I1226 13:02:51.434159 154317 net.cpp:612] pool1 <- norm1
I1226 13:02:51.434193 154317 net.cpp:586] pool1 -> pool1
I1226 13:02:51.434283 154317 net.cpp:228] Setting up pool1
I1226 13:02:51.434325 154317 net.cpp:235] Top shape: 32 96 27 27 (2239488)
I1226 13:02:51.434348 154317 net.cpp:243] Memory required for data: 140259072
I1226 13:02:51.434376 154317 layer_factory.hpp:114] Creating layer conv2
I1226 13:02:51.434453 154317 net.cpp:178] Creating Layer conv2
I1226 13:02:51.434484 154317 net.cpp:612] conv2 <- pool1
I1226 13:02:51.434561 154317 net.cpp:586] conv2 -> conv2
I1226 13:02:51.450623 90983 net.cpp:228] Setting up label_data_1_split
I1226 13:02:51.450758 90983 net.cpp:235] Top shape: 32 (32)
I1226 13:02:51.450794 90983 net.cpp:235] Top shape: 32 (32)
I1226 13:02:51.450839 90983 net.cpp:243] Memory required for data: 19787520
I1226 13:02:51.450880 90983 layer_factory.hpp:114] Creating layer conv1
I1226 13:02:51.451076 90983 net.cpp:178] Creating Layer conv1
I1226 13:02:51.451184 90983 net.cpp:612] conv1 <- data
I1226 13:02:51.451256 90983 net.cpp:586] conv1 -> conv1
I1226 13:02:51.445528 92018 net.cpp:228] Setting up label_data_1_split
I1226 13:02:51.445636 92018 net.cpp:235] Top shape: 32 (32)
I1226 13:02:51.445668 92018 net.cpp:235] Top shape: 32 (32)
I1226 13:02:51.445693 92018 net.cpp:243] Memory required for data: 19787520
I1226 13:02:51.445729 92018 layer_factory.hpp:114] Creating layer conv1
I1226 13:02:51.445853 92018 net.cpp:178] Creating Layer conv1
I1226 13:02:51.445894 92018 net.cpp:612] conv1 <- data
I1226 13:02:51.445935 92018 net.cpp:586] conv1 -> conv1
I1226 13:02:51.466563 93741 net.cpp:228] Setting up data
I1226 13:02:51.466678 93741 net.cpp:235] Top shape: 32 3 227 227 (4946784)
I1226 13:02:51.466711 93741 net.cpp:235] Top shape: 32 (32)
I1226 13:02:51.466735 93741 net.cpp:243] Memory required for data: 19787264
I1226 13:02:51.466773 93741 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 13:02:51.466852 93741 net.cpp:178] Creating Layer label_data_1_split
I1226 13:02:51.466976 93741 net.cpp:612] label_data_1_split <- label
I1226 13:02:51.467025 93741 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 13:02:51.467073 93741 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 13:02:51.482281 96103 net.cpp:228] Setting up data
I1226 13:02:51.482444 96103 net.cpp:235] Top shape: 32 3 227 227 (4946784)
I1226 13:02:51.482502 96103 net.cpp:235] Top shape: 32 (32)
I1226 13:02:51.482637 96103 net.cpp:243] Memory required for data: 19787264
I1226 13:02:51.482684 96103 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 13:02:51.482853 96103 net.cpp:178] Creating Layer label_data_1_split
I1226 13:02:51.483053 96103 net.cpp:612] label_data_1_split <- label
I1226 13:02:51.483121 96103 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 13:02:51.483196 96103 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 13:02:51.491680 94300 net.cpp:228] Setting up data
I1226 13:02:51.491814 94300 net.cpp:235] Top shape: 32 3 227 227 (4946784)
I1226 13:02:51.491909 94300 net.cpp:235] Top shape: 32 (32)
I1226 13:02:51.491945 94300 net.cpp:243] Memory required for data: 19787264
I1226 13:02:51.491989 94300 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 13:02:51.492107 94300 net.cpp:178] Creating Layer label_data_1_split
I1226 13:02:51.492270 94300 net.cpp:612] label_data_1_split <- label
I1226 13:02:51.492333 94300 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 13:02:51.492403 94300 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 13:02:51.479707 93741 net.cpp:228] Setting up label_data_1_split
I1226 13:02:51.479805 93741 net.cpp:235] Top shape: 32 (32)
I1226 13:02:51.479838 93741 net.cpp:235] Top shape: 32 (32)
I1226 13:02:51.479862 93741 net.cpp:243] Memory required for data: 19787520
I1226 13:02:51.479897 93741 layer_factory.hpp:114] Creating layer conv1
I1226 13:02:51.480018 93741 net.cpp:178] Creating Layer conv1
I1226 13:02:51.480059 93741 net.cpp:612] conv1 <- data
I1226 13:02:51.480100 93741 net.cpp:586] conv1 -> conv1
I1226 13:02:51.487975 122158 net.cpp:228] Setting up data
I1226 13:02:51.488124 122158 net.cpp:235] Top shape: 32 3 227 227 (4946784)
I1226 13:02:51.488175 122158 net.cpp:235] Top shape: 32 (32)
I1226 13:02:51.488210 122158 net.cpp:243] Memory required for data: 19787264
I1226 13:02:51.488260 122158 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 13:02:51.488476 122158 net.cpp:178] Creating Layer label_data_1_split
I1226 13:02:51.488528 122158 net.cpp:612] label_data_1_split <- label
I1226 13:02:51.488579 122158 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 13:02:51.488677 122158 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 13:02:51.505245 94300 net.cpp:228] Setting up label_data_1_split
I1226 13:02:51.505363 94300 net.cpp:235] Top shape: 32 (32)
I1226 13:02:51.505401 94300 net.cpp:235] Top shape: 32 (32)
I1226 13:02:51.505429 94300 net.cpp:243] Memory required for data: 19787520
I1226 13:02:51.505466 94300 layer_factory.hpp:114] Creating layer conv1
I1226 13:02:51.505578 94300 net.cpp:178] Creating Layer conv1
I1226 13:02:51.505622 94300 net.cpp:612] conv1 <- data
I1226 13:02:51.505667 94300 net.cpp:586] conv1 -> conv1
I1226 13:02:51.502617 96103 net.cpp:228] Setting up label_data_1_split
I1226 13:02:51.502765 96103 net.cpp:235] Top shape: 32 (32)
I1226 13:02:51.502995 96103 net.cpp:235] Top shape: 32 (32)
I1226 13:02:51.503059 96103 net.cpp:243] Memory required for data: 19787520
I1226 13:02:51.503113 96103 layer_factory.hpp:114] Creating layer conv1
I1226 13:02:51.503585 96103 net.cpp:178] Creating Layer conv1
I1226 13:02:51.503705 96103 net.cpp:612] conv1 <- data
I1226 13:02:51.503845 96103 net.cpp:586] conv1 -> conv1
I1226 13:02:51.509027 122158 net.cpp:228] Setting up label_data_1_split
I1226 13:02:51.509160 122158 net.cpp:235] Top shape: 32 (32)
I1226 13:02:51.509207 122158 net.cpp:235] Top shape: 32 (32)
I1226 13:02:51.509240 122158 net.cpp:243] Memory required for data: 19787520
I1226 13:02:51.509287 122158 layer_factory.hpp:114] Creating layer conv1
I1226 13:02:51.509426 122158 net.cpp:178] Creating Layer conv1
I1226 13:02:51.509480 122158 net.cpp:612] conv1 <- data
I1226 13:02:51.509536 122158 net.cpp:586] conv1 -> conv1
I1226 13:02:51.555394 85539 net.cpp:228] Setting up conv1
I1226 13:02:51.555505 85539 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 13:02:51.555533 85539 net.cpp:243] Memory required for data: 56958720
I1226 13:02:51.555711 85539 layer_factory.hpp:114] Creating layer relu1
I1226 13:02:51.555871 85539 net.cpp:178] Creating Layer relu1
I1226 13:02:51.555920 85539 net.cpp:612] relu1 <- conv1
I1226 13:02:51.555972 85539 net.cpp:573] relu1 -> conv1 (in-place)
I1226 13:02:51.556071 85539 net.cpp:228] Setting up relu1
I1226 13:02:51.556120 85539 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 13:02:51.556151 85539 net.cpp:243] Memory required for data: 94129920
I1226 13:02:51.556185 85539 layer_factory.hpp:114] Creating layer norm1
I1226 13:02:51.556247 85539 net.cpp:178] Creating Layer norm1
I1226 13:02:51.556277 85539 net.cpp:612] norm1 <- conv1
I1226 13:02:51.556334 85539 net.cpp:586] norm1 -> norm1
I1226 13:02:51.556426 85539 net.cpp:228] Setting up norm1
I1226 13:02:51.556466 85539 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 13:02:51.556502 85539 net.cpp:243] Memory required for data: 131301120
I1226 13:02:51.556540 85539 layer_factory.hpp:114] Creating layer pool1
I1226 13:02:51.556602 85539 net.cpp:178] Creating Layer pool1
I1226 13:02:51.556630 85539 net.cpp:612] pool1 <- norm1
I1226 13:02:51.556689 85539 net.cpp:586] pool1 -> pool1
I1226 13:02:51.556785 85539 net.cpp:228] Setting up pool1
I1226 13:02:51.556834 85539 net.cpp:235] Top shape: 32 96 27 27 (2239488)
I1226 13:02:51.556857 85539 net.cpp:243] Memory required for data: 140259072
I1226 13:02:51.556885 85539 layer_factory.hpp:114] Creating layer conv2
I1226 13:02:51.556957 85539 net.cpp:178] Creating Layer conv2
I1226 13:02:51.556993 85539 net.cpp:612] conv2 <- pool1
I1226 13:02:51.557040 85539 net.cpp:586] conv2 -> conv2
I1226 13:02:51.568230 154317 net.cpp:228] Setting up conv2
I1226 13:02:51.568341 154317 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 13:02:51.568370 154317 net.cpp:243] Memory required for data: 164146944
I1226 13:02:51.568444 154317 layer_factory.hpp:114] Creating layer relu2
I1226 13:02:51.568529 154317 net.cpp:178] Creating Layer relu2
I1226 13:02:51.568567 154317 net.cpp:612] relu2 <- conv2
I1226 13:02:51.568608 154317 net.cpp:573] relu2 -> conv2 (in-place)
I1226 13:02:51.568711 154317 net.cpp:228] Setting up relu2
I1226 13:02:51.568755 154317 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 13:02:51.568780 154317 net.cpp:243] Memory required for data: 188034816
I1226 13:02:51.568817 154317 layer_factory.hpp:114] Creating layer norm2
I1226 13:02:51.568874 154317 net.cpp:178] Creating Layer norm2
I1226 13:02:51.568907 154317 net.cpp:612] norm2 <- conv2
I1226 13:02:51.568941 154317 net.cpp:586] norm2 -> norm2
I1226 13:02:51.569058 154317 net.cpp:228] Setting up norm2
I1226 13:02:51.569106 154317 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 13:02:51.569131 154317 net.cpp:243] Memory required for data: 211922688
I1226 13:02:51.569159 154317 layer_factory.hpp:114] Creating layer pool2
I1226 13:02:51.569222 154317 net.cpp:178] Creating Layer pool2
I1226 13:02:51.569255 154317 net.cpp:612] pool2 <- norm2
I1226 13:02:51.569298 154317 net.cpp:586] pool2 -> pool2
I1226 13:02:51.569382 154317 net.cpp:228] Setting up pool2
I1226 13:02:51.569517 154317 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 13:02:51.569545 154317 net.cpp:243] Memory required for data: 217460480
I1226 13:02:51.569576 154317 layer_factory.hpp:114] Creating layer conv3
I1226 13:02:51.569659 154317 net.cpp:178] Creating Layer conv3
I1226 13:02:51.569694 154317 net.cpp:612] conv3 <- pool2
I1226 13:02:51.569737 154317 net.cpp:586] conv3 -> conv3
I1226 13:02:51.579345 91895 net.cpp:228] Setting up conv1
I1226 13:02:51.579465 91895 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 13:02:51.579502 91895 net.cpp:243] Memory required for data: 56958720
I1226 13:02:51.579617 91895 layer_factory.hpp:114] Creating layer relu1
I1226 13:02:51.579717 91895 net.cpp:178] Creating Layer relu1
I1226 13:02:51.579761 91895 net.cpp:612] relu1 <- conv1
I1226 13:02:51.579814 91895 net.cpp:573] relu1 -> conv1 (in-place)
I1226 13:02:51.579910 91895 net.cpp:228] Setting up relu1
I1226 13:02:51.579954 91895 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 13:02:51.579978 91895 net.cpp:243] Memory required for data: 94129920
I1226 13:02:51.580008 91895 layer_factory.hpp:114] Creating layer norm1
I1226 13:02:51.580070 91895 net.cpp:178] Creating Layer norm1
I1226 13:02:51.580106 91895 net.cpp:612] norm1 <- conv1
I1226 13:02:51.580158 91895 net.cpp:586] norm1 -> norm1
I1226 13:02:51.580251 91895 net.cpp:228] Setting up norm1
I1226 13:02:51.580322 91895 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 13:02:51.580350 91895 net.cpp:243] Memory required for data: 131301120
I1226 13:02:51.580392 91895 layer_factory.hpp:114] Creating layer pool1
I1226 13:02:51.580453 91895 net.cpp:178] Creating Layer pool1
I1226 13:02:51.580484 91895 net.cpp:612] pool1 <- norm1
I1226 13:02:51.580536 91895 net.cpp:586] pool1 -> pool1
I1226 13:02:51.580636 91895 net.cpp:228] Setting up pool1
I1226 13:02:51.580682 91895 net.cpp:235] Top shape: 32 96 27 27 (2239488)
I1226 13:02:51.580704 91895 net.cpp:243] Memory required for data: 140259072
I1226 13:02:51.580734 91895 layer_factory.hpp:114] Creating layer conv2
I1226 13:02:51.580826 91895 net.cpp:178] Creating Layer conv2
I1226 13:02:51.580863 91895 net.cpp:612] conv2 <- pool1
I1226 13:02:51.580919 91895 net.cpp:586] conv2 -> conv2
I1226 13:02:51.588461 92018 net.cpp:228] Setting up conv1
I1226 13:02:51.588577 92018 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 13:02:51.588604 92018 net.cpp:243] Memory required for data: 56958720
I1226 13:02:51.588742 92018 layer_factory.hpp:114] Creating layer relu1
I1226 13:02:51.588912 92018 net.cpp:178] Creating Layer relu1
I1226 13:02:51.588956 92018 net.cpp:612] relu1 <- conv1
I1226 13:02:51.589015 92018 net.cpp:573] relu1 -> conv1 (in-place)
I1226 13:02:51.589118 92018 net.cpp:228] Setting up relu1
I1226 13:02:51.589169 92018 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 13:02:51.589193 92018 net.cpp:243] Memory required for data: 94129920
I1226 13:02:51.589223 92018 layer_factory.hpp:114] Creating layer norm1
I1226 13:02:51.589300 92018 net.cpp:178] Creating Layer norm1
I1226 13:02:51.589334 92018 net.cpp:612] norm1 <- conv1
I1226 13:02:51.589381 92018 net.cpp:586] norm1 -> norm1
I1226 13:02:51.589471 92018 net.cpp:228] Setting up norm1
I1226 13:02:51.589514 92018 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 13:02:51.589539 92018 net.cpp:243] Memory required for data: 131301120
I1226 13:02:51.589567 92018 layer_factory.hpp:114] Creating layer pool1
I1226 13:02:51.589638 92018 net.cpp:178] Creating Layer pool1
I1226 13:02:51.589673 92018 net.cpp:612] pool1 <- norm1
I1226 13:02:51.589715 92018 net.cpp:586] pool1 -> pool1
I1226 13:02:51.589848 92018 net.cpp:228] Setting up pool1
I1226 13:02:51.589905 92018 net.cpp:235] Top shape: 32 96 27 27 (2239488)
I1226 13:02:51.589929 92018 net.cpp:243] Memory required for data: 140259072
I1226 13:02:51.589958 92018 layer_factory.hpp:114] Creating layer conv2
I1226 13:02:51.590031 92018 net.cpp:178] Creating Layer conv2
I1226 13:02:51.590066 92018 net.cpp:612] conv2 <- pool1
I1226 13:02:51.590107 92018 net.cpp:586] conv2 -> conv2
I1226 13:02:51.604753 90983 net.cpp:228] Setting up conv1
I1226 13:02:51.605170 90983 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 13:02:51.605267 90983 net.cpp:243] Memory required for data: 56958720
I1226 13:02:51.605407 90983 layer_factory.hpp:114] Creating layer relu1
I1226 13:02:51.605494 90983 net.cpp:178] Creating Layer relu1
I1226 13:02:51.605634 90983 net.cpp:612] relu1 <- conv1
I1226 13:02:51.605676 90983 net.cpp:573] relu1 -> conv1 (in-place)
I1226 13:02:51.605813 90983 net.cpp:228] Setting up relu1
I1226 13:02:51.605954 90983 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 13:02:51.605981 90983 net.cpp:243] Memory required for data: 94129920
I1226 13:02:51.606011 90983 layer_factory.hpp:114] Creating layer norm1
I1226 13:02:51.606082 90983 net.cpp:178] Creating Layer norm1
I1226 13:02:51.606112 90983 net.cpp:612] norm1 <- conv1
I1226 13:02:51.606168 90983 net.cpp:586] norm1 -> norm1
I1226 13:02:51.606410 90983 net.cpp:228] Setting up norm1
I1226 13:02:51.606462 90983 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 13:02:51.606484 90983 net.cpp:243] Memory required for data: 131301120
I1226 13:02:51.606514 90983 layer_factory.hpp:114] Creating layer pool1
I1226 13:02:51.607046 90983 net.cpp:178] Creating Layer pool1
I1226 13:02:51.607122 90983 net.cpp:612] pool1 <- norm1
I1226 13:02:51.607241 90983 net.cpp:586] pool1 -> pool1
I1226 13:02:51.607409 90983 net.cpp:228] Setting up pool1
I1226 13:02:51.607463 90983 net.cpp:235] Top shape: 32 96 27 27 (2239488)
I1226 13:02:51.607486 90983 net.cpp:243] Memory required for data: 140259072
I1226 13:02:51.607517 90983 layer_factory.hpp:114] Creating layer conv2
I1226 13:02:51.607614 90983 net.cpp:178] Creating Layer conv2
I1226 13:02:51.607662 90983 net.cpp:612] conv2 <- pool1
I1226 13:02:51.607707 90983 net.cpp:586] conv2 -> conv2
I1226 13:02:51.607118 93741 net.cpp:228] Setting up conv1
I1226 13:02:51.607267 93741 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 13:02:51.607298 93741 net.cpp:243] Memory required for data: 56958720
I1226 13:02:51.607446 93741 layer_factory.hpp:114] Creating layer relu1
I1226 13:02:51.607554 93741 net.cpp:178] Creating Layer relu1
I1226 13:02:51.607599 93741 net.cpp:612] relu1 <- conv1
I1226 13:02:51.607663 93741 net.cpp:573] relu1 -> conv1 (in-place)
I1226 13:02:51.607780 93741 net.cpp:228] Setting up relu1
I1226 13:02:51.607841 93741 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 13:02:51.607869 93741 net.cpp:243] Memory required for data: 94129920
I1226 13:02:51.607902 93741 layer_factory.hpp:114] Creating layer norm1
I1226 13:02:51.607978 93741 net.cpp:178] Creating Layer norm1
I1226 13:02:51.608012 93741 net.cpp:612] norm1 <- conv1
I1226 13:02:51.608062 93741 net.cpp:586] norm1 -> norm1
I1226 13:02:51.608162 93741 net.cpp:228] Setting up norm1
I1226 13:02:51.608219 93741 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 13:02:51.608245 93741 net.cpp:243] Memory required for data: 131301120
I1226 13:02:51.608273 93741 layer_factory.hpp:114] Creating layer pool1
I1226 13:02:51.608335 93741 net.cpp:178] Creating Layer pool1
I1226 13:02:51.608366 93741 net.cpp:612] pool1 <- norm1
I1226 13:02:51.608431 93741 net.cpp:586] pool1 -> pool1
I1226 13:02:51.608533 93741 net.cpp:228] Setting up pool1
I1226 13:02:51.608588 93741 net.cpp:235] Top shape: 32 96 27 27 (2239488)
I1226 13:02:51.608611 93741 net.cpp:243] Memory required for data: 140259072
I1226 13:02:51.608639 93741 layer_factory.hpp:114] Creating layer conv2
I1226 13:02:51.608714 93741 net.cpp:178] Creating Layer conv2
I1226 13:02:51.608743 93741 net.cpp:612] conv2 <- pool1
I1226 13:02:51.608798 93741 net.cpp:586] conv2 -> conv2
I1226 13:02:51.740216 85539 net.cpp:228] Setting up conv2
I1226 13:02:51.740391 85539 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 13:02:51.740442 85539 net.cpp:243] Memory required for data: 164146944
I1226 13:02:51.740588 85539 layer_factory.hpp:114] Creating layer relu2
I1226 13:02:51.740862 85539 net.cpp:178] Creating Layer relu2
I1226 13:02:51.741063 85539 net.cpp:612] relu2 <- conv2
I1226 13:02:51.741143 85539 net.cpp:573] relu2 -> conv2 (in-place)
I1226 13:02:51.741403 85539 net.cpp:228] Setting up relu2
I1226 13:02:51.742110 85539 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 13:02:51.742215 85539 net.cpp:243] Memory required for data: 188034816
I1226 13:02:51.742274 85539 layer_factory.hpp:114] Creating layer norm2
I1226 13:02:51.742352 85539 net.cpp:178] Creating Layer norm2
I1226 13:02:51.742422 85539 net.cpp:612] norm2 <- conv2
I1226 13:02:51.742504 85539 net.cpp:586] norm2 -> norm2
I1226 13:02:51.742691 85539 net.cpp:228] Setting up norm2
I1226 13:02:51.742797 85539 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 13:02:51.742856 85539 net.cpp:243] Memory required for data: 211922688
I1226 13:02:51.743070 85539 layer_factory.hpp:114] Creating layer pool2
I1226 13:02:51.743230 85539 net.cpp:178] Creating Layer pool2
I1226 13:02:51.743291 85539 net.cpp:612] pool2 <- norm2
I1226 13:02:51.743345 85539 net.cpp:586] pool2 -> pool2
I1226 13:02:51.743880 85539 net.cpp:228] Setting up pool2
I1226 13:02:51.744042 85539 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 13:02:51.744225 85539 net.cpp:243] Memory required for data: 217460480
I1226 13:02:51.744283 85539 layer_factory.hpp:114] Creating layer conv3
I1226 13:02:51.744410 85539 net.cpp:178] Creating Layer conv3
I1226 13:02:51.744454 85539 net.cpp:612] conv3 <- pool2
I1226 13:02:51.744551 85539 net.cpp:586] conv3 -> conv3
I1226 13:02:51.735326 122158 net.cpp:228] Setting up conv1
I1226 13:02:51.735468 122158 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 13:02:51.735508 122158 net.cpp:243] Memory required for data: 56958720
I1226 13:02:51.735697 122158 layer_factory.hpp:114] Creating layer relu1
I1226 13:02:51.735831 122158 net.cpp:178] Creating Layer relu1
I1226 13:02:51.735893 122158 net.cpp:612] relu1 <- conv1
I1226 13:02:51.735947 122158 net.cpp:573] relu1 -> conv1 (in-place)
I1226 13:02:51.736091 122158 net.cpp:228] Setting up relu1
I1226 13:02:51.736160 122158 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 13:02:51.736196 122158 net.cpp:243] Memory required for data: 94129920
I1226 13:02:51.736237 122158 layer_factory.hpp:114] Creating layer norm1
I1226 13:02:51.736343 122158 net.cpp:178] Creating Layer norm1
I1226 13:02:51.736387 122158 net.cpp:612] norm1 <- conv1
I1226 13:02:51.736450 122158 net.cpp:586] norm1 -> norm1
I1226 13:02:51.736585 122158 net.cpp:228] Setting up norm1
I1226 13:02:51.736675 122158 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 13:02:51.736712 122158 net.cpp:243] Memory required for data: 131301120
I1226 13:02:51.736753 122158 layer_factory.hpp:114] Creating layer pool1
I1226 13:02:51.736842 122158 net.cpp:178] Creating Layer pool1
I1226 13:02:51.736882 122158 net.cpp:612] pool1 <- norm1
I1226 13:02:51.736943 122158 net.cpp:586] pool1 -> pool1
I1226 13:02:51.737076 122158 net.cpp:228] Setting up pool1
I1226 13:02:51.737135 122158 net.cpp:235] Top shape: 32 96 27 27 (2239488)
I1226 13:02:51.737169 122158 net.cpp:243] Memory required for data: 140259072
I1226 13:02:51.737221 122158 layer_factory.hpp:114] Creating layer conv2
I1226 13:02:51.737315 122158 net.cpp:178] Creating Layer conv2
I1226 13:02:51.737354 122158 net.cpp:612] conv2 <- pool1
I1226 13:02:51.737408 122158 net.cpp:586] conv2 -> conv2
I1226 13:02:51.753329 154317 net.cpp:228] Setting up conv3
I1226 13:02:51.753437 154317 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:02:51.753463 154317 net.cpp:243] Memory required for data: 225767168
I1226 13:02:51.753533 154317 layer_factory.hpp:114] Creating layer relu3
I1226 13:02:51.753597 154317 net.cpp:178] Creating Layer relu3
I1226 13:02:51.753628 154317 net.cpp:612] relu3 <- conv3
I1226 13:02:51.753672 154317 net.cpp:573] relu3 -> conv3 (in-place)
I1226 13:02:51.753749 154317 net.cpp:228] Setting up relu3
I1226 13:02:51.753782 154317 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:02:51.753816 154317 net.cpp:243] Memory required for data: 234073856
I1226 13:02:51.753851 154317 layer_factory.hpp:114] Creating layer conv4
I1226 13:02:51.753916 154317 net.cpp:178] Creating Layer conv4
I1226 13:02:51.753943 154317 net.cpp:612] conv4 <- conv3
I1226 13:02:51.754019 154317 net.cpp:586] conv4 -> conv4
I1226 13:02:51.781975 92018 net.cpp:228] Setting up conv2
I1226 13:02:51.782088 92018 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 13:02:51.782121 92018 net.cpp:243] Memory required for data: 164146944
I1226 13:02:51.782224 92018 layer_factory.hpp:114] Creating layer relu2
I1226 13:02:51.782294 92018 net.cpp:178] Creating Layer relu2
I1226 13:02:51.782325 92018 net.cpp:612] relu2 <- conv2
I1226 13:02:51.782372 92018 net.cpp:573] relu2 -> conv2 (in-place)
I1226 13:02:51.782474 92018 net.cpp:228] Setting up relu2
I1226 13:02:51.782533 92018 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 13:02:51.782559 92018 net.cpp:243] Memory required for data: 188034816
I1226 13:02:51.782591 92018 layer_factory.hpp:114] Creating layer norm2
I1226 13:02:51.782640 92018 net.cpp:178] Creating Layer norm2
I1226 13:02:51.782763 92018 net.cpp:612] norm2 <- conv2
I1226 13:02:51.784215 91895 net.cpp:228] Setting up conv2
I1226 13:02:51.784343 91895 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 13:02:51.784371 91895 net.cpp:243] Memory required for data: 164146944
I1226 13:02:51.784471 91895 layer_factory.hpp:114] Creating layer relu2
I1226 13:02:51.784554 91895 net.cpp:178] Creating Layer relu2
I1226 13:02:51.784590 91895 net.cpp:612] relu2 <- conv2
I1226 13:02:51.784693 91895 net.cpp:573] relu2 -> conv2 (in-place)
I1226 13:02:51.784795 91895 net.cpp:228] Setting up relu2
I1226 13:02:51.784844 91895 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 13:02:51.784867 91895 net.cpp:243] Memory required for data: 188034816
I1226 13:02:51.784898 91895 layer_factory.hpp:114] Creating layer norm2
I1226 13:02:51.784947 91895 net.cpp:178] Creating Layer norm2
I1226 13:02:51.785085 91895 net.cpp:612] norm2 <- conv2
I1226 13:02:51.785141 91895 net.cpp:586] norm2 -> norm2
I1226 13:02:51.785348 91895 net.cpp:228] Setting up norm2
I1226 13:02:51.785410 91895 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 13:02:51.785434 91895 net.cpp:243] Memory required for data: 211922688
I1226 13:02:51.785465 91895 layer_factory.hpp:114] Creating layer pool2
I1226 13:02:51.785527 91895 net.cpp:178] Creating Layer pool2
I1226 13:02:51.785558 91895 net.cpp:612] pool2 <- norm2
I1226 13:02:51.785595 91895 net.cpp:586] pool2 -> pool2
I1226 13:02:51.785677 91895 net.cpp:228] Setting up pool2
I1226 13:02:51.785718 91895 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 13:02:51.785837 91895 net.cpp:243] Memory required for data: 217460480
I1226 13:02:51.785869 91895 layer_factory.hpp:114] Creating layer conv3
I1226 13:02:51.785950 91895 net.cpp:178] Creating Layer conv3
I1226 13:02:51.785990 91895 net.cpp:612] conv3 <- pool2
I1226 13:02:51.786041 91895 net.cpp:586] conv3 -> conv3
I1226 13:02:51.786732 92018 net.cpp:586] norm2 -> norm2
I1226 13:02:51.786983 92018 net.cpp:228] Setting up norm2
I1226 13:02:51.787096 92018 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 13:02:51.787236 92018 net.cpp:243] Memory required for data: 211922688
I1226 13:02:51.787272 92018 layer_factory.hpp:114] Creating layer pool2
I1226 13:02:51.787343 92018 net.cpp:178] Creating Layer pool2
I1226 13:02:51.787380 92018 net.cpp:612] pool2 <- norm2
I1226 13:02:51.787420 92018 net.cpp:586] pool2 -> pool2
I1226 13:02:51.787518 92018 net.cpp:228] Setting up pool2
I1226 13:02:51.787569 92018 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 13:02:51.787689 92018 net.cpp:243] Memory required for data: 217460480
I1226 13:02:51.787727 92018 layer_factory.hpp:114] Creating layer conv3
I1226 13:02:51.787840 92018 net.cpp:178] Creating Layer conv3
I1226 13:02:51.787883 92018 net.cpp:612] conv3 <- pool2
I1226 13:02:51.788007 92018 net.cpp:586] conv3 -> conv3
I1226 13:02:51.829108 90983 net.cpp:228] Setting up conv2
I1226 13:02:51.829248 90983 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 13:02:51.829279 90983 net.cpp:243] Memory required for data: 164146944
I1226 13:02:51.829355 90983 layer_factory.hpp:114] Creating layer relu2
I1226 13:02:51.829438 90983 net.cpp:178] Creating Layer relu2
I1226 13:02:51.829478 90983 net.cpp:612] relu2 <- conv2
I1226 13:02:51.829524 90983 net.cpp:573] relu2 -> conv2 (in-place)
I1226 13:02:51.829613 90983 net.cpp:228] Setting up relu2
I1226 13:02:51.829658 90983 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 13:02:51.829684 90983 net.cpp:243] Memory required for data: 188034816
I1226 13:02:51.829711 90983 layer_factory.hpp:114] Creating layer norm2
I1226 13:02:51.829797 90983 net.cpp:178] Creating Layer norm2
I1226 13:02:51.829831 90983 net.cpp:612] norm2 <- conv2
I1226 13:02:51.829866 90983 net.cpp:586] norm2 -> norm2
I1226 13:02:51.829946 90983 net.cpp:228] Setting up norm2
I1226 13:02:51.829983 90983 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 13:02:51.830013 90983 net.cpp:243] Memory required for data: 211922688
I1226 13:02:51.830040 90983 layer_factory.hpp:114] Creating layer pool2
I1226 13:02:51.830091 90983 net.cpp:178] Creating Layer pool2
I1226 13:02:51.830121 90983 net.cpp:612] pool2 <- norm2
I1226 13:02:51.830168 90983 net.cpp:586] pool2 -> pool2
I1226 13:02:51.830268 90983 net.cpp:228] Setting up pool2
I1226 13:02:51.830309 90983 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 13:02:51.830427 90983 net.cpp:243] Memory required for data: 217460480
I1226 13:02:51.830459 90983 layer_factory.hpp:114] Creating layer conv3
I1226 13:02:51.830548 90983 net.cpp:178] Creating Layer conv3
I1226 13:02:51.830582 90983 net.cpp:612] conv3 <- pool2
I1226 13:02:51.830626 90983 net.cpp:586] conv3 -> conv3
I1226 13:02:51.828250 93741 net.cpp:228] Setting up conv2
I1226 13:02:51.828429 93741 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 13:02:51.828470 93741 net.cpp:243] Memory required for data: 164146944
I1226 13:02:51.828563 93741 layer_factory.hpp:114] Creating layer relu2
I1226 13:02:51.828687 93741 net.cpp:178] Creating Layer relu2
I1226 13:02:51.828730 93741 net.cpp:612] relu2 <- conv2
I1226 13:02:51.828778 93741 net.cpp:573] relu2 -> conv2 (in-place)
I1226 13:02:51.828884 93741 net.cpp:228] Setting up relu2
I1226 13:02:51.828975 93741 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 13:02:51.829006 93741 net.cpp:243] Memory required for data: 188034816
I1226 13:02:51.829041 93741 layer_factory.hpp:114] Creating layer norm2
I1226 13:02:51.829097 93741 net.cpp:178] Creating Layer norm2
I1226 13:02:51.829128 93741 net.cpp:612] norm2 <- conv2
I1226 13:02:51.829186 93741 net.cpp:586] norm2 -> norm2
I1226 13:02:51.829288 93741 net.cpp:228] Setting up norm2
I1226 13:02:51.829354 93741 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 13:02:51.829421 93741 net.cpp:243] Memory required for data: 211922688
I1226 13:02:51.829457 93741 layer_factory.hpp:114] Creating layer pool2
I1226 13:02:51.829524 93741 net.cpp:178] Creating Layer pool2
I1226 13:02:51.829558 93741 net.cpp:612] pool2 <- norm2
I1226 13:02:51.829597 93741 net.cpp:586] pool2 -> pool2
I1226 13:02:51.829695 93741 net.cpp:228] Setting up pool2
I1226 13:02:51.829748 93741 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 13:02:51.829864 93741 net.cpp:243] Memory required for data: 217460480
I1226 13:02:51.829901 93741 layer_factory.hpp:114] Creating layer conv3
I1226 13:02:51.829995 93741 net.cpp:178] Creating Layer conv3
I1226 13:02:51.830036 93741 net.cpp:612] conv3 <- pool2
I1226 13:02:51.830090 93741 net.cpp:586] conv3 -> conv3
I1226 13:02:51.871749 96103 net.cpp:228] Setting up conv1
I1226 13:02:51.871896 96103 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 13:02:51.871929 96103 net.cpp:243] Memory required for data: 56958720
I1226 13:02:51.872052 96103 layer_factory.hpp:114] Creating layer relu1
I1226 13:02:51.872144 96103 net.cpp:178] Creating Layer relu1
I1226 13:02:51.872191 96103 net.cpp:612] relu1 <- conv1
I1226 13:02:51.872263 96103 net.cpp:573] relu1 -> conv1 (in-place)
I1226 13:02:51.872387 96103 net.cpp:228] Setting up relu1
I1226 13:02:51.872447 96103 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 13:02:51.872474 96103 net.cpp:243] Memory required for data: 94129920
I1226 13:02:51.872508 96103 layer_factory.hpp:114] Creating layer norm1
I1226 13:02:51.872581 96103 net.cpp:178] Creating Layer norm1
I1226 13:02:51.872618 96103 net.cpp:612] norm1 <- conv1
I1226 13:02:51.872668 96103 net.cpp:586] norm1 -> norm1
I1226 13:02:51.872815 96103 net.cpp:228] Setting up norm1
I1226 13:02:51.872872 96103 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 13:02:51.872898 96103 net.cpp:243] Memory required for data: 131301120
I1226 13:02:51.872930 96103 layer_factory.hpp:114] Creating layer pool1
I1226 13:02:51.873001 96103 net.cpp:178] Creating Layer pool1
I1226 13:02:51.873037 96103 net.cpp:612] pool1 <- norm1
I1226 13:02:51.873075 96103 net.cpp:586] pool1 -> pool1
I1226 13:02:51.873177 96103 net.cpp:228] Setting up pool1
I1226 13:02:51.873227 96103 net.cpp:235] Top shape: 32 96 27 27 (2239488)
I1226 13:02:51.873251 96103 net.cpp:243] Memory required for data: 140259072
I1226 13:02:51.873281 96103 layer_factory.hpp:114] Creating layer conv2
I1226 13:02:51.873373 96103 net.cpp:178] Creating Layer conv2
I1226 13:02:51.873409 96103 net.cpp:612] conv2 <- pool1
I1226 13:02:51.873455 96103 net.cpp:586] conv2 -> conv2
I1226 13:02:51.909281 94300 net.cpp:228] Setting up conv1
I1226 13:02:51.909400 94300 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 13:02:51.909431 94300 net.cpp:243] Memory required for data: 56958720
I1226 13:02:51.909554 94300 layer_factory.hpp:114] Creating layer relu1
I1226 13:02:51.909639 94300 net.cpp:178] Creating Layer relu1
I1226 13:02:51.909693 94300 net.cpp:612] relu1 <- conv1
I1226 13:02:51.909752 94300 net.cpp:573] relu1 -> conv1 (in-place)
I1226 13:02:51.909983 94300 net.cpp:228] Setting up relu1
I1226 13:02:51.910055 94300 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 13:02:51.910084 94300 net.cpp:243] Memory required for data: 94129920
I1226 13:02:51.910120 94300 layer_factory.hpp:114] Creating layer norm1
I1226 13:02:51.910184 94300 net.cpp:178] Creating Layer norm1
I1226 13:02:51.910223 94300 net.cpp:612] norm1 <- conv1
I1226 13:02:51.910275 94300 net.cpp:586] norm1 -> norm1
I1226 13:02:51.910377 94300 net.cpp:228] Setting up norm1
I1226 13:02:51.910429 94300 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 13:02:51.910454 94300 net.cpp:243] Memory required for data: 131301120
I1226 13:02:51.910485 94300 layer_factory.hpp:114] Creating layer pool1
I1226 13:02:51.910555 94300 net.cpp:178] Creating Layer pool1
I1226 13:02:51.910589 94300 net.cpp:612] pool1 <- norm1
I1226 13:02:51.910655 94300 net.cpp:586] pool1 -> pool1
I1226 13:02:51.910759 94300 net.cpp:228] Setting up pool1
I1226 13:02:51.910809 94300 net.cpp:235] Top shape: 32 96 27 27 (2239488)
I1226 13:02:51.910858 94300 net.cpp:243] Memory required for data: 140259072
I1226 13:02:51.910889 94300 layer_factory.hpp:114] Creating layer conv2
I1226 13:02:51.910979 94300 net.cpp:178] Creating Layer conv2
I1226 13:02:51.911016 94300 net.cpp:612] conv2 <- pool1
I1226 13:02:51.911059 94300 net.cpp:586] conv2 -> conv2
I1226 13:02:51.909092 154317 net.cpp:228] Setting up conv4
I1226 13:02:51.909230 154317 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:02:51.909256 154317 net.cpp:243] Memory required for data: 242380544
I1226 13:02:51.909338 154317 layer_factory.hpp:114] Creating layer relu4
I1226 13:02:51.909413 154317 net.cpp:178] Creating Layer relu4
I1226 13:02:51.909451 154317 net.cpp:612] relu4 <- conv4
I1226 13:02:51.909488 154317 net.cpp:573] relu4 -> conv4 (in-place)
I1226 13:02:51.909579 154317 net.cpp:228] Setting up relu4
I1226 13:02:51.909620 154317 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:02:51.909642 154317 net.cpp:243] Memory required for data: 250687232
I1226 13:02:51.909669 154317 layer_factory.hpp:114] Creating layer conv5
I1226 13:02:51.909759 154317 net.cpp:178] Creating Layer conv5
I1226 13:02:51.909795 154317 net.cpp:612] conv5 <- conv4
I1226 13:02:51.909835 154317 net.cpp:586] conv5 -> conv5
I1226 13:02:52.017216 154317 net.cpp:228] Setting up conv5
I1226 13:02:52.017323 154317 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 13:02:52.017350 154317 net.cpp:243] Memory required for data: 256225024
I1226 13:02:52.017439 154317 layer_factory.hpp:114] Creating layer relu5
I1226 13:02:52.017503 154317 net.cpp:178] Creating Layer relu5
I1226 13:02:52.017540 154317 net.cpp:612] relu5 <- conv5
I1226 13:02:52.017591 154317 net.cpp:573] relu5 -> conv5 (in-place)
I1226 13:02:52.017684 154317 net.cpp:228] Setting up relu5
I1226 13:02:52.017732 154317 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 13:02:52.017765 154317 net.cpp:243] Memory required for data: 261762816
I1226 13:02:52.017793 154317 layer_factory.hpp:114] Creating layer pool5
I1226 13:02:52.017851 154317 net.cpp:178] Creating Layer pool5
I1226 13:02:52.017885 154317 net.cpp:612] pool5 <- conv5
I1226 13:02:52.017922 154317 net.cpp:586] pool5 -> pool5
I1226 13:02:52.018035 154317 net.cpp:228] Setting up pool5
I1226 13:02:52.018081 154317 net.cpp:235] Top shape: 32 256 6 6 (294912)
I1226 13:02:52.018105 154317 net.cpp:243] Memory required for data: 262942464
I1226 13:02:52.018131 154317 layer_factory.hpp:114] Creating layer fc6
I1226 13:02:52.018199 154317 net.cpp:178] Creating Layer fc6
I1226 13:02:52.018234 154317 net.cpp:612] fc6 <- pool5
I1226 13:02:52.018283 154317 net.cpp:586] fc6 -> fc6
I1226 13:02:52.027078 122158 net.cpp:228] Setting up conv2
I1226 13:02:52.027218 122158 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 13:02:52.027259 122158 net.cpp:243] Memory required for data: 164146944
I1226 13:02:52.027405 122158 layer_factory.hpp:114] Creating layer relu2
I1226 13:02:52.027528 122158 net.cpp:178] Creating Layer relu2
I1226 13:02:52.027573 122158 net.cpp:612] relu2 <- conv2
I1226 13:02:52.027696 122158 net.cpp:573] relu2 -> conv2 (in-place)
I1226 13:02:52.027834 122158 net.cpp:228] Setting up relu2
I1226 13:02:52.028048 122158 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 13:02:52.028229 122158 net.cpp:243] Memory required for data: 188034816
I1226 13:02:52.028278 122158 layer_factory.hpp:114] Creating layer norm2
I1226 13:02:52.028854 122158 net.cpp:178] Creating Layer norm2
I1226 13:02:52.029023 122158 net.cpp:612] norm2 <- conv2
I1226 13:02:52.029120 122158 net.cpp:586] norm2 -> norm2
I1226 13:02:52.029346 122158 net.cpp:228] Setting up norm2
I1226 13:02:52.029434 122158 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 13:02:52.029470 122158 net.cpp:243] Memory required for data: 211922688
I1226 13:02:52.029515 122158 layer_factory.hpp:114] Creating layer pool2
I1226 13:02:52.029743 122158 net.cpp:178] Creating Layer pool2
I1226 13:02:52.029798 122158 net.cpp:612] pool2 <- norm2
I1226 13:02:52.029867 122158 net.cpp:586] pool2 -> pool2
I1226 13:02:52.030184 122158 net.cpp:228] Setting up pool2
I1226 13:02:52.030261 122158 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 13:02:52.030297 122158 net.cpp:243] Memory required for data: 217460480
I1226 13:02:52.030376 122158 layer_factory.hpp:114] Creating layer conv3
I1226 13:02:52.030511 122158 net.cpp:178] Creating Layer conv3
I1226 13:02:52.030562 122158 net.cpp:612] conv3 <- pool2
I1226 13:02:52.030668 122158 net.cpp:586] conv3 -> conv3
I1226 13:02:52.049984 85539 net.cpp:228] Setting up conv3
I1226 13:02:52.050104 85539 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:02:52.050135 85539 net.cpp:243] Memory required for data: 225767168
I1226 13:02:52.050257 85539 layer_factory.hpp:114] Creating layer relu3
I1226 13:02:52.050333 85539 net.cpp:178] Creating Layer relu3
I1226 13:02:52.050377 85539 net.cpp:612] relu3 <- conv3
I1226 13:02:52.050447 85539 net.cpp:573] relu3 -> conv3 (in-place)
I1226 13:02:52.050572 85539 net.cpp:228] Setting up relu3
I1226 13:02:52.050637 85539 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:02:52.050695 85539 net.cpp:243] Memory required for data: 234073856
I1226 13:02:52.050737 85539 layer_factory.hpp:114] Creating layer conv4
I1226 13:02:52.050859 85539 net.cpp:178] Creating Layer conv4
I1226 13:02:52.051056 85539 net.cpp:612] conv4 <- conv3
I1226 13:02:52.051128 85539 net.cpp:586] conv4 -> conv4
I1226 13:02:52.062682 91895 net.cpp:228] Setting up conv3
I1226 13:02:52.062798 91895 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:02:52.062856 91895 net.cpp:243] Memory required for data: 225767168
I1226 13:02:52.062937 91895 layer_factory.hpp:114] Creating layer relu3
I1226 13:02:52.063135 91895 net.cpp:178] Creating Layer relu3
I1226 13:02:52.063182 91895 net.cpp:612] relu3 <- conv3
I1226 13:02:52.063225 91895 net.cpp:573] relu3 -> conv3 (in-place)
I1226 13:02:52.063377 91895 net.cpp:228] Setting up relu3
I1226 13:02:52.063441 91895 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:02:52.063464 91895 net.cpp:243] Memory required for data: 234073856
I1226 13:02:52.063493 91895 layer_factory.hpp:114] Creating layer conv4
I1226 13:02:52.063586 91895 net.cpp:178] Creating Layer conv4
I1226 13:02:52.063953 91895 net.cpp:612] conv4 <- conv3
I1226 13:02:52.064049 91895 net.cpp:586] conv4 -> conv4
I1226 13:02:52.066691 92018 net.cpp:228] Setting up conv3
I1226 13:02:52.066834 92018 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:02:52.066866 92018 net.cpp:243] Memory required for data: 225767168
I1226 13:02:52.066941 92018 layer_factory.hpp:114] Creating layer relu3
I1226 13:02:52.067076 92018 net.cpp:178] Creating Layer relu3
I1226 13:02:52.067107 92018 net.cpp:612] relu3 <- conv3
I1226 13:02:52.067175 92018 net.cpp:573] relu3 -> conv3 (in-place)
I1226 13:02:52.067301 92018 net.cpp:228] Setting up relu3
I1226 13:02:52.067373 92018 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:02:52.067397 92018 net.cpp:243] Memory required for data: 234073856
I1226 13:02:52.067427 92018 layer_factory.hpp:114] Creating layer conv4
I1226 13:02:52.067520 92018 net.cpp:178] Creating Layer conv4
I1226 13:02:52.067559 92018 net.cpp:612] conv4 <- conv3
I1226 13:02:52.067610 92018 net.cpp:586] conv4 -> conv4
I1226 13:02:52.115113 93741 net.cpp:228] Setting up conv3
I1226 13:02:52.115234 93741 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:02:52.115268 93741 net.cpp:243] Memory required for data: 225767168
I1226 13:02:52.115351 93741 layer_factory.hpp:114] Creating layer relu3
I1226 13:02:52.115494 93741 net.cpp:178] Creating Layer relu3
I1226 13:02:52.115541 93741 net.cpp:612] relu3 <- conv3
I1226 13:02:52.115619 93741 net.cpp:573] relu3 -> conv3 (in-place)
I1226 13:02:52.115741 93741 net.cpp:228] Setting up relu3
I1226 13:02:52.115835 93741 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:02:52.115869 93741 net.cpp:243] Memory required for data: 234073856
I1226 13:02:52.115906 93741 layer_factory.hpp:114] Creating layer conv4
I1226 13:02:52.116021 93741 net.cpp:178] Creating Layer conv4
I1226 13:02:52.116091 93741 net.cpp:612] conv4 <- conv3
I1226 13:02:52.116657 93741 net.cpp:586] conv4 -> conv4
I1226 13:02:52.159890 90983 net.cpp:228] Setting up conv3
I1226 13:02:52.160007 90983 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:02:52.160033 90983 net.cpp:243] Memory required for data: 225767168
I1226 13:02:52.160135 90983 layer_factory.hpp:114] Creating layer relu3
I1226 13:02:52.160197 90983 net.cpp:178] Creating Layer relu3
I1226 13:02:52.160250 90983 net.cpp:612] relu3 <- conv3
I1226 13:02:52.160289 90983 net.cpp:573] relu3 -> conv3 (in-place)
I1226 13:02:52.160392 90983 net.cpp:228] Setting up relu3
I1226 13:02:52.160431 90983 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:02:52.160462 90983 net.cpp:243] Memory required for data: 234073856
I1226 13:02:52.160490 90983 layer_factory.hpp:114] Creating layer conv4
I1226 13:02:52.160573 90983 net.cpp:178] Creating Layer conv4
I1226 13:02:52.160606 90983 net.cpp:612] conv4 <- conv3
I1226 13:02:52.160645 90983 net.cpp:586] conv4 -> conv4
I1226 13:02:52.294420 91895 net.cpp:228] Setting up conv4
I1226 13:02:52.294528 91895 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:02:52.294574 91895 net.cpp:243] Memory required for data: 242380544
I1226 13:02:52.294657 91895 layer_factory.hpp:114] Creating layer relu4
I1226 13:02:52.294886 91895 net.cpp:178] Creating Layer relu4
I1226 13:02:52.294953 91895 net.cpp:612] relu4 <- conv4
I1226 13:02:52.295003 91895 net.cpp:573] relu4 -> conv4 (in-place)
I1226 13:02:52.295130 91895 net.cpp:228] Setting up relu4
I1226 13:02:52.295836 91895 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:02:52.295908 91895 net.cpp:243] Memory required for data: 250687232
I1226 13:02:52.296033 91895 layer_factory.hpp:114] Creating layer conv5
I1226 13:02:52.296170 91895 net.cpp:178] Creating Layer conv5
I1226 13:02:52.296217 91895 net.cpp:612] conv5 <- conv4
I1226 13:02:52.296283 91895 net.cpp:586] conv5 -> conv5
I1226 13:02:52.297096 92018 net.cpp:228] Setting up conv4
I1226 13:02:52.297222 92018 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:02:52.297261 92018 net.cpp:243] Memory required for data: 242380544
I1226 13:02:52.297333 92018 layer_factory.hpp:114] Creating layer relu4
I1226 13:02:52.297443 92018 net.cpp:178] Creating Layer relu4
I1226 13:02:52.297562 92018 net.cpp:612] relu4 <- conv4
I1226 13:02:52.297997 92018 net.cpp:573] relu4 -> conv4 (in-place)
I1226 13:02:52.298168 92018 net.cpp:228] Setting up relu4
I1226 13:02:52.298265 92018 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:02:52.298302 92018 net.cpp:243] Memory required for data: 250687232
I1226 13:02:52.298337 92018 layer_factory.hpp:114] Creating layer conv5
I1226 13:02:52.298466 92018 net.cpp:178] Creating Layer conv5
I1226 13:02:52.298573 92018 net.cpp:612] conv5 <- conv4
I1226 13:02:52.298633 92018 net.cpp:586] conv5 -> conv5
I1226 13:02:52.297502 85539 net.cpp:228] Setting up conv4
I1226 13:02:52.297618 85539 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:02:52.297652 85539 net.cpp:243] Memory required for data: 242380544
I1226 13:02:52.297797 85539 layer_factory.hpp:114] Creating layer relu4
I1226 13:02:52.298017 85539 net.cpp:178] Creating Layer relu4
I1226 13:02:52.298059 85539 net.cpp:612] relu4 <- conv4
I1226 13:02:52.298136 85539 net.cpp:573] relu4 -> conv4 (in-place)
I1226 13:02:52.298233 85539 net.cpp:228] Setting up relu4
I1226 13:02:52.298856 85539 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:02:52.298959 85539 net.cpp:243] Memory required for data: 250687232
I1226 13:02:52.299015 85539 layer_factory.hpp:114] Creating layer conv5
I1226 13:02:52.299146 85539 net.cpp:178] Creating Layer conv5
I1226 13:02:52.299273 85539 net.cpp:612] conv5 <- conv4
I1226 13:02:52.299330 85539 net.cpp:586] conv5 -> conv5
I1226 13:02:52.367040 93741 net.cpp:228] Setting up conv4
I1226 13:02:52.367182 93741 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:02:52.367225 93741 net.cpp:243] Memory required for data: 242380544
I1226 13:02:52.367918 93741 layer_factory.hpp:114] Creating layer relu4
I1226 13:02:52.368052 93741 net.cpp:178] Creating Layer relu4
I1226 13:02:52.368100 93741 net.cpp:612] relu4 <- conv4
I1226 13:02:52.368222 93741 net.cpp:573] relu4 -> conv4 (in-place)
I1226 13:02:52.368330 93741 net.cpp:228] Setting up relu4
I1226 13:02:52.368413 93741 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:02:52.368449 93741 net.cpp:243] Memory required for data: 250687232
I1226 13:02:52.368486 93741 layer_factory.hpp:114] Creating layer conv5
I1226 13:02:52.368587 93741 net.cpp:178] Creating Layer conv5
I1226 13:02:52.368641 93741 net.cpp:612] conv5 <- conv4
I1226 13:02:52.368739 93741 net.cpp:586] conv5 -> conv5
I1226 13:02:52.460850 90983 net.cpp:228] Setting up conv4
I1226 13:02:52.460978 90983 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:02:52.461007 90983 net.cpp:243] Memory required for data: 242380544
I1226 13:02:52.461064 90983 layer_factory.hpp:114] Creating layer relu4
I1226 13:02:52.461241 90983 net.cpp:178] Creating Layer relu4
I1226 13:02:52.461345 90983 net.cpp:612] relu4 <- conv4
I1226 13:02:52.461387 90983 net.cpp:573] relu4 -> conv4 (in-place)
I1226 13:02:52.461504 90983 net.cpp:228] Setting up relu4
I1226 13:02:52.461549 90983 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:02:52.461591 90983 net.cpp:243] Memory required for data: 250687232
I1226 13:02:52.461707 90983 layer_factory.hpp:114] Creating layer conv5
I1226 13:02:52.461804 90983 net.cpp:178] Creating Layer conv5
I1226 13:02:52.461865 90983 net.cpp:612] conv5 <- conv4
I1226 13:02:52.461941 90983 net.cpp:586] conv5 -> conv5
I1226 13:02:52.485327 92018 net.cpp:228] Setting up conv5
I1226 13:02:52.485437 92018 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 13:02:52.485466 92018 net.cpp:243] Memory required for data: 256225024
I1226 13:02:52.485565 92018 layer_factory.hpp:114] Creating layer relu5
I1226 13:02:52.485635 92018 net.cpp:178] Creating Layer relu5
I1226 13:02:52.485664 92018 net.cpp:612] relu5 <- conv5
I1226 13:02:52.485720 92018 net.cpp:573] relu5 -> conv5 (in-place)
I1226 13:02:52.485839 92018 net.cpp:228] Setting up relu5
I1226 13:02:52.485885 92018 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 13:02:52.485910 92018 net.cpp:243] Memory required for data: 261762816
I1226 13:02:52.485950 92018 layer_factory.hpp:114] Creating layer pool5
I1226 13:02:52.486028 92018 net.cpp:178] Creating Layer pool5
I1226 13:02:52.486066 92018 net.cpp:612] pool5 <- conv5
I1226 13:02:52.486114 92018 net.cpp:586] pool5 -> pool5
I1226 13:02:52.486212 92018 net.cpp:228] Setting up pool5
I1226 13:02:52.486253 92018 net.cpp:235] Top shape: 32 256 6 6 (294912)
I1226 13:02:52.486285 92018 net.cpp:243] Memory required for data: 262942464
I1226 13:02:52.486320 92018 layer_factory.hpp:114] Creating layer fc6
I1226 13:02:52.486379 92018 net.cpp:178] Creating Layer fc6
I1226 13:02:52.486413 92018 net.cpp:612] fc6 <- pool5
I1226 13:02:52.486470 92018 net.cpp:586] fc6 -> fc6
I1226 13:02:52.477641 91895 net.cpp:228] Setting up conv5
I1226 13:02:52.493782 91895 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 13:02:52.493839 91895 net.cpp:243] Memory required for data: 256225024
I1226 13:02:52.493988 91895 layer_factory.hpp:114] Creating layer relu5
I1226 13:02:52.494079 91895 net.cpp:178] Creating Layer relu5
I1226 13:02:52.494148 91895 net.cpp:612] relu5 <- conv5
I1226 13:02:52.494190 91895 net.cpp:573] relu5 -> conv5 (in-place)
I1226 13:02:52.494403 91895 net.cpp:228] Setting up relu5
I1226 13:02:52.494458 91895 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 13:02:52.494482 91895 net.cpp:243] Memory required for data: 261762816
I1226 13:02:52.494513 91895 layer_factory.hpp:114] Creating layer pool5
I1226 13:02:52.494575 91895 net.cpp:178] Creating Layer pool5
I1226 13:02:52.494611 91895 net.cpp:612] pool5 <- conv5
I1226 13:02:52.494649 91895 net.cpp:586] pool5 -> pool5
I1226 13:02:52.494745 91895 net.cpp:228] Setting up pool5
I1226 13:02:52.494786 91895 net.cpp:235] Top shape: 32 256 6 6 (294912)
I1226 13:02:52.494808 91895 net.cpp:243] Memory required for data: 262942464
I1226 13:02:52.494840 91895 layer_factory.hpp:114] Creating layer fc6
I1226 13:02:52.494910 91895 net.cpp:178] Creating Layer fc6
I1226 13:02:52.494940 91895 net.cpp:612] fc6 <- pool5
I1226 13:02:52.494982 91895 net.cpp:586] fc6 -> fc6
I1226 13:02:52.499769 85539 net.cpp:228] Setting up conv5
I1226 13:02:52.499886 85539 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 13:02:52.499918 85539 net.cpp:243] Memory required for data: 256225024
I1226 13:02:52.500016 85539 layer_factory.hpp:114] Creating layer relu5
I1226 13:02:52.500092 85539 net.cpp:178] Creating Layer relu5
I1226 13:02:52.500128 85539 net.cpp:612] relu5 <- conv5
I1226 13:02:52.500195 85539 net.cpp:573] relu5 -> conv5 (in-place)
I1226 13:02:52.500340 85539 net.cpp:228] Setting up relu5
I1226 13:02:52.500566 85539 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 13:02:52.500818 85539 net.cpp:243] Memory required for data: 261762816
I1226 13:02:52.500865 85539 layer_factory.hpp:114] Creating layer pool5
I1226 13:02:52.500967 85539 net.cpp:178] Creating Layer pool5
I1226 13:02:52.501394 85539 net.cpp:612] pool5 <- conv5
I1226 13:02:52.501472 85539 net.cpp:586] pool5 -> pool5
I1226 13:02:52.501713 85539 net.cpp:228] Setting up pool5
I1226 13:02:52.501823 85539 net.cpp:235] Top shape: 32 256 6 6 (294912)
I1226 13:02:52.501859 85539 net.cpp:243] Memory required for data: 262942464
I1226 13:02:52.501904 85539 layer_factory.hpp:114] Creating layer fc6
I1226 13:02:52.501988 85539 net.cpp:178] Creating Layer fc6
I1226 13:02:52.502038 85539 net.cpp:612] fc6 <- pool5
I1226 13:02:52.502112 85539 net.cpp:586] fc6 -> fc6
I1226 13:02:52.493726 122158 net.cpp:228] Setting up conv3
I1226 13:02:52.493923 122158 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:02:52.493971 122158 net.cpp:243] Memory required for data: 225767168
I1226 13:02:52.494112 122158 layer_factory.hpp:114] Creating layer relu3
I1226 13:02:52.494206 122158 net.cpp:178] Creating Layer relu3
I1226 13:02:52.494251 122158 net.cpp:612] relu3 <- conv3
I1226 13:02:52.494309 122158 net.cpp:573] relu3 -> conv3 (in-place)
I1226 13:02:52.494460 122158 net.cpp:228] Setting up relu3
I1226 13:02:52.494531 122158 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:02:52.494567 122158 net.cpp:243] Memory required for data: 234073856
I1226 13:02:52.494611 122158 layer_factory.hpp:114] Creating layer conv4
I1226 13:02:52.495142 122158 net.cpp:178] Creating Layer conv4
I1226 13:02:52.495208 122158 net.cpp:612] conv4 <- conv3
I1226 13:02:52.495293 122158 net.cpp:586] conv4 -> conv4
I1226 13:02:52.548943 93741 net.cpp:228] Setting up conv5
I1226 13:02:52.549096 93741 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 13:02:52.549134 93741 net.cpp:243] Memory required for data: 256225024
I1226 13:02:52.549541 93741 layer_factory.hpp:114] Creating layer relu5
I1226 13:02:52.549772 93741 net.cpp:178] Creating Layer relu5
I1226 13:02:52.549816 93741 net.cpp:612] relu5 <- conv5
I1226 13:02:52.562412 93741 net.cpp:573] relu5 -> conv5 (in-place)
I1226 13:02:52.562618 93741 net.cpp:228] Setting up relu5
I1226 13:02:52.562686 93741 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 13:02:52.562714 93741 net.cpp:243] Memory required for data: 261762816
I1226 13:02:52.562752 93741 layer_factory.hpp:114] Creating layer pool5
I1226 13:02:52.562832 93741 net.cpp:178] Creating Layer pool5
I1226 13:02:52.562870 93741 net.cpp:612] pool5 <- conv5
I1226 13:02:52.562916 93741 net.cpp:586] pool5 -> pool5
I1226 13:02:52.563022 93741 net.cpp:228] Setting up pool5
I1226 13:02:52.563073 93741 net.cpp:235] Top shape: 32 256 6 6 (294912)
I1226 13:02:52.563100 93741 net.cpp:243] Memory required for data: 262942464
I1226 13:02:52.563130 93741 layer_factory.hpp:114] Creating layer fc6
I1226 13:02:52.563194 93741 net.cpp:178] Creating Layer fc6
I1226 13:02:52.563222 93741 net.cpp:612] fc6 <- pool5
I1226 13:02:52.563282 93741 net.cpp:586] fc6 -> fc6
I1226 13:02:52.657196 90983 net.cpp:228] Setting up conv5
I1226 13:02:52.657337 90983 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 13:02:52.657367 90983 net.cpp:243] Memory required for data: 256225024
I1226 13:02:52.657445 90983 layer_factory.hpp:114] Creating layer relu5
I1226 13:02:52.657594 90983 net.cpp:178] Creating Layer relu5
I1226 13:02:52.657640 90983 net.cpp:612] relu5 <- conv5
I1226 13:02:52.657680 90983 net.cpp:573] relu5 -> conv5 (in-place)
I1226 13:02:52.657771 90983 net.cpp:228] Setting up relu5
I1226 13:02:52.657819 90983 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 13:02:52.657843 90983 net.cpp:243] Memory required for data: 261762816
I1226 13:02:52.657872 90983 layer_factory.hpp:114] Creating layer pool5
I1226 13:02:52.657949 90983 net.cpp:178] Creating Layer pool5
I1226 13:02:52.657980 90983 net.cpp:612] pool5 <- conv5
I1226 13:02:52.658017 90983 net.cpp:586] pool5 -> pool5
I1226 13:02:52.658120 90983 net.cpp:228] Setting up pool5
I1226 13:02:52.658169 90983 net.cpp:235] Top shape: 32 256 6 6 (294912)
I1226 13:02:52.658190 90983 net.cpp:243] Memory required for data: 262942464
I1226 13:02:52.658246 90983 layer_factory.hpp:114] Creating layer fc6
I1226 13:02:52.658309 90983 net.cpp:178] Creating Layer fc6
I1226 13:02:52.658349 90983 net.cpp:612] fc6 <- pool5
I1226 13:02:52.658753 90983 net.cpp:586] fc6 -> fc6
I1226 13:02:52.762550 96103 net.cpp:228] Setting up conv2
I1226 13:02:52.762667 96103 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 13:02:52.762696 96103 net.cpp:243] Memory required for data: 164146944
I1226 13:02:52.762773 96103 layer_factory.hpp:114] Creating layer relu2
I1226 13:02:52.762887 96103 net.cpp:178] Creating Layer relu2
I1226 13:02:52.762931 96103 net.cpp:612] relu2 <- conv2
I1226 13:02:52.762975 96103 net.cpp:573] relu2 -> conv2 (in-place)
I1226 13:02:52.763085 96103 net.cpp:228] Setting up relu2
I1226 13:02:52.763221 96103 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 13:02:52.763255 96103 net.cpp:243] Memory required for data: 188034816
I1226 13:02:52.763288 96103 layer_factory.hpp:114] Creating layer norm2
I1226 13:02:52.763344 96103 net.cpp:178] Creating Layer norm2
I1226 13:02:52.763373 96103 net.cpp:612] norm2 <- conv2
I1226 13:02:52.763411 96103 net.cpp:586] norm2 -> norm2
I1226 13:02:52.763514 96103 net.cpp:228] Setting up norm2
I1226 13:02:52.763564 96103 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 13:02:52.763591 96103 net.cpp:243] Memory required for data: 211922688
I1226 13:02:52.763619 96103 layer_factory.hpp:114] Creating layer pool2
I1226 13:02:52.763692 96103 net.cpp:178] Creating Layer pool2
I1226 13:02:52.763730 96103 net.cpp:612] pool2 <- norm2
I1226 13:02:52.763767 96103 net.cpp:586] pool2 -> pool2
I1226 13:02:52.772384 96103 net.cpp:228] Setting up pool2
I1226 13:02:52.772492 96103 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 13:02:52.772630 96103 net.cpp:243] Memory required for data: 217460480
I1226 13:02:52.772703 96103 layer_factory.hpp:114] Creating layer conv3
I1226 13:02:52.772902 96103 net.cpp:178] Creating Layer conv3
I1226 13:02:52.772948 96103 net.cpp:612] conv3 <- pool2
I1226 13:02:52.773108 96103 net.cpp:586] conv3 -> conv3
I1226 13:02:52.840282 94300 net.cpp:228] Setting up conv2
I1226 13:02:52.840436 94300 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 13:02:52.840471 94300 net.cpp:243] Memory required for data: 164146944
I1226 13:02:52.840728 94300 layer_factory.hpp:114] Creating layer relu2
I1226 13:02:52.840849 94300 net.cpp:178] Creating Layer relu2
I1226 13:02:52.840922 94300 net.cpp:612] relu2 <- conv2
I1226 13:02:52.840971 94300 net.cpp:573] relu2 -> conv2 (in-place)
I1226 13:02:52.841493 94300 net.cpp:228] Setting up relu2
I1226 13:02:52.841612 94300 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 13:02:52.841645 94300 net.cpp:243] Memory required for data: 188034816
I1226 13:02:52.841684 94300 layer_factory.hpp:114] Creating layer norm2
I1226 13:02:52.841756 94300 net.cpp:178] Creating Layer norm2
I1226 13:02:52.841814 94300 net.cpp:612] norm2 <- conv2
I1226 13:02:52.841910 94300 net.cpp:586] norm2 -> norm2
I1226 13:02:52.842173 94300 net.cpp:228] Setting up norm2
I1226 13:02:52.842404 94300 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 13:02:52.842443 94300 net.cpp:243] Memory required for data: 211922688
I1226 13:02:52.842485 94300 layer_factory.hpp:114] Creating layer pool2
I1226 13:02:52.842933 94300 net.cpp:178] Creating Layer pool2
I1226 13:02:52.843086 94300 net.cpp:612] pool2 <- norm2
I1226 13:02:52.843194 94300 net.cpp:586] pool2 -> pool2
I1226 13:02:52.843339 94300 net.cpp:228] Setting up pool2
I1226 13:02:52.843418 94300 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 13:02:52.843569 94300 net.cpp:243] Memory required for data: 217460480
I1226 13:02:52.843638 94300 layer_factory.hpp:114] Creating layer conv3
I1226 13:02:52.843780 94300 net.cpp:178] Creating Layer conv3
I1226 13:02:52.843855 94300 net.cpp:612] conv3 <- pool2
I1226 13:02:52.843950 94300 net.cpp:586] conv3 -> conv3
I1226 13:02:52.929234 122158 net.cpp:228] Setting up conv4
I1226 13:02:52.929751 122158 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:02:52.929852 122158 net.cpp:243] Memory required for data: 242380544
I1226 13:02:52.929994 122158 layer_factory.hpp:114] Creating layer relu4
I1226 13:02:52.930122 122158 net.cpp:178] Creating Layer relu4
I1226 13:02:52.930331 122158 net.cpp:612] relu4 <- conv4
I1226 13:02:52.930394 122158 net.cpp:573] relu4 -> conv4 (in-place)
I1226 13:02:52.930632 122158 net.cpp:228] Setting up relu4
I1226 13:02:52.930845 122158 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:02:52.930891 122158 net.cpp:243] Memory required for data: 250687232
I1226 13:02:52.930935 122158 layer_factory.hpp:114] Creating layer conv5
I1226 13:02:52.931058 122158 net.cpp:178] Creating Layer conv5
I1226 13:02:52.931169 122158 net.cpp:612] conv5 <- conv4
I1226 13:02:52.931257 122158 net.cpp:586] conv5 -> conv5
I1226 13:02:53.209740 122158 net.cpp:228] Setting up conv5
I1226 13:02:53.209929 122158 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 13:02:53.209982 122158 net.cpp:243] Memory required for data: 256225024
I1226 13:02:53.210153 122158 layer_factory.hpp:114] Creating layer relu5
I1226 13:02:53.210347 122158 net.cpp:178] Creating Layer relu5
I1226 13:02:53.210400 122158 net.cpp:612] relu5 <- conv5
I1226 13:02:53.210549 122158 net.cpp:573] relu5 -> conv5 (in-place)
I1226 13:02:53.211364 122158 net.cpp:228] Setting up relu5
I1226 13:02:53.211568 122158 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 13:02:53.211694 122158 net.cpp:243] Memory required for data: 261762816
I1226 13:02:53.211760 122158 layer_factory.hpp:114] Creating layer pool5
I1226 13:02:53.211932 122158 net.cpp:178] Creating Layer pool5
I1226 13:02:53.212256 122158 net.cpp:612] pool5 <- conv5
I1226 13:02:53.212375 122158 net.cpp:586] pool5 -> pool5
I1226 13:02:53.212687 122158 net.cpp:228] Setting up pool5
I1226 13:02:53.213394 122158 net.cpp:235] Top shape: 32 256 6 6 (294912)
I1226 13:02:53.213521 122158 net.cpp:243] Memory required for data: 262942464
I1226 13:02:53.213690 122158 layer_factory.hpp:114] Creating layer fc6
I1226 13:02:53.213933 122158 net.cpp:178] Creating Layer fc6
I1226 13:02:53.214035 122158 net.cpp:612] fc6 <- pool5
I1226 13:02:53.214118 122158 net.cpp:586] fc6 -> fc6
I1226 13:02:53.728888 96103 net.cpp:228] Setting up conv3
I1226 13:02:53.729017 96103 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:02:53.729050 96103 net.cpp:243] Memory required for data: 225767168
I1226 13:02:53.729131 96103 layer_factory.hpp:114] Creating layer relu3
I1226 13:02:53.729195 96103 net.cpp:178] Creating Layer relu3
I1226 13:02:53.729243 96103 net.cpp:612] relu3 <- conv3
I1226 13:02:53.729288 96103 net.cpp:573] relu3 -> conv3 (in-place)
I1226 13:02:53.729404 96103 net.cpp:228] Setting up relu3
I1226 13:02:53.729470 96103 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:02:53.729499 96103 net.cpp:243] Memory required for data: 234073856
I1226 13:02:53.729547 96103 layer_factory.hpp:114] Creating layer conv4
I1226 13:02:53.729631 96103 net.cpp:178] Creating Layer conv4
I1226 13:02:53.729672 96103 net.cpp:612] conv4 <- conv3
I1226 13:02:53.729729 96103 net.cpp:586] conv4 -> conv4
I1226 13:02:53.738638 94300 net.cpp:228] Setting up conv3
I1226 13:02:53.738760 94300 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:02:53.738801 94300 net.cpp:243] Memory required for data: 225767168
I1226 13:02:53.738962 94300 layer_factory.hpp:114] Creating layer relu3
I1226 13:02:53.739050 94300 net.cpp:178] Creating Layer relu3
I1226 13:02:53.739116 94300 net.cpp:612] relu3 <- conv3
I1226 13:02:53.739176 94300 net.cpp:573] relu3 -> conv3 (in-place)
I1226 13:02:53.739313 94300 net.cpp:228] Setting up relu3
I1226 13:02:53.739409 94300 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:02:53.739454 94300 net.cpp:243] Memory required for data: 234073856
I1226 13:02:53.739502 94300 layer_factory.hpp:114] Creating layer conv4
I1226 13:02:53.739637 94300 net.cpp:178] Creating Layer conv4
I1226 13:02:53.739697 94300 net.cpp:612] conv4 <- conv3
I1226 13:02:53.739769 94300 net.cpp:586] conv4 -> conv4
I1226 13:02:54.452769 96103 net.cpp:228] Setting up conv4
I1226 13:02:54.452911 96103 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:02:54.452946 96103 net.cpp:243] Memory required for data: 242380544
I1226 13:02:54.453012 96103 layer_factory.hpp:114] Creating layer relu4
I1226 13:02:54.453193 96103 net.cpp:178] Creating Layer relu4
I1226 13:02:54.453236 96103 net.cpp:612] relu4 <- conv4
I1226 13:02:54.453280 96103 net.cpp:573] relu4 -> conv4 (in-place)
I1226 13:02:54.453392 96103 net.cpp:228] Setting up relu4
I1226 13:02:54.453449 96103 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:02:54.453477 96103 net.cpp:243] Memory required for data: 250687232
I1226 13:02:54.453510 96103 layer_factory.hpp:114] Creating layer conv5
I1226 13:02:54.453608 96103 net.cpp:178] Creating Layer conv5
I1226 13:02:54.453652 96103 net.cpp:612] conv5 <- conv4
I1226 13:02:54.453702 96103 net.cpp:586] conv5 -> conv5
I1226 13:02:54.463922 94300 net.cpp:228] Setting up conv4
I1226 13:02:54.464052 94300 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:02:54.464097 94300 net.cpp:243] Memory required for data: 242380544
I1226 13:02:54.464186 94300 layer_factory.hpp:114] Creating layer relu4
I1226 13:02:54.464273 94300 net.cpp:178] Creating Layer relu4
I1226 13:02:54.464324 94300 net.cpp:612] relu4 <- conv4
I1226 13:02:54.464381 94300 net.cpp:573] relu4 -> conv4 (in-place)
I1226 13:02:54.464473 94300 net.cpp:228] Setting up relu4
I1226 13:02:54.464527 94300 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 13:02:54.464563 94300 net.cpp:243] Memory required for data: 250687232
I1226 13:02:54.464602 94300 layer_factory.hpp:114] Creating layer conv5
I1226 13:02:54.464692 94300 net.cpp:178] Creating Layer conv5
I1226 13:02:54.464741 94300 net.cpp:612] conv5 <- conv4
I1226 13:02:54.464800 94300 net.cpp:586] conv5 -> conv5
I1226 13:02:54.954911 96103 net.cpp:228] Setting up conv5
I1226 13:02:54.955029 96103 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 13:02:54.955061 96103 net.cpp:243] Memory required for data: 256225024
I1226 13:02:54.955142 96103 layer_factory.hpp:114] Creating layer relu5
I1226 13:02:54.955225 96103 net.cpp:178] Creating Layer relu5
I1226 13:02:54.955277 96103 net.cpp:612] relu5 <- conv5
I1226 13:02:54.955333 96103 net.cpp:573] relu5 -> conv5 (in-place)
I1226 13:02:54.955438 96103 net.cpp:228] Setting up relu5
I1226 13:02:54.955504 96103 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 13:02:54.955541 96103 net.cpp:243] Memory required for data: 261762816
I1226 13:02:54.955587 96103 layer_factory.hpp:114] Creating layer pool5
I1226 13:02:54.955657 96103 net.cpp:178] Creating Layer pool5
I1226 13:02:54.955698 96103 net.cpp:612] pool5 <- conv5
I1226 13:02:54.955742 96103 net.cpp:586] pool5 -> pool5
I1226 13:02:54.955868 96103 net.cpp:228] Setting up pool5
I1226 13:02:54.955929 96103 net.cpp:235] Top shape: 32 256 6 6 (294912)
I1226 13:02:54.955955 96103 net.cpp:243] Memory required for data: 262942464
I1226 13:02:54.955998 96103 layer_factory.hpp:114] Creating layer fc6
I1226 13:02:54.963601 94300 net.cpp:228] Setting up conv5
I1226 13:02:54.956075 96103 net.cpp:178] Creating Layer fc6
I1226 13:02:54.963729 94300 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 13:02:54.963770 94300 net.cpp:243] Memory required for data: 256225024
I1226 13:02:54.956113 96103 net.cpp:612] fc6 <- pool5
I1226 13:02:54.963907 94300 layer_factory.hpp:114] Creating layer relu5
I1226 13:02:54.956156 96103 net.cpp:586] fc6 -> fc6
I1226 13:02:54.964022 94300 net.cpp:178] Creating Layer relu5
I1226 13:02:54.964094 94300 net.cpp:612] relu5 <- conv5
I1226 13:02:54.964160 94300 net.cpp:573] relu5 -> conv5 (in-place)
I1226 13:02:54.964299 94300 net.cpp:228] Setting up relu5
I1226 13:02:54.964458 94300 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 13:02:54.964504 94300 net.cpp:243] Memory required for data: 261762816
I1226 13:02:54.964548 94300 layer_factory.hpp:114] Creating layer pool5
I1226 13:02:54.964625 94300 net.cpp:178] Creating Layer pool5
I1226 13:02:54.964689 94300 net.cpp:612] pool5 <- conv5
I1226 13:02:54.964779 94300 net.cpp:586] pool5 -> pool5
I1226 13:02:54.964968 94300 net.cpp:228] Setting up pool5
I1226 13:02:54.965067 94300 net.cpp:235] Top shape: 32 256 6 6 (294912)
I1226 13:02:54.965106 94300 net.cpp:243] Memory required for data: 262942464
I1226 13:02:54.965152 94300 layer_factory.hpp:114] Creating layer fc6
I1226 13:02:54.965275 94300 net.cpp:178] Creating Layer fc6
I1226 13:02:54.965327 94300 net.cpp:612] fc6 <- pool5
I1226 13:02:54.965386 94300 net.cpp:586] fc6 -> fc6
I1226 13:02:57.100455 154317 net.cpp:228] Setting up fc6
I1226 13:02:57.100569 154317 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:02:57.100605 154317 net.cpp:243] Memory required for data: 263466752
I1226 13:02:57.100669 154317 layer_factory.hpp:114] Creating layer relu6
I1226 13:02:57.100774 154317 net.cpp:178] Creating Layer relu6
I1226 13:02:57.100827 154317 net.cpp:612] relu6 <- fc6
I1226 13:02:57.100880 154317 net.cpp:573] relu6 -> fc6 (in-place)
I1226 13:02:57.101130 154317 net.cpp:228] Setting up relu6
I1226 13:02:57.101197 154317 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:02:57.101222 154317 net.cpp:243] Memory required for data: 263991040
I1226 13:02:57.101255 154317 layer_factory.hpp:114] Creating layer drop6
I1226 13:02:57.101331 154317 net.cpp:178] Creating Layer drop6
I1226 13:02:57.101373 154317 net.cpp:612] drop6 <- fc6
I1226 13:02:57.101414 154317 net.cpp:573] drop6 -> fc6 (in-place)
I1226 13:02:57.101475 154317 net.cpp:228] Setting up drop6
I1226 13:02:57.101519 154317 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:02:57.101550 154317 net.cpp:243] Memory required for data: 264515328
I1226 13:02:57.101579 154317 layer_factory.hpp:114] Creating layer fc7
I1226 13:02:57.101639 154317 net.cpp:178] Creating Layer fc7
I1226 13:02:57.101665 154317 net.cpp:612] fc7 <- fc6
I1226 13:02:57.101701 154317 net.cpp:586] fc7 -> fc7
I1226 13:02:57.770972 90983 net.cpp:228] Setting up fc6
I1226 13:02:57.771080 90983 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:02:57.771109 90983 net.cpp:243] Memory required for data: 263466752
I1226 13:02:57.771190 90983 layer_factory.hpp:114] Creating layer relu6
I1226 13:02:57.771287 90983 net.cpp:178] Creating Layer relu6
I1226 13:02:57.771323 90983 net.cpp:612] relu6 <- fc6
I1226 13:02:57.771395 90983 net.cpp:573] relu6 -> fc6 (in-place)
I1226 13:02:57.771493 90983 net.cpp:228] Setting up relu6
I1226 13:02:57.771633 90983 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:02:57.771663 90983 net.cpp:243] Memory required for data: 263991040
I1226 13:02:57.771695 90983 layer_factory.hpp:114] Creating layer drop6
I1226 13:02:57.771762 90983 net.cpp:178] Creating Layer drop6
I1226 13:02:57.771797 90983 net.cpp:612] drop6 <- fc6
I1226 13:02:57.771833 90983 net.cpp:573] drop6 -> fc6 (in-place)
I1226 13:02:57.771890 90983 net.cpp:228] Setting up drop6
I1226 13:02:57.771931 90983 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:02:57.771961 90983 net.cpp:243] Memory required for data: 264515328
I1226 13:02:57.771988 90983 layer_factory.hpp:114] Creating layer fc7
I1226 13:02:57.772061 90983 net.cpp:178] Creating Layer fc7
I1226 13:02:57.772101 90983 net.cpp:612] fc7 <- fc6
I1226 13:02:57.772146 90983 net.cpp:586] fc7 -> fc7
I1226 13:02:57.771998 92018 net.cpp:228] Setting up fc6
I1226 13:02:57.772111 92018 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:02:57.772141 92018 net.cpp:243] Memory required for data: 263466752
I1226 13:02:57.772228 92018 layer_factory.hpp:114] Creating layer relu6
I1226 13:02:57.772306 92018 net.cpp:178] Creating Layer relu6
I1226 13:02:57.772351 92018 net.cpp:612] relu6 <- fc6
I1226 13:02:57.772419 92018 net.cpp:573] relu6 -> fc6 (in-place)
I1226 13:02:57.772508 92018 net.cpp:228] Setting up relu6
I1226 13:02:57.772662 92018 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:02:57.772698 92018 net.cpp:243] Memory required for data: 263991040
I1226 13:02:57.772733 92018 layer_factory.hpp:114] Creating layer drop6
I1226 13:02:57.772819 92018 net.cpp:178] Creating Layer drop6
I1226 13:02:57.772855 92018 net.cpp:612] drop6 <- fc6
I1226 13:02:57.772894 92018 net.cpp:573] drop6 -> fc6 (in-place)
I1226 13:02:57.772971 92018 net.cpp:228] Setting up drop6
I1226 13:02:57.773010 92018 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:02:57.773036 92018 net.cpp:243] Memory required for data: 264515328
I1226 13:02:57.773071 92018 layer_factory.hpp:114] Creating layer fc7
I1226 13:02:57.773160 92018 net.cpp:178] Creating Layer fc7
I1226 13:02:57.773202 92018 net.cpp:612] fc7 <- fc6
I1226 13:02:57.773246 92018 net.cpp:586] fc7 -> fc7
I1226 13:02:57.770918 85539 net.cpp:228] Setting up fc6
I1226 13:02:57.771039 85539 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:02:57.771075 85539 net.cpp:243] Memory required for data: 263466752
I1226 13:02:57.771180 85539 layer_factory.hpp:114] Creating layer relu6
I1226 13:02:57.771270 85539 net.cpp:178] Creating Layer relu6
I1226 13:02:57.771328 85539 net.cpp:612] relu6 <- fc6
I1226 13:02:57.771404 85539 net.cpp:573] relu6 -> fc6 (in-place)
I1226 13:02:57.771513 85539 net.cpp:228] Setting up relu6
I1226 13:02:57.771706 85539 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:02:57.771750 85539 net.cpp:243] Memory required for data: 263991040
I1226 13:02:57.771791 85539 layer_factory.hpp:114] Creating layer drop6
I1226 13:02:57.771857 85539 net.cpp:178] Creating Layer drop6
I1226 13:02:57.771900 85539 net.cpp:612] drop6 <- fc6
I1226 13:02:57.771951 85539 net.cpp:573] drop6 -> fc6 (in-place)
I1226 13:02:57.772039 85539 net.cpp:228] Setting up drop6
I1226 13:02:57.772099 85539 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:02:57.772131 85539 net.cpp:243] Memory required for data: 264515328
I1226 13:02:57.772171 85539 layer_factory.hpp:114] Creating layer fc7
I1226 13:02:57.772267 85539 net.cpp:178] Creating Layer fc7
I1226 13:02:57.772315 85539 net.cpp:612] fc7 <- fc6
I1226 13:02:57.772367 85539 net.cpp:586] fc7 -> fc7
I1226 13:02:57.792250 91895 net.cpp:228] Setting up fc6
I1226 13:02:57.792387 91895 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:02:57.792418 91895 net.cpp:243] Memory required for data: 263466752
I1226 13:02:57.792482 91895 layer_factory.hpp:114] Creating layer relu6
I1226 13:02:57.792547 91895 net.cpp:178] Creating Layer relu6
I1226 13:02:57.792587 91895 net.cpp:612] relu6 <- fc6
I1226 13:02:57.792632 91895 net.cpp:573] relu6 -> fc6 (in-place)
I1226 13:02:57.792732 91895 net.cpp:228] Setting up relu6
I1226 13:02:57.792871 91895 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:02:57.792901 91895 net.cpp:243] Memory required for data: 263991040
I1226 13:02:57.792932 91895 layer_factory.hpp:114] Creating layer drop6
I1226 13:02:57.793005 91895 net.cpp:178] Creating Layer drop6
I1226 13:02:57.793035 91895 net.cpp:612] drop6 <- fc6
I1226 13:02:57.793086 91895 net.cpp:573] drop6 -> fc6 (in-place)
I1226 13:02:57.793149 91895 net.cpp:228] Setting up drop6
I1226 13:02:57.793191 91895 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:02:57.793215 91895 net.cpp:243] Memory required for data: 264515328
I1226 13:02:57.793242 91895 layer_factory.hpp:114] Creating layer fc7
I1226 13:02:57.793329 91895 net.cpp:178] Creating Layer fc7
I1226 13:02:57.793365 91895 net.cpp:612] fc7 <- fc6
I1226 13:02:57.793421 91895 net.cpp:586] fc7 -> fc7
I1226 13:02:57.942183 93741 net.cpp:228] Setting up fc6
I1226 13:02:57.942298 93741 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:02:57.942332 93741 net.cpp:243] Memory required for data: 263466752
I1226 13:02:57.942420 93741 layer_factory.hpp:114] Creating layer relu6
I1226 13:02:57.942524 93741 net.cpp:178] Creating Layer relu6
I1226 13:02:57.942575 93741 net.cpp:612] relu6 <- fc6
I1226 13:02:57.942625 93741 net.cpp:573] relu6 -> fc6 (in-place)
I1226 13:02:57.942724 93741 net.cpp:228] Setting up relu6
I1226 13:02:57.942878 93741 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:02:57.942914 93741 net.cpp:243] Memory required for data: 263991040
I1226 13:02:57.942947 93741 layer_factory.hpp:114] Creating layer drop6
I1226 13:02:57.943004 93741 net.cpp:178] Creating Layer drop6
I1226 13:02:57.943037 93741 net.cpp:612] drop6 <- fc6
I1226 13:02:57.943074 93741 net.cpp:573] drop6 -> fc6 (in-place)
I1226 13:02:57.943130 93741 net.cpp:228] Setting up drop6
I1226 13:02:57.943166 93741 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:02:57.943189 93741 net.cpp:243] Memory required for data: 264515328
I1226 13:02:57.943217 93741 layer_factory.hpp:114] Creating layer fc7
I1226 13:02:57.943294 93741 net.cpp:178] Creating Layer fc7
I1226 13:02:57.943323 93741 net.cpp:612] fc7 <- fc6
I1226 13:02:57.943361 93741 net.cpp:586] fc7 -> fc7
I1226 13:02:59.378806 154317 net.cpp:228] Setting up fc7
I1226 13:02:59.378917 154317 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:02:59.378957 154317 net.cpp:243] Memory required for data: 265039616
I1226 13:02:59.379053 154317 layer_factory.hpp:114] Creating layer relu7
I1226 13:02:59.379127 154317 net.cpp:178] Creating Layer relu7
I1226 13:02:59.379179 154317 net.cpp:612] relu7 <- fc7
I1226 13:02:59.379222 154317 net.cpp:573] relu7 -> fc7 (in-place)
I1226 13:02:59.379318 154317 net.cpp:228] Setting up relu7
I1226 13:02:59.379376 154317 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:02:59.379403 154317 net.cpp:243] Memory required for data: 265563904
I1226 13:02:59.379436 154317 layer_factory.hpp:114] Creating layer drop7
I1226 13:02:59.379487 154317 net.cpp:178] Creating Layer drop7
I1226 13:02:59.379515 154317 net.cpp:612] drop7 <- fc7
I1226 13:02:59.379552 154317 net.cpp:573] drop7 -> fc7 (in-place)
I1226 13:02:59.379597 154317 net.cpp:228] Setting up drop7
I1226 13:02:59.379639 154317 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:02:59.379662 154317 net.cpp:243] Memory required for data: 266088192
I1226 13:02:59.379689 154317 layer_factory.hpp:114] Creating layer fc8
I1226 13:02:59.379775 154317 net.cpp:178] Creating Layer fc8
I1226 13:02:59.379818 154317 net.cpp:612] fc8 <- fc7
I1226 13:02:59.379884 154317 net.cpp:586] fc8 -> fc8
I1226 13:02:59.938330 154317 net.cpp:228] Setting up fc8
I1226 13:02:59.938443 154317 net.cpp:235] Top shape: 32 1000 (32000)
I1226 13:02:59.938479 154317 net.cpp:243] Memory required for data: 266216192
I1226 13:02:59.938537 154317 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 13:02:59.938634 154317 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 13:02:59.938685 154317 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 13:02:59.938735 154317 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 13:02:59.938798 154317 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 13:02:59.938887 154317 net.cpp:228] Setting up fc8_fc8_0_split
I1226 13:02:59.938943 154317 net.cpp:235] Top shape: 32 1000 (32000)
I1226 13:02:59.938999 154317 net.cpp:235] Top shape: 32 1000 (32000)
I1226 13:02:59.939029 154317 net.cpp:243] Memory required for data: 266472192
I1226 13:02:59.939059 154317 layer_factory.hpp:114] Creating layer accuracy
I1226 13:02:59.939113 154317 net.cpp:178] Creating Layer accuracy
I1226 13:02:59.939152 154317 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 13:02:59.939198 154317 net.cpp:612] accuracy <- label_data_1_split_0
I1226 13:02:59.939239 154317 net.cpp:586] accuracy -> accuracy
I1226 13:02:59.939299 154317 net.cpp:228] Setting up accuracy
I1226 13:02:59.939342 154317 net.cpp:235] Top shape: (1)
I1226 13:02:59.939370 154317 net.cpp:243] Memory required for data: 266472196
I1226 13:02:59.939399 154317 layer_factory.hpp:114] Creating layer loss
I1226 13:02:59.939445 154317 net.cpp:178] Creating Layer loss
I1226 13:02:59.939479 154317 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 13:02:59.939513 154317 net.cpp:612] loss <- label_data_1_split_1
I1226 13:02:59.939551 154317 net.cpp:586] loss -> loss
I1226 13:02:59.939613 154317 layer_factory.hpp:114] Creating layer loss
I1226 13:02:59.968340 154317 net.cpp:228] Setting up loss
I1226 13:02:59.968442 154317 net.cpp:235] Top shape: (1)
I1226 13:02:59.968472 154317 net.cpp:238]     with loss weight 1
I1226 13:02:59.968592 154317 net.cpp:243] Memory required for data: 266472200
I1226 13:02:59.968636 154317 net.cpp:305] loss needs backward computation.
I1226 13:02:59.968680 154317 net.cpp:307] accuracy does not need backward computation.
I1226 13:02:59.968724 154317 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 13:02:59.968760 154317 net.cpp:305] fc8 needs backward computation.
I1226 13:02:59.968791 154317 net.cpp:305] drop7 needs backward computation.
I1226 13:02:59.968827 154317 net.cpp:305] relu7 needs backward computation.
I1226 13:02:59.968857 154317 net.cpp:305] fc7 needs backward computation.
I1226 13:02:59.968894 154317 net.cpp:305] drop6 needs backward computation.
I1226 13:02:59.968924 154317 net.cpp:305] relu6 needs backward computation.
I1226 13:02:59.968958 154317 net.cpp:305] fc6 needs backward computation.
I1226 13:02:59.969008 154317 net.cpp:305] pool5 needs backward computation.
I1226 13:02:59.969038 154317 net.cpp:305] relu5 needs backward computation.
I1226 13:02:59.969068 154317 net.cpp:305] conv5 needs backward computation.
I1226 13:02:59.969096 154317 net.cpp:305] relu4 needs backward computation.
I1226 13:02:59.969125 154317 net.cpp:305] conv4 needs backward computation.
I1226 13:02:59.969154 154317 net.cpp:305] relu3 needs backward computation.
I1226 13:02:59.969198 154317 net.cpp:305] conv3 needs backward computation.
I1226 13:02:59.969228 154317 net.cpp:305] pool2 needs backward computation.
I1226 13:02:59.969256 154317 net.cpp:305] norm2 needs backward computation.
I1226 13:02:59.969285 154317 net.cpp:305] relu2 needs backward computation.
I1226 13:02:59.969311 154317 net.cpp:305] conv2 needs backward computation.
I1226 13:02:59.969341 154317 net.cpp:305] pool1 needs backward computation.
I1226 13:02:59.969368 154317 net.cpp:305] norm1 needs backward computation.
I1226 13:02:59.969396 154317 net.cpp:305] relu1 needs backward computation.
I1226 13:02:59.969424 154317 net.cpp:305] conv1 needs backward computation.
I1226 13:02:59.969454 154317 net.cpp:307] label_data_1_split does not need backward computation.
I1226 13:02:59.969482 154317 net.cpp:307] data does not need backward computation.
I1226 13:02:59.969508 154317 net.cpp:349] This network produces output accuracy
I1226 13:02:59.969538 154317 net.cpp:349] This network produces output loss
I1226 13:02:59.969616 154317 net.cpp:363] Network initialization done.
I1226 13:02:59.970157 154317 solver.cpp:107] Solver scaffolding done.
I1226 13:02:59.970345 154317 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 13:03:00.034607 90983 net.cpp:228] Setting up fc7
I1226 13:03:00.034720 90983 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:03:00.034750 90983 net.cpp:243] Memory required for data: 265039616
I1226 13:03:00.034808 90983 layer_factory.hpp:114] Creating layer relu7
I1226 13:03:00.034916 90983 net.cpp:178] Creating Layer relu7
I1226 13:03:00.035019 90983 net.cpp:612] relu7 <- fc7
I1226 13:03:00.035063 90983 net.cpp:573] relu7 -> fc7 (in-place)
I1226 13:03:00.035146 90983 net.cpp:228] Setting up relu7
I1226 13:03:00.035188 90983 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:03:00.035240 90983 net.cpp:243] Memory required for data: 265563904
I1226 13:03:00.035276 90983 layer_factory.hpp:114] Creating layer drop7
I1226 13:03:00.035326 90983 net.cpp:178] Creating Layer drop7
I1226 13:03:00.035357 90983 net.cpp:612] drop7 <- fc7
I1226 13:03:00.035393 90983 net.cpp:573] drop7 -> fc7 (in-place)
I1226 13:03:00.035459 90983 net.cpp:228] Setting up drop7
I1226 13:03:00.035495 90983 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:03:00.035521 90983 net.cpp:243] Memory required for data: 266088192
I1226 13:03:00.035548 90983 layer_factory.hpp:114] Creating layer fc8
I1226 13:03:00.035706 90983 net.cpp:178] Creating Layer fc8
I1226 13:03:00.035740 90983 net.cpp:612] fc8 <- fc7
I1226 13:03:00.035780 90983 net.cpp:586] fc8 -> fc8
I1226 13:03:00.039782 85539 net.cpp:228] Setting up fc7
I1226 13:03:00.039894 85539 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:03:00.039928 85539 net.cpp:243] Memory required for data: 265039616
I1226 13:03:00.040033 85539 layer_factory.hpp:114] Creating layer relu7
I1226 13:03:00.040172 85539 net.cpp:178] Creating Layer relu7
I1226 13:03:00.040253 85539 net.cpp:612] relu7 <- fc7
I1226 13:03:00.040310 85539 net.cpp:573] relu7 -> fc7 (in-place)
I1226 13:03:00.040411 85539 net.cpp:228] Setting up relu7
I1226 13:03:00.040482 85539 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:03:00.040513 85539 net.cpp:243] Memory required for data: 265563904
I1226 13:03:00.040550 85539 layer_factory.hpp:114] Creating layer drop7
I1226 13:03:00.040619 85539 net.cpp:178] Creating Layer drop7
I1226 13:03:00.040680 85539 net.cpp:612] drop7 <- fc7
I1226 13:03:00.040732 85539 net.cpp:573] drop7 -> fc7 (in-place)
I1226 13:03:00.044098 92018 net.cpp:228] Setting up fc7
I1226 13:03:00.040792 85539 net.cpp:228] Setting up drop7
I1226 13:03:00.044212 92018 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:03:00.044242 92018 net.cpp:243] Memory required for data: 265039616
I1226 13:03:00.040843 85539 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:03:00.044327 92018 layer_factory.hpp:114] Creating layer relu7
I1226 13:03:00.040887 85539 net.cpp:243] Memory required for data: 266088192
I1226 13:03:00.040925 85539 layer_factory.hpp:114] Creating layer fc8
I1226 13:03:00.044425 92018 net.cpp:178] Creating Layer relu7
I1226 13:03:00.041002 85539 net.cpp:178] Creating Layer fc8
I1226 13:03:00.044471 92018 net.cpp:612] relu7 <- fc7
I1226 13:03:00.041048 85539 net.cpp:612] fc8 <- fc7
I1226 13:03:00.044513 92018 net.cpp:573] relu7 -> fc7 (in-place)
I1226 13:03:00.041100 85539 net.cpp:586] fc8 -> fc8
I1226 13:03:00.044613 92018 net.cpp:228] Setting up relu7
I1226 13:03:00.044667 92018 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:03:00.044695 92018 net.cpp:243] Memory required for data: 265563904
I1226 13:03:00.044865 92018 layer_factory.hpp:114] Creating layer drop7
I1226 13:03:00.044935 92018 net.cpp:178] Creating Layer drop7
I1226 13:03:00.044972 92018 net.cpp:612] drop7 <- fc7
I1226 13:03:00.045011 92018 net.cpp:573] drop7 -> fc7 (in-place)
I1226 13:03:00.045059 92018 net.cpp:228] Setting up drop7
I1226 13:03:00.045094 92018 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:03:00.045125 92018 net.cpp:243] Memory required for data: 266088192
I1226 13:03:00.045158 92018 layer_factory.hpp:114] Creating layer fc8
I1226 13:03:00.045215 92018 net.cpp:178] Creating Layer fc8
I1226 13:03:00.045250 92018 net.cpp:612] fc8 <- fc7
I1226 13:03:00.045290 92018 net.cpp:586] fc8 -> fc8
I1226 13:03:00.051939 91895 net.cpp:228] Setting up fc7
I1226 13:03:00.052055 91895 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:03:00.052085 91895 net.cpp:243] Memory required for data: 265039616
I1226 13:03:00.052141 91895 layer_factory.hpp:114] Creating layer relu7
I1226 13:03:00.052206 91895 net.cpp:178] Creating Layer relu7
I1226 13:03:00.052248 91895 net.cpp:612] relu7 <- fc7
I1226 13:03:00.052287 91895 net.cpp:573] relu7 -> fc7 (in-place)
I1226 13:03:00.052465 91895 net.cpp:228] Setting up relu7
I1226 13:03:00.052513 91895 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:03:00.052551 91895 net.cpp:243] Memory required for data: 265563904
I1226 13:03:00.052583 91895 layer_factory.hpp:114] Creating layer drop7
I1226 13:03:00.052628 91895 net.cpp:178] Creating Layer drop7
I1226 13:03:00.052654 91895 net.cpp:612] drop7 <- fc7
I1226 13:03:00.052690 91895 net.cpp:573] drop7 -> fc7 (in-place)
I1226 13:03:00.052743 91895 net.cpp:228] Setting up drop7
I1226 13:03:00.052784 91895 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:03:00.052808 91895 net.cpp:243] Memory required for data: 266088192
I1226 13:03:00.052834 91895 layer_factory.hpp:114] Creating layer fc8
I1226 13:03:00.052911 91895 net.cpp:178] Creating Layer fc8
I1226 13:03:00.052947 91895 net.cpp:612] fc8 <- fc7
I1226 13:03:00.052995 91895 net.cpp:586] fc8 -> fc8
I1226 13:03:00.245093 93741 net.cpp:228] Setting up fc7
I1226 13:03:00.245209 93741 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:03:00.245239 93741 net.cpp:243] Memory required for data: 265039616
I1226 13:03:00.245296 93741 layer_factory.hpp:114] Creating layer relu7
I1226 13:03:00.245391 93741 net.cpp:178] Creating Layer relu7
I1226 13:03:00.245445 93741 net.cpp:612] relu7 <- fc7
I1226 13:03:00.245493 93741 net.cpp:573] relu7 -> fc7 (in-place)
I1226 13:03:00.245574 93741 net.cpp:228] Setting up relu7
I1226 13:03:00.245621 93741 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:03:00.245648 93741 net.cpp:243] Memory required for data: 265563904
I1226 13:03:00.245678 93741 layer_factory.hpp:114] Creating layer drop7
I1226 13:03:00.245718 93741 net.cpp:178] Creating Layer drop7
I1226 13:03:00.245746 93741 net.cpp:612] drop7 <- fc7
I1226 13:03:00.245781 93741 net.cpp:573] drop7 -> fc7 (in-place)
I1226 13:03:00.245826 93741 net.cpp:228] Setting up drop7
I1226 13:03:00.245859 93741 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:03:00.245882 93741 net.cpp:243] Memory required for data: 266088192
I1226 13:03:00.245909 93741 layer_factory.hpp:114] Creating layer fc8
I1226 13:03:00.245965 93741 net.cpp:178] Creating Layer fc8
I1226 13:03:00.245991 93741 net.cpp:612] fc8 <- fc7
I1226 13:03:00.246026 93741 net.cpp:586] fc8 -> fc8
I1226 13:03:00.585903 90983 net.cpp:228] Setting up fc8
I1226 13:03:00.586025 90983 net.cpp:235] Top shape: 32 1000 (32000)
I1226 13:03:00.586053 90983 net.cpp:243] Memory required for data: 266216192
I1226 13:03:00.586133 90983 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 13:03:00.586231 90983 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 13:03:00.586267 90983 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 13:03:00.586307 90983 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 13:03:00.586360 90983 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 13:03:00.586454 90983 net.cpp:228] Setting up fc8_fc8_0_split
I1226 13:03:00.586500 90983 net.cpp:235] Top shape: 32 1000 (32000)
I1226 13:03:00.586530 90983 net.cpp:235] Top shape: 32 1000 (32000)
I1226 13:03:00.586554 90983 net.cpp:243] Memory required for data: 266472192
I1226 13:03:00.586590 90983 layer_factory.hpp:114] Creating layer accuracy
I1226 13:03:00.586639 90983 net.cpp:178] Creating Layer accuracy
I1226 13:03:00.586670 90983 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 13:03:00.586699 90983 net.cpp:612] accuracy <- label_data_1_split_0
I1226 13:03:00.586731 90983 net.cpp:586] accuracy -> accuracy
I1226 13:03:00.586773 90983 net.cpp:228] Setting up accuracy
I1226 13:03:00.586812 90983 net.cpp:235] Top shape: (1)
I1226 13:03:00.586844 90983 net.cpp:243] Memory required for data: 266472196
I1226 13:03:00.586871 90983 layer_factory.hpp:114] Creating layer loss
I1226 13:03:00.586911 90983 net.cpp:178] Creating Layer loss
I1226 13:03:00.586943 90983 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 13:03:00.586984 90983 net.cpp:612] loss <- label_data_1_split_1
I1226 13:03:00.587018 90983 net.cpp:586] loss -> loss
I1226 13:03:00.587074 90983 layer_factory.hpp:114] Creating layer loss
I1226 13:03:00.591095 85539 net.cpp:228] Setting up fc8
I1226 13:03:00.591239 85539 net.cpp:235] Top shape: 32 1000 (32000)
I1226 13:03:00.591308 85539 net.cpp:243] Memory required for data: 266216192
I1226 13:03:00.591383 85539 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 13:03:00.591466 85539 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 13:03:00.591516 85539 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 13:03:00.591562 85539 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 13:03:00.591639 85539 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 13:03:00.591792 85539 net.cpp:228] Setting up fc8_fc8_0_split
I1226 13:03:00.591867 85539 net.cpp:235] Top shape: 32 1000 (32000)
I1226 13:03:00.591909 85539 net.cpp:235] Top shape: 32 1000 (32000)
I1226 13:03:00.591943 85539 net.cpp:243] Memory required for data: 266472192
I1226 13:03:00.591984 85539 layer_factory.hpp:114] Creating layer accuracy
I1226 13:03:00.592063 85539 net.cpp:178] Creating Layer accuracy
I1226 13:03:00.592113 85539 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 13:03:00.592165 85539 net.cpp:612] accuracy <- label_data_1_split_0
I1226 13:03:00.592211 85539 net.cpp:586] accuracy -> accuracy
I1226 13:03:00.592274 85539 net.cpp:228] Setting up accuracy
I1226 13:03:00.592327 85539 net.cpp:235] Top shape: (1)
I1226 13:03:00.592371 85539 net.cpp:243] Memory required for data: 266472196
I1226 13:03:00.592409 85539 layer_factory.hpp:114] Creating layer loss
I1226 13:03:00.592473 85539 net.cpp:178] Creating Layer loss
I1226 13:03:00.592519 85539 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 13:03:00.592579 85539 net.cpp:612] loss <- label_data_1_split_1
I1226 13:03:00.592638 85539 net.cpp:586] loss -> loss
I1226 13:03:00.592754 85539 layer_factory.hpp:114] Creating layer loss
I1226 13:03:00.613482 90983 net.cpp:228] Setting up loss
I1226 13:03:00.613596 90983 net.cpp:235] Top shape: (1)
I1226 13:03:00.613761 90983 net.cpp:238]     with loss weight 1
I1226 13:03:00.613919 90983 net.cpp:243] Memory required for data: 266472200
I1226 13:03:00.613970 90983 net.cpp:305] loss needs backward computation.
I1226 13:03:00.614011 90983 net.cpp:307] accuracy does not need backward computation.
I1226 13:03:00.614047 90983 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 13:03:00.614078 90983 net.cpp:305] fc8 needs backward computation.
I1226 13:03:00.614109 90983 net.cpp:305] drop7 needs backward computation.
I1226 13:03:00.614141 90983 net.cpp:305] relu7 needs backward computation.
I1226 13:03:00.614169 90983 net.cpp:305] fc7 needs backward computation.
I1226 13:03:00.614248 90983 net.cpp:305] drop6 needs backward computation.
I1226 13:03:00.614282 90983 net.cpp:305] relu6 needs backward computation.
I1226 13:03:00.614312 90983 net.cpp:305] fc6 needs backward computation.
I1226 13:03:00.614351 90983 net.cpp:305] pool5 needs backward computation.
I1226 13:03:00.614383 90983 net.cpp:305] relu5 needs backward computation.
I1226 13:03:00.614418 90983 net.cpp:305] conv5 needs backward computation.
I1226 13:03:00.614449 90983 net.cpp:305] relu4 needs backward computation.
I1226 13:03:00.614488 90983 net.cpp:305] conv4 needs backward computation.
I1226 13:03:00.614521 90983 net.cpp:305] relu3 needs backward computation.
I1226 13:03:00.614557 90983 net.cpp:305] conv3 needs backward computation.
I1226 13:03:00.614589 90983 net.cpp:305] pool2 needs backward computation.
I1226 13:03:00.614626 90983 net.cpp:305] norm2 needs backward computation.
I1226 13:03:00.614662 90983 net.cpp:305] relu2 needs backward computation.
I1226 13:03:00.614701 90983 net.cpp:305] conv2 needs backward computation.
I1226 13:03:00.614740 90983 net.cpp:305] pool1 needs backward computation.
I1226 13:03:00.614775 90983 net.cpp:305] norm1 needs backward computation.
I1226 13:03:00.614810 90983 net.cpp:305] relu1 needs backward computation.
I1226 13:03:00.614840 90983 net.cpp:305] conv1 needs backward computation.
I1226 13:03:00.614894 90983 net.cpp:307] label_data_1_split does not need backward computation.
I1226 13:03:00.614931 90983 net.cpp:307] data does not need backward computation.
I1226 13:03:00.614959 90983 net.cpp:349] This network produces output accuracy
I1226 13:03:00.615002 90983 net.cpp:349] This network produces output loss
I1226 13:03:00.615092 90983 net.cpp:363] Network initialization done.
I1226 13:03:00.615559 90983 solver.cpp:107] Solver scaffolding done.
I1226 13:03:00.615782 90983 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 13:03:00.608491 92018 net.cpp:228] Setting up fc8
I1226 13:03:00.608618 92018 net.cpp:235] Top shape: 32 1000 (32000)
I1226 13:03:00.608654 92018 net.cpp:243] Memory required for data: 266216192
I1226 13:03:00.608712 92018 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 13:03:00.608813 92018 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 13:03:00.608855 92018 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 13:03:00.608896 92018 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 13:03:00.608959 92018 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 13:03:00.609058 92018 net.cpp:228] Setting up fc8_fc8_0_split
I1226 13:03:00.609107 92018 net.cpp:235] Top shape: 32 1000 (32000)
I1226 13:03:00.609144 92018 net.cpp:235] Top shape: 32 1000 (32000)
I1226 13:03:00.609169 92018 net.cpp:243] Memory required for data: 266472192
I1226 13:03:00.609197 92018 layer_factory.hpp:114] Creating layer accuracy
I1226 13:03:00.609251 92018 net.cpp:178] Creating Layer accuracy
I1226 13:03:00.609285 92018 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 13:03:00.609316 92018 net.cpp:612] accuracy <- label_data_1_split_0
I1226 13:03:00.609352 92018 net.cpp:586] accuracy -> accuracy
I1226 13:03:00.609400 92018 net.cpp:228] Setting up accuracy
I1226 13:03:00.609441 92018 net.cpp:235] Top shape: (1)
I1226 13:03:00.609463 92018 net.cpp:243] Memory required for data: 266472196
I1226 13:03:00.609496 92018 layer_factory.hpp:114] Creating layer loss
I1226 13:03:00.609550 92018 net.cpp:178] Creating Layer loss
I1226 13:03:00.609577 92018 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 13:03:00.609622 92018 net.cpp:612] loss <- label_data_1_split_1
I1226 13:03:00.609658 92018 net.cpp:586] loss -> loss
I1226 13:03:00.609724 92018 layer_factory.hpp:114] Creating layer loss
I1226 13:03:00.609881 91895 net.cpp:228] Setting up fc8
I1226 13:03:00.609995 91895 net.cpp:235] Top shape: 32 1000 (32000)
I1226 13:03:00.610021 91895 net.cpp:243] Memory required for data: 266216192
I1226 13:03:00.610101 91895 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 13:03:00.610190 91895 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 13:03:00.610229 91895 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 13:03:00.610277 91895 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 13:03:00.610352 91895 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 13:03:00.610445 91895 net.cpp:228] Setting up fc8_fc8_0_split
I1226 13:03:00.610496 91895 net.cpp:235] Top shape: 32 1000 (32000)
I1226 13:03:00.610532 91895 net.cpp:235] Top shape: 32 1000 (32000)
I1226 13:03:00.610554 91895 net.cpp:243] Memory required for data: 266472192
I1226 13:03:00.610582 91895 layer_factory.hpp:114] Creating layer accuracy
I1226 13:03:00.610635 91895 net.cpp:178] Creating Layer accuracy
I1226 13:03:00.610667 91895 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 13:03:00.610710 91895 net.cpp:612] accuracy <- label_data_1_split_0
I1226 13:03:00.610749 91895 net.cpp:586] accuracy -> accuracy
I1226 13:03:00.610793 91895 net.cpp:228] Setting up accuracy
I1226 13:03:00.610833 91895 net.cpp:235] Top shape: (1)
I1226 13:03:00.610865 91895 net.cpp:243] Memory required for data: 266472196
I1226 13:03:00.610893 91895 layer_factory.hpp:114] Creating layer loss
I1226 13:03:00.610936 91895 net.cpp:178] Creating Layer loss
I1226 13:03:00.610961 91895 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 13:03:00.610990 91895 net.cpp:612] loss <- label_data_1_split_1
I1226 13:03:00.611022 91895 net.cpp:586] loss -> loss
I1226 13:03:00.611076 91895 layer_factory.hpp:114] Creating layer loss
I1226 13:03:00.624548 85539 net.cpp:228] Setting up loss
I1226 13:03:00.624675 85539 net.cpp:235] Top shape: (1)
I1226 13:03:00.624974 85539 net.cpp:238]     with loss weight 1
I1226 13:03:00.625128 85539 net.cpp:243] Memory required for data: 266472200
I1226 13:03:00.625176 85539 net.cpp:305] loss needs backward computation.
I1226 13:03:00.625217 85539 net.cpp:307] accuracy does not need backward computation.
I1226 13:03:00.625253 85539 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 13:03:00.625293 85539 net.cpp:305] fc8 needs backward computation.
I1226 13:03:00.625324 85539 net.cpp:305] drop7 needs backward computation.
I1226 13:03:00.625365 85539 net.cpp:305] relu7 needs backward computation.
I1226 13:03:00.625401 85539 net.cpp:305] fc7 needs backward computation.
I1226 13:03:00.625432 85539 net.cpp:305] drop6 needs backward computation.
I1226 13:03:00.625463 85539 net.cpp:305] relu6 needs backward computation.
I1226 13:03:00.625500 85539 net.cpp:305] fc6 needs backward computation.
I1226 13:03:00.625532 85539 net.cpp:305] pool5 needs backward computation.
I1226 13:03:00.625563 85539 net.cpp:305] relu5 needs backward computation.
I1226 13:03:00.625602 85539 net.cpp:305] conv5 needs backward computation.
I1226 13:03:00.625633 85539 net.cpp:305] relu4 needs backward computation.
I1226 13:03:00.625689 85539 net.cpp:305] conv4 needs backward computation.
I1226 13:03:00.625722 85539 net.cpp:305] relu3 needs backward computation.
I1226 13:03:00.625759 85539 net.cpp:305] conv3 needs backward computation.
I1226 13:03:00.625792 85539 net.cpp:305] pool2 needs backward computation.
I1226 13:03:00.625829 85539 net.cpp:305] norm2 needs backward computation.
I1226 13:03:00.625867 85539 net.cpp:305] relu2 needs backward computation.
I1226 13:03:00.625897 85539 net.cpp:305] conv2 needs backward computation.
I1226 13:03:00.625927 85539 net.cpp:305] pool1 needs backward computation.
I1226 13:03:00.625965 85539 net.cpp:305] norm1 needs backward computation.
I1226 13:03:00.625996 85539 net.cpp:305] relu1 needs backward computation.
I1226 13:03:00.626029 85539 net.cpp:305] conv1 needs backward computation.
I1226 13:03:00.626080 85539 net.cpp:307] label_data_1_split does not need backward computation.
I1226 13:03:00.626121 85539 net.cpp:307] data does not need backward computation.
I1226 13:03:00.626150 85539 net.cpp:349] This network produces output accuracy
I1226 13:03:00.626191 85539 net.cpp:349] This network produces output loss
I1226 13:03:00.626273 85539 net.cpp:363] Network initialization done.
I1226 13:03:00.626739 85539 solver.cpp:107] Solver scaffolding done.
I1226 13:03:00.626960 85539 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 13:03:00.639513 91895 net.cpp:228] Setting up loss
I1226 13:03:00.639624 91895 net.cpp:235] Top shape: (1)
I1226 13:03:00.639789 91895 net.cpp:238]     with loss weight 1
I1226 13:03:00.639942 91895 net.cpp:243] Memory required for data: 266472200
I1226 13:03:00.639991 91895 net.cpp:305] loss needs backward computation.
I1226 13:03:00.640034 91895 net.cpp:307] accuracy does not need backward computation.
I1226 13:03:00.640072 91895 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 13:03:00.640105 91895 net.cpp:305] fc8 needs backward computation.
I1226 13:03:00.640136 91895 net.cpp:305] drop7 needs backward computation.
I1226 13:03:00.640182 91895 net.cpp:305] relu7 needs backward computation.
I1226 13:03:00.640213 91895 net.cpp:305] fc7 needs backward computation.
I1226 13:03:00.640250 91895 net.cpp:305] drop6 needs backward computation.
I1226 13:03:00.640280 91895 net.cpp:305] relu6 needs backward computation.
I1226 13:03:00.640352 91895 net.cpp:305] fc6 needs backward computation.
I1226 13:03:00.640388 91895 net.cpp:305] pool5 needs backward computation.
I1226 13:03:00.640419 91895 net.cpp:305] relu5 needs backward computation.
I1226 13:03:00.640447 91895 net.cpp:305] conv5 needs backward computation.
I1226 13:03:00.640477 91895 net.cpp:305] relu4 needs backward computation.
I1226 13:03:00.640529 91895 net.cpp:305] conv4 needs backward computation.
I1226 13:03:00.640566 91895 net.cpp:305] relu3 needs backward computation.
I1226 13:03:00.640597 91895 net.cpp:305] conv3 needs backward computation.
I1226 13:03:00.640630 91895 net.cpp:305] pool2 needs backward computation.
I1226 13:03:00.640662 91895 net.cpp:305] norm2 needs backward computation.
I1226 13:03:00.640704 91895 net.cpp:305] relu2 needs backward computation.
I1226 13:03:00.640743 91895 net.cpp:305] conv2 needs backward computation.
I1226 13:03:00.640775 91895 net.cpp:305] pool1 needs backward computation.
I1226 13:03:00.640816 91895 net.cpp:305] norm1 needs backward computation.
I1226 13:03:00.640856 91895 net.cpp:305] relu1 needs backward computation.
I1226 13:03:00.640892 91895 net.cpp:305] conv1 needs backward computation.
I1226 13:03:00.640933 91895 net.cpp:307] label_data_1_split does not need backward computation.
I1226 13:03:00.640974 91895 net.cpp:307] data does not need backward computation.
I1226 13:03:00.641010 91895 net.cpp:349] This network produces output accuracy
I1226 13:03:00.641047 91895 net.cpp:349] This network produces output loss
I1226 13:03:00.641137 91895 net.cpp:363] Network initialization done.
I1226 13:03:00.641594 91895 solver.cpp:107] Solver scaffolding done.
I1226 13:03:00.641819 91895 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 13:03:00.645699 92018 net.cpp:228] Setting up loss
I1226 13:03:00.645848 92018 net.cpp:235] Top shape: (1)
I1226 13:03:00.646140 92018 net.cpp:238]     with loss weight 1
I1226 13:03:00.646303 92018 net.cpp:243] Memory required for data: 266472200
I1226 13:03:00.646358 92018 net.cpp:305] loss needs backward computation.
I1226 13:03:00.646399 92018 net.cpp:307] accuracy does not need backward computation.
I1226 13:03:00.646435 92018 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 13:03:00.646467 92018 net.cpp:305] fc8 needs backward computation.
I1226 13:03:00.646498 92018 net.cpp:305] drop7 needs backward computation.
I1226 13:03:00.646545 92018 net.cpp:305] relu7 needs backward computation.
I1226 13:03:00.646579 92018 net.cpp:305] fc7 needs backward computation.
I1226 13:03:00.646620 92018 net.cpp:305] drop6 needs backward computation.
I1226 13:03:00.646657 92018 net.cpp:305] relu6 needs backward computation.
I1226 13:03:00.646688 92018 net.cpp:305] fc6 needs backward computation.
I1226 13:03:00.646725 92018 net.cpp:305] pool5 needs backward computation.
I1226 13:03:00.646757 92018 net.cpp:305] relu5 needs backward computation.
I1226 13:03:00.646788 92018 net.cpp:305] conv5 needs backward computation.
I1226 13:03:00.646848 92018 net.cpp:305] relu4 needs backward computation.
I1226 13:03:00.646886 92018 net.cpp:305] conv4 needs backward computation.
I1226 13:03:00.646919 92018 net.cpp:305] relu3 needs backward computation.
I1226 13:03:00.646955 92018 net.cpp:305] conv3 needs backward computation.
I1226 13:03:00.646988 92018 net.cpp:305] pool2 needs backward computation.
I1226 13:03:00.647027 92018 net.cpp:305] norm2 needs backward computation.
I1226 13:03:00.647056 92018 net.cpp:305] relu2 needs backward computation.
I1226 13:03:00.647094 92018 net.cpp:305] conv2 needs backward computation.
I1226 13:03:00.647125 92018 net.cpp:305] pool1 needs backward computation.
I1226 13:03:00.647156 92018 net.cpp:305] norm1 needs backward computation.
I1226 13:03:00.647218 92018 net.cpp:305] relu1 needs backward computation.
I1226 13:03:00.647255 92018 net.cpp:305] conv1 needs backward computation.
I1226 13:03:00.647289 92018 net.cpp:307] label_data_1_split does not need backward computation.
I1226 13:03:00.647330 92018 net.cpp:307] data does not need backward computation.
I1226 13:03:00.647363 92018 net.cpp:349] This network produces output accuracy
I1226 13:03:00.647400 92018 net.cpp:349] This network produces output loss
I1226 13:03:00.647485 92018 net.cpp:363] Network initialization done.
I1226 13:03:00.648717 92018 solver.cpp:107] Solver scaffolding done.
I1226 13:03:00.648995 92018 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 13:03:00.802722 93741 net.cpp:228] Setting up fc8
I1226 13:03:00.802845 93741 net.cpp:235] Top shape: 32 1000 (32000)
I1226 13:03:00.802873 93741 net.cpp:243] Memory required for data: 266216192
I1226 13:03:00.802927 93741 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 13:03:00.802996 93741 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 13:03:00.803035 93741 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 13:03:00.803078 93741 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 13:03:00.803125 93741 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 13:03:00.803190 93741 net.cpp:228] Setting up fc8_fc8_0_split
I1226 13:03:00.803231 93741 net.cpp:235] Top shape: 32 1000 (32000)
I1226 13:03:00.803261 93741 net.cpp:235] Top shape: 32 1000 (32000)
I1226 13:03:00.803282 93741 net.cpp:243] Memory required for data: 266472192
I1226 13:03:00.803311 93741 layer_factory.hpp:114] Creating layer accuracy
I1226 13:03:00.803418 93741 net.cpp:178] Creating Layer accuracy
I1226 13:03:00.803457 93741 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 13:03:00.803489 93741 net.cpp:612] accuracy <- label_data_1_split_0
I1226 13:03:00.803529 93741 net.cpp:586] accuracy -> accuracy
I1226 13:03:00.803575 93741 net.cpp:228] Setting up accuracy
I1226 13:03:00.803635 93741 net.cpp:235] Top shape: (1)
I1226 13:03:00.803658 93741 net.cpp:243] Memory required for data: 266472196
I1226 13:03:00.803683 93741 layer_factory.hpp:114] Creating layer loss
I1226 13:03:00.803724 93741 net.cpp:178] Creating Layer loss
I1226 13:03:00.803750 93741 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 13:03:00.803776 93741 net.cpp:612] loss <- label_data_1_split_1
I1226 13:03:00.803807 93741 net.cpp:586] loss -> loss
I1226 13:03:00.803860 93741 layer_factory.hpp:114] Creating layer loss
I1226 13:03:00.835048 93741 net.cpp:228] Setting up loss
I1226 13:03:00.835145 93741 net.cpp:235] Top shape: (1)
I1226 13:03:00.835259 93741 net.cpp:238]     with loss weight 1
I1226 13:03:00.835361 93741 net.cpp:243] Memory required for data: 266472200
I1226 13:03:00.835436 93741 net.cpp:305] loss needs backward computation.
I1226 13:03:00.835470 93741 net.cpp:307] accuracy does not need backward computation.
I1226 13:03:00.835496 93741 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 13:03:00.835520 93741 net.cpp:305] fc8 needs backward computation.
I1226 13:03:00.835543 93741 net.cpp:305] drop7 needs backward computation.
I1226 13:03:00.835566 93741 net.cpp:305] relu7 needs backward computation.
I1226 13:03:00.835589 93741 net.cpp:305] fc7 needs backward computation.
I1226 13:03:00.835613 93741 net.cpp:305] drop6 needs backward computation.
I1226 13:03:00.835634 93741 net.cpp:305] relu6 needs backward computation.
I1226 13:03:00.835656 93741 net.cpp:305] fc6 needs backward computation.
I1226 13:03:00.835680 93741 net.cpp:305] pool5 needs backward computation.
I1226 13:03:00.835703 93741 net.cpp:305] relu5 needs backward computation.
I1226 13:03:00.835726 93741 net.cpp:305] conv5 needs backward computation.
I1226 13:03:00.835752 93741 net.cpp:305] relu4 needs backward computation.
I1226 13:03:00.835774 93741 net.cpp:305] conv4 needs backward computation.
I1226 13:03:00.835799 93741 net.cpp:305] relu3 needs backward computation.
I1226 13:03:00.835824 93741 net.cpp:305] conv3 needs backward computation.
I1226 13:03:00.835849 93741 net.cpp:305] pool2 needs backward computation.
I1226 13:03:00.835872 93741 net.cpp:305] norm2 needs backward computation.
I1226 13:03:00.835896 93741 net.cpp:305] relu2 needs backward computation.
I1226 13:03:00.835918 93741 net.cpp:305] conv2 needs backward computation.
I1226 13:03:00.835942 93741 net.cpp:305] pool1 needs backward computation.
I1226 13:03:00.835966 93741 net.cpp:305] norm1 needs backward computation.
I1226 13:03:00.835990 93741 net.cpp:305] relu1 needs backward computation.
I1226 13:03:00.836012 93741 net.cpp:305] conv1 needs backward computation.
I1226 13:03:00.836037 93741 net.cpp:307] label_data_1_split does not need backward computation.
I1226 13:03:00.836061 93741 net.cpp:307] data does not need backward computation.
I1226 13:03:00.836083 93741 net.cpp:349] This network produces output accuracy
I1226 13:03:00.836109 93741 net.cpp:349] This network produces output loss
I1226 13:03:00.836165 93741 net.cpp:363] Network initialization done.
I1226 13:03:00.836591 93741 solver.cpp:107] Solver scaffolding done.
I1226 13:03:00.836752 93741 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 13:03:01.050921 122158 net.cpp:228] Setting up fc6
I1226 13:03:01.051075 122158 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:03:01.051137 122158 net.cpp:243] Memory required for data: 263466752
I1226 13:03:01.051245 122158 layer_factory.hpp:114] Creating layer relu6
I1226 13:03:01.051409 122158 net.cpp:178] Creating Layer relu6
I1226 13:03:01.051482 122158 net.cpp:612] relu6 <- fc6
I1226 13:03:01.051800 122158 net.cpp:573] relu6 -> fc6 (in-place)
I1226 13:03:01.052014 122158 net.cpp:228] Setting up relu6
I1226 13:03:01.052194 122158 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:03:01.052242 122158 net.cpp:243] Memory required for data: 263991040
I1226 13:03:01.052317 122158 layer_factory.hpp:114] Creating layer drop6
I1226 13:03:01.052407 122158 net.cpp:178] Creating Layer drop6
I1226 13:03:01.052479 122158 net.cpp:612] drop6 <- fc6
I1226 13:03:01.052583 122158 net.cpp:573] drop6 -> fc6 (in-place)
I1226 13:03:01.052736 122158 net.cpp:228] Setting up drop6
I1226 13:03:01.052826 122158 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:03:01.052871 122158 net.cpp:243] Memory required for data: 264515328
I1226 13:03:01.052925 122158 layer_factory.hpp:114] Creating layer fc7
I1226 13:03:01.053053 122158 net.cpp:178] Creating Layer fc7
I1226 13:03:01.053117 122158 net.cpp:612] fc7 <- fc6
I1226 13:03:01.053231 122158 net.cpp:586] fc7 -> fc7
I1226 13:03:01.641706 154317 caffe.cpp:376] Configuring multinode setup
I1226 13:03:01.644078 154317 caffe.cpp:386] Starting parameter server in mpi environment
I1226 13:03:04.503036 122158 net.cpp:228] Setting up fc7
I1226 13:03:04.503185 122158 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:03:04.503239 122158 net.cpp:243] Memory required for data: 265039616
I1226 13:03:04.503347 122158 layer_factory.hpp:114] Creating layer relu7
I1226 13:03:04.503494 122158 net.cpp:178] Creating Layer relu7
I1226 13:03:04.503775 122158 net.cpp:612] relu7 <- fc7
I1226 13:03:04.503852 122158 net.cpp:573] relu7 -> fc7 (in-place)
I1226 13:03:04.504001 122158 net.cpp:228] Setting up relu7
I1226 13:03:04.504098 122158 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:03:04.504150 122158 net.cpp:243] Memory required for data: 265563904
I1226 13:03:04.504209 122158 layer_factory.hpp:114] Creating layer drop7
I1226 13:03:04.504303 122158 net.cpp:178] Creating Layer drop7
I1226 13:03:04.504362 122158 net.cpp:612] drop7 <- fc7
I1226 13:03:04.504432 122158 net.cpp:573] drop7 -> fc7 (in-place)
I1226 13:03:04.504519 122158 net.cpp:228] Setting up drop7
I1226 13:03:04.504590 122158 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:03:04.504683 122158 net.cpp:243] Memory required for data: 266088192
I1226 13:03:04.504741 122158 layer_factory.hpp:114] Creating layer fc8
I1226 13:03:04.504850 122158 net.cpp:178] Creating Layer fc8
I1226 13:03:04.504925 122158 net.cpp:612] fc8 <- fc7
I1226 13:03:04.505007 122158 net.cpp:586] fc8 -> fc8
I1226 13:03:04.787533 91895 caffe.cpp:376] Configuring multinode setup
I1226 13:03:04.788933 91895 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 13:03:04.810956 90983 caffe.cpp:376] Configuring multinode setup
I1226 13:03:04.812592 90983 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 13:03:04.810035 92018 caffe.cpp:376] Configuring multinode setup
I1226 13:03:04.811519 92018 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 13:03:04.832016 85539 caffe.cpp:376] Configuring multinode setup
I1226 13:03:04.833688 85539 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 13:03:04.948519 93741 caffe.cpp:376] Configuring multinode setup
I1226 13:03:04.949965 93741 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 13:03:05.352121 122158 net.cpp:228] Setting up fc8
I1226 13:03:05.352295 122158 net.cpp:235] Top shape: 32 1000 (32000)
I1226 13:03:05.352352 122158 net.cpp:243] Memory required for data: 266216192
I1226 13:03:05.352504 122158 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 13:03:05.352831 122158 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 13:03:05.352891 122158 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 13:03:05.353018 122158 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 13:03:05.353281 122158 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 13:03:05.353490 122158 net.cpp:228] Setting up fc8_fc8_0_split
I1226 13:03:05.353576 122158 net.cpp:235] Top shape: 32 1000 (32000)
I1226 13:03:05.353680 122158 net.cpp:235] Top shape: 32 1000 (32000)
I1226 13:03:05.353729 122158 net.cpp:243] Memory required for data: 266472192
I1226 13:03:05.353786 122158 layer_factory.hpp:114] Creating layer accuracy
I1226 13:03:05.353878 122158 net.cpp:178] Creating Layer accuracy
I1226 13:03:05.353951 122158 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 13:03:05.354051 122158 net.cpp:612] accuracy <- label_data_1_split_0
I1226 13:03:05.354357 122158 net.cpp:586] accuracy -> accuracy
I1226 13:03:05.354444 122158 net.cpp:228] Setting up accuracy
I1226 13:03:05.354517 122158 net.cpp:235] Top shape: (1)
I1226 13:03:05.354567 122158 net.cpp:243] Memory required for data: 266472196
I1226 13:03:05.354631 122158 layer_factory.hpp:114] Creating layer loss
I1226 13:03:05.354753 122158 net.cpp:178] Creating Layer loss
I1226 13:03:05.354813 122158 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 13:03:05.354936 122158 net.cpp:612] loss <- label_data_1_split_1
I1226 13:03:05.355010 122158 net.cpp:586] loss -> loss
I1226 13:03:05.355275 122158 layer_factory.hpp:114] Creating layer loss
I1226 13:03:05.396162 122158 net.cpp:228] Setting up loss
I1226 13:03:05.396296 122158 net.cpp:235] Top shape: (1)
I1226 13:03:05.396351 122158 net.cpp:238]     with loss weight 1
I1226 13:03:05.396535 122158 net.cpp:243] Memory required for data: 266472200
I1226 13:03:05.396600 122158 net.cpp:305] loss needs backward computation.
I1226 13:03:05.396689 122158 net.cpp:307] accuracy does not need backward computation.
I1226 13:03:05.396749 122158 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 13:03:05.396796 122158 net.cpp:305] fc8 needs backward computation.
I1226 13:03:05.396842 122158 net.cpp:305] drop7 needs backward computation.
I1226 13:03:05.396886 122158 net.cpp:305] relu7 needs backward computation.
I1226 13:03:05.396932 122158 net.cpp:305] fc7 needs backward computation.
I1226 13:03:05.396977 122158 net.cpp:305] drop6 needs backward computation.
I1226 13:03:05.397022 122158 net.cpp:305] relu6 needs backward computation.
I1226 13:03:05.397068 122158 net.cpp:305] fc6 needs backward computation.
I1226 13:03:05.397115 122158 net.cpp:305] pool5 needs backward computation.
I1226 13:03:05.397162 122158 net.cpp:305] relu5 needs backward computation.
I1226 13:03:05.397254 122158 net.cpp:305] conv5 needs backward computation.
I1226 13:03:05.397301 122158 net.cpp:305] relu4 needs backward computation.
I1226 13:03:05.397557 122158 net.cpp:305] conv4 needs backward computation.
I1226 13:03:05.397660 122158 net.cpp:305] relu3 needs backward computation.
I1226 13:03:05.397745 122158 net.cpp:305] conv3 needs backward computation.
I1226 13:03:05.397794 122158 net.cpp:305] pool2 needs backward computation.
I1226 13:03:05.397840 122158 net.cpp:305] norm2 needs backward computation.
I1226 13:03:05.397886 122158 net.cpp:305] relu2 needs backward computation.
I1226 13:03:05.397930 122158 net.cpp:305] conv2 needs backward computation.
I1226 13:03:05.397975 122158 net.cpp:305] pool1 needs backward computation.
I1226 13:03:05.398021 122158 net.cpp:305] norm1 needs backward computation.
I1226 13:03:05.398066 122158 net.cpp:305] relu1 needs backward computation.
I1226 13:03:05.398109 122158 net.cpp:305] conv1 needs backward computation.
I1226 13:03:05.398185 122158 net.cpp:307] label_data_1_split does not need backward computation.
I1226 13:03:05.398273 122158 net.cpp:307] data does not need backward computation.
I1226 13:03:05.398319 122158 net.cpp:349] This network produces output accuracy
I1226 13:03:05.398370 122158 net.cpp:349] This network produces output loss
I1226 13:03:05.398524 122158 net.cpp:363] Network initialization done.
I1226 13:03:05.399255 122158 solver.cpp:107] Solver scaffolding done.
I1226 13:03:05.399555 122158 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 13:03:11.431583 122158 caffe.cpp:376] Configuring multinode setup
I1226 13:03:11.433728 122158 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 13:03:24.364424 96103 net.cpp:228] Setting up fc6
I1226 13:03:24.364696 96103 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:03:24.364740 96103 net.cpp:243] Memory required for data: 263466752
I1226 13:03:24.364838 96103 layer_factory.hpp:114] Creating layer relu6
I1226 13:03:24.364912 96103 net.cpp:178] Creating Layer relu6
I1226 13:03:24.364945 96103 net.cpp:612] relu6 <- fc6
I1226 13:03:24.364984 96103 net.cpp:573] relu6 -> fc6 (in-place)
I1226 13:03:24.365083 96103 net.cpp:228] Setting up relu6
I1226 13:03:24.365128 96103 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:03:24.365154 96103 net.cpp:243] Memory required for data: 263991040
I1226 13:03:24.365185 96103 layer_factory.hpp:114] Creating layer drop6
I1226 13:03:24.365236 96103 net.cpp:178] Creating Layer drop6
I1226 13:03:24.365267 96103 net.cpp:612] drop6 <- fc6
I1226 13:03:24.365301 96103 net.cpp:573] drop6 -> fc6 (in-place)
I1226 13:03:24.365355 96103 net.cpp:228] Setting up drop6
I1226 13:03:24.365389 96103 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:03:24.365413 96103 net.cpp:243] Memory required for data: 264515328
I1226 13:03:24.365442 96103 layer_factory.hpp:114] Creating layer fc7
I1226 13:03:24.365520 96103 net.cpp:178] Creating Layer fc7
I1226 13:03:24.365658 96103 net.cpp:612] fc7 <- fc6
I1226 13:03:24.365713 96103 net.cpp:586] fc7 -> fc7
I1226 13:03:24.378280 94300 net.cpp:228] Setting up fc6
I1226 13:03:24.378556 94300 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:03:24.378618 94300 net.cpp:243] Memory required for data: 263466752
I1226 13:03:24.378705 94300 layer_factory.hpp:114] Creating layer relu6
I1226 13:03:24.378957 94300 net.cpp:178] Creating Layer relu6
I1226 13:03:24.379014 94300 net.cpp:612] relu6 <- fc6
I1226 13:03:24.379070 94300 net.cpp:573] relu6 -> fc6 (in-place)
I1226 13:03:24.379215 94300 net.cpp:228] Setting up relu6
I1226 13:03:24.379305 94300 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:03:24.379348 94300 net.cpp:243] Memory required for data: 263991040
I1226 13:03:24.379392 94300 layer_factory.hpp:114] Creating layer drop6
I1226 13:03:24.379479 94300 net.cpp:178] Creating Layer drop6
I1226 13:03:24.379534 94300 net.cpp:612] drop6 <- fc6
I1226 13:03:24.379590 94300 net.cpp:573] drop6 -> fc6 (in-place)
I1226 13:03:24.379675 94300 net.cpp:228] Setting up drop6
I1226 13:03:24.379737 94300 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:03:24.379772 94300 net.cpp:243] Memory required for data: 264515328
I1226 13:03:24.379812 94300 layer_factory.hpp:114] Creating layer fc7
I1226 13:03:24.379940 94300 net.cpp:178] Creating Layer fc7
I1226 13:03:24.379990 94300 net.cpp:612] fc7 <- fc6
I1226 13:03:24.380074 94300 net.cpp:586] fc7 -> fc7
I1226 13:03:37.441018 96103 net.cpp:228] Setting up fc7
I1226 13:03:37.441136 96103 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:03:37.441174 96103 net.cpp:243] Memory required for data: 265039616
I1226 13:03:37.441236 96103 layer_factory.hpp:114] Creating layer relu7
I1226 13:03:37.441328 96103 net.cpp:178] Creating Layer relu7
I1226 13:03:37.441388 96103 net.cpp:612] relu7 <- fc7
I1226 13:03:37.441434 96103 net.cpp:573] relu7 -> fc7 (in-place)
I1226 13:03:37.441539 96103 net.cpp:228] Setting up relu7
I1226 13:03:37.441601 96103 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:03:37.441628 96103 net.cpp:243] Memory required for data: 265563904
I1226 13:03:37.441661 96103 layer_factory.hpp:114] Creating layer drop7
I1226 13:03:37.441714 96103 net.cpp:178] Creating Layer drop7
I1226 13:03:37.441751 96103 net.cpp:612] drop7 <- fc7
I1226 13:03:37.441841 96103 net.cpp:573] drop7 -> fc7 (in-place)
I1226 13:03:37.441900 96103 net.cpp:228] Setting up drop7
I1226 13:03:37.441936 96103 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:03:37.441961 96103 net.cpp:243] Memory required for data: 266088192
I1226 13:03:37.441989 96103 layer_factory.hpp:114] Creating layer fc8
I1226 13:03:37.442046 96103 net.cpp:178] Creating Layer fc8
I1226 13:03:37.442075 96103 net.cpp:612] fc8 <- fc7
I1226 13:03:37.442118 96103 net.cpp:586] fc8 -> fc8
I1226 13:03:37.457270 94300 net.cpp:228] Setting up fc7
I1226 13:03:37.457392 94300 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:03:37.457442 94300 net.cpp:243] Memory required for data: 265039616
I1226 13:03:37.457530 94300 layer_factory.hpp:114] Creating layer relu7
I1226 13:03:37.457659 94300 net.cpp:178] Creating Layer relu7
I1226 13:03:37.457721 94300 net.cpp:612] relu7 <- fc7
I1226 13:03:37.457778 94300 net.cpp:573] relu7 -> fc7 (in-place)
I1226 13:03:37.457957 94300 net.cpp:228] Setting up relu7
I1226 13:03:37.458046 94300 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:03:37.458087 94300 net.cpp:243] Memory required for data: 265563904
I1226 13:03:37.458137 94300 layer_factory.hpp:114] Creating layer drop7
I1226 13:03:37.458206 94300 net.cpp:178] Creating Layer drop7
I1226 13:03:37.458259 94300 net.cpp:612] drop7 <- fc7
I1226 13:03:37.458338 94300 net.cpp:573] drop7 -> fc7 (in-place)
I1226 13:03:37.458410 94300 net.cpp:228] Setting up drop7
I1226 13:03:37.458470 94300 net.cpp:235] Top shape: 32 4096 (131072)
I1226 13:03:37.458505 94300 net.cpp:243] Memory required for data: 266088192
I1226 13:03:37.458544 94300 layer_factory.hpp:114] Creating layer fc8
I1226 13:03:37.458631 94300 net.cpp:178] Creating Layer fc8
I1226 13:03:37.458674 94300 net.cpp:612] fc8 <- fc7
I1226 13:03:37.458726 94300 net.cpp:586] fc8 -> fc8
I1226 13:03:40.630508 96103 net.cpp:228] Setting up fc8
I1226 13:03:40.630631 96103 net.cpp:235] Top shape: 32 1000 (32000)
I1226 13:03:40.630666 96103 net.cpp:243] Memory required for data: 266216192
I1226 13:03:40.630728 96103 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 13:03:40.630959 96103 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 13:03:40.631008 96103 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 13:03:40.631055 96103 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 13:03:40.631117 96103 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 13:03:40.631217 96103 net.cpp:228] Setting up fc8_fc8_0_split
I1226 13:03:40.631280 96103 net.cpp:235] Top shape: 32 1000 (32000)
I1226 13:03:40.631314 96103 net.cpp:235] Top shape: 32 1000 (32000)
I1226 13:03:40.631338 96103 net.cpp:243] Memory required for data: 266472192
I1226 13:03:40.631381 96103 layer_factory.hpp:114] Creating layer accuracy
I1226 13:03:40.631443 96103 net.cpp:178] Creating Layer accuracy
I1226 13:03:40.631486 96103 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 13:03:40.631520 96103 net.cpp:612] accuracy <- label_data_1_split_0
I1226 13:03:40.631556 96103 net.cpp:586] accuracy -> accuracy
I1226 13:03:40.631603 96103 net.cpp:228] Setting up accuracy
I1226 13:03:40.631647 96103 net.cpp:235] Top shape: (1)
I1226 13:03:40.631680 96103 net.cpp:243] Memory required for data: 266472196
I1226 13:03:40.631721 96103 layer_factory.hpp:114] Creating layer loss
I1226 13:03:40.631914 96103 net.cpp:178] Creating Layer loss
I1226 13:03:40.631969 96103 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 13:03:40.632005 96103 net.cpp:612] loss <- label_data_1_split_1
I1226 13:03:40.632061 96103 net.cpp:586] loss -> loss
I1226 13:03:40.632141 96103 layer_factory.hpp:114] Creating layer loss
I1226 13:03:40.652482 94300 net.cpp:228] Setting up fc8
I1226 13:03:40.652613 94300 net.cpp:235] Top shape: 32 1000 (32000)
I1226 13:03:40.652669 94300 net.cpp:243] Memory required for data: 266216192
I1226 13:03:40.652762 94300 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 13:03:40.652926 94300 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 13:03:40.652992 94300 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 13:03:40.653060 94300 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 13:03:40.653129 94300 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 13:03:40.653268 94300 net.cpp:228] Setting up fc8_fc8_0_split
I1226 13:03:40.653354 94300 net.cpp:235] Top shape: 32 1000 (32000)
I1226 13:03:40.653405 94300 net.cpp:235] Top shape: 32 1000 (32000)
I1226 13:03:40.653439 94300 net.cpp:243] Memory required for data: 266472192
I1226 13:03:40.653481 94300 layer_factory.hpp:114] Creating layer accuracy
I1226 13:03:40.653568 94300 net.cpp:178] Creating Layer accuracy
I1226 13:03:40.653620 94300 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 13:03:40.653666 94300 net.cpp:612] accuracy <- label_data_1_split_0
I1226 13:03:40.653714 94300 net.cpp:586] accuracy -> accuracy
I1226 13:03:40.653779 94300 net.cpp:228] Setting up accuracy
I1226 13:03:40.653868 94300 net.cpp:235] Top shape: (1)
I1226 13:03:40.653908 94300 net.cpp:243] Memory required for data: 266472196
I1226 13:03:40.653949 94300 layer_factory.hpp:114] Creating layer loss
I1226 13:03:40.654137 94300 net.cpp:178] Creating Layer loss
I1226 13:03:40.654201 94300 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 13:03:40.654247 94300 net.cpp:612] loss <- label_data_1_split_1
I1226 13:03:40.654295 94300 net.cpp:586] loss -> loss
I1226 13:03:40.654395 94300 layer_factory.hpp:114] Creating layer loss
I1226 13:03:40.671449 96103 net.cpp:228] Setting up loss
I1226 13:03:40.671568 96103 net.cpp:235] Top shape: (1)
I1226 13:03:40.671617 96103 net.cpp:238]     with loss weight 1
I1226 13:03:40.671777 96103 net.cpp:243] Memory required for data: 266472200
I1226 13:03:40.671866 96103 net.cpp:305] loss needs backward computation.
I1226 13:03:40.671912 96103 net.cpp:307] accuracy does not need backward computation.
I1226 13:03:40.671964 96103 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 13:03:40.672013 96103 net.cpp:305] fc8 needs backward computation.
I1226 13:03:40.672055 96103 net.cpp:305] drop7 needs backward computation.
I1226 13:03:40.672086 96103 net.cpp:305] relu7 needs backward computation.
I1226 13:03:40.672116 96103 net.cpp:305] fc7 needs backward computation.
I1226 13:03:40.672147 96103 net.cpp:305] drop6 needs backward computation.
I1226 13:03:40.672188 96103 net.cpp:305] relu6 needs backward computation.
I1226 13:03:40.672219 96103 net.cpp:305] fc6 needs backward computation.
I1226 13:03:40.672250 96103 net.cpp:305] pool5 needs backward computation.
I1226 13:03:40.672281 96103 net.cpp:305] relu5 needs backward computation.
I1226 13:03:40.672312 96103 net.cpp:305] conv5 needs backward computation.
I1226 13:03:40.672344 96103 net.cpp:305] relu4 needs backward computation.
I1226 13:03:40.672376 96103 net.cpp:305] conv4 needs backward computation.
I1226 13:03:40.672407 96103 net.cpp:305] relu3 needs backward computation.
I1226 13:03:40.672436 96103 net.cpp:305] conv3 needs backward computation.
I1226 13:03:40.672469 96103 net.cpp:305] pool2 needs backward computation.
I1226 13:03:40.672513 96103 net.cpp:305] norm2 needs backward computation.
I1226 13:03:40.672545 96103 net.cpp:305] relu2 needs backward computation.
I1226 13:03:40.672585 96103 net.cpp:305] conv2 needs backward computation.
I1226 13:03:40.672628 96103 net.cpp:305] pool1 needs backward computation.
I1226 13:03:40.672659 96103 net.cpp:305] norm1 needs backward computation.
I1226 13:03:40.672690 96103 net.cpp:305] relu1 needs backward computation.
I1226 13:03:40.672727 96103 net.cpp:305] conv1 needs backward computation.
I1226 13:03:40.672760 96103 net.cpp:307] label_data_1_split does not need backward computation.
I1226 13:03:40.672826 96103 net.cpp:307] data does not need backward computation.
I1226 13:03:40.672857 96103 net.cpp:349] This network produces output accuracy
I1226 13:03:40.672893 96103 net.cpp:349] This network produces output loss
I1226 13:03:40.673007 96103 net.cpp:363] Network initialization done.
I1226 13:03:40.673460 96103 solver.cpp:107] Solver scaffolding done.
I1226 13:03:40.673683 96103 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 13:03:40.683686 94300 net.cpp:228] Setting up loss
I1226 13:03:40.683804 94300 net.cpp:235] Top shape: (1)
I1226 13:03:40.683893 94300 net.cpp:238]     with loss weight 1
I1226 13:03:40.684170 94300 net.cpp:243] Memory required for data: 266472200
I1226 13:03:40.684226 94300 net.cpp:305] loss needs backward computation.
I1226 13:03:40.684269 94300 net.cpp:307] accuracy does not need backward computation.
I1226 13:03:40.684322 94300 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 13:03:40.684355 94300 net.cpp:305] fc8 needs backward computation.
I1226 13:03:40.684386 94300 net.cpp:305] drop7 needs backward computation.
I1226 13:03:40.684417 94300 net.cpp:305] relu7 needs backward computation.
I1226 13:03:40.684447 94300 net.cpp:305] fc7 needs backward computation.
I1226 13:03:40.684489 94300 net.cpp:305] drop6 needs backward computation.
I1226 13:03:40.684520 94300 net.cpp:305] relu6 needs backward computation.
I1226 13:03:40.684551 94300 net.cpp:305] fc6 needs backward computation.
I1226 13:03:40.684581 94300 net.cpp:305] pool5 needs backward computation.
I1226 13:03:40.684619 94300 net.cpp:305] relu5 needs backward computation.
I1226 13:03:40.684649 94300 net.cpp:305] conv5 needs backward computation.
I1226 13:03:40.684680 94300 net.cpp:305] relu4 needs backward computation.
I1226 13:03:40.684721 94300 net.cpp:305] conv4 needs backward computation.
I1226 13:03:40.684751 94300 net.cpp:305] relu3 needs backward computation.
I1226 13:03:40.684792 94300 net.cpp:305] conv3 needs backward computation.
I1226 13:03:40.684845 94300 net.cpp:305] pool2 needs backward computation.
I1226 13:03:40.684878 94300 net.cpp:305] norm2 needs backward computation.
I1226 13:03:40.684908 94300 net.cpp:305] relu2 needs backward computation.
I1226 13:03:40.684939 94300 net.cpp:305] conv2 needs backward computation.
I1226 13:03:40.684983 94300 net.cpp:305] pool1 needs backward computation.
I1226 13:03:40.685016 94300 net.cpp:305] norm1 needs backward computation.
I1226 13:03:40.685047 94300 net.cpp:305] relu1 needs backward computation.
I1226 13:03:40.685076 94300 net.cpp:305] conv1 needs backward computation.
I1226 13:03:40.685117 94300 net.cpp:307] label_data_1_split does not need backward computation.
I1226 13:03:40.685150 94300 net.cpp:307] data does not need backward computation.
I1226 13:03:40.685186 94300 net.cpp:349] This network produces output accuracy
I1226 13:03:40.685221 94300 net.cpp:349] This network produces output loss
I1226 13:03:40.685329 94300 net.cpp:363] Network initialization done.
I1226 13:03:40.685798 94300 solver.cpp:107] Solver scaffolding done.
I1226 13:03:40.686064 94300 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 13:03:46.102830 96103 caffe.cpp:376] Configuring multinode setup
I1226 13:03:46.104606 96103 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 13:03:46.150260 94300 caffe.cpp:376] Configuring multinode setup
I1226 13:03:46.152153 94300 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 13:03:46.139392 93741 SynchronousNode.cpp:662] [0] [proc 0] solving
I1226 13:03:46.144767 96103 SynchronousNode.cpp:662] [4] [proc 4] solving
I1226 13:03:46.152379 94300 SynchronousNode.cpp:662] [1] [proc 1] solving
I1226 13:03:46.161700 90983 SynchronousNode.cpp:662] [7] [proc 7] solving
I1226 13:03:46.139685 93741 solver.cpp:354] Solving AlexNet
I1226 13:03:46.139760 93741 solver.cpp:355] Learning Rate Policy: step
I1226 13:03:46.144963 96103 solver.cpp:354] Solving AlexNet
I1226 13:03:46.152482 94300 solver.cpp:354] Solving AlexNet
I1226 13:03:46.153831 91895 SynchronousNode.cpp:662] [3] [proc 3] solving
I1226 13:03:46.161969 90983 solver.cpp:354] Solving AlexNet
I1226 13:03:46.162017 90983 solver.cpp:355] Learning Rate Policy: step
I1226 13:03:46.153838 92018 SynchronousNode.cpp:662] [5] [proc 5] solving
I1226 13:03:46.139992 93741 MultiSolver.cpp:76] [0] PROFILING BEGIN[Forward]
I1226 13:03:46.145228 96103 solver.cpp:355] Learning Rate Policy: step
I1226 13:03:46.152520 94300 solver.cpp:355] Learning Rate Policy: step
I1226 13:03:46.154116 91895 solver.cpp:354] Solving AlexNet
I1226 13:03:46.154165 91895 solver.cpp:355] Learning Rate Policy: step
I1226 13:03:46.162315 90983 MultiSolver.cpp:76] [7] PROFILING BEGIN[Forward]
I1226 13:03:46.154124 92018 solver.cpp:354] Solving AlexNet
I1226 13:03:46.145562 96103 MultiSolver.cpp:76] [4] PROFILING BEGIN[Forward]
I1226 13:03:46.152775 94300 MultiSolver.cpp:76] [1] PROFILING BEGIN[Forward]
I1226 13:03:46.154439 91895 MultiSolver.cpp:76] [3] PROFILING BEGIN[Forward]
I1226 13:03:46.154175 92018 solver.cpp:355] Learning Rate Policy: step
I1226 13:03:46.154418 92018 MultiSolver.cpp:76] [5] PROFILING BEGIN[Forward]
I1226 13:03:46.150394 85539 SynchronousNode.cpp:662] [6] [proc 6] solving
I1226 13:03:46.150719 85539 solver.cpp:354] Solving AlexNet
I1226 13:03:46.150779 85539 solver.cpp:355] Learning Rate Policy: step
I1226 13:03:46.151059 85539 MultiSolver.cpp:76] [6] PROFILING BEGIN[Forward]
I1226 13:03:46.138298 122158 SynchronousNode.cpp:662] [2] [proc 2] solving
I1226 13:03:46.138692 122158 solver.cpp:354] Solving AlexNet
I1226 13:03:46.138761 122158 solver.cpp:355] Learning Rate Policy: step
I1226 13:03:46.139092 122158 MultiSolver.cpp:76] [2] PROFILING BEGIN[Forward]
I1226 13:03:46.156394 94372 SynchronousNode.cpp:280] [1] Comm thread started 0 0
I1226 13:03:46.155202 96175 SynchronousNode.cpp:280] [4] Comm thread started 1 0
I1226 13:03:46.164834 91964 SynchronousNode.cpp:280] [3] Comm thread started 0 0
I1226 13:03:46.151738 93812 SynchronousNode.cpp:280] [0] Comm thread started 0 1
I1226 13:03:46.151430 122228 SynchronousNode.cpp:280] [2] Comm thread started 0 0
I1226 13:03:46.183904 91052 SynchronousNode.cpp:280] [7] Comm thread started 1 0
I1226 13:03:46.162626 85611 SynchronousNode.cpp:280] [6] Comm thread started 1 0
I1226 13:03:46.189787 92087 SynchronousNode.cpp:280] [5] Comm thread started 1 0
I1226 13:03:46.179281 93812 SynchronousNode.cpp:466] [0] initialized root of cluster with nodes: 9 and the total iter size is: 8
I1226 13:03:46.370491 93741 MultiSolver.cpp:93] [0] PROFILING END[Forward]
I1226 13:03:46.370610 93741 MultiSolver.cpp:95] [0] PROFILING BEGIN[Backward]
I1226 13:03:46.731494 93741 MultiSolver.cpp:109] [0] PROFILING END[Backward]
I1226 13:03:46.731582 93741 solver.cpp:291] [0] Iteration 1, loss = 2.17349
I1226 13:03:46.731638 93741 solver.cpp:317]     Train net output #0: accuracy = 0.4375
I1226 13:03:46.731690 93741 solver.cpp:317]     Train net output #1: loss = 2.17349 (* 1 = 2.17349 loss)
I1226 13:03:46.731737 93741 MultiSolver.cpp:76] [0] PROFILING BEGIN[Forward]
I1226 13:03:48.757216 92018 MultiSolver.cpp:93] [5] PROFILING END[Forward]
I1226 13:03:48.757321 92018 MultiSolver.cpp:95] [5] PROFILING BEGIN[Backward]
I1226 13:03:48.796166 85539 MultiSolver.cpp:93] [6] PROFILING END[Forward]
I1226 13:03:48.796277 85539 MultiSolver.cpp:95] [6] PROFILING BEGIN[Backward]
I1226 13:03:48.836560 122158 MultiSolver.cpp:93] [2] PROFILING END[Forward]
I1226 13:03:48.836753 122158 MultiSolver.cpp:95] [2] PROFILING BEGIN[Backward]
I1226 13:03:48.944208 92018 MultiSolver.cpp:109] [5] PROFILING END[Backward]
I1226 13:03:48.944308 92018 solver.cpp:291] [5] Iteration 1, loss = 3.61398
I1226 13:03:48.944396 92018 solver.cpp:317]     Train net output #0: accuracy = 0.25
I1226 13:03:48.944473 92018 solver.cpp:317]     Train net output #1: loss = 3.61398 (* 1 = 3.61398 loss)
I1226 13:03:48.944551 92018 MultiSolver.cpp:76] [5] PROFILING BEGIN[Forward]
I1226 13:03:49.074057 85539 MultiSolver.cpp:109] [6] PROFILING END[Backward]
I1226 13:03:49.074151 85539 solver.cpp:291] [6] Iteration 1, loss = 3.21916
I1226 13:03:49.074232 85539 solver.cpp:317]     Train net output #0: accuracy = 0.375
I1226 13:03:49.074328 85539 solver.cpp:317]     Train net output #1: loss = 3.21916 (* 1 = 3.21916 loss)
I1226 13:03:49.074404 85539 MultiSolver.cpp:76] [6] PROFILING BEGIN[Forward]
I1226 13:03:49.087594 90983 MultiSolver.cpp:93] [7] PROFILING END[Forward]
I1226 13:03:49.087702 90983 MultiSolver.cpp:95] [7] PROFILING BEGIN[Backward]
I1226 13:03:49.106544 91895 MultiSolver.cpp:93] [3] PROFILING END[Forward]
I1226 13:03:49.106647 91895 MultiSolver.cpp:95] [3] PROFILING BEGIN[Backward]
I1226 13:03:49.130805 122158 MultiSolver.cpp:109] [2] PROFILING END[Backward]
I1226 13:03:49.130997 122158 solver.cpp:291] [2] Iteration 1, loss = 2.71072
I1226 13:03:49.131502 122158 solver.cpp:317]     Train net output #0: accuracy = 0.40625
I1226 13:03:49.131660 122158 solver.cpp:317]     Train net output #1: loss = 2.71072 (* 1 = 2.71072 loss)
I1226 13:03:49.132215 122158 MultiSolver.cpp:76] [2] PROFILING BEGIN[Forward]
I1226 13:03:49.190438 96103 MultiSolver.cpp:93] [4] PROFILING END[Forward]
I1226 13:03:49.190563 96103 MultiSolver.cpp:95] [4] PROFILING BEGIN[Backward]
I1226 13:03:49.235973 91895 MultiSolver.cpp:109] [3] PROFILING END[Backward]
I1226 13:03:49.236065 91895 solver.cpp:291] [3] Iteration 1, loss = 2.86852
I1226 13:03:49.236133 91895 solver.cpp:317]     Train net output #0: accuracy = 0.40625
I1226 13:03:49.236197 91895 solver.cpp:317]     Train net output #1: loss = 2.86852 (* 1 = 2.86852 loss)
I1226 13:03:49.236611 91895 MultiSolver.cpp:76] [3] PROFILING BEGIN[Forward]
I1226 13:03:49.297302 94300 MultiSolver.cpp:93] [1] PROFILING END[Forward]
I1226 13:03:49.297420 94300 MultiSolver.cpp:95] [1] PROFILING BEGIN[Backward]
I1226 13:03:49.353792 90983 MultiSolver.cpp:109] [7] PROFILING END[Backward]
I1226 13:03:49.353888 90983 solver.cpp:291] [7] Iteration 1, loss = 3.573
I1226 13:03:49.353965 90983 solver.cpp:317]     Train net output #0: accuracy = 0.34375
I1226 13:03:49.354039 90983 solver.cpp:317]     Train net output #1: loss = 3.573 (* 1 = 3.573 loss)
I1226 13:03:49.354395 90983 MultiSolver.cpp:76] [7] PROFILING BEGIN[Forward]
I1226 13:03:50.001621 94300 MultiSolver.cpp:109] [1] PROFILING END[Backward]
I1226 13:03:50.001729 94300 solver.cpp:291] [1] Iteration 1, loss = 3.12467
I1226 13:03:50.001808 94300 solver.cpp:317]     Train net output #0: accuracy = 0.15625
I1226 13:03:50.001955 94300 solver.cpp:317]     Train net output #1: loss = 3.12467 (* 1 = 3.12467 loss)
I1226 13:03:50.002154 94300 MultiSolver.cpp:76] [1] PROFILING BEGIN[Forward]
I1226 13:03:50.071918 96103 MultiSolver.cpp:109] [4] PROFILING END[Backward]
I1226 13:03:50.072022 96103 solver.cpp:291] [4] Iteration 1, loss = 3.19563
I1226 13:03:50.072110 96103 solver.cpp:317]     Train net output #0: accuracy = 0.3125
I1226 13:03:50.072186 96103 solver.cpp:317]     Train net output #1: loss = 3.19563 (* 1 = 3.19563 loss)
I1226 13:03:50.072356 96103 MultiSolver.cpp:76] [4] PROFILING BEGIN[Forward]
I1226 13:03:56.688901 93741 MultiSolver.cpp:93] [0] PROFILING END[Forward]
I1226 13:03:56.689007 93741 MultiSolver.cpp:95] [0] PROFILING BEGIN[Backward]
I1226 13:03:56.755332 93741 MultiSolver.cpp:109] [0] PROFILING END[Backward]
I1226 13:03:56.755463 93741 solver.cpp:291] [0] Iteration 2, loss = 4.07322
I1226 13:03:56.755517 93741 solver.cpp:317]     Train net output #0: accuracy = 0.1875
I1226 13:03:56.755568 93741 solver.cpp:317]     Train net output #1: loss = 4.07322 (* 1 = 4.07322 loss)
I1226 13:03:56.755614 93741 MultiSolver.cpp:76] [0] PROFILING BEGIN[Forward]
I1226 13:03:57.896561 85539 MultiSolver.cpp:93] [6] PROFILING END[Forward]
I1226 13:03:57.896723 85539 MultiSolver.cpp:95] [6] PROFILING BEGIN[Backward]
I1226 13:03:57.900260 92018 MultiSolver.cpp:93] [5] PROFILING END[Forward]
I1226 13:03:57.900383 92018 MultiSolver.cpp:95] [5] PROFILING BEGIN[Backward]
I1226 13:03:57.886667 122158 MultiSolver.cpp:93] [2] PROFILING END[Forward]
I1226 13:03:57.886838 122158 MultiSolver.cpp:95] [2] PROFILING BEGIN[Backward]
I1226 13:03:57.912716 94300 MultiSolver.cpp:93] [1] PROFILING END[Forward]
I1226 13:03:57.913717 94300 MultiSolver.cpp:95] [1] PROFILING BEGIN[Backward]
I1226 13:03:57.965056 122158 MultiSolver.cpp:109] [2] PROFILING END[Backward]
I1226 13:03:57.965304 122158 solver.cpp:291] [2] Iteration 2, loss = 3.43304
I1226 13:03:57.965407 122158 solver.cpp:317]     Train net output #0: accuracy = 0.25
I1226 13:03:57.965522 122158 solver.cpp:317]     Train net output #1: loss = 3.43304 (* 1 = 3.43304 loss)
I1226 13:03:57.965620 122158 MultiSolver.cpp:76] [2] PROFILING BEGIN[Forward]
I1226 13:03:58.008244 92018 MultiSolver.cpp:109] [5] PROFILING END[Backward]
I1226 13:03:58.008337 92018 solver.cpp:291] [5] Iteration 2, loss = 3.35553
I1226 13:03:58.008414 92018 solver.cpp:317]     Train net output #0: accuracy = 0.25
I1226 13:03:58.008491 92018 solver.cpp:317]     Train net output #1: loss = 3.35553 (* 1 = 3.35553 loss)
I1226 13:03:58.008576 92018 MultiSolver.cpp:76] [5] PROFILING BEGIN[Forward]
I1226 13:03:58.009013 85539 MultiSolver.cpp:109] [6] PROFILING END[Backward]
I1226 13:03:58.009104 85539 solver.cpp:291] [6] Iteration 2, loss = 3.72109
I1226 13:03:58.009179 85539 solver.cpp:317]     Train net output #0: accuracy = 0.21875
I1226 13:03:58.009256 85539 solver.cpp:317]     Train net output #1: loss = 3.72109 (* 1 = 3.72109 loss)
I1226 13:03:58.009551 85539 MultiSolver.cpp:76] [6] PROFILING BEGIN[Forward]
I1226 13:03:58.293750 91895 MultiSolver.cpp:93] [3] PROFILING END[Forward]
I1226 13:03:58.293874 91895 MultiSolver.cpp:95] [3] PROFILING BEGIN[Backward]
I1226 13:03:58.302124 90983 MultiSolver.cpp:93] [7] PROFILING END[Forward]
I1226 13:03:58.302320 90983 MultiSolver.cpp:95] [7] PROFILING BEGIN[Backward]
I1226 13:03:58.323197 96103 MultiSolver.cpp:93] [4] PROFILING END[Forward]
I1226 13:03:58.324228 96103 MultiSolver.cpp:95] [4] PROFILING BEGIN[Backward]
I1226 13:03:58.348193 91895 MultiSolver.cpp:109] [3] PROFILING END[Backward]
I1226 13:03:58.348278 91895 solver.cpp:291] [3] Iteration 2, loss = 3.39195
I1226 13:03:58.348373 91895 solver.cpp:317]     Train net output #0: accuracy = 0.25
I1226 13:03:58.348434 91895 solver.cpp:317]     Train net output #1: loss = 3.39195 (* 1 = 3.39195 loss)
I1226 13:03:58.348526 91895 MultiSolver.cpp:76] [3] PROFILING BEGIN[Forward]
I1226 13:03:58.403861 90983 MultiSolver.cpp:109] [7] PROFILING END[Backward]
I1226 13:03:58.403949 90983 solver.cpp:291] [7] Iteration 2, loss = 2.87834
I1226 13:03:58.404024 90983 solver.cpp:317]     Train net output #0: accuracy = 0.375
I1226 13:03:58.404098 90983 solver.cpp:317]     Train net output #1: loss = 2.87834 (* 1 = 2.87834 loss)
I1226 13:03:58.404292 90983 MultiSolver.cpp:76] [7] PROFILING BEGIN[Forward]
I1226 13:03:58.395931 94300 MultiSolver.cpp:109] [1] PROFILING END[Backward]
I1226 13:03:58.396024 94300 solver.cpp:291] [1] Iteration 2, loss = 2.55185
I1226 13:03:58.396102 94300 solver.cpp:317]     Train net output #0: accuracy = 0.34375
I1226 13:03:58.396178 94300 solver.cpp:317]     Train net output #1: loss = 2.55185 (* 1 = 2.55185 loss)
I1226 13:03:58.396255 94300 MultiSolver.cpp:76] [1] PROFILING BEGIN[Forward]
I1226 13:03:58.853157 96103 MultiSolver.cpp:109] [4] PROFILING END[Backward]
I1226 13:03:58.853286 96103 solver.cpp:291] [4] Iteration 2, loss = 3.38186
I1226 13:03:58.853363 96103 solver.cpp:317]     Train net output #0: accuracy = 0.28125
I1226 13:03:58.853469 96103 solver.cpp:317]     Train net output #1: loss = 3.38186 (* 1 = 3.38186 loss)
I1226 13:03:58.853556 96103 MultiSolver.cpp:76] [4] PROFILING BEGIN[Forward]
I1226 13:04:05.556572 93741 MultiSolver.cpp:93] [0] PROFILING END[Forward]
I1226 13:04:05.556684 93741 MultiSolver.cpp:95] [0] PROFILING BEGIN[Backward]
I1226 13:04:05.621475 93741 MultiSolver.cpp:109] [0] PROFILING END[Backward]
I1226 13:04:05.621556 93741 solver.cpp:291] [0] Iteration 3, loss = 3.65623
I1226 13:04:05.621610 93741 solver.cpp:317]     Train net output #0: accuracy = 0.4375
I1226 13:04:05.621662 93741 solver.cpp:317]     Train net output #1: loss = 3.65623 (* 1 = 3.65623 loss)
I1226 13:04:05.621742 93741 MultiSolver.cpp:76] [0] PROFILING BEGIN[Forward]
I1226 13:04:06.923295 85539 MultiSolver.cpp:93] [6] PROFILING END[Forward]
I1226 13:04:06.926672 92018 MultiSolver.cpp:93] [5] PROFILING END[Forward]
I1226 13:04:06.923413 85539 MultiSolver.cpp:95] [6] PROFILING BEGIN[Backward]
I1226 13:04:06.926827 92018 MultiSolver.cpp:95] [5] PROFILING BEGIN[Backward]
I1226 13:04:06.913818 122158 MultiSolver.cpp:93] [2] PROFILING END[Forward]
I1226 13:04:06.914005 122158 MultiSolver.cpp:95] [2] PROFILING BEGIN[Backward]
I1226 13:04:06.939553 94300 MultiSolver.cpp:93] [1] PROFILING END[Forward]
I1226 13:04:06.939669 94300 MultiSolver.cpp:95] [1] PROFILING BEGIN[Backward]
I1226 13:04:06.991932 122158 MultiSolver.cpp:109] [2] PROFILING END[Backward]
I1226 13:04:06.992051 122158 solver.cpp:291] [2] Iteration 3, loss = 3.57926
I1226 13:04:06.992158 122158 solver.cpp:317]     Train net output #0: accuracy = 0.34375
I1226 13:04:06.992270 122158 solver.cpp:317]     Train net output #1: loss = 3.57926 (* 1 = 3.57926 loss)
I1226 13:04:06.992380 122158 MultiSolver.cpp:76] [2] PROFILING BEGIN[Forward]
I1226 13:04:07.035604 85539 MultiSolver.cpp:109] [6] PROFILING END[Backward]
I1226 13:04:07.035742 85539 solver.cpp:291] [6] Iteration 3, loss = 2.78271
I1226 13:04:07.035818 85539 solver.cpp:317]     Train net output #0: accuracy = 0.40625
I1226 13:04:07.035897 85539 solver.cpp:317]     Train net output #1: loss = 2.78271 (* 1 = 2.78271 loss)
I1226 13:04:07.035984 85539 MultiSolver.cpp:76] [6] PROFILING BEGIN[Forward]
I1226 13:04:07.045236 92018 MultiSolver.cpp:109] [5] PROFILING END[Backward]
I1226 13:04:07.045327 92018 solver.cpp:291] [5] Iteration 3, loss = 3.7088
I1226 13:04:07.045404 92018 solver.cpp:317]     Train net output #0: accuracy = 0.09375
I1226 13:04:07.045482 92018 solver.cpp:317]     Train net output #1: loss = 3.7088 (* 1 = 3.7088 loss)
I1226 13:04:07.045568 92018 MultiSolver.cpp:76] [5] PROFILING BEGIN[Forward]
I1226 13:04:07.178905 91895 MultiSolver.cpp:93] [3] PROFILING END[Forward]
I1226 13:04:07.179018 91895 MultiSolver.cpp:95] [3] PROFILING BEGIN[Backward]
I1226 13:04:07.186879 90983 MultiSolver.cpp:93] [7] PROFILING END[Forward]
I1226 13:04:07.186995 90983 MultiSolver.cpp:95] [7] PROFILING BEGIN[Backward]
I1226 13:04:07.184026 96103 MultiSolver.cpp:93] [4] PROFILING END[Forward]
I1226 13:04:07.184156 96103 MultiSolver.cpp:95] [4] PROFILING BEGIN[Backward]
I1226 13:04:07.233052 91895 MultiSolver.cpp:109] [3] PROFILING END[Backward]
I1226 13:04:07.233176 91895 solver.cpp:291] [3] Iteration 3, loss = 2.64931
I1226 13:04:07.233253 91895 solver.cpp:317]     Train net output #0: accuracy = 0.53125
I1226 13:04:07.233358 91895 solver.cpp:317]     Train net output #1: loss = 2.64931 (* 1 = 2.64931 loss)
I1226 13:04:07.233420 91895 MultiSolver.cpp:76] [3] PROFILING BEGIN[Forward]
I1226 13:04:07.290431 90983 MultiSolver.cpp:109] [7] PROFILING END[Backward]
I1226 13:04:07.290520 90983 solver.cpp:291] [7] Iteration 3, loss = 3.47425
I1226 13:04:07.290597 90983 solver.cpp:317]     Train net output #0: accuracy = 0.1875
I1226 13:04:07.290673 90983 solver.cpp:317]     Train net output #1: loss = 3.47425 (* 1 = 3.47425 loss)
I1226 13:04:07.290808 90983 MultiSolver.cpp:76] [7] PROFILING BEGIN[Forward]
I1226 13:04:07.424370 94300 MultiSolver.cpp:109] [1] PROFILING END[Backward]
I1226 13:04:07.424471 94300 solver.cpp:291] [1] Iteration 3, loss = 3.00108
I1226 13:04:07.424546 94300 solver.cpp:317]     Train net output #0: accuracy = 0.4375
I1226 13:04:07.424623 94300 solver.cpp:317]     Train net output #1: loss = 3.00108 (* 1 = 3.00108 loss)
I1226 13:04:07.424748 94300 MultiSolver.cpp:76] [1] PROFILING BEGIN[Forward]
I1226 13:04:07.721856 96103 MultiSolver.cpp:109] [4] PROFILING END[Backward]
I1226 13:04:07.721952 96103 solver.cpp:291] [4] Iteration 3, loss = 3.47808
I1226 13:04:07.722030 96103 solver.cpp:317]     Train net output #0: accuracy = 0.21875
I1226 13:04:07.722108 96103 solver.cpp:317]     Train net output #1: loss = 3.47808 (* 1 = 3.47808 loss)
I1226 13:04:07.722188 96103 MultiSolver.cpp:76] [4] PROFILING BEGIN[Forward]
I1226 13:04:14.755147 93741 MultiSolver.cpp:93] [0] PROFILING END[Forward]
I1226 13:04:14.755249 93741 MultiSolver.cpp:95] [0] PROFILING BEGIN[Backward]
I1226 13:04:14.823413 93741 MultiSolver.cpp:109] [0] PROFILING END[Backward]
I1226 13:04:14.823494 93741 solver.cpp:291] [0] Iteration 4, loss = 3.96137
I1226 13:04:14.823549 93741 solver.cpp:317]     Train net output #0: accuracy = 0.1875
I1226 13:04:14.823601 93741 solver.cpp:317]     Train net output #1: loss = 3.96137 (* 1 = 3.96137 loss)
I1226 13:04:14.823659 93741 MultiSolver.cpp:76] [0] PROFILING BEGIN[Forward]
I1226 13:04:16.061769 85539 MultiSolver.cpp:93] [6] PROFILING END[Forward]
I1226 13:04:16.061894 85539 MultiSolver.cpp:95] [6] PROFILING BEGIN[Backward]
I1226 13:04:16.065336 92018 MultiSolver.cpp:93] [5] PROFILING END[Forward]
I1226 13:04:16.065450 92018 MultiSolver.cpp:95] [5] PROFILING BEGIN[Backward]
I1226 13:04:16.051278 122158 MultiSolver.cpp:93] [2] PROFILING END[Forward]
I1226 13:04:16.051414 122158 MultiSolver.cpp:95] [2] PROFILING BEGIN[Backward]
I1226 13:04:16.078927 94300 MultiSolver.cpp:93] [1] PROFILING END[Forward]
I1226 13:04:16.079040 94300 MultiSolver.cpp:95] [1] PROFILING BEGIN[Backward]
I1226 13:04:16.128706 122158 MultiSolver.cpp:109] [2] PROFILING END[Backward]
I1226 13:04:16.128832 122158 solver.cpp:291] [2] Iteration 4, loss = 3.52482
I1226 13:04:16.128939 122158 solver.cpp:317]     Train net output #0: accuracy = 0.21875
I1226 13:04:16.129051 122158 solver.cpp:317]     Train net output #1: loss = 3.52482 (* 1 = 3.52482 loss)
I1226 13:04:16.129192 122158 MultiSolver.cpp:76] [2] PROFILING BEGIN[Forward]
I1226 13:04:16.181664 92018 MultiSolver.cpp:109] [5] PROFILING END[Backward]
I1226 13:04:16.182932 92018 solver.cpp:291] [5] Iteration 4, loss = 3.02693
I1226 13:04:16.183028 92018 solver.cpp:317]     Train net output #0: accuracy = 0.40625
I1226 13:04:16.183111 92018 solver.cpp:317]     Train net output #1: loss = 3.02693 (* 1 = 3.02693 loss)
I1226 13:04:16.183197 92018 MultiSolver.cpp:76] [5] PROFILING BEGIN[Forward]
I1226 13:04:16.183131 85539 MultiSolver.cpp:109] [6] PROFILING END[Backward]
I1226 13:04:16.184483 85539 solver.cpp:291] [6] Iteration 4, loss = 3.35713
I1226 13:04:16.184751 85539 solver.cpp:317]     Train net output #0: accuracy = 0.125
I1226 13:04:16.184835 85539 solver.cpp:317]     Train net output #1: loss = 3.35713 (* 1 = 3.35713 loss)
I1226 13:04:16.184926 85539 MultiSolver.cpp:76] [6] PROFILING BEGIN[Forward]
I1226 13:04:16.390637 91895 MultiSolver.cpp:93] [3] PROFILING END[Forward]
I1226 13:04:16.391798 91895 MultiSolver.cpp:95] [3] PROFILING BEGIN[Backward]
I1226 13:04:16.398720 90983 MultiSolver.cpp:93] [7] PROFILING END[Forward]
I1226 13:04:16.399909 90983 MultiSolver.cpp:95] [7] PROFILING BEGIN[Backward]
I1226 13:04:16.395671 96103 MultiSolver.cpp:93] [4] PROFILING END[Forward]
I1226 13:04:16.395838 96103 MultiSolver.cpp:95] [4] PROFILING BEGIN[Backward]
I1226 13:04:16.447264 91895 MultiSolver.cpp:109] [3] PROFILING END[Backward]
I1226 13:04:16.447391 91895 solver.cpp:291] [3] Iteration 4, loss = 2.83174
I1226 13:04:16.447460 91895 solver.cpp:317]     Train net output #0: accuracy = 0.375
I1226 13:04:16.447526 91895 solver.cpp:317]     Train net output #1: loss = 2.83174 (* 1 = 2.83174 loss)
I1226 13:04:16.447592 91895 MultiSolver.cpp:76] [3] PROFILING BEGIN[Forward]
I1226 13:04:16.499425 90983 MultiSolver.cpp:109] [7] PROFILING END[Backward]
I1226 13:04:16.499517 90983 solver.cpp:291] [7] Iteration 4, loss = 3.13621
I1226 13:04:16.499594 90983 solver.cpp:317]     Train net output #0: accuracy = 0.34375
I1226 13:04:16.499670 90983 solver.cpp:317]     Train net output #1: loss = 3.13621 (* 1 = 3.13621 loss)
I1226 13:04:16.499788 90983 MultiSolver.cpp:76] [7] PROFILING BEGIN[Forward]
I1226 13:04:16.563980 94300 MultiSolver.cpp:109] [1] PROFILING END[Backward]
I1226 13:04:16.564079 94300 solver.cpp:291] [1] Iteration 4, loss = 3.52669
I1226 13:04:16.564157 94300 solver.cpp:317]     Train net output #0: accuracy = 0.3125
I1226 13:04:16.564231 94300 solver.cpp:317]     Train net output #1: loss = 3.52669 (* 1 = 3.52669 loss)
I1226 13:04:16.564306 94300 MultiSolver.cpp:76] [1] PROFILING BEGIN[Forward]
I1226 13:04:16.946063 96103 MultiSolver.cpp:109] [4] PROFILING END[Backward]
I1226 13:04:16.946158 96103 solver.cpp:291] [4] Iteration 4, loss = 4.04951
I1226 13:04:16.946236 96103 solver.cpp:317]     Train net output #0: accuracy = 0.15625
I1226 13:04:16.946313 96103 solver.cpp:317]     Train net output #1: loss = 4.04951 (* 1 = 4.04951 loss)
I1226 13:04:16.946394 96103 MultiSolver.cpp:76] [4] PROFILING BEGIN[Forward]
I1226 13:04:24.058857 93741 MultiSolver.cpp:93] [0] PROFILING END[Forward]
I1226 13:04:24.060050 93741 MultiSolver.cpp:95] [0] PROFILING BEGIN[Backward]
I1226 13:04:24.129601 93741 MultiSolver.cpp:109] [0] PROFILING END[Backward]
I1226 13:04:24.129683 93741 solver.cpp:291] [0] Iteration 5, loss = 3.16228
I1226 13:04:24.129739 93741 solver.cpp:317]     Train net output #0: accuracy = 0.21875
I1226 13:04:24.129789 93741 solver.cpp:317]     Train net output #1: loss = 3.16228 (* 1 = 3.16228 loss)
I1226 13:04:24.129838 93741 MultiSolver.cpp:76] [0] PROFILING BEGIN[Forward]
I1226 13:04:25.362547 85539 MultiSolver.cpp:93] [6] PROFILING END[Forward]
I1226 13:04:25.366219 92018 MultiSolver.cpp:93] [5] PROFILING END[Forward]
I1226 13:04:25.362709 85539 MultiSolver.cpp:95] [6] PROFILING BEGIN[Backward]
I1226 13:04:25.366334 92018 MultiSolver.cpp:95] [5] PROFILING BEGIN[Backward]
I1226 13:04:25.352108 122158 MultiSolver.cpp:93] [2] PROFILING END[Forward]
I1226 13:04:25.353572 122158 MultiSolver.cpp:95] [2] PROFILING BEGIN[Backward]
I1226 13:04:25.380151 94300 MultiSolver.cpp:93] [1] PROFILING END[Forward]
I1226 13:04:25.380269 94300 MultiSolver.cpp:95] [1] PROFILING BEGIN[Backward]
I1226 13:04:25.433485 122158 MultiSolver.cpp:109] [2] PROFILING END[Backward]
I1226 13:04:25.433603 122158 solver.cpp:291] [2] Iteration 5, loss = 3.12003
I1226 13:04:25.433753 122158 solver.cpp:317]     Train net output #0: accuracy = 0.3125
I1226 13:04:25.433867 122158 solver.cpp:317]     Train net output #1: loss = 3.12003 (* 1 = 3.12003 loss)
I1226 13:04:25.433984 122158 MultiSolver.cpp:76] [2] PROFILING BEGIN[Forward]
I1226 13:04:25.480914 92018 MultiSolver.cpp:109] [5] PROFILING END[Backward]
I1226 13:04:25.481001 92018 solver.cpp:291] [5] Iteration 5, loss = 3.10456
I1226 13:04:25.481077 92018 solver.cpp:317]     Train net output #0: accuracy = 0.34375
I1226 13:04:25.481153 92018 solver.cpp:317]     Train net output #1: loss = 3.10456 (* 1 = 3.10456 loss)
I1226 13:04:25.481248 92018 MultiSolver.cpp:76] [5] PROFILING BEGIN[Forward]
I1226 13:04:25.479213 85539 MultiSolver.cpp:109] [6] PROFILING END[Backward]
I1226 13:04:25.479302 85539 solver.cpp:291] [6] Iteration 5, loss = 3.15397
I1226 13:04:25.479383 85539 solver.cpp:317]     Train net output #0: accuracy = 0.34375
I1226 13:04:25.479460 85539 solver.cpp:317]     Train net output #1: loss = 3.15397 (* 1 = 3.15397 loss)
I1226 13:04:25.479552 85539 MultiSolver.cpp:76] [6] PROFILING BEGIN[Forward]
I1226 13:04:25.764889 91895 MultiSolver.cpp:93] [3] PROFILING END[Forward]
I1226 13:04:25.772948 90983 MultiSolver.cpp:93] [7] PROFILING END[Forward]
I1226 13:04:25.765003 91895 MultiSolver.cpp:95] [3] PROFILING BEGIN[Backward]
I1226 13:04:25.773069 90983 MultiSolver.cpp:95] [7] PROFILING BEGIN[Backward]
I1226 13:04:25.785945 96103 MultiSolver.cpp:93] [4] PROFILING END[Forward]
I1226 13:04:25.786072 96103 MultiSolver.cpp:95] [4] PROFILING BEGIN[Backward]
I1226 13:04:25.815521 91895 MultiSolver.cpp:109] [3] PROFILING END[Backward]
I1226 13:04:25.815603 91895 solver.cpp:291] [3] Iteration 5, loss = 2.83504
I1226 13:04:25.815671 91895 solver.cpp:317]     Train net output #0: accuracy = 0.34375
I1226 13:04:25.815737 91895 solver.cpp:317]     Train net output #1: loss = 2.83504 (* 1 = 2.83504 loss)
I1226 13:04:25.815814 91895 MultiSolver.cpp:76] [3] PROFILING BEGIN[Forward]
I1226 13:04:25.874622 90983 MultiSolver.cpp:109] [7] PROFILING END[Backward]
I1226 13:04:25.874733 90983 solver.cpp:291] [7] Iteration 5, loss = 3.45003
I1226 13:04:25.874807 90983 solver.cpp:317]     Train net output #0: accuracy = 0.21875
I1226 13:04:25.874883 90983 solver.cpp:317]     Train net output #1: loss = 3.45003 (* 1 = 3.45003 loss)
I1226 13:04:25.874968 90983 MultiSolver.cpp:76] [7] PROFILING BEGIN[Forward]
I1226 13:04:25.867169 94300 MultiSolver.cpp:109] [1] PROFILING END[Backward]
I1226 13:04:25.867264 94300 solver.cpp:291] [1] Iteration 5, loss = 3.60539
I1226 13:04:25.867341 94300 solver.cpp:317]     Train net output #0: accuracy = 0.25
I1226 13:04:25.867422 94300 solver.cpp:317]     Train net output #1: loss = 3.60539 (* 1 = 3.60539 loss)
I1226 13:04:25.867501 94300 MultiSolver.cpp:76] [1] PROFILING BEGIN[Forward]
I1226 13:04:26.310622 96103 MultiSolver.cpp:109] [4] PROFILING END[Backward]
I1226 13:04:26.310724 96103 solver.cpp:291] [4] Iteration 5, loss = 2.69164
I1226 13:04:26.310834 96103 solver.cpp:317]     Train net output #0: accuracy = 0.40625
I1226 13:04:26.310912 96103 solver.cpp:317]     Train net output #1: loss = 2.69164 (* 1 = 2.69164 loss)
I1226 13:04:26.310997 96103 MultiSolver.cpp:76] [4] PROFILING BEGIN[Forward]
I1226 13:04:33.342061 93741 MultiSolver.cpp:93] [0] PROFILING END[Forward]
I1226 13:04:33.342166 93741 MultiSolver.cpp:95] [0] PROFILING BEGIN[Backward]
I1226 13:04:33.409656 93741 MultiSolver.cpp:109] [0] PROFILING END[Backward]
I1226 13:04:33.409737 93741 solver.cpp:291] [0] Iteration 6, loss = 3.72732
I1226 13:04:33.409792 93741 solver.cpp:317]     Train net output #0: accuracy = 0.25
I1226 13:04:33.409844 93741 solver.cpp:317]     Train net output #1: loss = 3.72732 (* 1 = 3.72732 loss)
I1226 13:04:33.409905 93741 MultiSolver.cpp:76] [0] PROFILING BEGIN[Forward]
I1226 13:04:34.610385 85539 MultiSolver.cpp:93] [6] PROFILING END[Forward]
I1226 13:04:34.610502 85539 MultiSolver.cpp:95] [6] PROFILING BEGIN[Backward]
I1226 13:04:34.614006 92018 MultiSolver.cpp:93] [5] PROFILING END[Forward]
I1226 13:04:34.614131 92018 MultiSolver.cpp:95] [5] PROFILING BEGIN[Backward]
I1226 13:04:34.600091 122158 MultiSolver.cpp:93] [2] PROFILING END[Forward]
I1226 13:04:34.600234 122158 MultiSolver.cpp:95] [2] PROFILING BEGIN[Backward]
I1226 13:04:34.627328 94300 MultiSolver.cpp:93] [1] PROFILING END[Forward]
I1226 13:04:34.628000 94300 MultiSolver.cpp:95] [1] PROFILING BEGIN[Backward]
I1226 13:04:34.677296 122158 MultiSolver.cpp:109] [2] PROFILING END[Backward]
I1226 13:04:34.677455 122158 solver.cpp:291] [2] Iteration 6, loss = 2.56845
I1226 13:04:34.677865 122158 solver.cpp:317]     Train net output #0: accuracy = 0.40625
I1226 13:04:34.678000 122158 solver.cpp:317]     Train net output #1: loss = 2.56845 (* 1 = 2.56845 loss)
I1226 13:04:34.678107 122158 MultiSolver.cpp:76] [2] PROFILING BEGIN[Forward]
I1226 13:04:34.724012 85539 MultiSolver.cpp:109] [6] PROFILING END[Backward]
I1226 13:04:34.727481 92018 MultiSolver.cpp:109] [5] PROFILING END[Backward]
I1226 13:04:34.724107 85539 solver.cpp:291] [6] Iteration 6, loss = 3.39122
I1226 13:04:34.727573 92018 solver.cpp:291] [5] Iteration 6, loss = 3.6052
I1226 13:04:34.724184 85539 solver.cpp:317]     Train net output #0: accuracy = 0.3125
I1226 13:04:34.727650 92018 solver.cpp:317]     Train net output #0: accuracy = 0.1875
I1226 13:04:34.724263 85539 solver.cpp:317]     Train net output #1: loss = 3.39122 (* 1 = 3.39122 loss)
I1226 13:04:34.727728 92018 solver.cpp:317]     Train net output #1: loss = 3.6052 (* 1 = 3.6052 loss)
I1226 13:04:34.724349 85539 MultiSolver.cpp:76] [6] PROFILING BEGIN[Forward]
I1226 13:04:34.727841 92018 MultiSolver.cpp:76] [5] PROFILING BEGIN[Forward]
I1226 13:04:35.108073 94300 MultiSolver.cpp:109] [1] PROFILING END[Backward]
I1226 13:04:35.108165 94300 solver.cpp:291] [1] Iteration 6, loss = 3.59348
I1226 13:04:35.108230 94300 solver.cpp:317]     Train net output #0: accuracy = 0.28125
I1226 13:04:35.108296 94300 solver.cpp:317]     Train net output #1: loss = 3.59348 (* 1 = 3.59348 loss)
I1226 13:04:35.108364 94300 MultiSolver.cpp:76] [1] PROFILING BEGIN[Forward]
I1226 13:04:35.110393 91895 MultiSolver.cpp:93] [3] PROFILING END[Forward]
I1226 13:04:35.110508 91895 MultiSolver.cpp:95] [3] PROFILING BEGIN[Backward]
I1226 13:04:35.119029 90983 MultiSolver.cpp:93] [7] PROFILING END[Forward]
I1226 13:04:35.119254 90983 MultiSolver.cpp:95] [7] PROFILING BEGIN[Backward]
I1226 13:04:35.115710 96103 MultiSolver.cpp:93] [4] PROFILING END[Forward]
I1226 13:04:35.116446 96103 MultiSolver.cpp:95] [4] PROFILING BEGIN[Backward]
I1226 13:04:35.168521 91895 MultiSolver.cpp:109] [3] PROFILING END[Backward]
I1226 13:04:35.168609 91895 solver.cpp:291] [3] Iteration 6, loss = 2.6398
I1226 13:04:35.168678 91895 solver.cpp:317]     Train net output #0: accuracy = 0.4375
I1226 13:04:35.168747 91895 solver.cpp:317]     Train net output #1: loss = 2.6398 (* 1 = 2.6398 loss)
I1226 13:04:35.168817 91895 MultiSolver.cpp:76] [3] PROFILING BEGIN[Forward]
I1226 13:04:35.219300 90983 MultiSolver.cpp:109] [7] PROFILING END[Backward]
I1226 13:04:35.219414 90983 solver.cpp:291] [7] Iteration 6, loss = 2.82838
I1226 13:04:35.219488 90983 solver.cpp:317]     Train net output #0: accuracy = 0.21875
I1226 13:04:35.219564 90983 solver.cpp:317]     Train net output #1: loss = 2.82838 (* 1 = 2.82838 loss)
I1226 13:04:35.219669 90983 MultiSolver.cpp:76] [7] PROFILING BEGIN[Forward]
I1226 13:04:35.643548 96103 MultiSolver.cpp:109] [4] PROFILING END[Backward]
I1226 13:04:35.643647 96103 solver.cpp:291] [4] Iteration 6, loss = 3.6593
I1226 13:04:35.643726 96103 solver.cpp:317]     Train net output #0: accuracy = 0.21875
I1226 13:04:35.643833 96103 solver.cpp:317]     Train net output #1: loss = 3.6593 (* 1 = 3.6593 loss)
I1226 13:04:35.643906 96103 MultiSolver.cpp:76] [4] PROFILING BEGIN[Forward]
I1226 13:04:42.580891 93741 MultiSolver.cpp:93] [0] PROFILING END[Forward]
I1226 13:04:42.580992 93741 MultiSolver.cpp:95] [0] PROFILING BEGIN[Backward]
I1226 13:04:42.645794 93741 MultiSolver.cpp:109] [0] PROFILING END[Backward]
I1226 13:04:42.645876 93741 solver.cpp:291] [0] Iteration 7, loss = 2.88747
I1226 13:04:42.645931 93741 solver.cpp:317]     Train net output #0: accuracy = 0.34375
I1226 13:04:42.646008 93741 solver.cpp:317]     Train net output #1: loss = 2.88747 (* 1 = 2.88747 loss)
I1226 13:04:42.646092 93741 MultiSolver.cpp:76] [0] PROFILING BEGIN[Forward]
User defined signal 2
