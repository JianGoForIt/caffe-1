Dec 26 09:06:49 2016 120047 3 10.1 NIOS_DEBUG: stdin_fd set to -1
Dec 26 09:06:49 2016 120047 3 10.1 NIOS_DEBUG: fds[0] has a value of -1
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 09:06:49.814494 90317 mpiutil.cpp:166] Process rank 6 from number of 9 processes running on knl-node048
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 09:06:53.084398 90178 mpiutil.cpp:166] Process rank 2 from number of 9 processes running on knl-node038
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 09:06:53.073773 120057 mpiutil.cpp:166] Process rank 0 from number of 9 processes running on knl-node035
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 09:06:53.076998 91281 mpiutil.cpp:166] Process rank 4 from number of 9 processes running on knl-node051
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 09:06:53.083389 94464 mpiutil.cpp:166] Process rank 1 from number of 9 processes running on knl-node079
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 09:06:53.079970 96748 mpiutil.cpp:166] Process rank 3 from number of 9 processes running on knl-node016
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 09:06:53.080478 88808 mpiutil.cpp:166] Process rank 7 from number of 9 processes running on knl-node054
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 09:06:53.079785 117050 mpiutil.cpp:166] Process rank 5 from number of 9 processes running on knl-node042
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 09:06:53.087255 120755 mpiutil.cpp:166] Process rank 8 from number of 9 processes running on knl-node013
I1226 09:06:49.827775 90317 caffe.cpp:316] Use CPU.
I1226 09:06:53.090142 91281 caffe.cpp:316] Use CPU.
I1226 09:06:53.097105 94464 caffe.cpp:316] Use CPU.
I1226 09:06:53.091289 91281 solver.cpp:105] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_5.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 09:06:49.828917 90317 solver.cpp:105] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_7.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 09:06:53.088062 120057 caffe.cpp:316] Use CPU.
I1226 09:06:53.092181 91281 solver.cpp:140] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_5.prototxt
I1226 09:06:49.829877 90317 solver.cpp:140] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_7.prototxt
I1226 09:06:53.099635 90178 caffe.cpp:316] Use CPU.
I1226 09:06:53.098526 94464 solver.cpp:105] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_2.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 09:06:53.089238 120057 solver.cpp:105] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_1.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 09:06:53.099678 94464 solver.cpp:140] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_2.prototxt
I1226 09:06:53.090227 120057 solver.cpp:140] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_1.prototxt
I1226 09:06:53.095674 96748 caffe.cpp:316] Use CPU.
I1226 09:06:53.100970 90178 solver.cpp:105] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_3.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 09:06:53.101964 90178 solver.cpp:140] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_3.prototxt
I1226 09:06:53.096180 117050 caffe.cpp:316] Use CPU.
I1226 09:06:53.096886 96748 solver.cpp:105] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_4.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 09:06:53.097861 96748 solver.cpp:140] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_4.prototxt
I1226 09:06:53.097640 117050 solver.cpp:105] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_6.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 09:06:53.105262 120755 caffe.cpp:316] Use CPU.
I1226 09:06:53.098708 117050 solver.cpp:140] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_6.prototxt
I1226 09:06:53.099050 88808 caffe.cpp:316] Use CPU.
I1226 09:06:53.106561 120755 solver.cpp:105] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_dummy.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 09:06:53.107688 120755 solver.cpp:140] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_dummy.prototxt
I1226 09:06:53.100296 88808 solver.cpp:105] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_8.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 09:06:53.101408 88808 solver.cpp:140] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_8.prototxt
I1226 09:06:53.121105 91281 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 09:06:53.121184 91281 cpu_info.cpp:455] Total number of sockets: 1
I1226 09:06:53.121215 91281 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 09:06:53.121242 91281 cpu_info.cpp:461] Total number of processors: 272
I1226 09:06:53.121268 91281 cpu_info.cpp:464] GPU is used: no
I1226 09:06:53.121296 91281 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 09:06:53.121325 91281 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 09:06:49.860342 90317 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 09:06:49.860416 90317 cpu_info.cpp:455] Total number of sockets: 1
I1226 09:06:49.860440 90317 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 09:06:49.860460 90317 cpu_info.cpp:461] Total number of processors: 272
I1226 09:06:49.860479 90317 cpu_info.cpp:464] GPU is used: no
I1226 09:06:49.860499 90317 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 09:06:49.860520 90317 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 09:06:53.121197 120057 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 09:06:53.121269 120057 cpu_info.cpp:455] Total number of sockets: 1
I1226 09:06:53.132261 90178 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 09:06:53.132339 90178 cpu_info.cpp:455] Total number of sockets: 1
I1226 09:06:53.121299 120057 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 09:06:53.121325 120057 cpu_info.cpp:461] Total number of processors: 272
I1226 09:06:53.121352 120057 cpu_info.cpp:464] GPU is used: no
I1226 09:06:53.121381 120057 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 09:06:53.132362 90178 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 09:06:53.132382 90178 cpu_info.cpp:461] Total number of processors: 272
I1226 09:06:53.132403 90178 cpu_info.cpp:464] GPU is used: no
I1226 09:06:53.121408 120057 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 09:06:53.132423 90178 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 09:06:53.132444 90178 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 09:06:53.129130 96748 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 09:06:53.129214 96748 cpu_info.cpp:455] Total number of sockets: 1
I1226 09:06:53.129236 96748 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 09:06:53.129257 96748 cpu_info.cpp:461] Total number of processors: 272
I1226 09:06:53.129277 96748 cpu_info.cpp:464] GPU is used: no
I1226 09:06:53.129297 96748 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 09:06:53.129325 96748 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 09:06:53.130121 117050 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 09:06:53.130203 117050 cpu_info.cpp:455] Total number of sockets: 1
I1226 09:06:53.130226 117050 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 09:06:53.130247 117050 cpu_info.cpp:461] Total number of processors: 272
I1226 09:06:53.130267 117050 cpu_info.cpp:464] GPU is used: no
I1226 09:06:53.130287 117050 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 09:06:53.130378 117050 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 09:06:53.136509 94464 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 09:06:53.136620 94464 cpu_info.cpp:455] Total number of sockets: 1
I1226 09:06:53.136656 94464 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 09:06:53.136689 94464 cpu_info.cpp:461] Total number of processors: 272
I1226 09:06:53.136720 94464 cpu_info.cpp:464] GPU is used: no
I1226 09:06:53.136750 94464 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 09:06:53.136780 94464 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 09:06:53.134593 88808 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 09:06:53.134665 88808 cpu_info.cpp:455] Total number of sockets: 1
I1226 09:06:53.144281 120755 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 09:06:53.144353 120755 cpu_info.cpp:455] Total number of sockets: 1
I1226 09:06:53.134687 88808 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 09:06:53.134707 88808 cpu_info.cpp:461] Total number of processors: 272
I1226 09:06:53.134727 88808 cpu_info.cpp:464] GPU is used: no
I1226 09:06:53.144376 120755 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 09:06:53.144395 120755 cpu_info.cpp:461] Total number of processors: 272
I1226 09:06:53.144414 120755 cpu_info.cpp:464] GPU is used: no
I1226 09:06:53.144433 120755 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 09:06:53.134748 88808 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 09:06:53.144482 120755 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 09:06:53.134768 88808 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 09:06:53.160683 94464 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 09:06:53.171180 117050 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 09:06:53.171300 91281 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 09:06:53.161058 94464 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 09:06:49.910406 90317 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 09:06:53.171645 117050 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 09:06:53.185267 120755 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 09:06:53.171639 91281 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 09:06:53.163146 94464 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_1"
    batch_size: 32
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 09:06:49.910840 90317 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 09:06:53.173835 117050 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_5"
    batch_size: 32
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 09:06:53.185581 120755 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 09:06:53.173192 91281 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_4"
    batch_size: 32
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 09:06:49.913147 90317 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_6"
    batch_size: 32
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 09:06:53.187726 120755 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "DummyData"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  dummy_data_param {
    data_filler {
      type: "gaussian"
      std: 1
    }
    data_filler {
      type: "constant"
      value: 0
    }
    shape {
      dim: 32
      dim: 3
      dim: 227
      dim: 227
    }
    shape {
      dim: 32
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 09:06:53.193753 90178 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 09:06:53.194200 90178 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 09:06:53.191153 96748 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 09:06:53.191449 96748 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 09:06:53.196465 90178 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_2"
    batch_size: 32
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 09:06:53.193472 96748 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_3"
    batch_size: 32
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 09:06:53.195024 117050 layer_factory.hpp:114] Creating layer data
I1226 09:06:53.189934 120057 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 09:06:53.190255 120057 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 09:06:53.192261 120057 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_0"
    batch_size: 32
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 09:06:53.211958 117050 net.cpp:178] Creating Layer data
I1226 09:06:53.212071 117050 net.cpp:586] data -> data
I1226 09:06:53.212152 117050 net.cpp:586] data -> label
I1226 09:06:53.223191 120755 layer_factory.hpp:114] Creating layer data
I1226 09:06:53.223372 120755 net.cpp:178] Creating Layer data
I1226 09:06:53.223417 120755 net.cpp:586] data -> data
I1226 09:06:53.223507 120755 net.cpp:586] data -> label
I1226 09:06:53.218925 88808 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 09:06:53.219282 88808 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 09:06:53.221469 88808 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_7"
    batch_size: 32
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 09:06:49.958796 90317 layer_factory.hpp:114] Creating layer data
I1226 09:06:53.247521 90178 layer_factory.hpp:114] Creating layer data
I1226 09:06:53.247256 117052 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_5
I1226 09:06:49.984071 90317 net.cpp:178] Creating Layer data
I1226 09:06:49.984174 90317 net.cpp:586] data -> data
I1226 09:06:49.984274 90317 net.cpp:586] data -> label
I1226 09:06:53.258146 90178 net.cpp:178] Creating Layer data
I1226 09:06:53.258222 90178 net.cpp:586] data -> data
I1226 09:06:53.258299 90178 net.cpp:586] data -> label
I1226 09:06:53.251662 91281 layer_factory.hpp:114] Creating layer data
I1226 09:06:53.268059 117050 data_layer.cpp:80] output data size: 32,3,227,227
I1226 09:06:53.271723 94464 layer_factory.hpp:114] Creating layer data
I1226 09:06:53.277281 120755 net.cpp:228] Setting up data
I1226 09:06:53.277422 120755 net.cpp:235] Top shape: 32 3 227 227 (4946784)
I1226 09:06:53.277458 120755 net.cpp:235] Top shape: 32 1 1 1 (32)
I1226 09:06:53.277508 120755 net.cpp:243] Memory required for data: 19787264
I1226 09:06:53.277544 120755 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 09:06:53.277627 120755 net.cpp:178] Creating Layer label_data_1_split
I1226 09:06:53.277732 120755 net.cpp:612] label_data_1_split <- label
I1226 09:06:53.277767 120755 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 09:06:53.277813 120755 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 09:06:53.269412 91281 net.cpp:178] Creating Layer data
I1226 09:06:53.269502 91281 net.cpp:586] data -> data
I1226 09:06:53.269582 91281 net.cpp:586] data -> label
I1226 09:06:53.288411 120755 net.cpp:228] Setting up label_data_1_split
I1226 09:06:53.288512 120755 net.cpp:235] Top shape: 32 1 1 1 (32)
I1226 09:06:53.288542 120755 net.cpp:235] Top shape: 32 1 1 1 (32)
I1226 09:06:53.288563 120755 net.cpp:243] Memory required for data: 19787520
I1226 09:06:53.288625 120755 layer_factory.hpp:114] Creating layer conv1
I1226 09:06:53.288724 120755 net.cpp:178] Creating Layer conv1
I1226 09:06:53.288816 120755 net.cpp:612] conv1 <- data
I1226 09:06:53.288856 120755 net.cpp:586] conv1 -> conv1
I1226 09:06:53.286605 94464 net.cpp:178] Creating Layer data
I1226 09:06:53.286733 94464 net.cpp:586] data -> data
I1226 09:06:53.286897 94464 net.cpp:586] data -> label
I1226 09:06:53.278884 120057 layer_factory.hpp:114] Creating layer data
I1226 09:06:53.285939 96748 layer_factory.hpp:114] Creating layer data
I1226 09:06:53.290455 120057 net.cpp:178] Creating Layer data
I1226 09:06:53.290540 120057 net.cpp:586] data -> data
I1226 09:06:53.290640 120057 net.cpp:586] data -> label
I1226 09:06:53.297832 96748 net.cpp:178] Creating Layer data
I1226 09:06:53.297919 96748 net.cpp:586] data -> data
I1226 09:06:53.297998 96748 net.cpp:586] data -> label
I1226 09:06:50.033205 90319 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_6
I1226 09:06:53.302920 88808 layer_factory.hpp:114] Creating layer data
I1226 09:06:53.304191 91283 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_4
I1226 09:06:53.315104 90180 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_2
I1226 09:06:53.311229 88808 net.cpp:178] Creating Layer data
I1226 09:06:53.311318 88808 net.cpp:586] data -> data
I1226 09:06:53.311398 88808 net.cpp:586] data -> label
I1226 09:06:50.048141 90317 data_layer.cpp:80] output data size: 32,3,227,227
I1226 09:06:53.323515 90178 data_layer.cpp:80] output data size: 32,3,227,227
I1226 09:06:53.327411 94466 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_1
I1226 09:06:53.335204 94464 data_layer.cpp:80] output data size: 32,3,227,227
I1226 09:06:53.330879 91281 data_layer.cpp:80] output data size: 32,3,227,227
I1226 09:06:53.338099 96750 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_3
I1226 09:06:53.345214 117050 net.cpp:228] Setting up data
I1226 09:06:53.345324 117050 net.cpp:235] Top shape: 32 3 227 227 (4946784)
I1226 09:06:53.345357 117050 net.cpp:235] Top shape: 32 (32)
I1226 09:06:53.345381 117050 net.cpp:243] Memory required for data: 19787264
I1226 09:06:53.345417 117050 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 09:06:53.345603 117050 net.cpp:178] Creating Layer label_data_1_split
I1226 09:06:53.345648 117050 net.cpp:612] label_data_1_split <- label
I1226 09:06:53.345685 117050 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 09:06:53.345736 117050 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 09:06:53.350585 120060 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_0
I1226 09:06:53.357453 117050 net.cpp:228] Setting up label_data_1_split
I1226 09:06:53.357580 117050 net.cpp:235] Top shape: 32 (32)
I1226 09:06:53.357615 117050 net.cpp:235] Top shape: 32 (32)
I1226 09:06:53.357640 117050 net.cpp:243] Memory required for data: 19787520
I1226 09:06:53.357676 117050 layer_factory.hpp:114] Creating layer conv1
I1226 09:06:53.357779 117050 net.cpp:178] Creating Layer conv1
I1226 09:06:53.357815 117050 net.cpp:612] conv1 <- data
I1226 09:06:53.357856 117050 net.cpp:586] conv1 -> conv1
I1226 09:06:53.363112 88810 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_7
I1226 09:06:53.358000 120057 data_layer.cpp:80] output data size: 32,3,227,227
I1226 09:06:53.371290 88808 data_layer.cpp:80] output data size: 32,3,227,227
I1226 09:06:53.373670 96748 data_layer.cpp:80] output data size: 32,3,227,227
I1226 09:06:53.397292 120755 net.cpp:228] Setting up conv1
I1226 09:06:53.397404 120755 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 09:06:53.397429 120755 net.cpp:243] Memory required for data: 56958720
I1226 09:06:53.397572 120755 layer_factory.hpp:114] Creating layer relu1
I1226 09:06:53.397641 120755 net.cpp:178] Creating Layer relu1
I1226 09:06:53.397685 120755 net.cpp:612] relu1 <- conv1
I1226 09:06:53.397730 120755 net.cpp:573] relu1 -> conv1 (in-place)
I1226 09:06:53.397832 120755 net.cpp:228] Setting up relu1
I1226 09:06:53.397877 120755 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 09:06:53.397920 120755 net.cpp:243] Memory required for data: 94129920
I1226 09:06:53.397951 120755 layer_factory.hpp:114] Creating layer norm1
I1226 09:06:53.398017 120755 net.cpp:178] Creating Layer norm1
I1226 09:06:53.398114 120755 net.cpp:612] norm1 <- conv1
I1226 09:06:53.398149 120755 net.cpp:586] norm1 -> norm1
I1226 09:06:53.398226 120755 net.cpp:228] Setting up norm1
I1226 09:06:53.398267 120755 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 09:06:53.398289 120755 net.cpp:243] Memory required for data: 131301120
I1226 09:06:53.398315 120755 layer_factory.hpp:114] Creating layer pool1
I1226 09:06:53.398365 120755 net.cpp:178] Creating Layer pool1
I1226 09:06:53.398396 120755 net.cpp:612] pool1 <- norm1
I1226 09:06:53.398428 120755 net.cpp:586] pool1 -> pool1
I1226 09:06:53.398505 120755 net.cpp:228] Setting up pool1
I1226 09:06:53.398538 120755 net.cpp:235] Top shape: 32 96 27 27 (2239488)
I1226 09:06:53.398566 120755 net.cpp:243] Memory required for data: 140259072
I1226 09:06:53.398597 120755 layer_factory.hpp:114] Creating layer conv2
I1226 09:06:53.398665 120755 net.cpp:178] Creating Layer conv2
I1226 09:06:53.398690 120755 net.cpp:612] conv2 <- pool1
I1226 09:06:53.398737 120755 net.cpp:586] conv2 -> conv2
I1226 09:06:50.126595 90317 net.cpp:228] Setting up data
I1226 09:06:50.126718 90317 net.cpp:235] Top shape: 32 3 227 227 (4946784)
I1226 09:06:50.126760 90317 net.cpp:235] Top shape: 32 (32)
I1226 09:06:50.126785 90317 net.cpp:243] Memory required for data: 19787264
I1226 09:06:50.126833 90317 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 09:06:50.126921 90317 net.cpp:178] Creating Layer label_data_1_split
I1226 09:06:50.127053 90317 net.cpp:612] label_data_1_split <- label
I1226 09:06:50.127095 90317 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 09:06:50.127151 90317 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 09:06:53.401198 90178 net.cpp:228] Setting up data
I1226 09:06:53.401315 90178 net.cpp:235] Top shape: 32 3 227 227 (4946784)
I1226 09:06:53.401350 90178 net.cpp:235] Top shape: 32 (32)
I1226 09:06:53.401374 90178 net.cpp:243] Memory required for data: 19787264
I1226 09:06:53.401410 90178 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 09:06:53.401494 90178 net.cpp:178] Creating Layer label_data_1_split
I1226 09:06:53.401609 90178 net.cpp:612] label_data_1_split <- label
I1226 09:06:53.401649 90178 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 09:06:53.401701 90178 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 09:06:50.139509 90317 net.cpp:228] Setting up label_data_1_split
I1226 09:06:50.139645 90317 net.cpp:235] Top shape: 32 (32)
I1226 09:06:50.139680 90317 net.cpp:235] Top shape: 32 (32)
I1226 09:06:50.139706 90317 net.cpp:243] Memory required for data: 19787520
I1226 09:06:50.139756 90317 layer_factory.hpp:114] Creating layer conv1
I1226 09:06:50.139854 90317 net.cpp:178] Creating Layer conv1
I1226 09:06:50.139892 90317 net.cpp:612] conv1 <- data
I1226 09:06:50.139935 90317 net.cpp:586] conv1 -> conv1
I1226 09:06:53.417623 90178 net.cpp:228] Setting up label_data_1_split
I1226 09:06:53.417733 90178 net.cpp:235] Top shape: 32 (32)
I1226 09:06:53.417768 90178 net.cpp:235] Top shape: 32 (32)
I1226 09:06:53.417793 90178 net.cpp:243] Memory required for data: 19787520
I1226 09:06:53.417856 90178 layer_factory.hpp:114] Creating layer conv1
I1226 09:06:53.417959 90178 net.cpp:178] Creating Layer conv1
I1226 09:06:53.418001 90178 net.cpp:612] conv1 <- data
I1226 09:06:53.418045 90178 net.cpp:586] conv1 -> conv1
I1226 09:06:53.429888 91281 net.cpp:228] Setting up data
I1226 09:06:53.430045 91281 net.cpp:235] Top shape: 32 3 227 227 (4946784)
I1226 09:06:53.430130 91281 net.cpp:235] Top shape: 32 (32)
I1226 09:06:53.430196 91281 net.cpp:243] Memory required for data: 19787264
I1226 09:06:53.430259 91281 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 09:06:53.430441 91281 net.cpp:178] Creating Layer label_data_1_split
I1226 09:06:53.430610 91281 net.cpp:612] label_data_1_split <- label
I1226 09:06:53.430672 91281 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 09:06:53.430732 91281 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 09:06:53.436287 120057 net.cpp:228] Setting up data
I1226 09:06:53.436403 120057 net.cpp:235] Top shape: 32 3 227 227 (4946784)
I1226 09:06:53.436439 120057 net.cpp:235] Top shape: 32 (32)
I1226 09:06:53.436462 120057 net.cpp:243] Memory required for data: 19787264
I1226 09:06:53.436497 120057 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 09:06:53.436703 120057 net.cpp:178] Creating Layer label_data_1_split
I1226 09:06:53.436756 120057 net.cpp:612] label_data_1_split <- label
I1226 09:06:53.436801 120057 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 09:06:53.436852 120057 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 09:06:53.443867 88808 net.cpp:228] Setting up data
I1226 09:06:53.443979 88808 net.cpp:235] Top shape: 32 3 227 227 (4946784)
I1226 09:06:53.444011 88808 net.cpp:235] Top shape: 32 (32)
I1226 09:06:53.444062 88808 net.cpp:243] Memory required for data: 19787264
I1226 09:06:53.444098 88808 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 09:06:53.444183 88808 net.cpp:178] Creating Layer label_data_1_split
I1226 09:06:53.444304 88808 net.cpp:612] label_data_1_split <- label
I1226 09:06:53.444342 88808 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 09:06:53.444399 88808 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 09:06:53.450569 120057 net.cpp:228] Setting up label_data_1_split
I1226 09:06:53.450701 120057 net.cpp:235] Top shape: 32 (32)
I1226 09:06:53.450737 120057 net.cpp:235] Top shape: 32 (32)
I1226 09:06:53.450764 120057 net.cpp:243] Memory required for data: 19787520
I1226 09:06:53.450801 120057 layer_factory.hpp:114] Creating layer conv1
I1226 09:06:53.450942 120057 net.cpp:178] Creating Layer conv1
I1226 09:06:53.451161 120057 net.cpp:612] conv1 <- data
I1226 09:06:53.451215 120057 net.cpp:586] conv1 -> conv1
I1226 09:06:53.458139 91281 net.cpp:228] Setting up label_data_1_split
I1226 09:06:53.458268 91281 net.cpp:235] Top shape: 32 (32)
I1226 09:06:53.458322 91281 net.cpp:235] Top shape: 32 (32)
I1226 09:06:53.458358 91281 net.cpp:243] Memory required for data: 19787520
I1226 09:06:53.458446 91281 layer_factory.hpp:114] Creating layer conv1
I1226 09:06:53.458590 91281 net.cpp:178] Creating Layer conv1
I1226 09:06:53.458663 91281 net.cpp:612] conv1 <- data
I1226 09:06:53.458736 91281 net.cpp:586] conv1 -> conv1
I1226 09:06:53.462736 96748 net.cpp:228] Setting up data
I1226 09:06:53.462852 96748 net.cpp:235] Top shape: 32 3 227 227 (4946784)
I1226 09:06:53.462888 96748 net.cpp:235] Top shape: 32 (32)
I1226 09:06:53.462940 96748 net.cpp:243] Memory required for data: 19787264
I1226 09:06:53.462977 96748 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 09:06:53.463090 96748 net.cpp:178] Creating Layer label_data_1_split
I1226 09:06:53.463225 96748 net.cpp:612] label_data_1_split <- label
I1226 09:06:53.463304 96748 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 09:06:53.463395 96748 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 09:06:53.467308 88808 net.cpp:228] Setting up label_data_1_split
I1226 09:06:53.467525 88808 net.cpp:235] Top shape: 32 (32)
I1226 09:06:53.467689 88808 net.cpp:235] Top shape: 32 (32)
I1226 09:06:53.467744 88808 net.cpp:243] Memory required for data: 19787520
I1226 09:06:53.468149 88808 layer_factory.hpp:114] Creating layer conv1
I1226 09:06:53.468317 88808 net.cpp:178] Creating Layer conv1
I1226 09:06:53.468401 88808 net.cpp:612] conv1 <- data
I1226 09:06:53.468452 88808 net.cpp:586] conv1 -> conv1
I1226 09:06:53.494732 94464 net.cpp:228] Setting up data
I1226 09:06:53.477898 96748 net.cpp:228] Setting up label_data_1_split
I1226 09:06:53.494879 94464 net.cpp:235] Top shape: 32 3 227 227 (4946784)
I1226 09:06:53.494923 94464 net.cpp:235] Top shape: 32 (32)
I1226 09:06:53.478008 96748 net.cpp:235] Top shape: 32 (32)
I1226 09:06:53.478040 96748 net.cpp:235] Top shape: 32 (32)
I1226 09:06:53.478117 96748 net.cpp:243] Memory required for data: 19787520
I1226 09:06:53.494952 94464 net.cpp:243] Memory required for data: 19787264
I1226 09:06:53.478160 96748 layer_factory.hpp:114] Creating layer conv1
I1226 09:06:53.494992 94464 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 09:06:53.478271 96748 net.cpp:178] Creating Layer conv1
I1226 09:06:53.495100 94464 net.cpp:178] Creating Layer label_data_1_split
I1226 09:06:53.478310 96748 net.cpp:612] conv1 <- data
I1226 09:06:53.495241 94464 net.cpp:612] label_data_1_split <- label
I1226 09:06:53.478355 96748 net.cpp:586] conv1 -> conv1
I1226 09:06:53.495292 94464 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 09:06:53.495357 94464 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 09:06:53.512501 94464 net.cpp:228] Setting up label_data_1_split
I1226 09:06:53.518311 117050 net.cpp:228] Setting up conv1
I1226 09:06:53.518425 117050 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 09:06:53.518452 117050 net.cpp:243] Memory required for data: 56958720
I1226 09:06:53.518579 117050 layer_factory.hpp:114] Creating layer relu1
I1226 09:06:53.518726 117050 net.cpp:178] Creating Layer relu1
I1226 09:06:53.518759 117050 net.cpp:612] relu1 <- conv1
I1226 09:06:53.518893 117050 net.cpp:573] relu1 -> conv1 (in-place)
I1226 09:06:53.518990 117050 net.cpp:228] Setting up relu1
I1226 09:06:53.519033 117050 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 09:06:53.519057 117050 net.cpp:243] Memory required for data: 94129920
I1226 09:06:53.519084 117050 layer_factory.hpp:114] Creating layer norm1
I1226 09:06:53.519210 117050 net.cpp:178] Creating Layer norm1
I1226 09:06:53.519315 117050 net.cpp:612] norm1 <- conv1
I1226 09:06:53.519438 117050 net.cpp:586] norm1 -> norm1
I1226 09:06:53.519523 117050 net.cpp:228] Setting up norm1
I1226 09:06:53.519587 117050 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 09:06:53.519611 117050 net.cpp:243] Memory required for data: 131301120
I1226 09:06:53.519639 117050 layer_factory.hpp:114] Creating layer pool1
I1226 09:06:53.519709 117050 net.cpp:178] Creating Layer pool1
I1226 09:06:53.519742 117050 net.cpp:612] pool1 <- norm1
I1226 09:06:53.519776 117050 net.cpp:586] pool1 -> pool1
I1226 09:06:53.519856 117050 net.cpp:228] Setting up pool1
I1226 09:06:53.519896 117050 net.cpp:235] Top shape: 32 96 27 27 (2239488)
I1226 09:06:53.519918 117050 net.cpp:243] Memory required for data: 140259072
I1226 09:06:53.519944 117050 layer_factory.hpp:114] Creating layer conv2
I1226 09:06:53.520018 117050 net.cpp:178] Creating Layer conv2
I1226 09:06:53.520050 117050 net.cpp:612] conv2 <- pool1
I1226 09:06:53.520086 117050 net.cpp:586] conv2 -> conv2
I1226 09:06:53.530674 120755 net.cpp:228] Setting up conv2
I1226 09:06:53.530786 120755 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 09:06:53.530812 120755 net.cpp:243] Memory required for data: 164146944
I1226 09:06:53.530882 120755 layer_factory.hpp:114] Creating layer relu2
I1226 09:06:53.530988 120755 net.cpp:178] Creating Layer relu2
I1226 09:06:53.531031 120755 net.cpp:612] relu2 <- conv2
I1226 09:06:53.531074 120755 net.cpp:573] relu2 -> conv2 (in-place)
I1226 09:06:53.531172 120755 net.cpp:228] Setting up relu2
I1226 09:06:53.531213 120755 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 09:06:53.531236 120755 net.cpp:243] Memory required for data: 188034816
I1226 09:06:53.531265 120755 layer_factory.hpp:114] Creating layer norm2
I1226 09:06:53.531312 120755 net.cpp:178] Creating Layer norm2
I1226 09:06:53.531342 120755 net.cpp:612] norm2 <- conv2
I1226 09:06:53.531375 120755 net.cpp:586] norm2 -> norm2
I1226 09:06:53.531457 120755 net.cpp:228] Setting up norm2
I1226 09:06:53.531500 120755 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 09:06:53.531523 120755 net.cpp:243] Memory required for data: 211922688
I1226 09:06:53.531551 120755 layer_factory.hpp:114] Creating layer pool2
I1226 09:06:53.531602 120755 net.cpp:178] Creating Layer pool2
I1226 09:06:53.531635 120755 net.cpp:612] pool2 <- norm2
I1226 09:06:53.531678 120755 net.cpp:586] pool2 -> pool2
I1226 09:06:53.531751 120755 net.cpp:228] Setting up pool2
I1226 09:06:53.531878 120755 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 09:06:53.531937 120755 net.cpp:243] Memory required for data: 217460480
I1226 09:06:53.531971 120755 layer_factory.hpp:114] Creating layer conv3
I1226 09:06:53.532058 120755 net.cpp:178] Creating Layer conv3
I1226 09:06:53.532090 120755 net.cpp:612] conv3 <- pool2
I1226 09:06:53.532130 120755 net.cpp:586] conv3 -> conv3
I1226 09:06:53.512635 94464 net.cpp:235] Top shape: 32 (32)
I1226 09:06:53.532755 94464 net.cpp:235] Top shape: 32 (32)
I1226 09:06:53.532861 94464 net.cpp:243] Memory required for data: 19787520
I1226 09:06:53.532996 94464 layer_factory.hpp:114] Creating layer conv1
I1226 09:06:53.533169 94464 net.cpp:178] Creating Layer conv1
I1226 09:06:53.533241 94464 net.cpp:612] conv1 <- data
I1226 09:06:53.533293 94464 net.cpp:586] conv1 -> conv1
I1226 09:06:50.284376 90317 net.cpp:228] Setting up conv1
I1226 09:06:50.284495 90317 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 09:06:50.284554 90317 net.cpp:243] Memory required for data: 56958720
I1226 09:06:50.284715 90317 layer_factory.hpp:114] Creating layer relu1
I1226 09:06:50.284921 90317 net.cpp:178] Creating Layer relu1
I1226 09:06:50.284968 90317 net.cpp:612] relu1 <- conv1
I1226 09:06:50.285018 90317 net.cpp:573] relu1 -> conv1 (in-place)
I1226 09:06:50.285126 90317 net.cpp:228] Setting up relu1
I1226 09:06:50.285181 90317 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 09:06:50.285204 90317 net.cpp:243] Memory required for data: 94129920
I1226 09:06:50.285234 90317 layer_factory.hpp:114] Creating layer norm1
I1226 09:06:50.285310 90317 net.cpp:178] Creating Layer norm1
I1226 09:06:50.285348 90317 net.cpp:612] norm1 <- conv1
I1226 09:06:50.285398 90317 net.cpp:586] norm1 -> norm1
I1226 09:06:50.285504 90317 net.cpp:228] Setting up norm1
I1226 09:06:50.285553 90317 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 09:06:50.285599 90317 net.cpp:243] Memory required for data: 131301120
I1226 09:06:50.285632 90317 layer_factory.hpp:114] Creating layer pool1
I1226 09:06:50.285699 90317 net.cpp:178] Creating Layer pool1
I1226 09:06:50.285735 90317 net.cpp:612] pool1 <- norm1
I1226 09:06:50.285774 90317 net.cpp:586] pool1 -> pool1
I1226 09:06:50.285863 90317 net.cpp:228] Setting up pool1
I1226 09:06:50.285915 90317 net.cpp:235] Top shape: 32 96 27 27 (2239488)
I1226 09:06:50.285939 90317 net.cpp:243] Memory required for data: 140259072
I1226 09:06:50.285967 90317 layer_factory.hpp:114] Creating layer conv2
I1226 09:06:50.286046 90317 net.cpp:178] Creating Layer conv2
I1226 09:06:50.286083 90317 net.cpp:612] conv2 <- pool1
I1226 09:06:50.286135 90317 net.cpp:586] conv2 -> conv2
I1226 09:06:53.578840 90178 net.cpp:228] Setting up conv1
I1226 09:06:53.578963 90178 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 09:06:53.578997 90178 net.cpp:243] Memory required for data: 56958720
I1226 09:06:53.579453 90178 layer_factory.hpp:114] Creating layer relu1
I1226 09:06:53.579713 90178 net.cpp:178] Creating Layer relu1
I1226 09:06:53.579784 90178 net.cpp:612] relu1 <- conv1
I1226 09:06:53.579916 90178 net.cpp:573] relu1 -> conv1 (in-place)
I1226 09:06:53.580329 90178 net.cpp:228] Setting up relu1
I1226 09:06:53.580404 90178 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 09:06:53.580441 90178 net.cpp:243] Memory required for data: 94129920
I1226 09:06:53.580484 90178 layer_factory.hpp:114] Creating layer norm1
I1226 09:06:53.580775 90178 net.cpp:178] Creating Layer norm1
I1226 09:06:53.580950 90178 net.cpp:612] norm1 <- conv1
I1226 09:06:53.582568 90178 net.cpp:586] norm1 -> norm1
I1226 09:06:53.582883 90178 net.cpp:228] Setting up norm1
I1226 09:06:53.583029 90178 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 09:06:53.583073 90178 net.cpp:243] Memory required for data: 131301120
I1226 09:06:53.583175 90178 layer_factory.hpp:114] Creating layer pool1
I1226 09:06:53.583293 90178 net.cpp:178] Creating Layer pool1
I1226 09:06:53.583526 90178 net.cpp:612] pool1 <- norm1
I1226 09:06:53.583627 90178 net.cpp:586] pool1 -> pool1
I1226 09:06:53.583781 90178 net.cpp:228] Setting up pool1
I1226 09:06:53.583876 90178 net.cpp:235] Top shape: 32 96 27 27 (2239488)
I1226 09:06:53.583971 90178 net.cpp:243] Memory required for data: 140259072
I1226 09:06:53.584020 90178 layer_factory.hpp:114] Creating layer conv2
I1226 09:06:53.584118 90178 net.cpp:178] Creating Layer conv2
I1226 09:06:53.584158 90178 net.cpp:612] conv2 <- pool1
I1226 09:06:53.584211 90178 net.cpp:586] conv2 -> conv2
I1226 09:06:53.614203 120057 net.cpp:228] Setting up conv1
I1226 09:06:53.614326 120057 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 09:06:53.614357 120057 net.cpp:243] Memory required for data: 56958720
I1226 09:06:53.614477 120057 layer_factory.hpp:114] Creating layer relu1
I1226 09:06:53.614586 120057 net.cpp:178] Creating Layer relu1
I1226 09:06:53.614661 120057 net.cpp:612] relu1 <- conv1
I1226 09:06:53.614729 120057 net.cpp:573] relu1 -> conv1 (in-place)
I1226 09:06:53.614845 120057 net.cpp:228] Setting up relu1
I1226 09:06:53.614913 120057 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 09:06:53.614944 120057 net.cpp:243] Memory required for data: 94129920
I1226 09:06:53.614979 120057 layer_factory.hpp:114] Creating layer norm1
I1226 09:06:53.615058 120057 net.cpp:178] Creating Layer norm1
I1226 09:06:53.615090 120057 net.cpp:612] norm1 <- conv1
I1226 09:06:53.615139 120057 net.cpp:586] norm1 -> norm1
I1226 09:06:53.615257 120057 net.cpp:228] Setting up norm1
I1226 09:06:53.615315 120057 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 09:06:53.615339 120057 net.cpp:243] Memory required for data: 131301120
I1226 09:06:53.615371 120057 layer_factory.hpp:114] Creating layer pool1
I1226 09:06:53.615442 120057 net.cpp:178] Creating Layer pool1
I1226 09:06:53.615470 120057 net.cpp:612] pool1 <- norm1
I1226 09:06:53.615509 120057 net.cpp:586] pool1 -> pool1
I1226 09:06:53.615639 120057 net.cpp:228] Setting up pool1
I1226 09:06:53.615697 120057 net.cpp:235] Top shape: 32 96 27 27 (2239488)
I1226 09:06:53.615722 120057 net.cpp:243] Memory required for data: 140259072
I1226 09:06:53.615751 120057 layer_factory.hpp:114] Creating layer conv2
I1226 09:06:53.615833 120057 net.cpp:178] Creating Layer conv2
I1226 09:06:53.615862 120057 net.cpp:612] conv2 <- pool1
I1226 09:06:53.615905 120057 net.cpp:586] conv2 -> conv2
I1226 09:06:53.622800 88808 net.cpp:228] Setting up conv1
I1226 09:06:53.622938 88808 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 09:06:53.622972 88808 net.cpp:243] Memory required for data: 56958720
I1226 09:06:53.623263 88808 layer_factory.hpp:114] Creating layer relu1
I1226 09:06:53.623399 88808 net.cpp:178] Creating Layer relu1
I1226 09:06:53.623531 88808 net.cpp:612] relu1 <- conv1
I1226 09:06:53.623615 88808 net.cpp:573] relu1 -> conv1 (in-place)
I1226 09:06:53.624117 88808 net.cpp:228] Setting up relu1
I1226 09:06:53.624297 88808 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 09:06:53.624390 88808 net.cpp:243] Memory required for data: 94129920
I1226 09:06:53.624441 88808 layer_factory.hpp:114] Creating layer norm1
I1226 09:06:53.624569 88808 net.cpp:178] Creating Layer norm1
I1226 09:06:53.624665 88808 net.cpp:612] norm1 <- conv1
I1226 09:06:53.624729 88808 net.cpp:586] norm1 -> norm1
I1226 09:06:53.624882 88808 net.cpp:228] Setting up norm1
I1226 09:06:53.624963 88808 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 09:06:53.624994 88808 net.cpp:243] Memory required for data: 131301120
I1226 09:06:53.625036 88808 layer_factory.hpp:114] Creating layer pool1
I1226 09:06:53.625134 88808 net.cpp:178] Creating Layer pool1
I1226 09:06:53.625193 88808 net.cpp:612] pool1 <- norm1
I1226 09:06:53.625247 88808 net.cpp:586] pool1 -> pool1
I1226 09:06:53.622244 91281 net.cpp:228] Setting up conv1
I1226 09:06:53.622357 91281 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 09:06:53.622421 91281 net.cpp:243] Memory required for data: 56958720
I1226 09:06:53.625892 88808 net.cpp:228] Setting up pool1
I1226 09:06:53.622539 91281 layer_factory.hpp:114] Creating layer relu1
I1226 09:06:53.626015 88808 net.cpp:235] Top shape: 32 96 27 27 (2239488)
I1226 09:06:53.622635 91281 net.cpp:178] Creating Layer relu1
I1226 09:06:53.626055 88808 net.cpp:243] Memory required for data: 140259072
I1226 09:06:53.622678 91281 net.cpp:612] relu1 <- conv1
I1226 09:06:53.626134 88808 layer_factory.hpp:114] Creating layer conv2
I1226 09:06:53.622720 91281 net.cpp:573] relu1 -> conv1 (in-place)
I1226 09:06:53.622828 91281 net.cpp:228] Setting up relu1
I1226 09:06:53.626292 88808 net.cpp:178] Creating Layer conv2
I1226 09:06:53.622874 91281 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 09:06:53.626345 88808 net.cpp:612] conv2 <- pool1
I1226 09:06:53.622898 91281 net.cpp:243] Memory required for data: 94129920
I1226 09:06:53.626404 88808 net.cpp:586] conv2 -> conv2
I1226 09:06:53.622926 91281 layer_factory.hpp:114] Creating layer norm1
I1226 09:06:53.622994 91281 net.cpp:178] Creating Layer norm1
I1226 09:06:53.623026 91281 net.cpp:612] norm1 <- conv1
I1226 09:06:53.623075 91281 net.cpp:586] norm1 -> norm1
I1226 09:06:53.623167 91281 net.cpp:228] Setting up norm1
I1226 09:06:53.623210 91281 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 09:06:53.623234 91281 net.cpp:243] Memory required for data: 131301120
I1226 09:06:53.623261 91281 layer_factory.hpp:114] Creating layer pool1
I1226 09:06:53.623324 91281 net.cpp:178] Creating Layer pool1
I1226 09:06:53.623356 91281 net.cpp:612] pool1 <- norm1
I1226 09:06:53.623431 91281 net.cpp:586] pool1 -> pool1
I1226 09:06:53.623535 91281 net.cpp:228] Setting up pool1
I1226 09:06:53.623577 91281 net.cpp:235] Top shape: 32 96 27 27 (2239488)
I1226 09:06:53.623601 91281 net.cpp:243] Memory required for data: 140259072
I1226 09:06:53.623631 91281 layer_factory.hpp:114] Creating layer conv2
I1226 09:06:53.623706 91281 net.cpp:178] Creating Layer conv2
I1226 09:06:53.623744 91281 net.cpp:612] conv2 <- pool1
I1226 09:06:53.623782 91281 net.cpp:586] conv2 -> conv2
I1226 09:06:53.646806 96748 net.cpp:228] Setting up conv1
I1226 09:06:53.646919 96748 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 09:06:53.646948 96748 net.cpp:243] Memory required for data: 56958720
I1226 09:06:53.647059 96748 layer_factory.hpp:114] Creating layer relu1
I1226 09:06:53.647189 96748 net.cpp:178] Creating Layer relu1
I1226 09:06:53.647233 96748 net.cpp:612] relu1 <- conv1
I1226 09:06:53.647275 96748 net.cpp:573] relu1 -> conv1 (in-place)
I1226 09:06:53.647385 96748 net.cpp:228] Setting up relu1
I1226 09:06:53.647466 96748 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 09:06:53.647491 96748 net.cpp:243] Memory required for data: 94129920
I1226 09:06:53.647522 96748 layer_factory.hpp:114] Creating layer norm1
I1226 09:06:53.647599 96748 net.cpp:178] Creating Layer norm1
I1226 09:06:53.647634 96748 net.cpp:612] norm1 <- conv1
I1226 09:06:53.647678 96748 net.cpp:586] norm1 -> norm1
I1226 09:06:53.648396 96748 net.cpp:228] Setting up norm1
I1226 09:06:53.648489 96748 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 09:06:53.648592 96748 net.cpp:243] Memory required for data: 131301120
I1226 09:06:53.648643 96748 layer_factory.hpp:114] Creating layer pool1
I1226 09:06:53.648767 96748 net.cpp:178] Creating Layer pool1
I1226 09:06:53.648804 96748 net.cpp:612] pool1 <- norm1
I1226 09:06:53.648864 96748 net.cpp:586] pool1 -> pool1
I1226 09:06:53.648973 96748 net.cpp:228] Setting up pool1
I1226 09:06:53.649030 96748 net.cpp:235] Top shape: 32 96 27 27 (2239488)
I1226 09:06:53.649054 96748 net.cpp:243] Memory required for data: 140259072
I1226 09:06:53.649144 96748 layer_factory.hpp:114] Creating layer conv2
I1226 09:06:53.649230 96748 net.cpp:178] Creating Layer conv2
I1226 09:06:53.649286 96748 net.cpp:612] conv2 <- pool1
I1226 09:06:53.649333 96748 net.cpp:586] conv2 -> conv2
I1226 09:06:53.696535 117050 net.cpp:228] Setting up conv2
I1226 09:06:53.696671 117050 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 09:06:53.696701 117050 net.cpp:243] Memory required for data: 164146944
I1226 09:06:53.696777 117050 layer_factory.hpp:114] Creating layer relu2
I1226 09:06:53.696851 117050 net.cpp:178] Creating Layer relu2
I1226 09:06:53.696889 117050 net.cpp:612] relu2 <- conv2
I1226 09:06:53.696938 117050 net.cpp:573] relu2 -> conv2 (in-place)
I1226 09:06:53.697046 117050 net.cpp:228] Setting up relu2
I1226 09:06:53.697104 117050 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 09:06:53.697130 117050 net.cpp:243] Memory required for data: 188034816
I1226 09:06:53.697161 117050 layer_factory.hpp:114] Creating layer norm2
I1226 09:06:53.697221 117050 net.cpp:178] Creating Layer norm2
I1226 09:06:53.697252 117050 net.cpp:612] norm2 <- conv2
I1226 09:06:53.697301 117050 net.cpp:586] norm2 -> norm2
I1226 09:06:53.697399 117050 net.cpp:228] Setting up norm2
I1226 09:06:53.697445 117050 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 09:06:53.697470 117050 net.cpp:243] Memory required for data: 211922688
I1226 09:06:53.697499 117050 layer_factory.hpp:114] Creating layer pool2
I1226 09:06:53.697585 117050 net.cpp:178] Creating Layer pool2
I1226 09:06:53.697628 117050 net.cpp:612] pool2 <- norm2
I1226 09:06:53.697665 117050 net.cpp:586] pool2 -> pool2
I1226 09:06:53.697870 117050 net.cpp:228] Setting up pool2
I1226 09:06:53.697922 117050 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 09:06:53.697947 117050 net.cpp:243] Memory required for data: 217460480
I1226 09:06:53.697988 117050 layer_factory.hpp:114] Creating layer conv3
I1226 09:06:53.698076 117050 net.cpp:178] Creating Layer conv3
I1226 09:06:53.698112 117050 net.cpp:612] conv3 <- pool2
I1226 09:06:53.698160 117050 net.cpp:586] conv3 -> conv3
I1226 09:06:53.716348 120755 net.cpp:228] Setting up conv3
I1226 09:06:53.716461 120755 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 09:06:53.716488 120755 net.cpp:243] Memory required for data: 225767168
I1226 09:06:53.716560 120755 layer_factory.hpp:114] Creating layer relu3
I1226 09:06:53.716650 120755 net.cpp:178] Creating Layer relu3
I1226 09:06:53.716691 120755 net.cpp:612] relu3 <- conv3
I1226 09:06:53.716730 120755 net.cpp:573] relu3 -> conv3 (in-place)
I1226 09:06:53.716884 120755 net.cpp:228] Setting up relu3
I1226 09:06:53.716953 120755 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 09:06:53.716979 120755 net.cpp:243] Memory required for data: 234073856
I1226 09:06:53.717007 120755 layer_factory.hpp:114] Creating layer conv4
I1226 09:06:53.717078 120755 net.cpp:178] Creating Layer conv4
I1226 09:06:53.717105 120755 net.cpp:612] conv4 <- conv3
I1226 09:06:53.717144 120755 net.cpp:586] conv4 -> conv4
I1226 09:06:50.503208 90317 net.cpp:228] Setting up conv2
I1226 09:06:50.503320 90317 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 09:06:50.503350 90317 net.cpp:243] Memory required for data: 164146944
I1226 09:06:50.503427 90317 layer_factory.hpp:114] Creating layer relu2
I1226 09:06:50.503501 90317 net.cpp:178] Creating Layer relu2
I1226 09:06:50.503537 90317 net.cpp:612] relu2 <- conv2
I1226 09:06:50.503618 90317 net.cpp:573] relu2 -> conv2 (in-place)
I1226 09:06:50.503723 90317 net.cpp:228] Setting up relu2
I1226 09:06:50.503787 90317 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 09:06:50.503816 90317 net.cpp:243] Memory required for data: 188034816
I1226 09:06:50.503847 90317 layer_factory.hpp:114] Creating layer norm2
I1226 09:06:50.503912 90317 net.cpp:178] Creating Layer norm2
I1226 09:06:50.503947 90317 net.cpp:612] norm2 <- conv2
I1226 09:06:50.503999 90317 net.cpp:586] norm2 -> norm2
I1226 09:06:50.504103 90317 net.cpp:228] Setting up norm2
I1226 09:06:50.504159 90317 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 09:06:50.504184 90317 net.cpp:243] Memory required for data: 211922688
I1226 09:06:50.504214 90317 layer_factory.hpp:114] Creating layer pool2
I1226 09:06:50.504283 90317 net.cpp:178] Creating Layer pool2
I1226 09:06:50.504312 90317 net.cpp:612] pool2 <- norm2
I1226 09:06:50.504351 90317 net.cpp:586] pool2 -> pool2
I1226 09:06:50.504441 90317 net.cpp:228] Setting up pool2
I1226 09:06:50.504489 90317 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 09:06:50.504652 90317 net.cpp:243] Memory required for data: 217460480
I1226 09:06:50.504693 90317 layer_factory.hpp:114] Creating layer conv3
I1226 09:06:50.504781 90317 net.cpp:178] Creating Layer conv3
I1226 09:06:50.504822 90317 net.cpp:612] conv3 <- pool2
I1226 09:06:50.504878 90317 net.cpp:586] conv3 -> conv3
I1226 09:06:53.789587 90178 net.cpp:228] Setting up conv2
I1226 09:06:53.789733 90178 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 09:06:53.789777 90178 net.cpp:243] Memory required for data: 164146944
I1226 09:06:53.789902 90178 layer_factory.hpp:114] Creating layer relu2
I1226 09:06:53.789986 90178 net.cpp:178] Creating Layer relu2
I1226 09:06:53.790249 90178 net.cpp:612] relu2 <- conv2
I1226 09:06:53.790321 90178 net.cpp:573] relu2 -> conv2 (in-place)
I1226 09:06:53.790467 90178 net.cpp:228] Setting up relu2
I1226 09:06:53.790549 90178 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 09:06:53.790585 90178 net.cpp:243] Memory required for data: 188034816
I1226 09:06:53.790634 90178 layer_factory.hpp:114] Creating layer norm2
I1226 09:06:53.790756 90178 net.cpp:178] Creating Layer norm2
I1226 09:06:53.790846 90178 net.cpp:612] norm2 <- conv2
I1226 09:06:53.790920 90178 net.cpp:586] norm2 -> norm2
I1226 09:06:53.791100 90178 net.cpp:228] Setting up norm2
I1226 09:06:53.791194 90178 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 09:06:53.791231 90178 net.cpp:243] Memory required for data: 211922688
I1226 09:06:53.791287 90178 layer_factory.hpp:114] Creating layer pool2
I1226 09:06:53.791357 90178 net.cpp:178] Creating Layer pool2
I1226 09:06:53.791398 90178 net.cpp:612] pool2 <- norm2
I1226 09:06:53.791471 90178 net.cpp:586] pool2 -> pool2
I1226 09:06:53.792006 90178 net.cpp:228] Setting up pool2
I1226 09:06:53.792158 90178 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 09:06:53.792345 90178 net.cpp:243] Memory required for data: 217460480
I1226 09:06:53.792402 90178 layer_factory.hpp:114] Creating layer conv3
I1226 09:06:53.792515 90178 net.cpp:178] Creating Layer conv3
I1226 09:06:53.792560 90178 net.cpp:612] conv3 <- pool2
I1226 09:06:53.792664 90178 net.cpp:586] conv3 -> conv3
I1226 09:06:53.819351 120057 net.cpp:228] Setting up conv2
I1226 09:06:53.819463 120057 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 09:06:53.819494 120057 net.cpp:243] Memory required for data: 164146944
I1226 09:06:53.819571 120057 layer_factory.hpp:114] Creating layer relu2
I1226 09:06:53.819675 120057 net.cpp:178] Creating Layer relu2
I1226 09:06:53.819722 120057 net.cpp:612] relu2 <- conv2
I1226 09:06:53.819777 120057 net.cpp:573] relu2 -> conv2 (in-place)
I1226 09:06:53.819885 120057 net.cpp:228] Setting up relu2
I1226 09:06:53.819968 120057 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 09:06:53.820001 120057 net.cpp:243] Memory required for data: 188034816
I1226 09:06:53.820036 120057 layer_factory.hpp:114] Creating layer norm2
I1226 09:06:53.820096 120057 net.cpp:178] Creating Layer norm2
I1226 09:06:53.820125 120057 net.cpp:612] norm2 <- conv2
I1226 09:06:53.820183 120057 net.cpp:586] norm2 -> norm2
I1226 09:06:53.820289 120057 net.cpp:228] Setting up norm2
I1226 09:06:53.820350 120057 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 09:06:53.820376 120057 net.cpp:243] Memory required for data: 211922688
I1226 09:06:53.820407 120057 layer_factory.hpp:114] Creating layer pool2
I1226 09:06:53.820477 120057 net.cpp:178] Creating Layer pool2
I1226 09:06:53.820508 120057 net.cpp:612] pool2 <- norm2
I1226 09:06:53.820549 120057 net.cpp:586] pool2 -> pool2
I1226 09:06:53.820808 120057 net.cpp:228] Setting up pool2
I1226 09:06:53.820976 120057 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 09:06:53.821002 120057 net.cpp:243] Memory required for data: 217460480
I1226 09:06:53.821035 120057 layer_factory.hpp:114] Creating layer conv3
I1226 09:06:53.821127 120057 net.cpp:178] Creating Layer conv3
I1226 09:06:53.821161 120057 net.cpp:612] conv3 <- pool2
I1226 09:06:53.821216 120057 net.cpp:586] conv3 -> conv3
I1226 09:06:53.861026 96748 net.cpp:228] Setting up conv2
I1226 09:06:53.861160 96748 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 09:06:53.861191 96748 net.cpp:243] Memory required for data: 164146944
I1226 09:06:53.861268 96748 layer_factory.hpp:114] Creating layer relu2
I1226 09:06:53.861343 96748 net.cpp:178] Creating Layer relu2
I1226 09:06:53.861387 96748 net.cpp:612] relu2 <- conv2
I1226 09:06:53.861433 96748 net.cpp:573] relu2 -> conv2 (in-place)
I1226 09:06:53.861541 96748 net.cpp:228] Setting up relu2
I1226 09:06:53.861590 96748 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 09:06:53.861615 96748 net.cpp:243] Memory required for data: 188034816
I1226 09:06:53.861646 96748 layer_factory.hpp:114] Creating layer norm2
I1226 09:06:53.861713 96748 net.cpp:178] Creating Layer norm2
I1226 09:06:53.861752 96748 net.cpp:612] norm2 <- conv2
I1226 09:06:53.861793 96748 net.cpp:586] norm2 -> norm2
I1226 09:06:53.861894 96748 net.cpp:228] Setting up norm2
I1226 09:06:53.861948 96748 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 09:06:53.861974 96748 net.cpp:243] Memory required for data: 211922688
I1226 09:06:53.862004 96748 layer_factory.hpp:114] Creating layer pool2
I1226 09:06:53.862057 96748 net.cpp:178] Creating Layer pool2
I1226 09:06:53.862112 96748 net.cpp:612] pool2 <- norm2
I1226 09:06:53.862193 96748 net.cpp:586] pool2 -> pool2
I1226 09:06:53.862277 96748 net.cpp:228] Setting up pool2
I1226 09:06:53.862325 96748 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 09:06:53.862447 96748 net.cpp:243] Memory required for data: 217460480
I1226 09:06:53.862486 96748 layer_factory.hpp:114] Creating layer conv3
I1226 09:06:53.862565 96748 net.cpp:178] Creating Layer conv3
I1226 09:06:53.862601 96748 net.cpp:612] conv3 <- pool2
I1226 09:06:53.862655 96748 net.cpp:586] conv3 -> conv3
I1226 09:06:53.870935 120755 net.cpp:228] Setting up conv4
I1226 09:06:53.871047 120755 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 09:06:53.871073 120755 net.cpp:243] Memory required for data: 242380544
I1226 09:06:53.871129 120755 layer_factory.hpp:114] Creating layer relu4
I1226 09:06:53.871187 120755 net.cpp:178] Creating Layer relu4
I1226 09:06:53.871217 120755 net.cpp:612] relu4 <- conv4
I1226 09:06:53.871255 120755 net.cpp:573] relu4 -> conv4 (in-place)
I1226 09:06:53.871335 120755 net.cpp:228] Setting up relu4
I1226 09:06:53.871373 120755 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 09:06:53.871397 120755 net.cpp:243] Memory required for data: 250687232
I1226 09:06:53.871424 120755 layer_factory.hpp:114] Creating layer conv5
I1226 09:06:53.871489 120755 net.cpp:178] Creating Layer conv5
I1226 09:06:53.871526 120755 net.cpp:612] conv5 <- conv4
I1226 09:06:53.871564 120755 net.cpp:586] conv5 -> conv5
I1226 09:06:53.872294 88808 net.cpp:228] Setting up conv2
I1226 09:06:53.872417 88808 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 09:06:53.872457 88808 net.cpp:243] Memory required for data: 164146944
I1226 09:06:53.872551 88808 layer_factory.hpp:114] Creating layer relu2
I1226 09:06:53.872661 88808 net.cpp:178] Creating Layer relu2
I1226 09:06:53.872711 88808 net.cpp:612] relu2 <- conv2
I1226 09:06:53.872758 88808 net.cpp:573] relu2 -> conv2 (in-place)
I1226 09:06:53.872880 88808 net.cpp:228] Setting up relu2
I1226 09:06:53.872938 88808 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 09:06:53.872969 88808 net.cpp:243] Memory required for data: 188034816
I1226 09:06:53.873005 88808 layer_factory.hpp:114] Creating layer norm2
I1226 09:06:53.873085 88808 net.cpp:178] Creating Layer norm2
I1226 09:06:53.873124 88808 net.cpp:612] norm2 <- conv2
I1226 09:06:53.873172 88808 net.cpp:586] norm2 -> norm2
I1226 09:06:53.873277 88808 net.cpp:228] Setting up norm2
I1226 09:06:53.873334 88808 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 09:06:53.873365 88808 net.cpp:243] Memory required for data: 211922688
I1226 09:06:53.873399 88808 layer_factory.hpp:114] Creating layer pool2
I1226 09:06:53.873450 88808 net.cpp:178] Creating Layer pool2
I1226 09:06:53.873488 88808 net.cpp:612] pool2 <- norm2
I1226 09:06:53.873543 88808 net.cpp:586] pool2 -> pool2
I1226 09:06:53.873687 88808 net.cpp:228] Setting up pool2
I1226 09:06:53.873751 88808 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 09:06:53.873885 88808 net.cpp:243] Memory required for data: 217460480
I1226 09:06:53.873927 88808 layer_factory.hpp:114] Creating layer conv3
I1226 09:06:53.874038 88808 net.cpp:178] Creating Layer conv3
I1226 09:06:53.874081 88808 net.cpp:612] conv3 <- pool2
I1226 09:06:53.874143 88808 net.cpp:586] conv3 -> conv3
I1226 09:06:53.887313 91281 net.cpp:228] Setting up conv2
I1226 09:06:53.887450 91281 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 09:06:53.887481 91281 net.cpp:243] Memory required for data: 164146944
I1226 09:06:53.887559 91281 layer_factory.hpp:114] Creating layer relu2
I1226 09:06:53.887625 91281 net.cpp:178] Creating Layer relu2
I1226 09:06:53.887655 91281 net.cpp:612] relu2 <- conv2
I1226 09:06:53.887713 91281 net.cpp:573] relu2 -> conv2 (in-place)
I1226 09:06:53.887819 91281 net.cpp:228] Setting up relu2
I1226 09:06:53.887984 91281 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 09:06:53.888008 91281 net.cpp:243] Memory required for data: 188034816
I1226 09:06:53.888037 91281 layer_factory.hpp:114] Creating layer norm2
I1226 09:06:53.888094 91281 net.cpp:178] Creating Layer norm2
I1226 09:06:53.888124 91281 net.cpp:612] norm2 <- conv2
I1226 09:06:53.888171 91281 net.cpp:586] norm2 -> norm2
I1226 09:06:53.888259 91281 net.cpp:228] Setting up norm2
I1226 09:06:53.888301 91281 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 09:06:53.888324 91281 net.cpp:243] Memory required for data: 211922688
I1226 09:06:53.888350 91281 layer_factory.hpp:114] Creating layer pool2
I1226 09:06:53.895488 91281 net.cpp:178] Creating Layer pool2
I1226 09:06:53.895614 91281 net.cpp:612] pool2 <- norm2
I1226 09:06:53.895701 91281 net.cpp:586] pool2 -> pool2
I1226 09:06:53.895817 91281 net.cpp:228] Setting up pool2
I1226 09:06:53.896589 91281 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 09:06:53.896795 91281 net.cpp:243] Memory required for data: 217460480
I1226 09:06:53.896862 91281 layer_factory.hpp:114] Creating layer conv3
I1226 09:06:53.897020 91281 net.cpp:178] Creating Layer conv3
I1226 09:06:53.897578 91281 net.cpp:612] conv3 <- pool2
I1226 09:06:53.897856 91281 net.cpp:586] conv3 -> conv3
I1226 09:06:53.931027 94464 net.cpp:228] Setting up conv1
I1226 09:06:53.931172 94464 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 09:06:53.931215 94464 net.cpp:243] Memory required for data: 56958720
I1226 09:06:53.931361 94464 layer_factory.hpp:114] Creating layer relu1
I1226 09:06:53.931509 94464 net.cpp:178] Creating Layer relu1
I1226 09:06:53.931557 94464 net.cpp:612] relu1 <- conv1
I1226 09:06:53.931653 94464 net.cpp:573] relu1 -> conv1 (in-place)
I1226 09:06:53.931859 94464 net.cpp:228] Setting up relu1
I1226 09:06:53.932097 94464 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 09:06:53.932152 94464 net.cpp:243] Memory required for data: 94129920
I1226 09:06:53.932199 94464 layer_factory.hpp:114] Creating layer norm1
I1226 09:06:53.932281 94464 net.cpp:178] Creating Layer norm1
I1226 09:06:53.932319 94464 net.cpp:612] norm1 <- conv1
I1226 09:06:53.932361 94464 net.cpp:586] norm1 -> norm1
I1226 09:06:53.932476 94464 net.cpp:228] Setting up norm1
I1226 09:06:53.932533 94464 net.cpp:235] Top shape: 32 96 55 55 (9292800)
I1226 09:06:53.932559 94464 net.cpp:243] Memory required for data: 131301120
I1226 09:06:53.932590 94464 layer_factory.hpp:114] Creating layer pool1
I1226 09:06:53.932660 94464 net.cpp:178] Creating Layer pool1
I1226 09:06:53.932696 94464 net.cpp:612] pool1 <- norm1
I1226 09:06:53.932746 94464 net.cpp:586] pool1 -> pool1
I1226 09:06:53.932886 94464 net.cpp:228] Setting up pool1
I1226 09:06:53.932942 94464 net.cpp:235] Top shape: 32 96 27 27 (2239488)
I1226 09:06:53.932966 94464 net.cpp:243] Memory required for data: 140259072
I1226 09:06:53.932996 94464 layer_factory.hpp:114] Creating layer conv2
I1226 09:06:53.933079 94464 net.cpp:178] Creating Layer conv2
I1226 09:06:53.933120 94464 net.cpp:612] conv2 <- pool1
I1226 09:06:53.933166 94464 net.cpp:586] conv2 -> conv2
I1226 09:06:53.978731 120755 net.cpp:228] Setting up conv5
I1226 09:06:53.978840 120755 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 09:06:53.978865 120755 net.cpp:243] Memory required for data: 256225024
I1226 09:06:53.979008 120755 layer_factory.hpp:114] Creating layer relu5
I1226 09:06:53.979079 120755 net.cpp:178] Creating Layer relu5
I1226 09:06:53.979110 120755 net.cpp:612] relu5 <- conv5
I1226 09:06:53.979169 120755 net.cpp:573] relu5 -> conv5 (in-place)
I1226 09:06:53.979272 120755 net.cpp:228] Setting up relu5
I1226 09:06:53.979315 120755 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 09:06:53.979341 120755 net.cpp:243] Memory required for data: 261762816
I1226 09:06:53.979368 120755 layer_factory.hpp:114] Creating layer pool5
I1226 09:06:53.979435 120755 net.cpp:178] Creating Layer pool5
I1226 09:06:53.979470 120755 net.cpp:612] pool5 <- conv5
I1226 09:06:53.979506 120755 net.cpp:586] pool5 -> pool5
I1226 09:06:53.979595 120755 net.cpp:228] Setting up pool5
I1226 09:06:53.979636 120755 net.cpp:235] Top shape: 32 256 6 6 (294912)
I1226 09:06:53.979658 120755 net.cpp:243] Memory required for data: 262942464
I1226 09:06:53.979686 120755 layer_factory.hpp:114] Creating layer fc6
I1226 09:06:53.979751 120755 net.cpp:178] Creating Layer fc6
I1226 09:06:53.979785 120755 net.cpp:612] fc6 <- pool5
I1226 09:06:53.979836 120755 net.cpp:586] fc6 -> fc6
I1226 09:06:54.004158 117050 net.cpp:228] Setting up conv3
I1226 09:06:54.004273 117050 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 09:06:54.004302 117050 net.cpp:243] Memory required for data: 225767168
I1226 09:06:54.004377 117050 layer_factory.hpp:114] Creating layer relu3
I1226 09:06:54.004447 117050 net.cpp:178] Creating Layer relu3
I1226 09:06:54.004480 117050 net.cpp:612] relu3 <- conv3
I1226 09:06:54.004568 117050 net.cpp:573] relu3 -> conv3 (in-place)
I1226 09:06:54.004676 117050 net.cpp:228] Setting up relu3
I1226 09:06:54.004730 117050 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 09:06:54.004755 117050 net.cpp:243] Memory required for data: 234073856
I1226 09:06:54.004786 117050 layer_factory.hpp:114] Creating layer conv4
I1226 09:06:54.004875 117050 net.cpp:178] Creating Layer conv4
I1226 09:06:54.004918 117050 net.cpp:612] conv4 <- conv3
I1226 09:06:54.004971 117050 net.cpp:586] conv4 -> conv4
I1226 09:06:50.802060 90317 net.cpp:228] Setting up conv3
I1226 09:06:50.802175 90317 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 09:06:50.802206 90317 net.cpp:243] Memory required for data: 225767168
I1226 09:06:50.802285 90317 layer_factory.hpp:114] Creating layer relu3
I1226 09:06:50.802359 90317 net.cpp:178] Creating Layer relu3
I1226 09:06:50.802403 90317 net.cpp:612] relu3 <- conv3
I1226 09:06:50.802474 90317 net.cpp:573] relu3 -> conv3 (in-place)
I1226 09:06:50.802603 90317 net.cpp:228] Setting up relu3
I1226 09:06:50.802659 90317 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 09:06:50.802693 90317 net.cpp:243] Memory required for data: 234073856
I1226 09:06:50.802726 90317 layer_factory.hpp:114] Creating layer conv4
I1226 09:06:50.802819 90317 net.cpp:178] Creating Layer conv4
I1226 09:06:50.802861 90317 net.cpp:612] conv4 <- conv3
I1226 09:06:50.802920 90317 net.cpp:586] conv4 -> conv4
I1226 09:06:54.091804 120057 net.cpp:228] Setting up conv3
I1226 09:06:54.091955 120057 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 09:06:54.092016 120057 net.cpp:243] Memory required for data: 225767168
I1226 09:06:54.092110 120057 layer_factory.hpp:114] Creating layer relu3
I1226 09:06:54.092193 120057 net.cpp:178] Creating Layer relu3
I1226 09:06:54.092262 120057 net.cpp:612] relu3 <- conv3
I1226 09:06:54.092344 120057 net.cpp:573] relu3 -> conv3 (in-place)
I1226 09:06:54.092505 120057 net.cpp:228] Setting up relu3
I1226 09:06:54.092567 120057 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 09:06:54.092595 120057 net.cpp:243] Memory required for data: 234073856
I1226 09:06:54.092695 120057 layer_factory.hpp:114] Creating layer conv4
I1226 09:06:54.092808 120057 net.cpp:178] Creating Layer conv4
I1226 09:06:54.092844 120057 net.cpp:612] conv4 <- conv3
I1226 09:06:54.092938 120057 net.cpp:586] conv4 -> conv4
I1226 09:06:54.097470 91281 net.cpp:228] Setting up conv3
I1226 09:06:54.097584 91281 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 09:06:54.097611 91281 net.cpp:243] Memory required for data: 225767168
I1226 09:06:54.097741 91281 layer_factory.hpp:114] Creating layer relu3
I1226 09:06:54.097810 91281 net.cpp:178] Creating Layer relu3
I1226 09:06:54.097842 91281 net.cpp:612] relu3 <- conv3
I1226 09:06:54.097960 91281 net.cpp:573] relu3 -> conv3 (in-place)
I1226 09:06:54.098072 91281 net.cpp:228] Setting up relu3
I1226 09:06:54.098119 91281 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 09:06:54.098141 91281 net.cpp:243] Memory required for data: 234073856
I1226 09:06:54.098170 91281 layer_factory.hpp:114] Creating layer conv4
I1226 09:06:54.098256 91281 net.cpp:178] Creating Layer conv4
I1226 09:06:54.098294 91281 net.cpp:612] conv4 <- conv3
I1226 09:06:54.098347 91281 net.cpp:586] conv4 -> conv4
I1226 09:06:54.110293 90178 net.cpp:228] Setting up conv3
I1226 09:06:54.110474 90178 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 09:06:54.110517 90178 net.cpp:243] Memory required for data: 225767168
I1226 09:06:54.110641 90178 layer_factory.hpp:114] Creating layer relu3
I1226 09:06:54.110849 90178 net.cpp:178] Creating Layer relu3
I1226 09:06:54.110906 90178 net.cpp:612] relu3 <- conv3
I1226 09:06:54.111212 90178 net.cpp:573] relu3 -> conv3 (in-place)
I1226 09:06:54.111361 90178 net.cpp:228] Setting up relu3
I1226 09:06:54.111986 90178 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 09:06:54.112068 90178 net.cpp:243] Memory required for data: 234073856
I1226 09:06:54.112118 90178 layer_factory.hpp:114] Creating layer conv4
I1226 09:06:54.112288 90178 net.cpp:178] Creating Layer conv4
I1226 09:06:54.112339 90178 net.cpp:612] conv4 <- conv3
I1226 09:06:54.112431 90178 net.cpp:586] conv4 -> conv4
I1226 09:06:54.172147 96748 net.cpp:228] Setting up conv3
I1226 09:06:54.172255 96748 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 09:06:54.172281 96748 net.cpp:243] Memory required for data: 225767168
I1226 09:06:54.172354 96748 layer_factory.hpp:114] Creating layer relu3
I1226 09:06:54.172417 96748 net.cpp:178] Creating Layer relu3
I1226 09:06:54.172446 96748 net.cpp:612] relu3 <- conv3
I1226 09:06:54.172487 96748 net.cpp:573] relu3 -> conv3 (in-place)
I1226 09:06:54.172585 96748 net.cpp:228] Setting up relu3
I1226 09:06:54.172622 96748 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 09:06:54.172646 96748 net.cpp:243] Memory required for data: 234073856
I1226 09:06:54.172674 96748 layer_factory.hpp:114] Creating layer conv4
I1226 09:06:54.172765 96748 net.cpp:178] Creating Layer conv4
I1226 09:06:54.172798 96748 net.cpp:612] conv4 <- conv3
I1226 09:06:54.172837 96748 net.cpp:586] conv4 -> conv4
I1226 09:06:54.217953 88808 net.cpp:228] Setting up conv3
I1226 09:06:54.218188 88808 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 09:06:54.218390 88808 net.cpp:243] Memory required for data: 225767168
I1226 09:06:54.218509 88808 layer_factory.hpp:114] Creating layer relu3
I1226 09:06:54.218574 88808 net.cpp:178] Creating Layer relu3
I1226 09:06:54.218664 88808 net.cpp:612] relu3 <- conv3
I1226 09:06:54.218709 88808 net.cpp:573] relu3 -> conv3 (in-place)
I1226 09:06:54.218837 88808 net.cpp:228] Setting up relu3
I1226 09:06:54.218915 88808 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 09:06:54.218950 88808 net.cpp:243] Memory required for data: 234073856
I1226 09:06:54.218998 88808 layer_factory.hpp:114] Creating layer conv4
I1226 09:06:54.219138 88808 net.cpp:178] Creating Layer conv4
I1226 09:06:54.219199 88808 net.cpp:612] conv4 <- conv3
I1226 09:06:54.219281 88808 net.cpp:586] conv4 -> conv4
I1226 09:06:54.250583 91281 net.cpp:228] Setting up conv4
I1226 09:06:54.250692 91281 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 09:06:54.250717 91281 net.cpp:243] Memory required for data: 242380544
I1226 09:06:54.250774 91281 layer_factory.hpp:114] Creating layer relu4
I1226 09:06:54.250854 91281 net.cpp:178] Creating Layer relu4
I1226 09:06:54.250892 91281 net.cpp:612] relu4 <- conv4
I1226 09:06:54.250936 91281 net.cpp:573] relu4 -> conv4 (in-place)
I1226 09:06:54.251029 91281 net.cpp:228] Setting up relu4
I1226 09:06:54.251073 91281 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 09:06:54.251106 91281 net.cpp:243] Memory required for data: 250687232
I1226 09:06:54.251134 91281 layer_factory.hpp:114] Creating layer conv5
I1226 09:06:54.251215 91281 net.cpp:178] Creating Layer conv5
I1226 09:06:54.251248 91281 net.cpp:612] conv5 <- conv4
I1226 09:06:54.251289 91281 net.cpp:586] conv5 -> conv5
I1226 09:06:54.262286 117050 net.cpp:228] Setting up conv4
I1226 09:06:54.262395 117050 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 09:06:54.262423 117050 net.cpp:243] Memory required for data: 242380544
I1226 09:06:54.262480 117050 layer_factory.hpp:114] Creating layer relu4
I1226 09:06:54.262578 117050 net.cpp:178] Creating Layer relu4
I1226 09:06:54.262624 117050 net.cpp:612] relu4 <- conv4
I1226 09:06:54.262689 117050 net.cpp:573] relu4 -> conv4 (in-place)
I1226 09:06:54.262789 117050 net.cpp:228] Setting up relu4
I1226 09:06:54.262831 117050 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 09:06:54.262856 117050 net.cpp:243] Memory required for data: 250687232
I1226 09:06:54.262886 117050 layer_factory.hpp:114] Creating layer conv5
I1226 09:06:54.262972 117050 net.cpp:178] Creating Layer conv5
I1226 09:06:54.263006 117050 net.cpp:612] conv5 <- conv4
I1226 09:06:54.263056 117050 net.cpp:586] conv5 -> conv5
I1226 09:06:51.060325 90317 net.cpp:228] Setting up conv4
I1226 09:06:51.060441 90317 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 09:06:51.060472 90317 net.cpp:243] Memory required for data: 242380544
I1226 09:06:51.060533 90317 layer_factory.hpp:114] Creating layer relu4
I1226 09:06:51.060642 90317 net.cpp:178] Creating Layer relu4
I1226 09:06:51.060695 90317 net.cpp:612] relu4 <- conv4
I1226 09:06:51.060756 90317 net.cpp:573] relu4 -> conv4 (in-place)
I1226 09:06:51.060847 90317 net.cpp:228] Setting up relu4
I1226 09:06:51.060889 90317 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 09:06:51.060914 90317 net.cpp:243] Memory required for data: 250687232
I1226 09:06:51.060943 90317 layer_factory.hpp:114] Creating layer conv5
I1226 09:06:51.061035 90317 net.cpp:178] Creating Layer conv5
I1226 09:06:51.061074 90317 net.cpp:612] conv5 <- conv4
I1226 09:06:51.061127 90317 net.cpp:586] conv5 -> conv5
I1226 09:06:54.334198 120057 net.cpp:228] Setting up conv4
I1226 09:06:54.334334 120057 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 09:06:54.334370 120057 net.cpp:243] Memory required for data: 242380544
I1226 09:06:54.334434 120057 layer_factory.hpp:114] Creating layer relu4
I1226 09:06:54.334527 120057 net.cpp:178] Creating Layer relu4
I1226 09:06:54.334780 120057 net.cpp:612] relu4 <- conv4
I1226 09:06:54.334893 120057 net.cpp:573] relu4 -> conv4 (in-place)
I1226 09:06:54.335041 120057 net.cpp:228] Setting up relu4
I1226 09:06:54.335093 120057 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 09:06:54.335117 120057 net.cpp:243] Memory required for data: 250687232
I1226 09:06:54.335147 120057 layer_factory.hpp:114] Creating layer conv5
I1226 09:06:54.335269 120057 net.cpp:178] Creating Layer conv5
I1226 09:06:54.335346 120057 net.cpp:612] conv5 <- conv4
I1226 09:06:54.335443 120057 net.cpp:586] conv5 -> conv5
I1226 09:06:54.362133 91281 net.cpp:228] Setting up conv5
I1226 09:06:54.362241 91281 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 09:06:54.362268 91281 net.cpp:243] Memory required for data: 256225024
I1226 09:06:54.362342 91281 layer_factory.hpp:114] Creating layer relu5
I1226 09:06:54.362453 91281 net.cpp:178] Creating Layer relu5
I1226 09:06:54.362498 91281 net.cpp:612] relu5 <- conv5
I1226 09:06:54.362547 91281 net.cpp:573] relu5 -> conv5 (in-place)
I1226 09:06:54.362644 91281 net.cpp:228] Setting up relu5
I1226 09:06:54.362682 91281 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 09:06:54.362705 91281 net.cpp:243] Memory required for data: 261762816
I1226 09:06:54.362746 91281 layer_factory.hpp:114] Creating layer pool5
I1226 09:06:54.362818 91281 net.cpp:178] Creating Layer pool5
I1226 09:06:54.362854 91281 net.cpp:612] pool5 <- conv5
I1226 09:06:54.362890 91281 net.cpp:586] pool5 -> pool5
I1226 09:06:54.362980 91281 net.cpp:228] Setting up pool5
I1226 09:06:54.363031 91281 net.cpp:235] Top shape: 32 256 6 6 (294912)
I1226 09:06:54.363054 91281 net.cpp:243] Memory required for data: 262942464
I1226 09:06:54.363091 91281 layer_factory.hpp:114] Creating layer fc6
I1226 09:06:54.363147 91281 net.cpp:178] Creating Layer fc6
I1226 09:06:54.363173 91281 net.cpp:612] fc6 <- pool5
I1226 09:06:54.363209 91281 net.cpp:586] fc6 -> fc6
I1226 09:06:54.384434 90178 net.cpp:228] Setting up conv4
I1226 09:06:54.384554 90178 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 09:06:54.384587 90178 net.cpp:243] Memory required for data: 242380544
I1226 09:06:54.384667 90178 layer_factory.hpp:114] Creating layer relu4
I1226 09:06:54.384742 90178 net.cpp:178] Creating Layer relu4
I1226 09:06:54.384791 90178 net.cpp:612] relu4 <- conv4
I1226 09:06:54.384869 90178 net.cpp:573] relu4 -> conv4 (in-place)
I1226 09:06:54.384968 90178 net.cpp:228] Setting up relu4
I1226 09:06:54.385032 90178 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 09:06:54.385066 90178 net.cpp:243] Memory required for data: 250687232
I1226 09:06:54.385107 90178 layer_factory.hpp:114] Creating layer conv5
I1226 09:06:54.385207 90178 net.cpp:178] Creating Layer conv5
I1226 09:06:54.385258 90178 net.cpp:612] conv5 <- conv4
I1226 09:06:54.385318 90178 net.cpp:586] conv5 -> conv5
I1226 09:06:54.405817 96748 net.cpp:228] Setting up conv4
I1226 09:06:54.405927 96748 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 09:06:54.405952 96748 net.cpp:243] Memory required for data: 242380544
I1226 09:06:54.406034 96748 layer_factory.hpp:114] Creating layer relu4
I1226 09:06:54.406141 96748 net.cpp:178] Creating Layer relu4
I1226 09:06:54.406184 96748 net.cpp:612] relu4 <- conv4
I1226 09:06:54.406224 96748 net.cpp:573] relu4 -> conv4 (in-place)
I1226 09:06:54.406314 96748 net.cpp:228] Setting up relu4
I1226 09:06:54.406349 96748 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 09:06:54.406383 96748 net.cpp:243] Memory required for data: 250687232
I1226 09:06:54.406419 96748 layer_factory.hpp:114] Creating layer conv5
I1226 09:06:54.406499 96748 net.cpp:178] Creating Layer conv5
I1226 09:06:54.406534 96748 net.cpp:612] conv5 <- conv4
I1226 09:06:54.406584 96748 net.cpp:586] conv5 -> conv5
I1226 09:06:54.458465 117050 net.cpp:228] Setting up conv5
I1226 09:06:54.458596 117050 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 09:06:54.458623 117050 net.cpp:243] Memory required for data: 256225024
I1226 09:06:54.458695 117050 layer_factory.hpp:114] Creating layer relu5
I1226 09:06:54.458762 117050 net.cpp:178] Creating Layer relu5
I1226 09:06:54.458796 117050 net.cpp:612] relu5 <- conv5
I1226 09:06:54.458855 117050 net.cpp:573] relu5 -> conv5 (in-place)
I1226 09:06:54.458946 117050 net.cpp:228] Setting up relu5
I1226 09:06:54.459002 117050 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 09:06:54.459028 117050 net.cpp:243] Memory required for data: 261762816
I1226 09:06:54.459054 117050 layer_factory.hpp:114] Creating layer pool5
I1226 09:06:54.459103 117050 net.cpp:178] Creating Layer pool5
I1226 09:06:54.459136 117050 net.cpp:612] pool5 <- conv5
I1226 09:06:54.459182 117050 net.cpp:586] pool5 -> pool5
I1226 09:06:54.459277 117050 net.cpp:228] Setting up pool5
I1226 09:06:54.459318 117050 net.cpp:235] Top shape: 32 256 6 6 (294912)
I1226 09:06:54.459345 117050 net.cpp:243] Memory required for data: 262942464
I1226 09:06:54.459379 117050 layer_factory.hpp:114] Creating layer fc6
I1226 09:06:54.459435 117050 net.cpp:178] Creating Layer fc6
I1226 09:06:54.459462 117050 net.cpp:612] fc6 <- pool5
I1226 09:06:54.459496 117050 net.cpp:586] fc6 -> fc6
I1226 09:06:51.252204 90317 net.cpp:228] Setting up conv5
I1226 09:06:51.252348 90317 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 09:06:51.252382 90317 net.cpp:243] Memory required for data: 256225024
I1226 09:06:51.252454 90317 layer_factory.hpp:114] Creating layer relu5
I1226 09:06:51.252601 90317 net.cpp:178] Creating Layer relu5
I1226 09:06:51.252646 90317 net.cpp:612] relu5 <- conv5
I1226 09:06:51.252732 90317 net.cpp:573] relu5 -> conv5 (in-place)
I1226 09:06:51.252825 90317 net.cpp:228] Setting up relu5
I1226 09:06:51.252895 90317 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 09:06:51.252954 90317 net.cpp:243] Memory required for data: 261762816
I1226 09:06:51.252985 90317 layer_factory.hpp:114] Creating layer pool5
I1226 09:06:51.253046 90317 net.cpp:178] Creating Layer pool5
I1226 09:06:51.253103 90317 net.cpp:612] pool5 <- conv5
I1226 09:06:51.253142 90317 net.cpp:586] pool5 -> pool5
I1226 09:06:51.253233 90317 net.cpp:228] Setting up pool5
I1226 09:06:51.253335 90317 net.cpp:235] Top shape: 32 256 6 6 (294912)
I1226 09:06:51.253360 90317 net.cpp:243] Memory required for data: 262942464
I1226 09:06:51.253803 90317 layer_factory.hpp:114] Creating layer fc6
I1226 09:06:51.253927 90317 net.cpp:178] Creating Layer fc6
I1226 09:06:51.254006 90317 net.cpp:612] fc6 <- pool5
I1226 09:06:51.254075 90317 net.cpp:586] fc6 -> fc6
I1226 09:06:54.521600 88808 net.cpp:228] Setting up conv4
I1226 09:06:54.521711 88808 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 09:06:54.521740 88808 net.cpp:243] Memory required for data: 242380544
I1226 09:06:54.521800 88808 layer_factory.hpp:114] Creating layer relu4
I1226 09:06:54.521880 88808 net.cpp:178] Creating Layer relu4
I1226 09:06:54.521914 88808 net.cpp:612] relu4 <- conv4
I1226 09:06:54.521965 88808 net.cpp:573] relu4 -> conv4 (in-place)
I1226 09:06:54.522069 88808 net.cpp:228] Setting up relu4
I1226 09:06:54.522121 88808 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 09:06:54.522145 88808 net.cpp:243] Memory required for data: 250687232
I1226 09:06:54.522172 88808 layer_factory.hpp:114] Creating layer conv5
I1226 09:06:54.522249 88808 net.cpp:178] Creating Layer conv5
I1226 09:06:54.522286 88808 net.cpp:612] conv5 <- conv4
I1226 09:06:54.522336 88808 net.cpp:586] conv5 -> conv5
I1226 09:06:54.526962 120057 net.cpp:228] Setting up conv5
I1226 09:06:54.527072 120057 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 09:06:54.527099 120057 net.cpp:243] Memory required for data: 256225024
I1226 09:06:54.527174 120057 layer_factory.hpp:114] Creating layer relu5
I1226 09:06:54.527240 120057 net.cpp:178] Creating Layer relu5
I1226 09:06:54.527279 120057 net.cpp:612] relu5 <- conv5
I1226 09:06:54.527343 120057 net.cpp:573] relu5 -> conv5 (in-place)
I1226 09:06:54.527451 120057 net.cpp:228] Setting up relu5
I1226 09:06:54.527516 120057 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 09:06:54.527544 120057 net.cpp:243] Memory required for data: 261762816
I1226 09:06:54.527576 120057 layer_factory.hpp:114] Creating layer pool5
I1226 09:06:54.527673 120057 net.cpp:178] Creating Layer pool5
I1226 09:06:54.527709 120057 net.cpp:612] pool5 <- conv5
I1226 09:06:54.527752 120057 net.cpp:586] pool5 -> pool5
I1226 09:06:54.527856 120057 net.cpp:228] Setting up pool5
I1226 09:06:54.527904 120057 net.cpp:235] Top shape: 32 256 6 6 (294912)
I1226 09:06:54.527926 120057 net.cpp:243] Memory required for data: 262942464
I1226 09:06:54.527956 120057 layer_factory.hpp:114] Creating layer fc6
I1226 09:06:54.528017 120057 net.cpp:178] Creating Layer fc6
I1226 09:06:54.528043 120057 net.cpp:612] fc6 <- pool5
I1226 09:06:54.528096 120057 net.cpp:586] fc6 -> fc6
I1226 09:06:54.552057 90178 net.cpp:228] Setting up conv5
I1226 09:06:54.552207 90178 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 09:06:54.552264 90178 net.cpp:243] Memory required for data: 256225024
I1226 09:06:54.552368 90178 layer_factory.hpp:114] Creating layer relu5
I1226 09:06:54.552873 90178 net.cpp:178] Creating Layer relu5
I1226 09:06:54.552979 90178 net.cpp:612] relu5 <- conv5
I1226 09:06:54.553114 90178 net.cpp:573] relu5 -> conv5 (in-place)
I1226 09:06:54.553282 90178 net.cpp:228] Setting up relu5
I1226 09:06:54.553442 90178 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 09:06:54.553474 90178 net.cpp:243] Memory required for data: 261762816
I1226 09:06:54.553527 90178 layer_factory.hpp:114] Creating layer pool5
I1226 09:06:54.553593 90178 net.cpp:178] Creating Layer pool5
I1226 09:06:54.553627 90178 net.cpp:612] pool5 <- conv5
I1226 09:06:54.553707 90178 net.cpp:586] pool5 -> pool5
I1226 09:06:54.553902 90178 net.cpp:228] Setting up pool5
I1226 09:06:54.553983 90178 net.cpp:235] Top shape: 32 256 6 6 (294912)
I1226 09:06:54.554018 90178 net.cpp:243] Memory required for data: 262942464
I1226 09:06:54.554081 90178 layer_factory.hpp:114] Creating layer fc6
I1226 09:06:54.554162 90178 net.cpp:178] Creating Layer fc6
I1226 09:06:54.554566 90178 net.cpp:612] fc6 <- pool5
I1226 09:06:54.554728 90178 net.cpp:586] fc6 -> fc6
I1226 09:06:54.568342 96748 net.cpp:228] Setting up conv5
I1226 09:06:54.568465 96748 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 09:06:54.568495 96748 net.cpp:243] Memory required for data: 256225024
I1226 09:06:54.568568 96748 layer_factory.hpp:114] Creating layer relu5
I1226 09:06:54.568634 96748 net.cpp:178] Creating Layer relu5
I1226 09:06:54.568672 96748 net.cpp:612] relu5 <- conv5
I1226 09:06:54.568727 96748 net.cpp:573] relu5 -> conv5 (in-place)
I1226 09:06:54.568823 96748 net.cpp:228] Setting up relu5
I1226 09:06:54.568900 96748 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 09:06:54.568929 96748 net.cpp:243] Memory required for data: 261762816
I1226 09:06:54.568961 96748 layer_factory.hpp:114] Creating layer pool5
I1226 09:06:54.569015 96748 net.cpp:178] Creating Layer pool5
I1226 09:06:54.569041 96748 net.cpp:612] pool5 <- conv5
I1226 09:06:54.569135 96748 net.cpp:586] pool5 -> pool5
I1226 09:06:54.569288 96748 net.cpp:228] Setting up pool5
I1226 09:06:54.569397 96748 net.cpp:235] Top shape: 32 256 6 6 (294912)
I1226 09:06:54.569473 96748 net.cpp:243] Memory required for data: 262942464
I1226 09:06:54.569576 96748 layer_factory.hpp:114] Creating layer fc6
I1226 09:06:54.569654 96748 net.cpp:178] Creating Layer fc6
I1226 09:06:54.569705 96748 net.cpp:612] fc6 <- pool5
I1226 09:06:54.569774 96748 net.cpp:586] fc6 -> fc6
I1226 09:06:54.699673 88808 net.cpp:228] Setting up conv5
I1226 09:06:54.699784 88808 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 09:06:54.699815 88808 net.cpp:243] Memory required for data: 256225024
I1226 09:06:54.699890 88808 layer_factory.hpp:114] Creating layer relu5
I1226 09:06:54.699980 88808 net.cpp:178] Creating Layer relu5
I1226 09:06:54.700023 88808 net.cpp:612] relu5 <- conv5
I1226 09:06:54.700067 88808 net.cpp:573] relu5 -> conv5 (in-place)
I1226 09:06:54.700160 88808 net.cpp:228] Setting up relu5
I1226 09:06:54.700201 88808 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 09:06:54.700225 88808 net.cpp:243] Memory required for data: 261762816
I1226 09:06:54.700254 88808 layer_factory.hpp:114] Creating layer pool5
I1226 09:06:54.700323 88808 net.cpp:178] Creating Layer pool5
I1226 09:06:54.700350 88808 net.cpp:612] pool5 <- conv5
I1226 09:06:54.700387 88808 net.cpp:586] pool5 -> pool5
I1226 09:06:54.700479 88808 net.cpp:228] Setting up pool5
I1226 09:06:54.700532 88808 net.cpp:235] Top shape: 32 256 6 6 (294912)
I1226 09:06:54.700556 88808 net.cpp:243] Memory required for data: 262942464
I1226 09:06:54.700611 88808 layer_factory.hpp:114] Creating layer fc6
I1226 09:06:54.700673 88808 net.cpp:178] Creating Layer fc6
I1226 09:06:54.700711 88808 net.cpp:612] fc6 <- pool5
I1226 09:06:54.700748 88808 net.cpp:586] fc6 -> fc6
I1226 09:06:54.836587 94464 net.cpp:228] Setting up conv2
I1226 09:06:54.836705 94464 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 09:06:54.836736 94464 net.cpp:243] Memory required for data: 164146944
I1226 09:06:54.836841 94464 layer_factory.hpp:114] Creating layer relu2
I1226 09:06:54.836905 94464 net.cpp:178] Creating Layer relu2
I1226 09:06:54.836949 94464 net.cpp:612] relu2 <- conv2
I1226 09:06:54.837020 94464 net.cpp:573] relu2 -> conv2 (in-place)
I1226 09:06:54.837131 94464 net.cpp:228] Setting up relu2
I1226 09:06:54.837193 94464 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 09:06:54.837221 94464 net.cpp:243] Memory required for data: 188034816
I1226 09:06:54.837263 94464 layer_factory.hpp:114] Creating layer norm2
I1226 09:06:54.837316 94464 net.cpp:178] Creating Layer norm2
I1226 09:06:54.837352 94464 net.cpp:612] norm2 <- conv2
I1226 09:06:54.837393 94464 net.cpp:586] norm2 -> norm2
I1226 09:06:54.837488 94464 net.cpp:228] Setting up norm2
I1226 09:06:54.837539 94464 net.cpp:235] Top shape: 32 256 27 27 (5971968)
I1226 09:06:54.837565 94464 net.cpp:243] Memory required for data: 211922688
I1226 09:06:54.837594 94464 layer_factory.hpp:114] Creating layer pool2
I1226 09:06:54.837643 94464 net.cpp:178] Creating Layer pool2
I1226 09:06:54.837678 94464 net.cpp:612] pool2 <- norm2
I1226 09:06:54.837734 94464 net.cpp:586] pool2 -> pool2
I1226 09:06:54.837858 94464 net.cpp:228] Setting up pool2
I1226 09:06:54.837913 94464 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 09:06:54.838038 94464 net.cpp:243] Memory required for data: 217460480
I1226 09:06:54.838075 94464 layer_factory.hpp:114] Creating layer conv3
I1226 09:06:54.838165 94464 net.cpp:178] Creating Layer conv3
I1226 09:06:54.838204 94464 net.cpp:612] conv3 <- pool2
I1226 09:06:54.838260 94464 net.cpp:586] conv3 -> conv3
I1226 09:06:55.714550 94464 net.cpp:228] Setting up conv3
I1226 09:06:55.714668 94464 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 09:06:55.714702 94464 net.cpp:243] Memory required for data: 225767168
I1226 09:06:55.714840 94464 layer_factory.hpp:114] Creating layer relu3
I1226 09:06:55.714931 94464 net.cpp:178] Creating Layer relu3
I1226 09:06:55.714968 94464 net.cpp:612] relu3 <- conv3
I1226 09:06:55.715013 94464 net.cpp:573] relu3 -> conv3 (in-place)
I1226 09:06:55.715131 94464 net.cpp:228] Setting up relu3
I1226 09:06:55.715204 94464 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 09:06:55.715234 94464 net.cpp:243] Memory required for data: 234073856
I1226 09:06:55.715267 94464 layer_factory.hpp:114] Creating layer conv4
I1226 09:06:55.715360 94464 net.cpp:178] Creating Layer conv4
I1226 09:06:55.715404 94464 net.cpp:612] conv4 <- conv3
I1226 09:06:55.715466 94464 net.cpp:586] conv4 -> conv4
I1226 09:06:56.437021 94464 net.cpp:228] Setting up conv4
I1226 09:06:56.437141 94464 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 09:06:56.437178 94464 net.cpp:243] Memory required for data: 242380544
I1226 09:06:56.437249 94464 layer_factory.hpp:114] Creating layer relu4
I1226 09:06:56.437443 94464 net.cpp:178] Creating Layer relu4
I1226 09:06:56.437487 94464 net.cpp:612] relu4 <- conv4
I1226 09:06:56.437533 94464 net.cpp:573] relu4 -> conv4 (in-place)
I1226 09:06:56.437631 94464 net.cpp:228] Setting up relu4
I1226 09:06:56.437697 94464 net.cpp:235] Top shape: 32 384 13 13 (2076672)
I1226 09:06:56.437726 94464 net.cpp:243] Memory required for data: 250687232
I1226 09:06:56.437759 94464 layer_factory.hpp:114] Creating layer conv5
I1226 09:06:56.437901 94464 net.cpp:178] Creating Layer conv5
I1226 09:06:56.437961 94464 net.cpp:612] conv5 <- conv4
I1226 09:06:56.438015 94464 net.cpp:586] conv5 -> conv5
I1226 09:06:56.938138 94464 net.cpp:228] Setting up conv5
I1226 09:06:56.938257 94464 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 09:06:56.938289 94464 net.cpp:243] Memory required for data: 256225024
I1226 09:06:56.938371 94464 layer_factory.hpp:114] Creating layer relu5
I1226 09:06:56.938469 94464 net.cpp:178] Creating Layer relu5
I1226 09:06:56.938515 94464 net.cpp:612] relu5 <- conv5
I1226 09:06:56.938570 94464 net.cpp:573] relu5 -> conv5 (in-place)
I1226 09:06:56.938676 94464 net.cpp:228] Setting up relu5
I1226 09:06:56.938735 94464 net.cpp:235] Top shape: 32 256 13 13 (1384448)
I1226 09:06:56.938763 94464 net.cpp:243] Memory required for data: 261762816
I1226 09:06:56.938829 94464 layer_factory.hpp:114] Creating layer pool5
I1226 09:06:56.938902 94464 net.cpp:178] Creating Layer pool5
I1226 09:06:56.938947 94464 net.cpp:612] pool5 <- conv5
I1226 09:06:56.938990 94464 net.cpp:586] pool5 -> pool5
I1226 09:06:56.939087 94464 net.cpp:228] Setting up pool5
I1226 09:06:56.939141 94464 net.cpp:235] Top shape: 32 256 6 6 (294912)
I1226 09:06:56.939167 94464 net.cpp:243] Memory required for data: 262942464
I1226 09:06:56.939196 94464 layer_factory.hpp:114] Creating layer fc6
I1226 09:06:56.939277 94464 net.cpp:178] Creating Layer fc6
I1226 09:06:56.939314 94464 net.cpp:612] fc6 <- pool5
I1226 09:06:56.939358 94464 net.cpp:586] fc6 -> fc6
I1226 09:06:59.081490 120755 net.cpp:228] Setting up fc6
I1226 09:06:59.081603 120755 net.cpp:235] Top shape: 32 4096 (131072)
I1226 09:06:59.081634 120755 net.cpp:243] Memory required for data: 263466752
I1226 09:06:59.081693 120755 layer_factory.hpp:114] Creating layer relu6
I1226 09:06:59.081785 120755 net.cpp:178] Creating Layer relu6
I1226 09:06:59.081833 120755 net.cpp:612] relu6 <- fc6
I1226 09:06:59.081884 120755 net.cpp:573] relu6 -> fc6 (in-place)
I1226 09:06:59.082126 120755 net.cpp:228] Setting up relu6
I1226 09:06:59.082243 120755 net.cpp:235] Top shape: 32 4096 (131072)
I1226 09:06:59.082271 120755 net.cpp:243] Memory required for data: 263991040
I1226 09:06:59.082304 120755 layer_factory.hpp:114] Creating layer drop6
I1226 09:06:59.082375 120755 net.cpp:178] Creating Layer drop6
I1226 09:06:59.082406 120755 net.cpp:612] drop6 <- fc6
I1226 09:06:59.082443 120755 net.cpp:573] drop6 -> fc6 (in-place)
I1226 09:06:59.082509 120755 net.cpp:228] Setting up drop6
I1226 09:06:59.082551 120755 net.cpp:235] Top shape: 32 4096 (131072)
I1226 09:06:59.082573 120755 net.cpp:243] Memory required for data: 264515328
I1226 09:06:59.082602 120755 layer_factory.hpp:114] Creating layer fc7
I1226 09:06:59.082659 120755 net.cpp:178] Creating Layer fc7
I1226 09:06:59.082691 120755 net.cpp:612] fc7 <- fc6
I1226 09:06:59.082728 120755 net.cpp:586] fc7 -> fc7
I1226 09:06:59.470233 91281 net.cpp:228] Setting up fc6
I1226 09:06:59.470345 91281 net.cpp:235] Top shape: 32 4096 (131072)
I1226 09:06:59.470398 91281 net.cpp:243] Memory required for data: 263466752
I1226 09:06:59.470466 91281 layer_factory.hpp:114] Creating layer relu6
I1226 09:06:59.470531 91281 net.cpp:178] Creating Layer relu6
I1226 09:06:59.470578 91281 net.cpp:612] relu6 <- fc6
I1226 09:06:59.470638 91281 net.cpp:573] relu6 -> fc6 (in-place)
I1226 09:06:59.470736 91281 net.cpp:228] Setting up relu6
I1226 09:06:59.470877 91281 net.cpp:235] Top shape: 32 4096 (131072)
I1226 09:06:59.470908 91281 net.cpp:243] Memory required for data: 263991040
I1226 09:06:59.470942 91281 layer_factory.hpp:114] Creating layer drop6
I1226 09:06:59.471007 91281 net.cpp:178] Creating Layer drop6
I1226 09:06:59.471040 91281 net.cpp:612] drop6 <- fc6
I1226 09:06:59.471086 91281 net.cpp:573] drop6 -> fc6 (in-place)
I1226 09:06:59.471148 91281 net.cpp:228] Setting up drop6
I1226 09:06:59.471181 91281 net.cpp:235] Top shape: 32 4096 (131072)
I1226 09:06:59.471204 91281 net.cpp:243] Memory required for data: 264515328
I1226 09:06:59.471231 91281 layer_factory.hpp:114] Creating layer fc7
I1226 09:06:59.471312 91281 net.cpp:178] Creating Layer fc7
I1226 09:06:59.471345 91281 net.cpp:612] fc7 <- fc6
I1226 09:06:59.471408 91281 net.cpp:586] fc7 -> fc7
I1226 09:06:59.765272 90178 net.cpp:228] Setting up fc6
I1226 09:06:59.765396 90178 net.cpp:235] Top shape: 32 4096 (131072)
I1226 09:06:59.765430 90178 net.cpp:243] Memory required for data: 263466752
I1226 09:06:59.765540 90178 layer_factory.hpp:114] Creating layer relu6
I1226 09:06:59.765633 90178 net.cpp:178] Creating Layer relu6
I1226 09:06:59.765883 90178 net.cpp:612] relu6 <- fc6
I1226 09:06:59.765990 90178 net.cpp:573] relu6 -> fc6 (in-place)
I1226 09:06:59.766109 90178 net.cpp:228] Setting up relu6
I1226 09:06:59.766301 90178 net.cpp:235] Top shape: 32 4096 (131072)
I1226 09:06:59.766369 90178 net.cpp:243] Memory required for data: 263991040
I1226 09:06:59.766422 90178 layer_factory.hpp:114] Creating layer drop6
I1226 09:06:59.766489 90178 net.cpp:178] Creating Layer drop6
I1226 09:06:59.766530 90178 net.cpp:612] drop6 <- fc6
I1226 09:06:59.766577 90178 net.cpp:573] drop6 -> fc6 (in-place)
I1226 09:06:59.766654 90178 net.cpp:228] Setting up drop6
I1226 09:06:59.766708 90178 net.cpp:235] Top shape: 32 4096 (131072)
I1226 09:06:59.766743 90178 net.cpp:243] Memory required for data: 264515328
I1226 09:06:59.766778 90178 layer_factory.hpp:114] Creating layer fc7
I1226 09:06:59.766898 90178 net.cpp:178] Creating Layer fc7
I1226 09:06:59.766948 90178 net.cpp:612] fc7 <- fc6
I1226 09:06:59.767015 90178 net.cpp:586] fc7 -> fc7
I1226 09:06:59.769585 117050 net.cpp:228] Setting up fc6
I1226 09:06:59.769697 117050 net.cpp:235] Top shape: 32 4096 (131072)
I1226 09:06:59.769727 117050 net.cpp:243] Memory required for data: 263466752
I1226 09:06:56.505079 90317 net.cpp:228] Setting up fc6
I1226 09:06:59.769783 117050 layer_factory.hpp:114] Creating layer relu6
I1226 09:06:56.505193 90317 net.cpp:235] Top shape: 32 4096 (131072)
I1226 09:06:56.505223 90317 net.cpp:243] Memory required for data: 263466752
I1226 09:06:59.769850 117050 net.cpp:178] Creating Layer relu6
I1226 09:06:56.505280 90317 layer_factory.hpp:114] Creating layer relu6
I1226 09:06:59.769896 117050 net.cpp:612] relu6 <- fc6
I1226 09:06:56.505355 90317 net.cpp:178] Creating Layer relu6
I1226 09:06:59.770066 117050 net.cpp:573] relu6 -> fc6 (in-place)
I1226 09:06:56.505389 90317 net.cpp:612] relu6 <- fc6
I1226 09:06:59.770170 117050 net.cpp:228] Setting up relu6
I1226 09:06:56.505451 90317 net.cpp:573] relu6 -> fc6 (in-place)
I1226 09:06:59.770226 117050 net.cpp:235] Top shape: 32 4096 (131072)
I1226 09:06:59.770251 117050 net.cpp:243] Memory required for data: 263991040
I1226 09:06:56.505539 90317 net.cpp:228] Setting up relu6
I1226 09:06:59.770283 117050 layer_factory.hpp:114] Creating layer drop6
I1226 09:06:56.505720 90317 net.cpp:235] Top shape: 32 4096 (131072)
I1226 09:06:59.770346 117050 net.cpp:178] Creating Layer drop6
I1226 09:06:56.505750 90317 net.cpp:243] Memory required for data: 263991040
I1226 09:06:59.770383 117050 net.cpp:612] drop6 <- fc6
I1226 09:06:56.505789 90317 layer_factory.hpp:114] Creating layer drop6
I1226 09:06:59.770427 117050 net.cpp:573] drop6 -> fc6 (in-place)
I1226 09:06:56.505844 90317 net.cpp:178] Creating Layer drop6
I1226 09:06:56.505873 90317 net.cpp:612] drop6 <- fc6
I1226 09:06:59.770490 117050 net.cpp:228] Setting up drop6
I1226 09:06:56.505913 90317 net.cpp:573] drop6 -> fc6 (in-place)
I1226 09:06:59.770531 117050 net.cpp:235] Top shape: 32 4096 (131072)
I1226 09:06:59.770578 117050 net.cpp:243] Memory required for data: 264515328
I1226 09:06:59.770612 117050 layer_factory.hpp:114] Creating layer fc7
I1226 09:06:56.505980 90317 net.cpp:228] Setting up drop6
I1226 09:06:59.770691 117050 net.cpp:178] Creating Layer fc7
I1226 09:06:56.506016 90317 net.cpp:235] Top shape: 32 4096 (131072)
I1226 09:06:56.506038 90317 net.cpp:243] Memory required for data: 264515328
I1226 09:06:59.770730 117050 net.cpp:612] fc7 <- fc6
I1226 09:06:56.506067 90317 layer_factory.hpp:114] Creating layer fc7
I1226 09:06:59.770771 117050 net.cpp:586] fc7 -> fc7
I1226 09:06:56.506150 90317 net.cpp:178] Creating Layer fc7
I1226 09:06:56.506186 90317 net.cpp:612] fc7 <- fc6
I1226 09:06:56.506225 90317 net.cpp:586] fc7 -> fc7
I1226 09:06:59.829813 96748 net.cpp:228] Setting up fc6
I1226 09:06:59.829928 96748 net.cpp:235] Top shape: 32 4096 (131072)
I1226 09:06:59.829957 96748 net.cpp:243] Memory required for data: 263466752
I1226 09:06:59.830044 96748 layer_factory.hpp:114] Creating layer relu6
I1226 09:06:59.830149 96748 net.cpp:178] Creating Layer relu6
I1226 09:06:59.830296 96748 net.cpp:612] relu6 <- fc6
I1226 09:06:59.830355 96748 net.cpp:573] relu6 -> fc6 (in-place)
I1226 09:06:59.830451 96748 net.cpp:228] Setting up relu6
I1226 09:06:59.830638 96748 net.cpp:235] Top shape: 32 4096 (131072)
I1226 09:06:59.830698 96748 net.cpp:243] Memory required for data: 263991040
I1226 09:06:59.830744 96748 layer_factory.hpp:114] Creating layer drop6
I1226 09:06:59.830806 96748 net.cpp:178] Creating Layer drop6
I1226 09:06:59.830837 96748 net.cpp:612] drop6 <- fc6
I1226 09:06:59.830878 96748 net.cpp:573] drop6 -> fc6 (in-place)
I1226 09:06:59.830945 96748 net.cpp:228] Setting up drop6
I1226 09:06:59.830993 96748 net.cpp:235] Top shape: 32 4096 (131072)
I1226 09:06:59.831018 96748 net.cpp:243] Memory required for data: 264515328
I1226 09:06:59.831051 96748 layer_factory.hpp:114] Creating layer fc7
I1226 09:06:59.831153 96748 net.cpp:178] Creating Layer fc7
I1226 09:06:59.831192 96748 net.cpp:612] fc7 <- fc6
I1226 09:06:59.831233 96748 net.cpp:586] fc7 -> fc7
I1226 09:06:59.847319 88808 net.cpp:228] Setting up fc6
I1226 09:06:59.847432 88808 net.cpp:235] Top shape: 32 4096 (131072)
I1226 09:06:59.847465 88808 net.cpp:243] Memory required for data: 263466752
I1226 09:06:59.847525 88808 layer_factory.hpp:114] Creating layer relu6
I1226 09:06:59.847628 88808 net.cpp:178] Creating Layer relu6
I1226 09:06:59.847669 88808 net.cpp:612] relu6 <- fc6
I1226 09:06:59.847736 88808 net.cpp:573] relu6 -> fc6 (in-place)
I1226 09:06:59.847851 88808 net.cpp:228] Setting up relu6
I1226 09:06:59.848001 88808 net.cpp:235] Top shape: 32 4096 (131072)
I1226 09:06:59.848032 88808 net.cpp:243] Memory required for data: 263991040
I1226 09:06:59.848065 88808 layer_factory.hpp:114] Creating layer drop6
I1226 09:06:59.848122 88808 net.cpp:178] Creating Layer drop6
I1226 09:06:59.848150 88808 net.cpp:612] drop6 <- fc6
I1226 09:06:59.848196 88808 net.cpp:573] drop6 -> fc6 (in-place)
I1226 09:06:59.848251 88808 net.cpp:228] Setting up drop6
I1226 09:06:59.848290 88808 net.cpp:235] Top shape: 32 4096 (131072)
I1226 09:06:59.848323 88808 net.cpp:243] Memory required for data: 264515328
I1226 09:06:59.848358 88808 layer_factory.hpp:114] Creating layer fc7
I1226 09:06:59.848433 88808 net.cpp:178] Creating Layer fc7
I1226 09:06:59.848469 88808 net.cpp:612] fc7 <- fc6
I1226 09:06:59.848515 88808 net.cpp:586] fc7 -> fc7
I1226 09:06:59.878804 120057 net.cpp:228] Setting up fc6
I1226 09:06:59.878917 120057 net.cpp:235] Top shape: 32 4096 (131072)
I1226 09:06:59.878948 120057 net.cpp:243] Memory required for data: 263466752
I1226 09:06:59.879005 120057 layer_factory.hpp:114] Creating layer relu6
I1226 09:06:59.879099 120057 net.cpp:178] Creating Layer relu6
I1226 09:06:59.879146 120057 net.cpp:612] relu6 <- fc6
I1226 09:06:59.879320 120057 net.cpp:573] relu6 -> fc6 (in-place)
I1226 09:06:59.879444 120057 net.cpp:228] Setting up relu6
I1226 09:06:59.879511 120057 net.cpp:235] Top shape: 32 4096 (131072)
I1226 09:06:59.879541 120057 net.cpp:243] Memory required for data: 263991040
I1226 09:06:59.879575 120057 layer_factory.hpp:114] Creating layer drop6
I1226 09:06:59.879663 120057 net.cpp:178] Creating Layer drop6
I1226 09:06:59.879698 120057 net.cpp:612] drop6 <- fc6
I1226 09:06:59.879736 120057 net.cpp:573] drop6 -> fc6 (in-place)
I1226 09:06:59.879801 120057 net.cpp:228] Setting up drop6
I1226 09:06:59.879840 120057 net.cpp:235] Top shape: 32 4096 (131072)
I1226 09:06:59.879864 120057 net.cpp:243] Memory required for data: 264515328
I1226 09:06:59.879894 120057 layer_factory.hpp:114] Creating layer fc7
I1226 09:06:59.879969 120057 net.cpp:178] Creating Layer fc7
I1226 09:06:59.880002 120057 net.cpp:612] fc7 <- fc6
I1226 09:06:59.880045 120057 net.cpp:586] fc7 -> fc7
I1226 09:07:01.359990 120755 net.cpp:228] Setting up fc7
I1226 09:07:01.360095 120755 net.cpp:235] Top shape: 32 4096 (131072)
I1226 09:07:01.360123 120755 net.cpp:243] Memory required for data: 265039616
I1226 09:07:01.360208 120755 layer_factory.hpp:114] Creating layer relu7
I1226 09:07:01.360290 120755 net.cpp:178] Creating Layer relu7
I1226 09:07:01.360334 120755 net.cpp:612] relu7 <- fc7
I1226 09:07:01.360380 120755 net.cpp:573] relu7 -> fc7 (in-place)
I1226 09:07:01.360474 120755 net.cpp:228] Setting up relu7
I1226 09:07:01.360520 120755 net.cpp:235] Top shape: 32 4096 (131072)
I1226 09:07:01.360545 120755 net.cpp:243] Memory required for data: 265563904
I1226 09:07:01.360575 120755 layer_factory.hpp:114] Creating layer drop7
I1226 09:07:01.360621 120755 net.cpp:178] Creating Layer drop7
I1226 09:07:01.360648 120755 net.cpp:612] drop7 <- fc7
I1226 09:07:01.360682 120755 net.cpp:573] drop7 -> fc7 (in-place)
I1226 09:07:01.360730 120755 net.cpp:228] Setting up drop7
I1226 09:07:01.360772 120755 net.cpp:235] Top shape: 32 4096 (131072)
I1226 09:07:01.360796 120755 net.cpp:243] Memory required for data: 266088192
I1226 09:07:01.360823 120755 layer_factory.hpp:114] Creating layer fc8
I1226 09:07:01.360893 120755 net.cpp:178] Creating Layer fc8
I1226 09:07:01.360949 120755 net.cpp:612] fc8 <- fc7
I1226 09:07:01.360997 120755 net.cpp:586] fc8 -> fc8
I1226 09:07:01.741704 91281 net.cpp:228] Setting up fc7
I1226 09:07:01.741817 91281 net.cpp:235] Top shape: 32 4096 (131072)
I1226 09:07:01.741845 91281 net.cpp:243] Memory required for data: 265039616
I1226 09:07:01.741901 91281 layer_factory.hpp:114] Creating layer relu7
I1226 09:07:01.741994 91281 net.cpp:178] Creating Layer relu7
I1226 09:07:01.742039 91281 net.cpp:612] relu7 <- fc7
I1226 09:07:01.742089 91281 net.cpp:573] relu7 -> fc7 (in-place)
I1226 09:07:01.742172 91281 net.cpp:228] Setting up relu7
I1226 09:07:01.742220 91281 net.cpp:235] Top shape: 32 4096 (131072)
I1226 09:07:01.742244 91281 net.cpp:243] Memory required for data: 265563904
I1226 09:07:01.742272 91281 layer_factory.hpp:114] Creating layer drop7
I1226 09:07:01.742319 91281 net.cpp:178] Creating Layer drop7
I1226 09:07:01.742347 91281 net.cpp:612] drop7 <- fc7
I1226 09:07:01.742431 91281 net.cpp:573] drop7 -> fc7 (in-place)
I1226 09:07:01.742481 91281 net.cpp:228] Setting up drop7
I1226 09:07:01.742516 91281 net.cpp:235] Top shape: 32 4096 (131072)
I1226 09:07:01.742547 91281 net.cpp:243] Memory required for data: 266088192
I1226 09:07:01.742575 91281 layer_factory.hpp:114] Creating layer fc8
I1226 09:07:01.742630 91281 net.cpp:178] Creating Layer fc8
I1226 09:07:01.742663 91281 net.cpp:612] fc8 <- fc7
I1226 09:07:01.742699 91281 net.cpp:586] fc8 -> fc8
I1226 09:07:01.917551 120755 net.cpp:228] Setting up fc8
I1226 09:07:01.917661 120755 net.cpp:235] Top shape: 32 1000 (32000)
I1226 09:07:01.917692 120755 net.cpp:243] Memory required for data: 266216192
I1226 09:07:01.917748 120755 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 09:07:01.917842 120755 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 09:07:01.917888 120755 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 09:07:01.917963 120755 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 09:07:01.918036 120755 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 09:07:01.918138 120755 net.cpp:228] Setting up fc8_fc8_0_split
I1226 09:07:01.918184 120755 net.cpp:235] Top shape: 32 1000 (32000)
I1226 09:07:01.918218 120755 net.cpp:235] Top shape: 32 1000 (32000)
I1226 09:07:01.918257 120755 net.cpp:243] Memory required for data: 266472192
I1226 09:07:01.918290 120755 layer_factory.hpp:114] Creating layer accuracy
I1226 09:07:01.918341 120755 net.cpp:178] Creating Layer accuracy
I1226 09:07:01.918376 120755 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 09:07:01.918419 120755 net.cpp:612] accuracy <- label_data_1_split_0
I1226 09:07:01.918459 120755 net.cpp:586] accuracy -> accuracy
I1226 09:07:01.918514 120755 net.cpp:228] Setting up accuracy
I1226 09:07:01.918550 120755 net.cpp:235] Top shape: (1)
I1226 09:07:01.918573 120755 net.cpp:243] Memory required for data: 266472196
I1226 09:07:01.918607 120755 layer_factory.hpp:114] Creating layer loss
I1226 09:07:01.918653 120755 net.cpp:178] Creating Layer loss
I1226 09:07:01.918680 120755 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 09:07:01.918715 120755 net.cpp:612] loss <- label_data_1_split_1
I1226 09:07:01.918748 120755 net.cpp:586] loss -> loss
I1226 09:07:01.918807 120755 layer_factory.hpp:114] Creating layer loss
I1226 09:07:01.946014 120755 net.cpp:228] Setting up loss
I1226 09:07:01.946132 120755 net.cpp:235] Top shape: (1)
I1226 09:07:01.946173 120755 net.cpp:238]     with loss weight 1
I1226 09:07:01.946332 120755 net.cpp:243] Memory required for data: 266472200
I1226 09:07:01.946398 120755 net.cpp:305] loss needs backward computation.
I1226 09:07:01.946457 120755 net.cpp:307] accuracy does not need backward computation.
I1226 09:07:01.946502 120755 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 09:07:01.946547 120755 net.cpp:305] fc8 needs backward computation.
I1226 09:07:01.946581 120755 net.cpp:305] drop7 needs backward computation.
I1226 09:07:01.946614 120755 net.cpp:305] relu7 needs backward computation.
I1226 09:07:01.946643 120755 net.cpp:305] fc7 needs backward computation.
I1226 09:07:01.946682 120755 net.cpp:305] drop6 needs backward computation.
I1226 09:07:01.946714 120755 net.cpp:305] relu6 needs backward computation.
I1226 09:07:01.946751 120755 net.cpp:305] fc6 needs backward computation.
I1226 09:07:01.946784 120755 net.cpp:305] pool5 needs backward computation.
I1226 09:07:01.946825 120755 net.cpp:305] relu5 needs backward computation.
I1226 09:07:01.946856 120755 net.cpp:305] conv5 needs backward computation.
I1226 09:07:01.946893 120755 net.cpp:305] relu4 needs backward computation.
I1226 09:07:01.946949 120755 net.cpp:305] conv4 needs backward computation.
I1226 09:07:01.946980 120755 net.cpp:305] relu3 needs backward computation.
I1226 09:07:01.947041 120755 net.cpp:305] conv3 needs backward computation.
I1226 09:07:01.947074 120755 net.cpp:305] pool2 needs backward computation.
I1226 09:07:01.947104 120755 net.cpp:305] norm2 needs backward computation.
I1226 09:07:01.947134 120755 net.cpp:305] relu2 needs backward computation.
I1226 09:07:01.947162 120755 net.cpp:305] conv2 needs backward computation.
I1226 09:07:01.947192 120755 net.cpp:305] pool1 needs backward computation.
I1226 09:07:01.947221 120755 net.cpp:305] norm1 needs backward computation.
I1226 09:07:01.947250 120755 net.cpp:305] relu1 needs backward computation.
I1226 09:07:01.947278 120755 net.cpp:305] conv1 needs backward computation.
I1226 09:07:01.947309 120755 net.cpp:307] label_data_1_split does not need backward computation.
I1226 09:07:01.947340 120755 net.cpp:307] data does not need backward computation.
I1226 09:07:01.947367 120755 net.cpp:349] This network produces output accuracy
I1226 09:07:01.947399 120755 net.cpp:349] This network produces output loss
I1226 09:07:01.947484 120755 net.cpp:363] Network initialization done.
I1226 09:07:01.948086 120755 solver.cpp:119] Solver scaffolding done.
I1226 09:07:01.948323 120755 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 09:06:58.757256 90317 net.cpp:228] Setting up fc7
I1226 09:06:58.757367 90317 net.cpp:235] Top shape: 32 4096 (131072)
I1226 09:06:58.757396 90317 net.cpp:243] Memory required for data: 265039616
I1226 09:06:58.757454 90317 layer_factory.hpp:114] Creating layer relu7
I1226 09:06:58.757547 90317 net.cpp:178] Creating Layer relu7
I1226 09:06:58.757614 90317 net.cpp:612] relu7 <- fc7
I1226 09:06:58.757660 90317 net.cpp:573] relu7 -> fc7 (in-place)
I1226 09:06:58.757743 90317 net.cpp:228] Setting up relu7
I1226 09:06:58.757789 90317 net.cpp:235] Top shape: 32 4096 (131072)
I1226 09:06:58.757814 90317 net.cpp:243] Memory required for data: 265563904
I1226 09:06:58.757844 90317 layer_factory.hpp:114] Creating layer drop7
I1226 09:06:58.757899 90317 net.cpp:178] Creating Layer drop7
I1226 09:06:58.757936 90317 net.cpp:612] drop7 <- fc7
I1226 09:06:58.757972 90317 net.cpp:573] drop7 -> fc7 (in-place)
I1226 09:06:58.758025 90317 net.cpp:228] Setting up drop7
I1226 09:06:58.758065 90317 net.cpp:235] Top shape: 32 4096 (131072)
I1226 09:06:58.758087 90317 net.cpp:243] Memory required for data: 266088192
I1226 09:06:58.758116 90317 layer_factory.hpp:114] Creating layer fc8
I1226 09:06:58.758172 90317 net.cpp:178] Creating Layer fc8
I1226 09:06:58.758203 90317 net.cpp:612] fc8 <- fc7
I1226 09:06:58.758258 90317 net.cpp:586] fc8 -> fc8
I1226 09:07:02.051203 90178 net.cpp:228] Setting up fc7
I1226 09:07:02.051331 90178 net.cpp:235] Top shape: 32 4096 (131072)
I1226 09:07:02.051365 90178 net.cpp:243] Memory required for data: 265039616
I1226 09:07:02.051445 90178 layer_factory.hpp:114] Creating layer relu7
I1226 09:07:02.051550 90178 net.cpp:178] Creating Layer relu7
I1226 09:07:02.051601 90178 net.cpp:612] relu7 <- fc7
I1226 09:07:02.051650 90178 net.cpp:573] relu7 -> fc7 (in-place)
I1226 09:07:02.051745 90178 net.cpp:228] Setting up relu7
I1226 09:07:02.051805 90178 net.cpp:235] Top shape: 32 4096 (131072)
I1226 09:07:02.051872 90178 net.cpp:243] Memory required for data: 265563904
I1226 09:07:02.051954 90178 layer_factory.hpp:114] Creating layer drop7
I1226 09:07:02.052217 90178 net.cpp:178] Creating Layer drop7
I1226 09:07:02.052281 90178 net.cpp:612] drop7 <- fc7
I1226 09:07:02.052367 90178 net.cpp:573] drop7 -> fc7 (in-place)
I1226 09:07:02.052534 90178 net.cpp:228] Setting up drop7
I1226 09:07:02.052775 90178 net.cpp:235] Top shape: 32 4096 (131072)
I1226 09:07:02.052835 90178 net.cpp:243] Memory required for data: 266088192
I1226 09:07:02.052882 90178 layer_factory.hpp:114] Creating layer fc8
I1226 09:07:02.052961 90178 net.cpp:178] Creating Layer fc8
I1226 09:07:02.053009 90178 net.cpp:612] fc8 <- fc7
I1226 09:07:02.053066 90178 net.cpp:586] fc8 -> fc8
I1226 09:07:02.064375 117050 net.cpp:228] Setting up fc7
I1226 09:07:02.064484 117050 net.cpp:235] Top shape: 32 4096 (131072)
I1226 09:07:02.064514 117050 net.cpp:243] Memory required for data: 265039616
I1226 09:07:02.064597 117050 layer_factory.hpp:114] Creating layer relu7
I1226 09:07:02.064683 117050 net.cpp:178] Creating Layer relu7
I1226 09:07:02.064730 117050 net.cpp:612] relu7 <- fc7
I1226 09:07:02.064769 117050 net.cpp:573] relu7 -> fc7 (in-place)
I1226 09:07:02.064857 117050 net.cpp:228] Setting up relu7
I1226 09:07:02.064905 117050 net.cpp:235] Top shape: 32 4096 (131072)
I1226 09:07:02.064930 117050 net.cpp:243] Memory required for data: 265563904
I1226 09:07:02.064960 117050 layer_factory.hpp:114] Creating layer drop7
I1226 09:07:02.065014 117050 net.cpp:178] Creating Layer drop7
I1226 09:07:02.065050 117050 net.cpp:612] drop7 <- fc7
I1226 09:07:02.065085 117050 net.cpp:573] drop7 -> fc7 (in-place)
I1226 09:07:02.065129 117050 net.cpp:228] Setting up drop7
I1226 09:07:02.065168 117050 net.cpp:235] Top shape: 32 4096 (131072)
I1226 09:07:02.065196 117050 net.cpp:243] Memory required for data: 266088192
I1226 09:07:02.065223 117050 layer_factory.hpp:114] Creating layer fc8
I1226 09:07:02.065281 117050 net.cpp:178] Creating Layer fc8
I1226 09:07:02.065313 117050 net.cpp:612] fc8 <- fc7
I1226 09:07:02.065364 117050 net.cpp:586] fc8 -> fc8
I1226 09:07:02.092777 96748 net.cpp:228] Setting up fc7
I1226 09:07:02.092890 96748 net.cpp:235] Top shape: 32 4096 (131072)
I1226 09:07:02.092922 96748 net.cpp:243] Memory required for data: 265039616
I1226 09:07:02.092981 96748 layer_factory.hpp:114] Creating layer relu7
I1226 09:07:02.093104 96748 net.cpp:178] Creating Layer relu7
I1226 09:07:02.093149 96748 net.cpp:612] relu7 <- fc7
I1226 09:07:02.093192 96748 net.cpp:573] relu7 -> fc7 (in-place)
I1226 09:07:02.093277 96748 net.cpp:228] Setting up relu7
I1226 09:07:02.093318 96748 net.cpp:235] Top shape: 32 4096 (131072)
I1226 09:07:02.093341 96748 net.cpp:243] Memory required for data: 265563904
I1226 09:07:02.093371 96748 layer_factory.hpp:114] Creating layer drop7
I1226 09:07:02.093426 96748 net.cpp:178] Creating Layer drop7
I1226 09:07:02.093456 96748 net.cpp:612] drop7 <- fc7
I1226 09:07:02.093489 96748 net.cpp:573] drop7 -> fc7 (in-place)
I1226 09:07:02.093549 96748 net.cpp:228] Setting up drop7
I1226 09:07:02.093585 96748 net.cpp:235] Top shape: 32 4096 (131072)
I1226 09:07:02.093618 96748 net.cpp:243] Memory required for data: 266088192
I1226 09:07:02.093648 96748 layer_factory.hpp:114] Creating layer fc8
I1226 09:07:02.093709 96748 net.cpp:178] Creating Layer fc8
I1226 09:07:02.093739 96748 net.cpp:612] fc8 <- fc7
I1226 09:07:02.093780 96748 net.cpp:586] fc8 -> fc8
I1226 09:07:02.147300 88808 net.cpp:228] Setting up fc7
I1226 09:07:02.147416 88808 net.cpp:235] Top shape: 32 4096 (131072)
I1226 09:07:02.147446 88808 net.cpp:243] Memory required for data: 265039616
I1226 09:07:02.147505 88808 layer_factory.hpp:114] Creating layer relu7
I1226 09:07:02.147621 88808 net.cpp:178] Creating Layer relu7
I1226 09:07:02.147666 88808 net.cpp:612] relu7 <- fc7
I1226 09:07:02.147722 88808 net.cpp:573] relu7 -> fc7 (in-place)
I1226 09:07:02.147819 88808 net.cpp:228] Setting up relu7
I1226 09:07:02.147868 88808 net.cpp:235] Top shape: 32 4096 (131072)
I1226 09:07:02.147893 88808 net.cpp:243] Memory required for data: 265563904
I1226 09:07:02.147924 88808 layer_factory.hpp:114] Creating layer drop7
I1226 09:07:02.147982 88808 net.cpp:178] Creating Layer drop7
I1226 09:07:02.148015 88808 net.cpp:612] drop7 <- fc7
I1226 09:07:02.148052 88808 net.cpp:573] drop7 -> fc7 (in-place)
I1226 09:07:02.148098 88808 net.cpp:228] Setting up drop7
I1226 09:07:02.148131 88808 net.cpp:235] Top shape: 32 4096 (131072)
I1226 09:07:02.148155 88808 net.cpp:243] Memory required for data: 266088192
I1226 09:07:02.148191 88808 layer_factory.hpp:114] Creating layer fc8
I1226 09:07:02.148254 88808 net.cpp:178] Creating Layer fc8
I1226 09:07:02.148283 88808 net.cpp:612] fc8 <- fc7
I1226 09:07:02.148332 88808 net.cpp:586] fc8 -> fc8
I1226 09:07:02.179738 120057 net.cpp:228] Setting up fc7
I1226 09:07:02.179853 120057 net.cpp:235] Top shape: 32 4096 (131072)
I1226 09:07:02.179885 120057 net.cpp:243] Memory required for data: 265039616
I1226 09:07:02.179947 120057 layer_factory.hpp:114] Creating layer relu7
I1226 09:07:02.180044 120057 net.cpp:178] Creating Layer relu7
I1226 09:07:02.180094 120057 net.cpp:612] relu7 <- fc7
I1226 09:07:02.180143 120057 net.cpp:573] relu7 -> fc7 (in-place)
I1226 09:07:02.180240 120057 net.cpp:228] Setting up relu7
I1226 09:07:02.180310 120057 net.cpp:235] Top shape: 32 4096 (131072)
I1226 09:07:02.180343 120057 net.cpp:243] Memory required for data: 265563904
I1226 09:07:02.180382 120057 layer_factory.hpp:114] Creating layer drop7
I1226 09:07:02.180443 120057 net.cpp:178] Creating Layer drop7
I1226 09:07:02.180477 120057 net.cpp:612] drop7 <- fc7
I1226 09:07:02.180518 120057 net.cpp:573] drop7 -> fc7 (in-place)
I1226 09:07:02.180567 120057 net.cpp:228] Setting up drop7
I1226 09:07:02.180632 120057 net.cpp:235] Top shape: 32 4096 (131072)
I1226 09:07:02.180665 120057 net.cpp:243] Memory required for data: 266088192
I1226 09:07:02.180696 120057 layer_factory.hpp:114] Creating layer fc8
I1226 09:07:02.180759 120057 net.cpp:178] Creating Layer fc8
I1226 09:07:02.180789 120057 net.cpp:612] fc8 <- fc7
I1226 09:07:02.180848 120057 net.cpp:586] fc8 -> fc8
I1226 09:07:02.307533 91281 net.cpp:228] Setting up fc8
I1226 09:07:02.307656 91281 net.cpp:235] Top shape: 32 1000 (32000)
I1226 09:07:02.307687 91281 net.cpp:243] Memory required for data: 266216192
I1226 09:07:02.307746 91281 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 09:07:02.307818 91281 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 09:07:02.307857 91281 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 09:07:02.307903 91281 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 09:07:02.307961 91281 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 09:07:02.308159 91281 net.cpp:228] Setting up fc8_fc8_0_split
I1226 09:07:02.308209 91281 net.cpp:235] Top shape: 32 1000 (32000)
I1226 09:07:02.308238 91281 net.cpp:235] Top shape: 32 1000 (32000)
I1226 09:07:02.308264 91281 net.cpp:243] Memory required for data: 266472192
I1226 09:07:02.308298 91281 layer_factory.hpp:114] Creating layer accuracy
I1226 09:07:02.308348 91281 net.cpp:178] Creating Layer accuracy
I1226 09:07:02.308403 91281 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 09:07:02.308436 91281 net.cpp:612] accuracy <- label_data_1_split_0
I1226 09:07:02.308470 91281 net.cpp:586] accuracy -> accuracy
I1226 09:07:02.308516 91281 net.cpp:228] Setting up accuracy
I1226 09:07:02.308557 91281 net.cpp:235] Top shape: (1)
I1226 09:07:02.308581 91281 net.cpp:243] Memory required for data: 266472196
I1226 09:07:02.308609 91281 layer_factory.hpp:114] Creating layer loss
I1226 09:07:02.308662 91281 net.cpp:178] Creating Layer loss
I1226 09:07:02.308693 91281 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 09:07:02.308744 91281 net.cpp:612] loss <- label_data_1_split_1
I1226 09:07:02.308784 91281 net.cpp:586] loss -> loss
I1226 09:07:02.308845 91281 layer_factory.hpp:114] Creating layer loss
I1226 09:07:02.336573 91281 net.cpp:228] Setting up loss
I1226 09:07:02.336678 91281 net.cpp:235] Top shape: (1)
I1226 09:07:02.336976 91281 net.cpp:238]     with loss weight 1
I1226 09:07:02.337098 91281 net.cpp:243] Memory required for data: 266472200
I1226 09:07:02.337138 91281 net.cpp:305] loss needs backward computation.
I1226 09:07:02.337175 91281 net.cpp:307] accuracy does not need backward computation.
I1226 09:07:02.337206 91281 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 09:07:02.337234 91281 net.cpp:305] fc8 needs backward computation.
I1226 09:07:02.337271 91281 net.cpp:305] drop7 needs backward computation.
I1226 09:07:02.337297 91281 net.cpp:305] relu7 needs backward computation.
I1226 09:07:02.337327 91281 net.cpp:305] fc7 needs backward computation.
I1226 09:07:02.337352 91281 net.cpp:305] drop6 needs backward computation.
I1226 09:07:02.337393 91281 net.cpp:305] relu6 needs backward computation.
I1226 09:07:02.337424 91281 net.cpp:305] fc6 needs backward computation.
I1226 09:07:02.337461 91281 net.cpp:305] pool5 needs backward computation.
I1226 09:07:02.337487 91281 net.cpp:305] relu5 needs backward computation.
I1226 09:07:02.337512 91281 net.cpp:305] conv5 needs backward computation.
I1226 09:07:02.337554 91281 net.cpp:305] relu4 needs backward computation.
I1226 09:07:02.337579 91281 net.cpp:305] conv4 needs backward computation.
I1226 09:07:02.337604 91281 net.cpp:305] relu3 needs backward computation.
I1226 09:07:02.337637 91281 net.cpp:305] conv3 needs backward computation.
I1226 09:07:02.337662 91281 net.cpp:305] pool2 needs backward computation.
I1226 09:07:02.337687 91281 net.cpp:305] norm2 needs backward computation.
I1226 09:07:02.337718 91281 net.cpp:305] relu2 needs backward computation.
I1226 09:07:02.337741 91281 net.cpp:305] conv2 needs backward computation.
I1226 09:07:02.337771 91281 net.cpp:305] pool1 needs backward computation.
I1226 09:07:02.337802 91281 net.cpp:305] norm1 needs backward computation.
I1226 09:07:02.337827 91281 net.cpp:305] relu1 needs backward computation.
I1226 09:07:02.337854 91281 net.cpp:305] conv1 needs backward computation.
I1226 09:07:02.337898 91281 net.cpp:307] label_data_1_split does not need backward computation.
I1226 09:07:02.337934 91281 net.cpp:307] data does not need backward computation.
I1226 09:07:02.337956 91281 net.cpp:349] This network produces output accuracy
I1226 09:07:02.337990 91281 net.cpp:349] This network produces output loss
I1226 09:07:02.338066 91281 net.cpp:363] Network initialization done.
I1226 09:07:02.338510 91281 solver.cpp:119] Solver scaffolding done.
I1226 09:07:02.338690 91281 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 09:06:59.306838 90317 net.cpp:228] Setting up fc8
I1226 09:06:59.306948 90317 net.cpp:235] Top shape: 32 1000 (32000)
I1226 09:06:59.306975 90317 net.cpp:243] Memory required for data: 266216192
I1226 09:06:59.307059 90317 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 09:06:59.307129 90317 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 09:06:59.307173 90317 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 09:06:59.307214 90317 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 09:06:59.307339 90317 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 09:06:59.307427 90317 net.cpp:228] Setting up fc8_fc8_0_split
I1226 09:06:59.307478 90317 net.cpp:235] Top shape: 32 1000 (32000)
I1226 09:06:59.307508 90317 net.cpp:235] Top shape: 32 1000 (32000)
I1226 09:06:59.307531 90317 net.cpp:243] Memory required for data: 266472192
I1226 09:06:59.307569 90317 layer_factory.hpp:114] Creating layer accuracy
I1226 09:06:59.307646 90317 net.cpp:178] Creating Layer accuracy
I1226 09:06:59.307675 90317 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 09:06:59.307716 90317 net.cpp:612] accuracy <- label_data_1_split_0
I1226 09:06:59.307757 90317 net.cpp:586] accuracy -> accuracy
I1226 09:06:59.307804 90317 net.cpp:228] Setting up accuracy
I1226 09:06:59.307840 90317 net.cpp:235] Top shape: (1)
I1226 09:06:59.307863 90317 net.cpp:243] Memory required for data: 266472196
I1226 09:06:59.307896 90317 layer_factory.hpp:114] Creating layer loss
I1226 09:06:59.307958 90317 net.cpp:178] Creating Layer loss
I1226 09:06:59.307986 90317 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 09:06:59.308017 90317 net.cpp:612] loss <- label_data_1_split_1
I1226 09:06:59.308049 90317 net.cpp:586] loss -> loss
I1226 09:06:59.308106 90317 layer_factory.hpp:114] Creating layer loss
I1226 09:07:02.603979 90178 net.cpp:228] Setting up fc8
I1226 09:07:02.604122 90178 net.cpp:235] Top shape: 32 1000 (32000)
I1226 09:07:02.604189 90178 net.cpp:243] Memory required for data: 266216192
I1226 09:07:02.604265 90178 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 09:07:02.604343 90178 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 09:07:02.604394 90178 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 09:07:02.604445 90178 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 09:07:02.604564 90178 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 09:07:02.604676 90178 net.cpp:228] Setting up fc8_fc8_0_split
I1226 09:07:02.604733 90178 net.cpp:235] Top shape: 32 1000 (32000)
I1226 09:07:02.604774 90178 net.cpp:235] Top shape: 32 1000 (32000)
I1226 09:07:02.604805 90178 net.cpp:243] Memory required for data: 266472192
I1226 09:07:02.604873 90178 layer_factory.hpp:114] Creating layer accuracy
I1226 09:07:02.604995 90178 net.cpp:178] Creating Layer accuracy
I1226 09:07:02.605043 90178 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 09:07:02.605083 90178 net.cpp:612] accuracy <- label_data_1_split_0
I1226 09:07:02.605131 90178 net.cpp:586] accuracy -> accuracy
I1226 09:07:02.605198 90178 net.cpp:228] Setting up accuracy
I1226 09:07:02.605257 90178 net.cpp:235] Top shape: (1)
I1226 09:07:02.605288 90178 net.cpp:243] Memory required for data: 266472196
I1226 09:07:02.605324 90178 layer_factory.hpp:114] Creating layer loss
I1226 09:07:02.605381 90178 net.cpp:178] Creating Layer loss
I1226 09:07:02.605427 90178 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 09:07:02.605485 90178 net.cpp:612] loss <- label_data_1_split_1
I1226 09:07:02.605538 90178 net.cpp:586] loss -> loss
I1226 09:07:02.605681 90178 layer_factory.hpp:114] Creating layer loss
I1226 09:06:59.338985 90317 net.cpp:228] Setting up loss
I1226 09:06:59.339104 90317 net.cpp:235] Top shape: (1)
I1226 09:06:59.339277 90317 net.cpp:238]     with loss weight 1
I1226 09:06:59.339440 90317 net.cpp:243] Memory required for data: 266472200
I1226 09:06:59.339493 90317 net.cpp:305] loss needs backward computation.
I1226 09:06:59.339535 90317 net.cpp:307] accuracy does not need backward computation.
I1226 09:06:59.339617 90317 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 09:06:59.339663 90317 net.cpp:305] fc8 needs backward computation.
I1226 09:06:59.339697 90317 net.cpp:305] drop7 needs backward computation.
I1226 09:06:59.339743 90317 net.cpp:305] relu7 needs backward computation.
I1226 09:06:59.339784 90317 net.cpp:305] fc7 needs backward computation.
I1226 09:06:59.339817 90317 net.cpp:305] drop6 needs backward computation.
I1226 09:06:59.339848 90317 net.cpp:305] relu6 needs backward computation.
I1226 09:06:59.339876 90317 net.cpp:305] fc6 needs backward computation.
I1226 09:06:59.339907 90317 net.cpp:305] pool5 needs backward computation.
I1226 09:06:59.339949 90317 net.cpp:305] relu5 needs backward computation.
I1226 09:06:59.339987 90317 net.cpp:305] conv5 needs backward computation.
I1226 09:06:59.340028 90317 net.cpp:305] relu4 needs backward computation.
I1226 09:06:59.340056 90317 net.cpp:305] conv4 needs backward computation.
I1226 09:06:59.340090 90317 net.cpp:305] relu3 needs backward computation.
I1226 09:06:59.340119 90317 net.cpp:305] conv3 needs backward computation.
I1226 09:06:59.340150 90317 net.cpp:305] pool2 needs backward computation.
I1226 09:06:59.340209 90317 net.cpp:305] norm2 needs backward computation.
I1226 09:06:59.340245 90317 net.cpp:305] relu2 needs backward computation.
I1226 09:06:59.340276 90317 net.cpp:305] conv2 needs backward computation.
I1226 09:06:59.340308 90317 net.cpp:305] pool1 needs backward computation.
I1226 09:06:59.340339 90317 net.cpp:305] norm1 needs backward computation.
I1226 09:06:59.340379 90317 net.cpp:305] relu1 needs backward computation.
I1226 09:06:59.340420 90317 net.cpp:305] conv1 needs backward computation.
I1226 09:06:59.340458 90317 net.cpp:307] label_data_1_split does not need backward computation.
I1226 09:06:59.340492 90317 net.cpp:307] data does not need backward computation.
I1226 09:06:59.340525 90317 net.cpp:349] This network produces output accuracy
I1226 09:06:59.340560 90317 net.cpp:349] This network produces output loss
I1226 09:06:59.340679 90317 net.cpp:363] Network initialization done.
I1226 09:06:59.341115 90317 solver.cpp:119] Solver scaffolding done.
I1226 09:06:59.341334 90317 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 09:07:02.636909 90178 net.cpp:228] Setting up loss
I1226 09:07:02.637022 90178 net.cpp:235] Top shape: (1)
I1226 09:07:02.637187 90178 net.cpp:238]     with loss weight 1
I1226 09:07:02.637339 90178 net.cpp:243] Memory required for data: 266472200
I1226 09:07:02.637392 90178 net.cpp:305] loss needs backward computation.
I1226 09:07:02.637435 90178 net.cpp:307] accuracy does not need backward computation.
I1226 09:07:02.637475 90178 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 09:07:02.637508 90178 net.cpp:305] fc8 needs backward computation.
I1226 09:07:02.637547 90178 net.cpp:305] drop7 needs backward computation.
I1226 09:07:02.637578 90178 net.cpp:305] relu7 needs backward computation.
I1226 09:07:02.637609 90178 net.cpp:305] fc7 needs backward computation.
I1226 09:07:02.637641 90178 net.cpp:305] drop6 needs backward computation.
I1226 09:07:02.637684 90178 net.cpp:305] relu6 needs backward computation.
I1226 09:07:02.637715 90178 net.cpp:305] fc6 needs backward computation.
I1226 09:07:02.637747 90178 net.cpp:305] pool5 needs backward computation.
I1226 09:07:02.637785 90178 net.cpp:305] relu5 needs backward computation.
I1226 09:07:02.637845 90178 net.cpp:305] conv5 needs backward computation.
I1226 09:07:02.637882 90178 net.cpp:305] relu4 needs backward computation.
I1226 09:07:02.637923 90178 net.cpp:305] conv4 needs backward computation.
I1226 09:07:02.637953 90178 net.cpp:305] relu3 needs backward computation.
I1226 09:07:02.637989 90178 net.cpp:305] conv3 needs backward computation.
I1226 09:07:02.638020 90178 net.cpp:305] pool2 needs backward computation.
I1226 09:07:02.638058 90178 net.cpp:305] norm2 needs backward computation.
I1226 09:07:02.638089 90178 net.cpp:305] relu2 needs backward computation.
I1226 09:07:02.638118 90178 net.cpp:305] conv2 needs backward computation.
I1226 09:07:02.638150 90178 net.cpp:305] pool1 needs backward computation.
I1226 09:07:02.638181 90178 net.cpp:305] norm1 needs backward computation.
I1226 09:07:02.638211 90178 net.cpp:305] relu1 needs backward computation.
I1226 09:07:02.638239 90178 net.cpp:305] conv1 needs backward computation.
I1226 09:07:02.638300 90178 net.cpp:307] label_data_1_split does not need backward computation.
I1226 09:07:02.638341 90178 net.cpp:307] data does not need backward computation.
I1226 09:07:02.638377 90178 net.cpp:349] This network produces output accuracy
I1226 09:07:02.638412 90178 net.cpp:349] This network produces output loss
I1226 09:07:02.638507 90178 net.cpp:363] Network initialization done.
I1226 09:07:02.638983 90178 solver.cpp:119] Solver scaffolding done.
I1226 09:07:02.639214 90178 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 09:07:02.636656 117050 net.cpp:228] Setting up fc8
I1226 09:07:02.636776 117050 net.cpp:235] Top shape: 32 1000 (32000)
I1226 09:07:02.636806 117050 net.cpp:243] Memory required for data: 266216192
I1226 09:07:02.636859 117050 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 09:07:02.636927 117050 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 09:07:02.637027 117050 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 09:07:02.637102 117050 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 09:07:02.637166 117050 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 09:07:02.637254 117050 net.cpp:228] Setting up fc8_fc8_0_split
I1226 09:07:02.637305 117050 net.cpp:235] Top shape: 32 1000 (32000)
I1226 09:07:02.637339 117050 net.cpp:235] Top shape: 32 1000 (32000)
I1226 09:07:02.637364 117050 net.cpp:243] Memory required for data: 266472192
I1226 09:07:02.637394 117050 layer_factory.hpp:114] Creating layer accuracy
I1226 09:07:02.637455 117050 net.cpp:178] Creating Layer accuracy
I1226 09:07:02.637490 117050 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 09:07:02.637522 117050 net.cpp:612] accuracy <- label_data_1_split_0
I1226 09:07:02.637586 117050 net.cpp:586] accuracy -> accuracy
I1226 09:07:02.637670 117050 net.cpp:228] Setting up accuracy
I1226 09:07:02.637703 117050 net.cpp:235] Top shape: (1)
I1226 09:07:02.637812 117050 net.cpp:243] Memory required for data: 266472196
I1226 09:07:02.637840 117050 layer_factory.hpp:114] Creating layer loss
I1226 09:07:02.637907 117050 net.cpp:178] Creating Layer loss
I1226 09:07:02.637940 117050 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 09:07:02.637970 117050 net.cpp:612] loss <- label_data_1_split_1
I1226 09:07:02.638005 117050 net.cpp:586] loss -> loss
I1226 09:07:02.638172 117050 layer_factory.hpp:114] Creating layer loss
I1226 09:07:02.644628 96748 net.cpp:228] Setting up fc8
I1226 09:07:02.644757 96748 net.cpp:235] Top shape: 32 1000 (32000)
I1226 09:07:02.644785 96748 net.cpp:243] Memory required for data: 266216192
I1226 09:07:02.644870 96748 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 09:07:02.644943 96748 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 09:07:02.644975 96748 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 09:07:02.645015 96748 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 09:07:02.645086 96748 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 09:07:02.645198 96748 net.cpp:228] Setting up fc8_fc8_0_split
I1226 09:07:02.645253 96748 net.cpp:235] Top shape: 32 1000 (32000)
I1226 09:07:02.645287 96748 net.cpp:235] Top shape: 32 1000 (32000)
I1226 09:07:02.645316 96748 net.cpp:243] Memory required for data: 266472192
I1226 09:07:02.645351 96748 layer_factory.hpp:114] Creating layer accuracy
I1226 09:07:02.645406 96748 net.cpp:178] Creating Layer accuracy
I1226 09:07:02.645433 96748 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 09:07:02.645464 96748 net.cpp:612] accuracy <- label_data_1_split_0
I1226 09:07:02.645505 96748 net.cpp:586] accuracy -> accuracy
I1226 09:07:02.645551 96748 net.cpp:228] Setting up accuracy
I1226 09:07:02.645589 96748 net.cpp:235] Top shape: (1)
I1226 09:07:02.645612 96748 net.cpp:243] Memory required for data: 266472196
I1226 09:07:02.645645 96748 layer_factory.hpp:114] Creating layer loss
I1226 09:07:02.645689 96748 net.cpp:178] Creating Layer loss
I1226 09:07:02.645715 96748 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 09:07:02.645757 96748 net.cpp:612] loss <- label_data_1_split_1
I1226 09:07:02.645792 96748 net.cpp:586] loss -> loss
I1226 09:07:02.645853 96748 layer_factory.hpp:114] Creating layer loss
I1226 09:07:02.665815 117050 net.cpp:228] Setting up loss
I1226 09:07:02.665918 117050 net.cpp:235] Top shape: (1)
I1226 09:07:02.665947 117050 net.cpp:238]     with loss weight 1
I1226 09:07:02.666074 117050 net.cpp:243] Memory required for data: 266472200
I1226 09:07:02.666112 117050 net.cpp:305] loss needs backward computation.
I1226 09:07:02.666151 117050 net.cpp:307] accuracy does not need backward computation.
I1226 09:07:02.666191 117050 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 09:07:02.666229 117050 net.cpp:305] fc8 needs backward computation.
I1226 09:07:02.666261 117050 net.cpp:305] drop7 needs backward computation.
I1226 09:07:02.666297 117050 net.cpp:305] relu7 needs backward computation.
I1226 09:07:02.666333 117050 net.cpp:305] fc7 needs backward computation.
I1226 09:07:02.666373 117050 net.cpp:305] drop6 needs backward computation.
I1226 09:07:02.666404 117050 net.cpp:305] relu6 needs backward computation.
I1226 09:07:02.666440 117050 net.cpp:305] fc6 needs backward computation.
I1226 09:07:02.666478 117050 net.cpp:305] pool5 needs backward computation.
I1226 09:07:02.666515 117050 net.cpp:305] relu5 needs backward computation.
I1226 09:07:02.666570 117050 net.cpp:305] conv5 needs backward computation.
I1226 09:07:02.666604 117050 net.cpp:305] relu4 needs backward computation.
I1226 09:07:02.666633 117050 net.cpp:305] conv4 needs backward computation.
I1226 09:07:02.666663 117050 net.cpp:305] relu3 needs backward computation.
I1226 09:07:02.666692 117050 net.cpp:305] conv3 needs backward computation.
I1226 09:07:02.666720 117050 net.cpp:305] pool2 needs backward computation.
I1226 09:07:02.666748 117050 net.cpp:305] norm2 needs backward computation.
I1226 09:07:02.666774 117050 net.cpp:305] relu2 needs backward computation.
I1226 09:07:02.666798 117050 net.cpp:305] conv2 needs backward computation.
I1226 09:07:02.666824 117050 net.cpp:305] pool1 needs backward computation.
I1226 09:07:02.666864 117050 net.cpp:305] norm1 needs backward computation.
I1226 09:07:02.666895 117050 net.cpp:305] relu1 needs backward computation.
I1226 09:07:02.666924 117050 net.cpp:305] conv1 needs backward computation.
I1226 09:07:02.666959 117050 net.cpp:307] label_data_1_split does not need backward computation.
I1226 09:07:02.666997 117050 net.cpp:307] data does not need backward computation.
I1226 09:07:02.667034 117050 net.cpp:349] This network produces output accuracy
I1226 09:07:02.667069 117050 net.cpp:349] This network produces output loss
I1226 09:07:02.667158 117050 net.cpp:363] Network initialization done.
I1226 09:07:02.667588 117050 solver.cpp:119] Solver scaffolding done.
I1226 09:07:02.667768 117050 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 09:07:02.669641 96748 net.cpp:228] Setting up loss
I1226 09:07:02.669751 96748 net.cpp:235] Top shape: (1)
I1226 09:07:02.670056 96748 net.cpp:238]     with loss weight 1
I1226 09:07:02.670222 96748 net.cpp:243] Memory required for data: 266472200
I1226 09:07:02.670264 96748 net.cpp:305] loss needs backward computation.
I1226 09:07:02.670307 96748 net.cpp:307] accuracy does not need backward computation.
I1226 09:07:02.670344 96748 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 09:07:02.670378 96748 net.cpp:305] fc8 needs backward computation.
I1226 09:07:02.670410 96748 net.cpp:305] drop7 needs backward computation.
I1226 09:07:02.670440 96748 net.cpp:305] relu7 needs backward computation.
I1226 09:07:02.670485 96748 net.cpp:305] fc7 needs backward computation.
I1226 09:07:02.670517 96748 net.cpp:305] drop6 needs backward computation.
I1226 09:07:02.670547 96748 net.cpp:305] relu6 needs backward computation.
I1226 09:07:02.670578 96748 net.cpp:305] fc6 needs backward computation.
I1226 09:07:02.670608 96748 net.cpp:305] pool5 needs backward computation.
I1226 09:07:02.670647 96748 net.cpp:305] relu5 needs backward computation.
I1226 09:07:02.670688 96748 net.cpp:305] conv5 needs backward computation.
I1226 09:07:02.670727 96748 net.cpp:305] relu4 needs backward computation.
I1226 09:07:02.670766 96748 net.cpp:305] conv4 needs backward computation.
I1226 09:07:02.670799 96748 net.cpp:305] relu3 needs backward computation.
I1226 09:07:02.670828 96748 net.cpp:305] conv3 needs backward computation.
I1226 09:07:02.670869 96748 net.cpp:305] pool2 needs backward computation.
I1226 09:07:02.670907 96748 net.cpp:305] norm2 needs backward computation.
I1226 09:07:02.670938 96748 net.cpp:305] relu2 needs backward computation.
I1226 09:07:02.670966 96748 net.cpp:305] conv2 needs backward computation.
I1226 09:07:02.670999 96748 net.cpp:305] pool1 needs backward computation.
I1226 09:07:02.671039 96748 net.cpp:305] norm1 needs backward computation.
I1226 09:07:02.671092 96748 net.cpp:305] relu1 needs backward computation.
I1226 09:07:02.671123 96748 net.cpp:305] conv1 needs backward computation.
I1226 09:07:02.671186 96748 net.cpp:307] label_data_1_split does not need backward computation.
I1226 09:07:02.671236 96748 net.cpp:307] data does not need backward computation.
I1226 09:07:02.671267 96748 net.cpp:349] This network produces output accuracy
I1226 09:07:02.671305 96748 net.cpp:349] This network produces output loss
I1226 09:07:02.671398 96748 net.cpp:363] Network initialization done.
I1226 09:07:02.671840 96748 solver.cpp:119] Solver scaffolding done.
I1226 09:07:02.672051 96748 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 09:07:02.714682 88808 net.cpp:228] Setting up fc8
I1226 09:07:02.714803 88808 net.cpp:235] Top shape: 32 1000 (32000)
I1226 09:07:02.714838 88808 net.cpp:243] Memory required for data: 266216192
I1226 09:07:02.714895 88808 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 09:07:02.714968 88808 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 09:07:02.715008 88808 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 09:07:02.715056 88808 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 09:07:02.715128 88808 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 09:07:02.715230 88808 net.cpp:228] Setting up fc8_fc8_0_split
I1226 09:07:02.715286 88808 net.cpp:235] Top shape: 32 1000 (32000)
I1226 09:07:02.715323 88808 net.cpp:235] Top shape: 32 1000 (32000)
I1226 09:07:02.715347 88808 net.cpp:243] Memory required for data: 266472192
I1226 09:07:02.715374 88808 layer_factory.hpp:114] Creating layer accuracy
I1226 09:07:02.715427 88808 net.cpp:178] Creating Layer accuracy
I1226 09:07:02.715461 88808 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 09:07:02.715492 88808 net.cpp:612] accuracy <- label_data_1_split_0
I1226 09:07:02.715530 88808 net.cpp:586] accuracy -> accuracy
I1226 09:07:02.715600 88808 net.cpp:228] Setting up accuracy
I1226 09:07:02.715641 88808 net.cpp:235] Top shape: (1)
I1226 09:07:02.715664 88808 net.cpp:243] Memory required for data: 266472196
I1226 09:07:02.715692 88808 layer_factory.hpp:114] Creating layer loss
I1226 09:07:02.715752 88808 net.cpp:178] Creating Layer loss
I1226 09:07:02.715782 88808 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 09:07:02.715821 88808 net.cpp:612] loss <- label_data_1_split_1
I1226 09:07:02.715862 88808 net.cpp:586] loss -> loss
I1226 09:07:02.715934 88808 layer_factory.hpp:114] Creating layer loss
I1226 09:07:02.738978 88808 net.cpp:228] Setting up loss
I1226 09:07:02.739096 88808 net.cpp:235] Top shape: (1)
I1226 09:07:02.739280 88808 net.cpp:238]     with loss weight 1
I1226 09:07:02.739444 88808 net.cpp:243] Memory required for data: 266472200
I1226 09:07:02.739497 88808 net.cpp:305] loss needs backward computation.
I1226 09:07:02.739542 88808 net.cpp:307] accuracy does not need backward computation.
I1226 09:07:02.739612 88808 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 09:07:02.739650 88808 net.cpp:305] fc8 needs backward computation.
I1226 09:07:02.739684 88808 net.cpp:305] drop7 needs backward computation.
I1226 09:07:02.739715 88808 net.cpp:305] relu7 needs backward computation.
I1226 09:07:02.739745 88808 net.cpp:305] fc7 needs backward computation.
I1226 09:07:02.739776 88808 net.cpp:305] drop6 needs backward computation.
I1226 09:07:02.739806 88808 net.cpp:305] relu6 needs backward computation.
I1226 09:07:02.739836 88808 net.cpp:305] fc6 needs backward computation.
I1226 09:07:02.739867 88808 net.cpp:305] pool5 needs backward computation.
I1226 09:07:02.739912 88808 net.cpp:305] relu5 needs backward computation.
I1226 09:07:02.739944 88808 net.cpp:305] conv5 needs backward computation.
I1226 09:07:02.739984 88808 net.cpp:305] relu4 needs backward computation.
I1226 09:07:02.740015 88808 net.cpp:305] conv4 needs backward computation.
I1226 09:07:02.740056 88808 net.cpp:305] relu3 needs backward computation.
I1226 09:07:02.740092 88808 net.cpp:305] conv3 needs backward computation.
I1226 09:07:02.740131 88808 net.cpp:305] pool2 needs backward computation.
I1226 09:07:02.740170 88808 net.cpp:305] norm2 needs backward computation.
I1226 09:07:02.740209 88808 net.cpp:305] relu2 needs backward computation.
I1226 09:07:02.740245 88808 net.cpp:305] conv2 needs backward computation.
I1226 09:07:02.740304 88808 net.cpp:305] pool1 needs backward computation.
I1226 09:07:02.740345 88808 net.cpp:305] norm1 needs backward computation.
I1226 09:07:02.740375 88808 net.cpp:305] relu1 needs backward computation.
I1226 09:07:02.740406 88808 net.cpp:305] conv1 needs backward computation.
I1226 09:07:02.740453 88808 net.cpp:307] label_data_1_split does not need backward computation.
I1226 09:07:02.740495 88808 net.cpp:307] data does not need backward computation.
I1226 09:07:02.740530 88808 net.cpp:349] This network produces output accuracy
I1226 09:07:02.740563 88808 net.cpp:349] This network produces output loss
I1226 09:07:02.740679 88808 net.cpp:363] Network initialization done.
I1226 09:07:02.741120 88808 solver.cpp:119] Solver scaffolding done.
I1226 09:07:02.741335 88808 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 09:07:02.753895 120057 net.cpp:228] Setting up fc8
I1226 09:07:02.754009 120057 net.cpp:235] Top shape: 32 1000 (32000)
I1226 09:07:02.754040 120057 net.cpp:243] Memory required for data: 266216192
I1226 09:07:02.754093 120057 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 09:07:02.754164 120057 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 09:07:02.754204 120057 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 09:07:02.754252 120057 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 09:07:02.754324 120057 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 09:07:02.754412 120057 net.cpp:228] Setting up fc8_fc8_0_split
I1226 09:07:02.754475 120057 net.cpp:235] Top shape: 32 1000 (32000)
I1226 09:07:02.754513 120057 net.cpp:235] Top shape: 32 1000 (32000)
I1226 09:07:02.754536 120057 net.cpp:243] Memory required for data: 266472192
I1226 09:07:02.754567 120057 layer_factory.hpp:114] Creating layer accuracy
I1226 09:07:02.754647 120057 net.cpp:178] Creating Layer accuracy
I1226 09:07:02.754681 120057 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 09:07:02.754714 120057 net.cpp:612] accuracy <- label_data_1_split_0
I1226 09:07:02.754750 120057 net.cpp:586] accuracy -> accuracy
I1226 09:07:02.754799 120057 net.cpp:228] Setting up accuracy
I1226 09:07:02.754839 120057 net.cpp:235] Top shape: (1)
I1226 09:07:02.754863 120057 net.cpp:243] Memory required for data: 266472196
I1226 09:07:02.754890 120057 layer_factory.hpp:114] Creating layer loss
I1226 09:07:02.754952 120057 net.cpp:178] Creating Layer loss
I1226 09:07:02.754982 120057 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 09:07:02.755012 120057 net.cpp:612] loss <- label_data_1_split_1
I1226 09:07:02.755048 120057 net.cpp:586] loss -> loss
I1226 09:07:02.755223 120057 layer_factory.hpp:114] Creating layer loss
I1226 09:07:02.782893 120057 net.cpp:228] Setting up loss
I1226 09:07:02.782995 120057 net.cpp:235] Top shape: (1)
I1226 09:07:02.783025 120057 net.cpp:238]     with loss weight 1
I1226 09:07:02.783138 120057 net.cpp:243] Memory required for data: 266472200
I1226 09:07:02.783179 120057 net.cpp:305] loss needs backward computation.
I1226 09:07:02.783218 120057 net.cpp:307] accuracy does not need backward computation.
I1226 09:07:02.783254 120057 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 09:07:02.783287 120057 net.cpp:305] fc8 needs backward computation.
I1226 09:07:02.783319 120057 net.cpp:305] drop7 needs backward computation.
I1226 09:07:02.783349 120057 net.cpp:305] relu7 needs backward computation.
I1226 09:07:02.783380 120057 net.cpp:305] fc7 needs backward computation.
I1226 09:07:02.783411 120057 net.cpp:305] drop6 needs backward computation.
I1226 09:07:02.783442 120057 net.cpp:305] relu6 needs backward computation.
I1226 09:07:02.783473 120057 net.cpp:305] fc6 needs backward computation.
I1226 09:07:02.783505 120057 net.cpp:305] pool5 needs backward computation.
I1226 09:07:02.783538 120057 net.cpp:305] relu5 needs backward computation.
I1226 09:07:02.783571 120057 net.cpp:305] conv5 needs backward computation.
I1226 09:07:02.783627 120057 net.cpp:305] relu4 needs backward computation.
I1226 09:07:02.783660 120057 net.cpp:305] conv4 needs backward computation.
I1226 09:07:02.783690 120057 net.cpp:305] relu3 needs backward computation.
I1226 09:07:02.783721 120057 net.cpp:305] conv3 needs backward computation.
I1226 09:07:02.783754 120057 net.cpp:305] pool2 needs backward computation.
I1226 09:07:02.783804 120057 net.cpp:305] norm2 needs backward computation.
I1226 09:07:02.783836 120057 net.cpp:305] relu2 needs backward computation.
I1226 09:07:02.783867 120057 net.cpp:305] conv2 needs backward computation.
I1226 09:07:02.783900 120057 net.cpp:305] pool1 needs backward computation.
I1226 09:07:02.783932 120057 net.cpp:305] norm1 needs backward computation.
I1226 09:07:02.783963 120057 net.cpp:305] relu1 needs backward computation.
I1226 09:07:02.783994 120057 net.cpp:305] conv1 needs backward computation.
I1226 09:07:02.784029 120057 net.cpp:307] label_data_1_split does not need backward computation.
I1226 09:07:02.784065 120057 net.cpp:307] data does not need backward computation.
I1226 09:07:02.784093 120057 net.cpp:349] This network produces output accuracy
I1226 09:07:02.784129 120057 net.cpp:349] This network produces output loss
I1226 09:07:02.784214 120057 net.cpp:363] Network initialization done.
I1226 09:07:02.784658 120057 solver.cpp:119] Solver scaffolding done.
I1226 09:07:02.784852 120057 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 09:07:04.012822 91281 caffe.cpp:376] Configuring multinode setup
I1226 09:07:04.014230 91281 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 09:07:02.575611 90317 caffe.cpp:376] Configuring multinode setup
I1226 09:07:02.577285 90317 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 09:07:06.053202 120057 caffe.cpp:376] Configuring multinode setup
I1226 09:07:06.054713 120057 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 09:07:06.113009 120755 caffe.cpp:376] Configuring multinode setup
I1226 09:07:06.115525 120755 caffe.cpp:386] Starting parameter server in mpi environment
I1226 09:07:06.688959 90178 caffe.cpp:376] Configuring multinode setup
I1226 09:07:06.690529 90178 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 09:07:06.710278 117050 caffe.cpp:376] Configuring multinode setup
I1226 09:07:06.711773 117050 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 09:07:06.715953 96748 caffe.cpp:376] Configuring multinode setup
I1226 09:07:06.717597 96748 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 09:07:06.827172 88808 caffe.cpp:376] Configuring multinode setup
I1226 09:07:06.828702 88808 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 09:07:26.348158 94464 net.cpp:228] Setting up fc6
I1226 09:07:26.348423 94464 net.cpp:235] Top shape: 32 4096 (131072)
I1226 09:07:26.348465 94464 net.cpp:243] Memory required for data: 263466752
I1226 09:07:26.348533 94464 layer_factory.hpp:114] Creating layer relu6
I1226 09:07:26.348701 94464 net.cpp:178] Creating Layer relu6
I1226 09:07:26.348750 94464 net.cpp:612] relu6 <- fc6
I1226 09:07:26.348817 94464 net.cpp:573] relu6 -> fc6 (in-place)
I1226 09:07:26.348933 94464 net.cpp:228] Setting up relu6
I1226 09:07:26.348979 94464 net.cpp:235] Top shape: 32 4096 (131072)
I1226 09:07:26.349005 94464 net.cpp:243] Memory required for data: 263991040
I1226 09:07:26.349035 94464 layer_factory.hpp:114] Creating layer drop6
I1226 09:07:26.349093 94464 net.cpp:178] Creating Layer drop6
I1226 09:07:26.349123 94464 net.cpp:612] drop6 <- fc6
I1226 09:07:26.349160 94464 net.cpp:573] drop6 -> fc6 (in-place)
I1226 09:07:26.349215 94464 net.cpp:228] Setting up drop6
I1226 09:07:26.349249 94464 net.cpp:235] Top shape: 32 4096 (131072)
I1226 09:07:26.349273 94464 net.cpp:243] Memory required for data: 264515328
I1226 09:07:26.349301 94464 layer_factory.hpp:114] Creating layer fc7
I1226 09:07:26.349373 94464 net.cpp:178] Creating Layer fc7
I1226 09:07:26.349403 94464 net.cpp:612] fc7 <- fc6
I1226 09:07:26.349441 94464 net.cpp:586] fc7 -> fc7
I1226 09:07:39.424681 94464 net.cpp:228] Setting up fc7
I1226 09:07:39.424825 94464 net.cpp:235] Top shape: 32 4096 (131072)
I1226 09:07:39.424865 94464 net.cpp:243] Memory required for data: 265039616
I1226 09:07:39.424926 94464 layer_factory.hpp:114] Creating layer relu7
I1226 09:07:39.425024 94464 net.cpp:178] Creating Layer relu7
I1226 09:07:39.425081 94464 net.cpp:612] relu7 <- fc7
I1226 09:07:39.425125 94464 net.cpp:573] relu7 -> fc7 (in-place)
I1226 09:07:39.425227 94464 net.cpp:228] Setting up relu7
I1226 09:07:39.425287 94464 net.cpp:235] Top shape: 32 4096 (131072)
I1226 09:07:39.425315 94464 net.cpp:243] Memory required for data: 265563904
I1226 09:07:39.425349 94464 layer_factory.hpp:114] Creating layer drop7
I1226 09:07:39.425400 94464 net.cpp:178] Creating Layer drop7
I1226 09:07:39.425439 94464 net.cpp:612] drop7 <- fc7
I1226 09:07:39.425508 94464 net.cpp:573] drop7 -> fc7 (in-place)
I1226 09:07:39.425568 94464 net.cpp:228] Setting up drop7
I1226 09:07:39.425710 94464 net.cpp:235] Top shape: 32 4096 (131072)
I1226 09:07:39.425737 94464 net.cpp:243] Memory required for data: 266088192
I1226 09:07:39.425768 94464 layer_factory.hpp:114] Creating layer fc8
I1226 09:07:39.425861 94464 net.cpp:178] Creating Layer fc8
I1226 09:07:39.425901 94464 net.cpp:612] fc8 <- fc7
I1226 09:07:39.425945 94464 net.cpp:586] fc8 -> fc8
I1226 09:07:42.619910 94464 net.cpp:228] Setting up fc8
I1226 09:07:42.620034 94464 net.cpp:235] Top shape: 32 1000 (32000)
I1226 09:07:42.620074 94464 net.cpp:243] Memory required for data: 266216192
I1226 09:07:42.620137 94464 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 09:07:42.620244 94464 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 09:07:42.620299 94464 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 09:07:42.620348 94464 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 09:07:42.620403 94464 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 09:07:42.620510 94464 net.cpp:228] Setting up fc8_fc8_0_split
I1226 09:07:42.620573 94464 net.cpp:235] Top shape: 32 1000 (32000)
I1226 09:07:42.620610 94464 net.cpp:235] Top shape: 32 1000 (32000)
I1226 09:07:42.620638 94464 net.cpp:243] Memory required for data: 266472192
I1226 09:07:42.620677 94464 layer_factory.hpp:114] Creating layer accuracy
I1226 09:07:42.620738 94464 net.cpp:178] Creating Layer accuracy
I1226 09:07:42.620777 94464 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 09:07:42.620841 94464 net.cpp:612] accuracy <- label_data_1_split_0
I1226 09:07:42.620878 94464 net.cpp:586] accuracy -> accuracy
I1226 09:07:42.620931 94464 net.cpp:228] Setting up accuracy
I1226 09:07:42.620975 94464 net.cpp:235] Top shape: (1)
I1226 09:07:42.621000 94464 net.cpp:243] Memory required for data: 266472196
I1226 09:07:42.621043 94464 layer_factory.hpp:114] Creating layer loss
I1226 09:07:42.621212 94464 net.cpp:178] Creating Layer loss
I1226 09:07:42.621254 94464 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 09:07:42.621300 94464 net.cpp:612] loss <- label_data_1_split_1
I1226 09:07:42.621371 94464 net.cpp:586] loss -> loss
I1226 09:07:42.621450 94464 layer_factory.hpp:114] Creating layer loss
I1226 09:07:42.656157 94464 net.cpp:228] Setting up loss
I1226 09:07:42.656275 94464 net.cpp:235] Top shape: (1)
I1226 09:07:42.656319 94464 net.cpp:238]     with loss weight 1
I1226 09:07:42.656478 94464 net.cpp:243] Memory required for data: 266472200
I1226 09:07:42.656533 94464 net.cpp:305] loss needs backward computation.
I1226 09:07:42.656594 94464 net.cpp:307] accuracy does not need backward computation.
I1226 09:07:42.656643 94464 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 09:07:42.656687 94464 net.cpp:305] fc8 needs backward computation.
I1226 09:07:42.656730 94464 net.cpp:305] drop7 needs backward computation.
I1226 09:07:42.656770 94464 net.cpp:305] relu7 needs backward computation.
I1226 09:07:42.656828 94464 net.cpp:305] fc7 needs backward computation.
I1226 09:07:42.656862 94464 net.cpp:305] drop6 needs backward computation.
I1226 09:07:42.656893 94464 net.cpp:305] relu6 needs backward computation.
I1226 09:07:42.656939 94464 net.cpp:305] fc6 needs backward computation.
I1226 09:07:42.656970 94464 net.cpp:305] pool5 needs backward computation.
I1226 09:07:42.657011 94464 net.cpp:305] relu5 needs backward computation.
I1226 09:07:42.657052 94464 net.cpp:305] conv5 needs backward computation.
I1226 09:07:42.657083 94464 net.cpp:305] relu4 needs backward computation.
I1226 09:07:42.657117 94464 net.cpp:305] conv4 needs backward computation.
I1226 09:07:42.657151 94464 net.cpp:305] relu3 needs backward computation.
I1226 09:07:42.657182 94464 net.cpp:305] conv3 needs backward computation.
I1226 09:07:42.657214 94464 net.cpp:305] pool2 needs backward computation.
I1226 09:07:42.657246 94464 net.cpp:305] norm2 needs backward computation.
I1226 09:07:42.657290 94464 net.cpp:305] relu2 needs backward computation.
I1226 09:07:42.657322 94464 net.cpp:305] conv2 needs backward computation.
I1226 09:07:42.657354 94464 net.cpp:305] pool1 needs backward computation.
I1226 09:07:42.657385 94464 net.cpp:305] norm1 needs backward computation.
I1226 09:07:42.657416 94464 net.cpp:305] relu1 needs backward computation.
I1226 09:07:42.657446 94464 net.cpp:305] conv1 needs backward computation.
I1226 09:07:42.657492 94464 net.cpp:307] label_data_1_split does not need backward computation.
I1226 09:07:42.657538 94464 net.cpp:307] data does not need backward computation.
I1226 09:07:42.657575 94464 net.cpp:349] This network produces output accuracy
I1226 09:07:42.657610 94464 net.cpp:349] This network produces output loss
I1226 09:07:42.657711 94464 net.cpp:363] Network initialization done.
I1226 09:07:42.658200 94464 solver.cpp:119] Solver scaffolding done.
I1226 09:07:42.658432 94464 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 09:07:47.234984 94464 caffe.cpp:376] Configuring multinode setup
I1226 09:07:47.236870 94464 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 09:07:47.227685 120057 SynchronousNode.cpp:662] [0] [proc 0] solving
I1226 09:07:47.237092 94464 SynchronousNode.cpp:662] [1] [proc 1] solving
I1226 09:07:47.227998 120057 solver.cpp:366] Solving AlexNet
I1226 09:07:47.228049 120057 solver.cpp:367] Learning Rate Policy: step
I1226 09:07:47.231307 91281 SynchronousNode.cpp:662] [4] [proc 4] solving
I1226 09:07:47.237196 94464 solver.cpp:366] Solving AlexNet
I1226 09:07:47.233992 96748 SynchronousNode.cpp:662] [3] [proc 3] solving
I1226 09:07:43.968904 90317 SynchronousNode.cpp:662] [6] [proc 6] solving
I1226 09:07:47.234529 88808 SynchronousNode.cpp:662] [7] [proc 7] solving
I1226 09:07:47.238831 90178 SynchronousNode.cpp:662] [2] [proc 2] solving
I1226 09:07:47.233886 117050 SynchronousNode.cpp:662] [5] [proc 5] solving
I1226 09:07:47.228304 120057 MultiSolver.cpp:76] [0] PROFILING BEGIN[Forward]
I1226 09:07:47.231662 91281 solver.cpp:366] Solving AlexNet
I1226 09:07:47.231698 91281 solver.cpp:367] Learning Rate Policy: step
I1226 09:07:47.237234 94464 solver.cpp:367] Learning Rate Policy: step
I1226 09:07:47.234305 96748 solver.cpp:366] Solving AlexNet
I1226 09:07:43.969189 90317 solver.cpp:366] Solving AlexNet
I1226 09:07:43.969236 90317 solver.cpp:367] Learning Rate Policy: step
I1226 09:07:47.234848 88808 solver.cpp:366] Solving AlexNet
I1226 09:07:47.234899 88808 solver.cpp:367] Learning Rate Policy: step
I1226 09:07:47.239125 90178 solver.cpp:366] Solving AlexNet
I1226 09:07:47.239176 90178 solver.cpp:367] Learning Rate Policy: step
I1226 09:07:47.234170 117050 solver.cpp:366] Solving AlexNet
I1226 09:07:47.234213 117050 solver.cpp:367] Learning Rate Policy: step
I1226 09:07:47.241188 120755 async_param_server.cpp:187] PS: Comm loop
I1226 09:07:47.231940 91281 MultiSolver.cpp:76] [4] PROFILING BEGIN[Forward]
I1226 09:07:47.237468 94464 MultiSolver.cpp:76] [1] PROFILING BEGIN[Forward]
I1226 09:07:47.234386 96748 solver.cpp:367] Learning Rate Policy: step
I1226 09:07:43.969473 90317 MultiSolver.cpp:76] [6] PROFILING BEGIN[Forward]
I1226 09:07:47.235126 88808 MultiSolver.cpp:76] [7] PROFILING BEGIN[Forward]
I1226 09:07:47.239406 90178 MultiSolver.cpp:76] [2] PROFILING BEGIN[Forward]
I1226 09:07:47.234452 117050 MultiSolver.cpp:76] [5] PROFILING BEGIN[Forward]
I1226 09:07:47.234676 96748 MultiSolver.cpp:76] [3] PROFILING BEGIN[Forward]
I1226 09:07:47.241219 94533 SynchronousNode.cpp:280] [1] Comm thread started 0 0
I1226 09:07:47.241792 91353 SynchronousNode.cpp:280] [4] Comm thread started 1 0
I1226 09:07:47.245741 117122 SynchronousNode.cpp:280] [5] Comm thread started 1 0
I1226 09:07:47.253793 120826 async_param_server.cpp:175] PS: Compute loop
I1226 09:07:47.250753 90248 SynchronousNode.cpp:280] [2] Comm thread started 0 0
I1226 09:07:47.255513 96821 SynchronousNode.cpp:280] [3] Comm thread started 0 0
I1226 09:07:47.246515 88880 SynchronousNode.cpp:280] [7] Comm thread started 1 0
I1226 09:07:47.249794 120130 SynchronousNode.cpp:280] [0] Comm thread started 0 1
I1226 09:07:43.994190 90388 SynchronousNode.cpp:280] [6] Comm thread started 1 0
I1226 09:07:47.283715 120130 SynchronousNode.cpp:466] [0] initialized root of cluster with nodes: 9 and the total iter size is: 8
I1226 09:07:47.440024 120057 MultiSolver.cpp:93] [0] PROFILING END[Forward]
I1226 09:07:47.440129 120057 MultiSolver.cpp:95] [0] PROFILING BEGIN[Backward]
I1226 09:07:47.776267 120057 MultiSolver.cpp:109] [0] PROFILING END[Backward]
I1226 09:07:47.776360 120057 solver.cpp:303] [0] Iteration 1, loss = 2.93429
I1226 09:07:47.776424 120057 solver.cpp:329]     Train net output #0: accuracy = 0.375
I1226 09:07:47.776510 120057 solver.cpp:329]     Train net output #1: loss = 2.93429 (* 1 = 2.93429 loss)
I1226 09:07:47.776582 120057 MultiSolver.cpp:76] [0] PROFILING BEGIN[Forward]
I1226 09:07:49.954620 90178 MultiSolver.cpp:93] [2] PROFILING END[Forward]
I1226 09:07:49.954701 90178 MultiSolver.cpp:95] [2] PROFILING BEGIN[Backward]
I1226 09:07:46.684885 90317 MultiSolver.cpp:93] [6] PROFILING END[Forward]
I1226 09:07:46.684994 90317 MultiSolver.cpp:95] [6] PROFILING BEGIN[Backward]
I1226 09:07:49.952417 117050 MultiSolver.cpp:93] [5] PROFILING END[Forward]
I1226 09:07:49.952538 117050 MultiSolver.cpp:95] [5] PROFILING BEGIN[Backward]
I1226 09:07:50.032904 90178 MultiSolver.cpp:109] [2] PROFILING END[Backward]
I1226 09:07:50.032997 90178 solver.cpp:303] [2] Iteration 1, loss = 2.67397
I1226 09:07:50.033067 90178 solver.cpp:329]     Train net output #0: accuracy = 0.34375
I1226 09:07:50.033135 90178 solver.cpp:329]     Train net output #1: loss = 2.67397 (* 1 = 2.67397 loss)
I1226 09:07:50.033229 90178 MultiSolver.cpp:76] [2] PROFILING BEGIN[Forward]
I1226 09:07:50.083986 96748 MultiSolver.cpp:93] [3] PROFILING END[Forward]
I1226 09:07:50.084123 96748 MultiSolver.cpp:95] [3] PROFILING BEGIN[Backward]
I1226 09:07:50.119455 88808 MultiSolver.cpp:93] [7] PROFILING END[Forward]
I1226 09:07:50.119562 88808 MultiSolver.cpp:95] [7] PROFILING BEGIN[Backward]
I1226 09:07:46.857980 90317 MultiSolver.cpp:109] [6] PROFILING END[Backward]
I1226 09:07:46.858078 90317 solver.cpp:303] [6] Iteration 1, loss = 2.83391
I1226 09:07:46.858160 90317 solver.cpp:329]     Train net output #0: accuracy = 0.40625
I1226 09:07:46.858238 90317 solver.cpp:329]     Train net output #1: loss = 2.83391 (* 1 = 2.83391 loss)
I1226 09:07:46.858332 90317 MultiSolver.cpp:76] [6] PROFILING BEGIN[Forward]
I1226 09:07:50.128264 117050 MultiSolver.cpp:109] [5] PROFILING END[Backward]
I1226 09:07:50.128360 117050 solver.cpp:303] [5] Iteration 1, loss = 3.63598
I1226 09:07:50.128438 117050 solver.cpp:329]     Train net output #0: accuracy = 0.25
I1226 09:07:50.128527 117050 solver.cpp:329]     Train net output #1: loss = 3.63598 (* 1 = 3.63598 loss)
I1226 09:07:50.128665 117050 MultiSolver.cpp:76] [5] PROFILING BEGIN[Forward]
I1226 09:07:50.135560 91281 MultiSolver.cpp:93] [4] PROFILING END[Forward]
I1226 09:07:50.135659 91281 MultiSolver.cpp:95] [4] PROFILING BEGIN[Backward]
I1226 09:07:50.269364 96748 MultiSolver.cpp:109] [3] PROFILING END[Backward]
I1226 09:07:50.269449 96748 solver.cpp:303] [3] Iteration 1, loss = 2.74898
I1226 09:07:50.269517 96748 solver.cpp:329]     Train net output #0: accuracy = 0.40625
I1226 09:07:50.269584 96748 solver.cpp:329]     Train net output #1: loss = 2.74898 (* 1 = 2.74898 loss)
I1226 09:07:50.269706 96748 MultiSolver.cpp:76] [3] PROFILING BEGIN[Forward]
I1226 09:07:50.327329 94464 MultiSolver.cpp:93] [1] PROFILING END[Forward]
I1226 09:07:50.327445 94464 MultiSolver.cpp:95] [1] PROFILING BEGIN[Backward]
I1226 09:07:50.374017 88808 MultiSolver.cpp:109] [7] PROFILING END[Backward]
I1226 09:07:50.374141 88808 solver.cpp:303] [7] Iteration 1, loss = 3.61206
I1226 09:07:50.374228 88808 solver.cpp:329]     Train net output #0: accuracy = 0.375
I1226 09:07:50.374310 88808 solver.cpp:329]     Train net output #1: loss = 3.61206 (* 1 = 3.61206 loss)
I1226 09:07:50.374394 88808 MultiSolver.cpp:76] [7] PROFILING BEGIN[Forward]
I1226 09:07:50.435711 91281 MultiSolver.cpp:109] [4] PROFILING END[Backward]
I1226 09:07:50.435797 91281 solver.cpp:303] [4] Iteration 1, loss = 3.05398
I1226 09:07:50.435859 91281 solver.cpp:329]     Train net output #0: accuracy = 0.34375
I1226 09:07:50.435930 91281 solver.cpp:329]     Train net output #1: loss = 3.05398 (* 1 = 3.05398 loss)
I1226 09:07:50.436009 91281 MultiSolver.cpp:76] [4] PROFILING BEGIN[Forward]
I1226 09:07:51.280493 94464 MultiSolver.cpp:109] [1] PROFILING END[Backward]
I1226 09:07:51.280596 94464 solver.cpp:303] [1] Iteration 1, loss = 3.0757
I1226 09:07:51.280685 94464 solver.cpp:329]     Train net output #0: accuracy = 0.40625
I1226 09:07:51.280776 94464 solver.cpp:329]     Train net output #1: loss = 3.0757 (* 1 = 3.0757 loss)
I1226 09:07:51.280961 94464 MultiSolver.cpp:76] [1] PROFILING BEGIN[Forward]
I1226 09:07:57.781914 120057 MultiSolver.cpp:93] [0] PROFILING END[Forward]
I1226 09:07:57.782022 120057 MultiSolver.cpp:95] [0] PROFILING BEGIN[Backward]
I1226 09:07:57.846209 120057 MultiSolver.cpp:109] [0] PROFILING END[Backward]
I1226 09:07:57.846297 120057 solver.cpp:303] [0] Iteration 2, loss = 4.2085
I1226 09:07:57.846359 120057 solver.cpp:329]     Train net output #0: accuracy = 0.15625
I1226 09:07:57.846438 120057 solver.cpp:329]     Train net output #1: loss = 4.2085 (* 1 = 4.2085 loss)
I1226 09:07:57.846535 120057 MultiSolver.cpp:76] [0] PROFILING BEGIN[Forward]
I1226 09:07:55.940186 90317 MultiSolver.cpp:93] [6] PROFILING END[Forward]
I1226 09:07:59.210029 90178 MultiSolver.cpp:93] [2] PROFILING END[Forward]
I1226 09:07:59.205245 117050 MultiSolver.cpp:93] [5] PROFILING END[Forward]
I1226 09:07:55.940305 90317 MultiSolver.cpp:95] [6] PROFILING BEGIN[Backward]
I1226 09:07:59.210142 90178 MultiSolver.cpp:95] [2] PROFILING BEGIN[Backward]
I1226 09:07:59.205356 117050 MultiSolver.cpp:95] [5] PROFILING BEGIN[Backward]
I1226 09:07:59.224318 94464 MultiSolver.cpp:93] [1] PROFILING END[Forward]
I1226 09:07:59.225337 94464 MultiSolver.cpp:95] [1] PROFILING BEGIN[Backward]
I1226 09:07:59.262991 90178 MultiSolver.cpp:109] [2] PROFILING END[Backward]
I1226 09:07:59.263077 90178 solver.cpp:303] [2] Iteration 2, loss = 3.35071
I1226 09:07:59.263144 90178 solver.cpp:329]     Train net output #0: accuracy = 0.25
I1226 09:07:59.263211 90178 solver.cpp:329]     Train net output #1: loss = 3.35071 (* 1 = 3.35071 loss)
I1226 09:07:59.263310 90178 MultiSolver.cpp:76] [2] PROFILING BEGIN[Forward]
I1226 09:07:56.050236 90317 MultiSolver.cpp:109] [6] PROFILING END[Backward]
I1226 09:07:56.050331 90317 solver.cpp:303] [6] Iteration 2, loss = 3.23013
I1226 09:07:56.050411 90317 solver.cpp:329]     Train net output #0: accuracy = 0.28125
I1226 09:07:56.050489 90317 solver.cpp:329]     Train net output #1: loss = 3.23013 (* 1 = 3.23013 loss)
I1226 09:07:56.050732 90317 MultiSolver.cpp:76] [6] PROFILING BEGIN[Forward]
I1226 09:07:59.316808 117050 MultiSolver.cpp:109] [5] PROFILING END[Backward]
I1226 09:07:59.316897 117050 solver.cpp:303] [5] Iteration 2, loss = 2.99185
I1226 09:07:59.316975 117050 solver.cpp:329]     Train net output #0: accuracy = 0.28125
I1226 09:07:59.317052 117050 solver.cpp:329]     Train net output #1: loss = 2.99185 (* 1 = 2.99185 loss)
I1226 09:07:59.317139 117050 MultiSolver.cpp:76] [5] PROFILING BEGIN[Forward]
I1226 09:07:59.534441 96748 MultiSolver.cpp:93] [3] PROFILING END[Forward]
I1226 09:07:59.534994 88808 MultiSolver.cpp:93] [7] PROFILING END[Forward]
I1226 09:07:59.534548 96748 MultiSolver.cpp:95] [3] PROFILING BEGIN[Backward]
I1226 09:07:59.535112 88808 MultiSolver.cpp:95] [7] PROFILING BEGIN[Backward]
I1226 09:07:59.532166 91281 MultiSolver.cpp:93] [4] PROFILING END[Forward]
I1226 09:07:59.532313 91281 MultiSolver.cpp:95] [4] PROFILING BEGIN[Backward]
I1226 09:07:59.583716 96748 MultiSolver.cpp:109] [3] PROFILING END[Backward]
I1226 09:07:59.583811 96748 solver.cpp:303] [3] Iteration 2, loss = 3.4267
I1226 09:07:59.583874 96748 solver.cpp:329]     Train net output #0: accuracy = 0.28125
I1226 09:07:59.583941 96748 solver.cpp:329]     Train net output #1: loss = 3.4267 (* 1 = 3.4267 loss)
I1226 09:07:59.584014 96748 MultiSolver.cpp:76] [3] PROFILING BEGIN[Forward]
I1226 09:07:59.634270 88808 MultiSolver.cpp:109] [7] PROFILING END[Backward]
I1226 09:07:59.634361 88808 solver.cpp:303] [7] Iteration 2, loss = 3.00927
I1226 09:07:59.634439 88808 solver.cpp:329]     Train net output #0: accuracy = 0.375
I1226 09:07:59.634516 88808 solver.cpp:329]     Train net output #1: loss = 3.00927 (* 1 = 3.00927 loss)
I1226 09:07:59.634637 88808 MultiSolver.cpp:76] [7] PROFILING BEGIN[Forward]
I1226 09:07:59.658493 91281 MultiSolver.cpp:109] [4] PROFILING END[Backward]
I1226 09:07:59.658573 91281 solver.cpp:303] [4] Iteration 2, loss = 3.524
I1226 09:07:59.658638 91281 solver.cpp:329]     Train net output #0: accuracy = 0.21875
I1226 09:07:59.658699 91281 solver.cpp:329]     Train net output #1: loss = 3.524 (* 1 = 3.524 loss)
I1226 09:07:59.658772 91281 MultiSolver.cpp:76] [4] PROFILING BEGIN[Forward]
I1226 09:07:59.708628 94464 MultiSolver.cpp:109] [1] PROFILING END[Backward]
I1226 09:07:59.708727 94464 solver.cpp:303] [1] Iteration 2, loss = 2.68376
I1226 09:07:59.708837 94464 solver.cpp:329]     Train net output #0: accuracy = 0.34375
I1226 09:07:59.708916 94464 solver.cpp:329]     Train net output #1: loss = 2.68376 (* 1 = 2.68376 loss)
I1226 09:07:59.708997 94464 MultiSolver.cpp:76] [1] PROFILING BEGIN[Forward]
I1226 09:08:06.836272 120057 MultiSolver.cpp:93] [0] PROFILING END[Forward]
I1226 09:08:06.836382 120057 MultiSolver.cpp:95] [0] PROFILING BEGIN[Backward]
I1226 09:08:06.900020 120057 MultiSolver.cpp:109] [0] PROFILING END[Backward]
I1226 09:08:06.900116 120057 solver.cpp:303] [0] Iteration 3, loss = 3.1361
I1226 09:08:06.900180 120057 solver.cpp:329]     Train net output #0: accuracy = 0.375
I1226 09:08:06.900261 120057 solver.cpp:329]     Train net output #1: loss = 3.1361 (* 1 = 3.1361 loss)
I1226 09:08:06.900352 120057 MultiSolver.cpp:76] [0] PROFILING BEGIN[Forward]
I1226 09:08:08.086345 90178 MultiSolver.cpp:93] [2] PROFILING END[Forward]
I1226 09:08:08.086460 90178 MultiSolver.cpp:95] [2] PROFILING BEGIN[Backward]
I1226 09:08:04.816692 90317 MultiSolver.cpp:93] [6] PROFILING END[Forward]
I1226 09:08:08.081459 117050 MultiSolver.cpp:93] [5] PROFILING END[Forward]
I1226 09:08:08.081609 117050 MultiSolver.cpp:95] [5] PROFILING BEGIN[Backward]
I1226 09:08:04.816807 90317 MultiSolver.cpp:95] [6] PROFILING BEGIN[Backward]
I1226 09:08:08.099210 94464 MultiSolver.cpp:93] [1] PROFILING END[Forward]
I1226 09:08:08.099408 94464 MultiSolver.cpp:95] [1] PROFILING BEGIN[Backward]
I1226 09:08:08.143041 90178 MultiSolver.cpp:109] [2] PROFILING END[Backward]
I1226 09:08:08.143123 90178 solver.cpp:303] [2] Iteration 3, loss = 3.34451
I1226 09:08:08.143189 90178 solver.cpp:329]     Train net output #0: accuracy = 0.28125
I1226 09:08:08.143257 90178 solver.cpp:329]     Train net output #1: loss = 3.34451 (* 1 = 3.34451 loss)
I1226 09:08:08.143378 90178 MultiSolver.cpp:76] [2] PROFILING BEGIN[Forward]
I1226 09:08:04.928604 90317 MultiSolver.cpp:109] [6] PROFILING END[Backward]
I1226 09:08:04.928704 90317 solver.cpp:303] [6] Iteration 3, loss = 2.84532
I1226 09:08:04.928781 90317 solver.cpp:329]     Train net output #0: accuracy = 0.40625
I1226 09:08:04.928858 90317 solver.cpp:329]     Train net output #1: loss = 2.84532 (* 1 = 2.84532 loss)
I1226 09:08:04.928967 90317 MultiSolver.cpp:76] [6] PROFILING BEGIN[Forward]
I1226 09:08:08.195519 117050 MultiSolver.cpp:109] [5] PROFILING END[Backward]
I1226 09:08:08.195652 117050 solver.cpp:303] [5] Iteration 3, loss = 3.51411
I1226 09:08:08.195732 117050 solver.cpp:329]     Train net output #0: accuracy = 0.21875
I1226 09:08:08.195808 117050 solver.cpp:329]     Train net output #1: loss = 3.51411 (* 1 = 3.51411 loss)
I1226 09:08:08.195899 117050 MultiSolver.cpp:76] [5] PROFILING BEGIN[Forward]
I1226 09:08:08.403807 91281 MultiSolver.cpp:93] [4] PROFILING END[Forward]
I1226 09:08:08.406472 96748 MultiSolver.cpp:93] [3] PROFILING END[Forward]
I1226 09:08:08.406930 88808 MultiSolver.cpp:93] [7] PROFILING END[Forward]
I1226 09:08:08.403923 91281 MultiSolver.cpp:95] [4] PROFILING BEGIN[Backward]
I1226 09:08:08.406579 96748 MultiSolver.cpp:95] [3] PROFILING BEGIN[Backward]
I1226 09:08:08.407057 88808 MultiSolver.cpp:95] [7] PROFILING BEGIN[Backward]
I1226 09:08:08.457734 96748 MultiSolver.cpp:109] [3] PROFILING END[Backward]
I1226 09:08:08.457828 96748 solver.cpp:303] [3] Iteration 3, loss = 2.72168
I1226 09:08:08.457922 96748 solver.cpp:329]     Train net output #0: accuracy = 0.375
I1226 09:08:08.457990 96748 solver.cpp:329]     Train net output #1: loss = 2.72168 (* 1 = 2.72168 loss)
I1226 09:08:08.458225 96748 MultiSolver.cpp:76] [3] PROFILING BEGIN[Forward]
I1226 09:08:08.506225 88808 MultiSolver.cpp:109] [7] PROFILING END[Backward]
I1226 09:08:08.506363 88808 solver.cpp:303] [7] Iteration 3, loss = 3.6975
I1226 09:08:08.506448 88808 solver.cpp:329]     Train net output #0: accuracy = 0.125
I1226 09:08:08.506531 88808 solver.cpp:329]     Train net output #1: loss = 3.6975 (* 1 = 3.6975 loss)
I1226 09:08:08.506674 88808 MultiSolver.cpp:76] [7] PROFILING BEGIN[Forward]
I1226 09:08:08.526072 91281 MultiSolver.cpp:109] [4] PROFILING END[Backward]
I1226 09:08:08.526157 91281 solver.cpp:303] [4] Iteration 3, loss = 3.7565
I1226 09:08:08.526221 91281 solver.cpp:329]     Train net output #0: accuracy = 0.25
I1226 09:08:08.526283 91281 solver.cpp:329]     Train net output #1: loss = 3.7565 (* 1 = 3.7565 loss)
I1226 09:08:08.526337 91281 MultiSolver.cpp:76] [4] PROFILING BEGIN[Forward]
I1226 09:08:08.582046 94464 MultiSolver.cpp:109] [1] PROFILING END[Backward]
I1226 09:08:08.582142 94464 solver.cpp:303] [1] Iteration 3, loss = 3.55225
I1226 09:08:08.582219 94464 solver.cpp:329]     Train net output #0: accuracy = 0.28125
I1226 09:08:08.582295 94464 solver.cpp:329]     Train net output #1: loss = 3.55225 (* 1 = 3.55225 loss)
I1226 09:08:08.582377 94464 MultiSolver.cpp:76] [1] PROFILING BEGIN[Forward]
I1226 09:08:15.679069 120057 MultiSolver.cpp:93] [0] PROFILING END[Forward]
I1226 09:08:15.679193 120057 MultiSolver.cpp:95] [0] PROFILING BEGIN[Backward]
I1226 09:08:15.745208 120057 MultiSolver.cpp:109] [0] PROFILING END[Backward]
I1226 09:08:15.745302 120057 solver.cpp:303] [0] Iteration 4, loss = 3.77123
I1226 09:08:15.745368 120057 solver.cpp:329]     Train net output #0: accuracy = 0.375
I1226 09:08:15.745447 120057 solver.cpp:329]     Train net output #1: loss = 3.77123 (* 1 = 3.77123 loss)
I1226 09:08:15.745543 120057 MultiSolver.cpp:76] [0] PROFILING BEGIN[Forward]
I1226 09:08:13.867509 90317 MultiSolver.cpp:93] [6] PROFILING END[Forward]
I1226 09:08:17.137349 90178 MultiSolver.cpp:93] [2] PROFILING END[Forward]
I1226 09:08:17.132493 117050 MultiSolver.cpp:93] [5] PROFILING END[Forward]
I1226 09:08:13.867656 90317 MultiSolver.cpp:95] [6] PROFILING BEGIN[Backward]
I1226 09:08:17.137465 90178 MultiSolver.cpp:95] [2] PROFILING BEGIN[Backward]
I1226 09:08:17.132643 117050 MultiSolver.cpp:95] [5] PROFILING BEGIN[Backward]
I1226 09:08:17.152691 94464 MultiSolver.cpp:93] [1] PROFILING END[Forward]
I1226 09:08:17.152833 94464 MultiSolver.cpp:95] [1] PROFILING BEGIN[Backward]
I1226 09:08:17.190310 90178 MultiSolver.cpp:109] [2] PROFILING END[Backward]
I1226 09:08:17.190397 90178 solver.cpp:303] [2] Iteration 4, loss = 3.48388
I1226 09:08:17.190462 90178 solver.cpp:329]     Train net output #0: accuracy = 0.3125
I1226 09:08:17.190528 90178 solver.cpp:329]     Train net output #1: loss = 3.48388 (* 1 = 3.48388 loss)
I1226 09:08:17.190618 90178 MultiSolver.cpp:76] [2] PROFILING BEGIN[Forward]
I1226 09:08:13.981397 90317 MultiSolver.cpp:109] [6] PROFILING END[Backward]
I1226 09:08:13.982681 90317 solver.cpp:303] [6] Iteration 4, loss = 3.1265
I1226 09:08:13.982775 90317 solver.cpp:329]     Train net output #0: accuracy = 0.3125
I1226 09:08:13.982856 90317 solver.cpp:329]     Train net output #1: loss = 3.1265 (* 1 = 3.1265 loss)
I1226 09:08:13.982930 90317 MultiSolver.cpp:76] [6] PROFILING BEGIN[Forward]
I1226 09:08:17.248755 117050 MultiSolver.cpp:109] [5] PROFILING END[Backward]
I1226 09:08:17.250603 117050 solver.cpp:303] [5] Iteration 4, loss = 3.20921
I1226 09:08:17.250699 117050 solver.cpp:329]     Train net output #0: accuracy = 0.40625
I1226 09:08:17.250784 117050 solver.cpp:329]     Train net output #1: loss = 3.20921 (* 1 = 3.20921 loss)
I1226 09:08:17.250875 117050 MultiSolver.cpp:76] [5] PROFILING BEGIN[Forward]
I1226 09:08:17.347664 91281 MultiSolver.cpp:93] [4] PROFILING END[Forward]
I1226 09:08:17.348917 91281 MultiSolver.cpp:95] [4] PROFILING BEGIN[Backward]
I1226 09:08:17.351181 88808 MultiSolver.cpp:93] [7] PROFILING END[Forward]
I1226 09:08:17.352335 88808 MultiSolver.cpp:95] [7] PROFILING BEGIN[Backward]
I1226 09:08:17.350222 96748 MultiSolver.cpp:93] [3] PROFILING END[Forward]
I1226 09:08:17.352833 96748 MultiSolver.cpp:95] [3] PROFILING BEGIN[Backward]
I1226 09:08:17.406882 96748 MultiSolver.cpp:109] [3] PROFILING END[Backward]
I1226 09:08:17.406966 96748 solver.cpp:303] [3] Iteration 4, loss = 2.74343
I1226 09:08:17.407035 96748 solver.cpp:329]     Train net output #0: accuracy = 0.40625
I1226 09:08:17.407122 96748 solver.cpp:329]     Train net output #1: loss = 2.74343 (* 1 = 2.74343 loss)
I1226 09:08:17.407178 96748 MultiSolver.cpp:76] [3] PROFILING BEGIN[Forward]
I1226 09:08:17.463114 88808 MultiSolver.cpp:109] [7] PROFILING END[Backward]
I1226 09:08:17.463205 88808 solver.cpp:303] [7] Iteration 4, loss = 3.18347
I1226 09:08:17.463284 88808 solver.cpp:329]     Train net output #0: accuracy = 0.375
I1226 09:08:17.463358 88808 solver.cpp:329]     Train net output #1: loss = 3.18347 (* 1 = 3.18347 loss)
I1226 09:08:17.463500 88808 MultiSolver.cpp:76] [7] PROFILING BEGIN[Forward]
I1226 09:08:17.473934 91281 MultiSolver.cpp:109] [4] PROFILING END[Backward]
I1226 09:08:17.474019 91281 solver.cpp:303] [4] Iteration 4, loss = 4.25016
I1226 09:08:17.474079 91281 solver.cpp:329]     Train net output #0: accuracy = 0.0625
I1226 09:08:17.474141 91281 solver.cpp:329]     Train net output #1: loss = 4.25016 (* 1 = 4.25016 loss)
I1226 09:08:17.474222 91281 MultiSolver.cpp:76] [4] PROFILING BEGIN[Forward]
I1226 09:08:17.637436 94464 MultiSolver.cpp:109] [1] PROFILING END[Backward]
I1226 09:08:17.637533 94464 solver.cpp:303] [1] Iteration 4, loss = 3.62433
I1226 09:08:17.637609 94464 solver.cpp:329]     Train net output #0: accuracy = 0.25
I1226 09:08:17.637683 94464 solver.cpp:329]     Train net output #1: loss = 3.62433 (* 1 = 3.62433 loss)
I1226 09:08:17.637748 94464 MultiSolver.cpp:76] [1] PROFILING BEGIN[Forward]
I1226 09:08:24.872310 120057 MultiSolver.cpp:93] [0] PROFILING END[Forward]
I1226 09:08:24.873431 120057 MultiSolver.cpp:95] [0] PROFILING BEGIN[Backward]
I1226 09:08:24.937065 120057 MultiSolver.cpp:109] [0] PROFILING END[Backward]
I1226 09:08:24.937155 120057 solver.cpp:303] [0] Iteration 5, loss = 3.04337
I1226 09:08:24.937221 120057 solver.cpp:329]     Train net output #0: accuracy = 0.3125
I1226 09:08:24.937297 120057 solver.cpp:329]     Train net output #1: loss = 3.04337 (* 1 = 3.04337 loss)
I1226 09:08:24.937393 120057 MultiSolver.cpp:76] [0] PROFILING BEGIN[Forward]
I1226 09:08:23.031662 90317 MultiSolver.cpp:93] [6] PROFILING END[Forward]
I1226 09:08:26.296727 117050 MultiSolver.cpp:93] [5] PROFILING END[Forward]
I1226 09:08:23.031777 90317 MultiSolver.cpp:95] [6] PROFILING BEGIN[Backward]
I1226 09:08:26.301700 90178 MultiSolver.cpp:93] [2] PROFILING END[Forward]
I1226 09:08:26.296844 117050 MultiSolver.cpp:95] [5] PROFILING BEGIN[Backward]
I1226 09:08:26.302944 90178 MultiSolver.cpp:95] [2] PROFILING BEGIN[Backward]
I1226 09:08:26.314779 94464 MultiSolver.cpp:93] [1] PROFILING END[Forward]
I1226 09:08:26.315088 94464 MultiSolver.cpp:95] [1] PROFILING BEGIN[Backward]
I1226 09:08:26.367343 90178 MultiSolver.cpp:109] [2] PROFILING END[Backward]
I1226 09:08:26.367435 90178 solver.cpp:303] [2] Iteration 5, loss = 3.08288
I1226 09:08:26.367501 90178 solver.cpp:329]     Train net output #0: accuracy = 0.34375
I1226 09:08:26.367569 90178 solver.cpp:329]     Train net output #1: loss = 3.08288 (* 1 = 3.08288 loss)
I1226 09:08:26.367848 90178 MultiSolver.cpp:76] [2] PROFILING BEGIN[Forward]
I1226 09:08:23.133786 90317 MultiSolver.cpp:109] [6] PROFILING END[Backward]
I1226 09:08:23.133867 90317 solver.cpp:303] [6] Iteration 5, loss = 3.38387
I1226 09:08:23.133935 90317 solver.cpp:329]     Train net output #0: accuracy = 0.3125
I1226 09:08:23.134002 90317 solver.cpp:329]     Train net output #1: loss = 3.38387 (* 1 = 3.38387 loss)
I1226 09:08:23.134084 90317 MultiSolver.cpp:76] [6] PROFILING BEGIN[Forward]
I1226 09:08:26.403488 117050 MultiSolver.cpp:109] [5] PROFILING END[Backward]
I1226 09:08:26.403614 117050 solver.cpp:303] [5] Iteration 5, loss = 2.98676
I1226 09:08:26.403686 117050 solver.cpp:329]     Train net output #0: accuracy = 0.3125
I1226 09:08:26.403756 117050 solver.cpp:329]     Train net output #1: loss = 2.98676 (* 1 = 2.98676 loss)
I1226 09:08:26.403820 117050 MultiSolver.cpp:76] [5] PROFILING BEGIN[Forward]
I1226 09:08:26.634353 91281 MultiSolver.cpp:93] [4] PROFILING END[Forward]
I1226 09:08:26.636863 96748 MultiSolver.cpp:93] [3] PROFILING END[Forward]
I1226 09:08:26.637409 88808 MultiSolver.cpp:93] [7] PROFILING END[Forward]
I1226 09:08:26.634487 91281 MultiSolver.cpp:95] [4] PROFILING BEGIN[Backward]
I1226 09:08:26.637524 88808 MultiSolver.cpp:95] [7] PROFILING BEGIN[Backward]
I1226 09:08:26.636986 96748 MultiSolver.cpp:95] [3] PROFILING BEGIN[Backward]
I1226 09:08:26.687248 96748 MultiSolver.cpp:109] [3] PROFILING END[Backward]
I1226 09:08:26.687335 96748 solver.cpp:303] [3] Iteration 5, loss = 2.75289
I1226 09:08:26.687402 96748 solver.cpp:329]     Train net output #0: accuracy = 0.4375
I1226 09:08:26.687469 96748 solver.cpp:329]     Train net output #1: loss = 2.75289 (* 1 = 2.75289 loss)
I1226 09:08:26.687533 96748 MultiSolver.cpp:76] [3] PROFILING BEGIN[Forward]
I1226 09:08:26.738512 88808 MultiSolver.cpp:109] [7] PROFILING END[Backward]
I1226 09:08:26.738628 88808 solver.cpp:303] [7] Iteration 5, loss = 3.09684
I1226 09:08:26.738706 88808 solver.cpp:329]     Train net output #0: accuracy = 0.34375
I1226 09:08:26.738785 88808 solver.cpp:329]     Train net output #1: loss = 3.09684 (* 1 = 3.09684 loss)
I1226 09:08:26.738883 88808 MultiSolver.cpp:76] [7] PROFILING BEGIN[Forward]
I1226 09:08:26.757256 91281 MultiSolver.cpp:109] [4] PROFILING END[Backward]
I1226 09:08:26.757338 91281 solver.cpp:303] [4] Iteration 5, loss = 2.95639
I1226 09:08:26.757433 91281 solver.cpp:329]     Train net output #0: accuracy = 0.3125
I1226 09:08:26.757496 91281 solver.cpp:329]     Train net output #1: loss = 2.95639 (* 1 = 2.95639 loss)
I1226 09:08:26.757757 91281 MultiSolver.cpp:76] [4] PROFILING BEGIN[Forward]
I1226 09:08:26.797945 94464 MultiSolver.cpp:109] [1] PROFILING END[Backward]
I1226 09:08:26.798035 94464 solver.cpp:303] [1] Iteration 5, loss = 3.71501
I1226 09:08:26.798115 94464 solver.cpp:329]     Train net output #0: accuracy = 0.15625
I1226 09:08:26.798190 94464 solver.cpp:329]     Train net output #1: loss = 3.71501 (* 1 = 3.71501 loss)
I1226 09:08:26.798305 94464 MultiSolver.cpp:76] [1] PROFILING BEGIN[Forward]
I1226 09:08:34.213877 120057 MultiSolver.cpp:93] [0] PROFILING END[Forward]
I1226 09:08:34.213990 120057 MultiSolver.cpp:95] [0] PROFILING BEGIN[Backward]
I1226 09:08:34.278904 120057 MultiSolver.cpp:109] [0] PROFILING END[Backward]
I1226 09:08:34.278990 120057 solver.cpp:303] [0] Iteration 6, loss = 3.6387
I1226 09:08:34.279052 120057 solver.cpp:329]     Train net output #0: accuracy = 0.1875
I1226 09:08:34.279131 120057 solver.cpp:329]     Train net output #1: loss = 3.6387 (* 1 = 3.6387 loss)
I1226 09:08:34.279227 120057 MultiSolver.cpp:76] [0] PROFILING BEGIN[Forward]
I1226 09:08:35.613634 90178 MultiSolver.cpp:93] [2] PROFILING END[Forward]
I1226 09:08:35.608825 117050 MultiSolver.cpp:93] [5] PROFILING END[Forward]
I1226 09:08:35.613751 90178 MultiSolver.cpp:95] [2] PROFILING BEGIN[Backward]
I1226 09:08:35.608942 117050 MultiSolver.cpp:95] [5] PROFILING BEGIN[Backward]
I1226 09:08:32.343920 90317 MultiSolver.cpp:93] [6] PROFILING END[Forward]
I1226 09:08:32.344048 90317 MultiSolver.cpp:95] [6] PROFILING BEGIN[Backward]
I1226 09:08:35.626091 94464 MultiSolver.cpp:93] [1] PROFILING END[Forward]
I1226 09:08:35.626812 94464 MultiSolver.cpp:95] [1] PROFILING BEGIN[Backward]
I1226 09:08:35.669585 90178 MultiSolver.cpp:109] [2] PROFILING END[Backward]
I1226 09:08:35.669680 90178 solver.cpp:303] [2] Iteration 6, loss = 2.61307
I1226 09:08:35.669749 90178 solver.cpp:329]     Train net output #0: accuracy = 0.40625
I1226 09:08:35.669855 90178 solver.cpp:329]     Train net output #1: loss = 2.61307 (* 1 = 2.61307 loss)
I1226 09:08:35.669919 90178 MultiSolver.cpp:76] [2] PROFILING BEGIN[Forward]
I1226 09:08:32.454392 90317 MultiSolver.cpp:109] [6] PROFILING END[Backward]
I1226 09:08:32.454483 90317 solver.cpp:303] [6] Iteration 6, loss = 2.80696
I1226 09:08:32.454563 90317 solver.cpp:329]     Train net output #0: accuracy = 0.46875
I1226 09:08:32.454675 90317 solver.cpp:329]     Train net output #1: loss = 2.80696 (* 1 = 2.80696 loss)
I1226 09:08:32.454757 90317 MultiSolver.cpp:76] [6] PROFILING BEGIN[Forward]
I1226 09:08:35.721922 117050 MultiSolver.cpp:109] [5] PROFILING END[Backward]
I1226 09:08:35.722018 117050 solver.cpp:303] [5] Iteration 6, loss = 3.31863
I1226 09:08:35.722091 117050 solver.cpp:329]     Train net output #0: accuracy = 0.28125
I1226 09:08:35.722168 117050 solver.cpp:329]     Train net output #1: loss = 3.31863 (* 1 = 3.31863 loss)
I1226 09:08:35.722245 117050 MultiSolver.cpp:76] [5] PROFILING BEGIN[Forward]
I1226 09:08:35.797736 91281 MultiSolver.cpp:93] [4] PROFILING END[Forward]
I1226 09:08:35.797842 91281 MultiSolver.cpp:95] [4] PROFILING BEGIN[Backward]
I1226 09:08:35.800380 96748 MultiSolver.cpp:93] [3] PROFILING END[Forward]
I1226 09:08:35.800488 96748 MultiSolver.cpp:95] [3] PROFILING BEGIN[Backward]
I1226 09:08:35.801071 88808 MultiSolver.cpp:93] [7] PROFILING END[Forward]
I1226 09:08:35.801187 88808 MultiSolver.cpp:95] [7] PROFILING BEGIN[Backward]
I1226 09:08:35.852540 96748 MultiSolver.cpp:109] [3] PROFILING END[Backward]
I1226 09:08:35.852650 96748 solver.cpp:303] [3] Iteration 6, loss = 2.55337
I1226 09:08:35.852716 96748 solver.cpp:329]     Train net output #0: accuracy = 0.4375
I1226 09:08:35.852783 96748 solver.cpp:329]     Train net output #1: loss = 2.55337 (* 1 = 2.55337 loss)
I1226 09:08:35.852847 96748 MultiSolver.cpp:76] [3] PROFILING BEGIN[Forward]
I1226 09:08:35.904636 88808 MultiSolver.cpp:109] [7] PROFILING END[Backward]
I1226 09:08:35.904731 88808 solver.cpp:303] [7] Iteration 6, loss = 3.05428
I1226 09:08:35.904808 88808 solver.cpp:329]     Train net output #0: accuracy = 0.3125
I1226 09:08:35.904884 88808 solver.cpp:329]     Train net output #1: loss = 3.05428 (* 1 = 3.05428 loss)
I1226 09:08:35.904971 88808 MultiSolver.cpp:76] [7] PROFILING BEGIN[Forward]
I1226 09:08:35.915570 91281 MultiSolver.cpp:109] [4] PROFILING END[Backward]
I1226 09:08:35.915652 91281 solver.cpp:303] [4] Iteration 6, loss = 3.44939
I1226 09:08:35.915714 91281 solver.cpp:329]     Train net output #0: accuracy = 0.25
I1226 09:08:35.915777 91281 solver.cpp:329]     Train net output #1: loss = 3.44939 (* 1 = 3.44939 loss)
I1226 09:08:35.915843 91281 MultiSolver.cpp:76] [4] PROFILING BEGIN[Forward]
I1226 09:08:36.113077 94464 MultiSolver.cpp:109] [1] PROFILING END[Backward]
I1226 09:08:36.113171 94464 solver.cpp:303] [1] Iteration 6, loss = 3.63193
I1226 09:08:36.113250 94464 solver.cpp:329]     Train net output #0: accuracy = 0.1875
I1226 09:08:36.113327 94464 solver.cpp:329]     Train net output #1: loss = 3.63193 (* 1 = 3.63193 loss)
I1226 09:08:36.113402 94464 MultiSolver.cpp:76] [1] PROFILING BEGIN[Forward]
I1226 09:08:43.379350 120057 MultiSolver.cpp:93] [0] PROFILING END[Forward]
I1226 09:08:43.379472 120057 MultiSolver.cpp:95] [0] PROFILING BEGIN[Backward]
I1226 09:08:43.443850 120057 MultiSolver.cpp:109] [0] PROFILING END[Backward]
I1226 09:08:43.443945 120057 solver.cpp:303] [0] Iteration 7, loss = 2.83622
I1226 09:08:43.444010 120057 solver.cpp:329]     Train net output #0: accuracy = 0.375
I1226 09:08:43.444089 120057 solver.cpp:329]     Train net output #1: loss = 2.83622 (* 1 = 2.83622 loss)
I1226 09:08:43.444169 120057 MultiSolver.cpp:76] [0] PROFILING BEGIN[Forward]
I1226 09:08:44.724364 90178 MultiSolver.cpp:93] [2] PROFILING END[Forward]
I1226 09:08:44.719522 117050 MultiSolver.cpp:93] [5] PROFILING END[Forward]
I1226 09:08:41.454571 90317 MultiSolver.cpp:93] [6] PROFILING END[Forward]
I1226 09:08:44.724475 90178 MultiSolver.cpp:95] [2] PROFILING BEGIN[Backward]
I1226 09:08:44.719673 117050 MultiSolver.cpp:95] [5] PROFILING BEGIN[Backward]
I1226 09:08:41.454718 90317 MultiSolver.cpp:95] [6] PROFILING BEGIN[Backward]
I1226 09:08:44.737233 94464 MultiSolver.cpp:93] [1] PROFILING END[Forward]
I1226 09:08:44.737349 94464 MultiSolver.cpp:95] [1] PROFILING BEGIN[Backward]
I1226 09:08:44.777194 90178 MultiSolver.cpp:109] [2] PROFILING END[Backward]
I1226 09:08:44.777328 90178 solver.cpp:303] [2] Iteration 7, loss = 3.71079
I1226 09:08:44.777401 90178 solver.cpp:329]     Train net output #0: accuracy = 0.1875
I1226 09:08:44.777477 90178 solver.cpp:329]     Train net output #1: loss = 3.71079 (* 1 = 3.71079 loss)
I1226 09:08:44.777540 90178 MultiSolver.cpp:76] [2] PROFILING BEGIN[Forward]
I1226 09:08:44.830492 117050 MultiSolver.cpp:109] [5] PROFILING END[Backward]
I1226 09:08:44.830623 117050 solver.cpp:303] [5] Iteration 7, loss = 2.92217
I1226 09:08:44.830700 117050 solver.cpp:329]     Train net output #0: accuracy = 0.34375
I1226 09:08:44.830792 117050 solver.cpp:329]     Train net output #1: loss = 2.92217 (* 1 = 2.92217 loss)
I1226 09:08:44.830878 117050 MultiSolver.cpp:76] [5] PROFILING BEGIN[Forward]
I1226 09:08:41.567262 90317 MultiSolver.cpp:109] [6] PROFILING END[Backward]
I1226 09:08:41.567359 90317 solver.cpp:303] [6] Iteration 7, loss = 3.50108
I1226 09:08:41.567440 90317 solver.cpp:329]     Train net output #0: accuracy = 0.15625
I1226 09:08:41.567517 90317 solver.cpp:329]     Train net output #1: loss = 3.50108 (* 1 = 3.50108 loss)
I1226 09:08:41.567632 90317 MultiSolver.cpp:76] [6] PROFILING BEGIN[Forward]
I1226 09:08:44.904549 91281 MultiSolver.cpp:93] [4] PROFILING END[Forward]
I1226 09:08:44.907492 88808 MultiSolver.cpp:93] [7] PROFILING END[Forward]
I1226 09:08:44.904655 91281 MultiSolver.cpp:95] [4] PROFILING BEGIN[Backward]
I1226 09:08:44.907649 88808 MultiSolver.cpp:95] [7] PROFILING BEGIN[Backward]
I1226 09:08:44.907380 96748 MultiSolver.cpp:93] [3] PROFILING END[Forward]
I1226 09:08:44.907488 96748 MultiSolver.cpp:95] [3] PROFILING BEGIN[Backward]
I1226 09:08:44.957950 96748 MultiSolver.cpp:109] [3] PROFILING END[Backward]
I1226 09:08:44.958036 96748 solver.cpp:303] [3] Iteration 7, loss = 3.48568
I1226 09:08:44.958137 96748 solver.cpp:329]     Train net output #0: accuracy = 0.28125
I1226 09:08:44.958206 96748 solver.cpp:329]     Train net output #1: loss = 3.48568 (* 1 = 3.48568 loss)
I1226 09:08:44.958276 96748 MultiSolver.cpp:76] [3] PROFILING BEGIN[Forward]
I1226 09:08:45.011813 88808 MultiSolver.cpp:109] [7] PROFILING END[Backward]
I1226 09:08:45.011909 88808 solver.cpp:303] [7] Iteration 7, loss = 3.2852
I1226 09:08:45.011986 88808 solver.cpp:329]     Train net output #0: accuracy = 0.25
I1226 09:08:45.012063 88808 solver.cpp:329]     Train net output #1: loss = 3.2852 (* 1 = 3.2852 loss)
I1226 09:08:45.012151 88808 MultiSolver.cpp:76] [7] PROFILING BEGIN[Forward]
I1226 09:08:45.019487 91281 MultiSolver.cpp:109] [4] PROFILING END[Backward]
I1226 09:08:45.019570 91281 solver.cpp:303] [4] Iteration 7, loss = 3.46712
I1226 09:08:45.019634 91281 solver.cpp:329]     Train net output #0: accuracy = 0.21875
I1226 09:08:45.019695 91281 solver.cpp:329]     Train net output #1: loss = 3.46712 (* 1 = 3.46712 loss)
I1226 09:08:45.019762 91281 MultiSolver.cpp:76] [4] PROFILING BEGIN[Forward]
I1226 09:08:45.226138 94464 MultiSolver.cpp:109] [1] PROFILING END[Backward]
I1226 09:08:45.226233 94464 solver.cpp:303] [1] Iteration 7, loss = 3.5258
I1226 09:08:45.226311 94464 solver.cpp:329]     Train net output #0: accuracy = 0.3125
I1226 09:08:45.226388 94464 solver.cpp:329]     Train net output #1: loss = 3.5258 (* 1 = 3.5258 loss)
I1226 09:08:45.226470 94464 MultiSolver.cpp:76] [1] PROFILING BEGIN[Forward]
/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/8_node_run.sh: line 57: 120041 User defined signal 2   mpirun -configfile $nodeconfig

real	1m57.494s
user	0m0.011s
sys	0m0.028s
