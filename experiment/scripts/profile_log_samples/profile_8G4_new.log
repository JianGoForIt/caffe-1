Dec 26 14:16:39 2016 93408 3 10.1 NIOS_DEBUG: stdin_fd set to -1
Dec 26 14:16:39 2016 93408 3 10.1 NIOS_DEBUG: fds[0] has a value of -1
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 14:16:41.838054 93418 mpiutil.cpp:166] Process rank 0 from number of 9 processes running on knl-node051
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 14:16:41.837024 98928 mpiutil.cpp:166] Process rank 6 from number of 9 processes running on knl-node016
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 14:16:41.846668 91408 mpiutil.cpp:166] Process rank 5 from number of 9 processes running on knl-node063
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 14:16:41.832507 91497 mpiutil.cpp:166] Process rank 1 from number of 9 processes running on knl-node027
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 14:16:41.824137 97044 mpiutil.cpp:166] Process rank 7 from number of 9 processes running on knl-node079
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 14:16:41.824216 97473 mpiutil.cpp:166] Process rank 2 from number of 9 processes running on knl-node078
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 14:16:41.836781 95408 mpiutil.cpp:166] Process rank 4 from number of 9 processes running on knl-node084
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 14:16:41.836045 92503 mpiutil.cpp:166] Process rank 3 from number of 9 processes running on knl-node048
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 14:16:41.837749 93636 mpiutil.cpp:166] Process rank 8 from number of 9 processes running on knl-node024
I1226 14:16:41.849897 95408 caffe.cpp:316] Use CPU.
I1226 14:16:41.837952 97473 caffe.cpp:316] Use CPU.
I1226 14:16:41.851157 95408 solver.cpp:93] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_5.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 14:16:41.852185 93418 caffe.cpp:316] Use CPU.
I1226 14:16:41.851300 95408 solver.cpp:128] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_5.prototxt
I1226 14:16:41.847443 91497 caffe.cpp:316] Use CPU.
I1226 14:16:41.862040 91408 caffe.cpp:316] Use CPU.
I1226 14:16:41.848597 91497 solver.cpp:93] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_2.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 14:16:41.839268 97473 solver.cpp:93] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_3.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 14:16:41.848809 91497 solver.cpp:128] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_2.prototxt
I1226 14:16:41.840380 97473 solver.cpp:128] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_3.prototxt
I1226 14:16:41.853765 93418 solver.cpp:93] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_1.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 14:16:41.854915 93418 solver.cpp:128] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_1.prototxt
I1226 14:16:41.863152 91408 solver.cpp:93] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_6.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 14:16:41.864038 91408 solver.cpp:128] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_6.prototxt
I1226 14:16:41.841142 97044 caffe.cpp:316] Use CPU.
I1226 14:16:41.854436 98928 caffe.cpp:316] Use CPU.
I1226 14:16:41.854847 93636 caffe.cpp:316] Use CPU.
I1226 14:16:41.855499 98928 solver.cpp:93] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_7.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 14:16:41.855582 98928 solver.cpp:128] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_7.prototxt
I1226 14:16:41.842537 97044 solver.cpp:93] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_8.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 14:16:41.842695 97044 solver.cpp:128] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_8.prototxt
I1226 14:16:41.856230 93636 solver.cpp:93] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_dummy.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 14:16:41.855015 92503 caffe.cpp:316] Use CPU.
I1226 14:16:41.857383 93636 solver.cpp:128] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_dummy.prototxt
I1226 14:16:41.856406 92503 solver.cpp:93] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_4.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 14:16:41.857704 92503 solver.cpp:128] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_4.prototxt
I1226 14:16:41.879014 91497 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 14:16:41.879084 91497 cpu_info.cpp:455] Total number of sockets: 1
I1226 14:16:41.879106 91497 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 14:16:41.879123 91497 cpu_info.cpp:461] Total number of processors: 272
I1226 14:16:41.879142 91497 cpu_info.cpp:464] GPU is used: no
I1226 14:16:41.879159 91497 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 14:16:41.879205 91497 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 14:16:41.884701 95408 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 14:16:41.884788 95408 cpu_info.cpp:455] Total number of sockets: 1
I1226 14:16:41.886777 93418 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 14:16:41.886857 93418 cpu_info.cpp:455] Total number of sockets: 1
I1226 14:16:41.884814 95408 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 14:16:41.884861 95408 cpu_info.cpp:461] Total number of processors: 272
I1226 14:16:41.884884 95408 cpu_info.cpp:464] GPU is used: no
I1226 14:16:41.886890 93418 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 14:16:41.886917 93418 cpu_info.cpp:461] Total number of processors: 272
I1226 14:16:41.886945 93418 cpu_info.cpp:464] GPU is used: no
I1226 14:16:41.886973 93418 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 14:16:41.884907 95408 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 14:16:41.884927 95408 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 14:16:41.887001 93418 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 14:16:41.885905 98928 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 14:16:41.885977 98928 cpu_info.cpp:455] Total number of sockets: 1
I1226 14:16:41.886000 98928 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 14:16:41.886020 98928 cpu_info.cpp:461] Total number of processors: 272
I1226 14:16:41.886040 98928 cpu_info.cpp:464] GPU is used: no
I1226 14:16:41.886076 98928 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 14:16:41.886101 98928 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 14:16:41.896492 91408 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 14:16:41.896565 91408 cpu_info.cpp:455] Total number of sockets: 1
I1226 14:16:41.896591 91408 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 14:16:41.896612 91408 cpu_info.cpp:461] Total number of processors: 272
I1226 14:16:41.896633 91408 cpu_info.cpp:464] GPU is used: no
I1226 14:16:41.896654 91408 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 14:16:41.896675 91408 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 14:16:41.874115 97473 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 14:16:41.874207 97473 cpu_info.cpp:455] Total number of sockets: 1
I1226 14:16:41.874233 97473 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 14:16:41.874256 97473 cpu_info.cpp:461] Total number of processors: 272
I1226 14:16:41.874277 97473 cpu_info.cpp:464] GPU is used: no
I1226 14:16:41.874300 97473 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 14:16:41.874322 97473 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 14:16:41.888254 93636 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 14:16:41.888331 93636 cpu_info.cpp:455] Total number of sockets: 1
I1226 14:16:41.888355 93636 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 14:16:41.888373 93636 cpu_info.cpp:461] Total number of processors: 272
I1226 14:16:41.888393 93636 cpu_info.cpp:464] GPU is used: no
I1226 14:16:41.888413 93636 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 14:16:41.888433 93636 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 14:16:41.889045 92503 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 14:16:41.889117 92503 cpu_info.cpp:455] Total number of sockets: 1
I1226 14:16:41.889140 92503 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 14:16:41.889159 92503 cpu_info.cpp:461] Total number of processors: 272
I1226 14:16:41.889179 92503 cpu_info.cpp:464] GPU is used: no
I1226 14:16:41.889199 92503 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 14:16:41.889219 92503 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 14:16:41.878131 97044 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 14:16:41.878221 97044 cpu_info.cpp:455] Total number of sockets: 1
I1226 14:16:41.878248 97044 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 14:16:41.878270 97044 cpu_info.cpp:461] Total number of processors: 272
I1226 14:16:41.878293 97044 cpu_info.cpp:464] GPU is used: no
I1226 14:16:41.878314 97044 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 14:16:41.878335 97044 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 14:16:41.907711 95408 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 14:16:41.908105 95408 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 14:16:41.897579 97473 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 14:16:41.910176 95408 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_4"
    batch_size: 128
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 14:16:41.897994 97473 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 14:16:41.910456 95408 layer_factory.hpp:114] Creating layer data
I1226 14:16:41.899479 97044 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 14:16:41.899901 97044 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 14:16:41.900104 97473 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_2"
    batch_size: 128
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 14:16:41.913069 95408 net.cpp:178] Creating Layer data
I1226 14:16:41.913192 95408 net.cpp:586] data -> data
I1226 14:16:41.913302 95408 net.cpp:586] data -> label
I1226 14:16:41.901985 97044 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_7"
    batch_size: 128
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 14:16:41.902267 97044 layer_factory.hpp:114] Creating layer data
I1226 14:16:41.904865 97044 net.cpp:178] Creating Layer data
I1226 14:16:41.904996 97044 net.cpp:586] data -> data
I1226 14:16:41.905117 97044 net.cpp:586] data -> label
I1226 14:16:41.930790 91408 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 14:16:41.931077 91408 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 14:16:41.908221 97473 layer_factory.hpp:114] Creating layer data
I1226 14:16:41.932648 91408 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_5"
    batch_size: 128
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 14:16:41.924496 93418 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 14:16:41.924887 93418 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 14:16:41.925421 93636 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 14:16:41.925792 93636 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 14:16:41.926427 93418 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_0"
    batch_size: 128
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 14:16:41.925851 95411 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_4
I1226 14:16:41.913790 97473 net.cpp:178] Creating Layer data
I1226 14:16:41.913923 97473 net.cpp:586] data -> data
I1226 14:16:41.914044 97473 net.cpp:586] data -> label
I1226 14:16:41.927656 93636 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "DummyData"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  dummy_data_param {
    data_filler {
      type: "gaussian"
      std: 1
    }
    data_filler {
      type: "constant"
      value: 0
    }
    shape {
      dim: 128
      dim: 3
      dim: 227
      dim: 227
    }
    shape {
      dim: 128
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 14:16:41.930435 95408 data_layer.cpp:80] output data size: 128,3,227,227
I1226 14:16:41.934795 92503 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 14:16:41.935093 92503 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 14:16:41.947180 91408 layer_factory.hpp:114] Creating layer data
I1226 14:16:41.926369 97047 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_7
I1226 14:16:41.942488 93418 layer_factory.hpp:114] Creating layer data
I1226 14:16:41.925462 97475 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_2
I1226 14:16:41.936687 92503 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_3"
    batch_size: 128
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 14:16:41.930207 97473 data_layer.cpp:80] output data size: 128,3,227,227
I1226 14:16:41.938874 91497 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 14:16:41.939172 91497 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 14:16:41.940865 91497 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_1"
    batch_size: 128
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 14:16:41.941056 91497 layer_factory.hpp:114] Creating layer data
I1226 14:16:41.943397 91497 net.cpp:178] Creating Layer data
I1226 14:16:41.943467 91497 net.cpp:586] data -> data
I1226 14:16:41.943542 91497 net.cpp:586] data -> label
I1226 14:16:41.953140 93418 net.cpp:178] Creating Layer data
I1226 14:16:41.953225 93418 net.cpp:586] data -> data
I1226 14:16:41.953299 93418 net.cpp:586] data -> label
I1226 14:16:41.963167 91408 net.cpp:178] Creating Layer data
I1226 14:16:41.963248 91408 net.cpp:586] data -> data
I1226 14:16:41.963345 91408 net.cpp:586] data -> label
I1226 14:16:41.942243 97044 data_layer.cpp:80] output data size: 128,3,227,227
I1226 14:16:41.956420 93636 layer_factory.hpp:114] Creating layer data
I1226 14:16:41.956565 93636 net.cpp:178] Creating Layer data
I1226 14:16:41.956611 93636 net.cpp:586] data -> data
I1226 14:16:41.956754 93636 net.cpp:586] data -> label
I1226 14:16:41.959260 92503 layer_factory.hpp:114] Creating layer data
I1226 14:16:41.965991 92503 net.cpp:178] Creating Layer data
I1226 14:16:41.966078 92503 net.cpp:586] data -> data
I1226 14:16:41.966158 92503 net.cpp:586] data -> label
I1226 14:16:41.986991 91410 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_5
I1226 14:16:41.979974 98928 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 14:16:41.980402 98928 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 14:16:41.982692 98928 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_6"
    batch_size: 128
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 14:16:41.982939 98928 layer_factory.hpp:114] Creating layer data
I1226 14:16:41.995268 91408 data_layer.cpp:80] output data size: 128,3,227,227
I1226 14:16:41.986240 98928 net.cpp:178] Creating Layer data
I1226 14:16:41.986337 98928 net.cpp:586] data -> data
I1226 14:16:41.986439 98928 net.cpp:586] data -> label
I1226 14:16:41.982384 91500 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_1
I1226 14:16:41.987179 93636 net.cpp:228] Setting up data
I1226 14:16:41.987287 93636 net.cpp:235] Top shape: 128 3 227 227 (19787136)
I1226 14:16:41.987321 93636 net.cpp:235] Top shape: 128 1 1 1 (128)
I1226 14:16:41.987344 93636 net.cpp:243] Memory required for data: 79149056
I1226 14:16:41.987380 93636 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 14:16:41.987462 93636 net.cpp:178] Creating Layer label_data_1_split
I1226 14:16:41.987570 93636 net.cpp:612] label_data_1_split <- label
I1226 14:16:41.987606 93636 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 14:16:41.987650 93636 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 14:16:41.988378 92505 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_3
I1226 14:16:41.991260 91497 data_layer.cpp:80] output data size: 128,3,227,227
I1226 14:16:41.995959 92503 data_layer.cpp:80] output data size: 128,3,227,227
I1226 14:16:41.998739 93636 net.cpp:228] Setting up label_data_1_split
I1226 14:16:41.998839 93636 net.cpp:235] Top shape: 128 1 1 1 (128)
I1226 14:16:41.998872 93636 net.cpp:235] Top shape: 128 1 1 1 (128)
I1226 14:16:41.998893 93636 net.cpp:243] Memory required for data: 79150080
I1226 14:16:41.998925 93636 layer_factory.hpp:114] Creating layer conv1
I1226 14:16:41.999017 93636 net.cpp:178] Creating Layer conv1
I1226 14:16:41.999048 93636 net.cpp:612] conv1 <- data
I1226 14:16:41.999088 93636 net.cpp:586] conv1 -> conv1
I1226 14:16:42.012804 93421 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_0
I1226 14:16:42.033435 93418 data_layer.cpp:80] output data size: 128,3,227,227
I1226 14:16:42.033025 98931 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_6
I1226 14:16:42.041846 98928 data_layer.cpp:80] output data size: 128,3,227,227
I1226 14:16:42.125141 91497 net.cpp:228] Setting up data
I1226 14:16:42.125322 91497 net.cpp:235] Top shape: 128 3 227 227 (19787136)
I1226 14:16:42.125373 91497 net.cpp:235] Top shape: 128 (128)
I1226 14:16:42.125535 91497 net.cpp:243] Memory required for data: 79149056
I1226 14:16:42.125586 91497 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 14:16:42.125741 91497 net.cpp:178] Creating Layer label_data_1_split
I1226 14:16:42.125919 91497 net.cpp:612] label_data_1_split <- label
I1226 14:16:42.125980 91497 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 14:16:42.126392 91497 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 14:16:42.147511 93636 net.cpp:228] Setting up conv1
I1226 14:16:42.147619 93636 net.cpp:235] Top shape: 128 96 55 55 (37171200)
I1226 14:16:42.147644 93636 net.cpp:243] Memory required for data: 227834880
I1226 14:16:42.147807 93636 layer_factory.hpp:114] Creating layer relu1
I1226 14:16:42.147938 93636 net.cpp:178] Creating Layer relu1
I1226 14:16:42.147979 93636 net.cpp:612] relu1 <- conv1
I1226 14:16:42.148036 93636 net.cpp:573] relu1 -> conv1 (in-place)
I1226 14:16:42.148221 93636 net.cpp:228] Setting up relu1
I1226 14:16:42.148290 93636 net.cpp:235] Top shape: 128 96 55 55 (37171200)
I1226 14:16:42.148314 93636 net.cpp:243] Memory required for data: 376519680
I1226 14:16:42.148340 93636 layer_factory.hpp:114] Creating layer norm1
I1226 14:16:42.148409 93636 net.cpp:178] Creating Layer norm1
I1226 14:16:42.148439 93636 net.cpp:612] norm1 <- conv1
I1226 14:16:42.148471 93636 net.cpp:586] norm1 -> norm1
I1226 14:16:42.148553 93636 net.cpp:228] Setting up norm1
I1226 14:16:42.148600 93636 net.cpp:235] Top shape: 128 96 55 55 (37171200)
I1226 14:16:42.148624 93636 net.cpp:243] Memory required for data: 525204480
I1226 14:16:42.148651 93636 layer_factory.hpp:114] Creating layer pool1
I1226 14:16:42.148717 93636 net.cpp:178] Creating Layer pool1
I1226 14:16:42.148752 93636 net.cpp:612] pool1 <- norm1
I1226 14:16:42.148803 93636 net.cpp:586] pool1 -> pool1
I1226 14:16:42.148883 93636 net.cpp:228] Setting up pool1
I1226 14:16:42.148922 93636 net.cpp:235] Top shape: 128 96 27 27 (8957952)
I1226 14:16:42.148944 93636 net.cpp:243] Memory required for data: 561036288
I1226 14:16:42.148977 93636 layer_factory.hpp:114] Creating layer conv2
I1226 14:16:42.149041 93636 net.cpp:178] Creating Layer conv2
I1226 14:16:42.149091 93636 net.cpp:612] conv2 <- pool1
I1226 14:16:42.149138 93636 net.cpp:586] conv2 -> conv2
I1226 14:16:42.144632 91497 net.cpp:228] Setting up label_data_1_split
I1226 14:16:42.144759 91497 net.cpp:235] Top shape: 128 (128)
I1226 14:16:42.144798 91497 net.cpp:235] Top shape: 128 (128)
I1226 14:16:42.144824 91497 net.cpp:243] Memory required for data: 79150080
I1226 14:16:42.144860 91497 layer_factory.hpp:114] Creating layer conv1
I1226 14:16:42.144996 91497 net.cpp:178] Creating Layer conv1
I1226 14:16:42.145081 91497 net.cpp:612] conv1 <- data
I1226 14:16:42.145129 91497 net.cpp:586] conv1 -> conv1
I1226 14:16:42.170581 91408 net.cpp:228] Setting up data
I1226 14:16:42.170689 91408 net.cpp:235] Top shape: 128 3 227 227 (19787136)
I1226 14:16:42.170727 91408 net.cpp:235] Top shape: 128 (128)
I1226 14:16:42.170750 91408 net.cpp:243] Memory required for data: 79149056
I1226 14:16:42.170791 91408 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 14:16:42.170886 91408 net.cpp:178] Creating Layer label_data_1_split
I1226 14:16:42.171011 91408 net.cpp:612] label_data_1_split <- label
I1226 14:16:42.171056 91408 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 14:16:42.171106 91408 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 14:16:42.161973 92503 net.cpp:228] Setting up data
I1226 14:16:42.162106 92503 net.cpp:235] Top shape: 128 3 227 227 (19787136)
I1226 14:16:42.162158 92503 net.cpp:235] Top shape: 128 (128)
I1226 14:16:42.162190 92503 net.cpp:243] Memory required for data: 79149056
I1226 14:16:42.162255 92503 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 14:16:42.162400 92503 net.cpp:178] Creating Layer label_data_1_split
I1226 14:16:42.162573 92503 net.cpp:612] label_data_1_split <- label
I1226 14:16:42.162647 92503 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 14:16:42.162730 92503 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 14:16:42.191030 91408 net.cpp:228] Setting up label_data_1_split
I1226 14:16:42.191164 91408 net.cpp:235] Top shape: 128 (128)
I1226 14:16:42.191202 91408 net.cpp:235] Top shape: 128 (128)
I1226 14:16:42.191226 91408 net.cpp:243] Memory required for data: 79150080
I1226 14:16:42.191403 91408 layer_factory.hpp:114] Creating layer conv1
I1226 14:16:42.191560 91408 net.cpp:178] Creating Layer conv1
I1226 14:16:42.191650 91408 net.cpp:612] conv1 <- data
I1226 14:16:42.191786 91408 net.cpp:586] conv1 -> conv1
I1226 14:16:42.188525 92503 net.cpp:228] Setting up label_data_1_split
I1226 14:16:42.188632 92503 net.cpp:235] Top shape: 128 (128)
I1226 14:16:42.188665 92503 net.cpp:235] Top shape: 128 (128)
I1226 14:16:42.188689 92503 net.cpp:243] Memory required for data: 79150080
I1226 14:16:42.188724 92503 layer_factory.hpp:114] Creating layer conv1
I1226 14:16:42.188959 92503 net.cpp:178] Creating Layer conv1
I1226 14:16:42.189033 92503 net.cpp:612] conv1 <- data
I1226 14:16:42.189077 92503 net.cpp:586] conv1 -> conv1
I1226 14:16:42.214071 93418 net.cpp:228] Setting up data
I1226 14:16:42.214192 93418 net.cpp:235] Top shape: 128 3 227 227 (19787136)
I1226 14:16:42.214227 93418 net.cpp:235] Top shape: 128 (128)
I1226 14:16:42.214252 93418 net.cpp:243] Memory required for data: 79149056
I1226 14:16:42.214289 93418 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 14:16:42.214406 93418 net.cpp:178] Creating Layer label_data_1_split
I1226 14:16:42.214552 93418 net.cpp:612] label_data_1_split <- label
I1226 14:16:42.214612 93418 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 14:16:42.214669 93418 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 14:16:42.218458 98928 net.cpp:228] Setting up data
I1226 14:16:42.218607 98928 net.cpp:235] Top shape: 128 3 227 227 (19787136)
I1226 14:16:42.218680 98928 net.cpp:235] Top shape: 128 (128)
I1226 14:16:42.218719 98928 net.cpp:243] Memory required for data: 79149056
I1226 14:16:42.218768 98928 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 14:16:42.219043 98928 net.cpp:178] Creating Layer label_data_1_split
I1226 14:16:42.219233 98928 net.cpp:612] label_data_1_split <- label
I1226 14:16:42.219293 98928 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 14:16:42.219365 98928 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 14:16:42.252759 98928 net.cpp:228] Setting up label_data_1_split
I1226 14:16:42.252871 98928 net.cpp:235] Top shape: 128 (128)
I1226 14:16:42.252905 98928 net.cpp:235] Top shape: 128 (128)
I1226 14:16:42.252931 98928 net.cpp:243] Memory required for data: 79150080
I1226 14:16:42.252969 98928 layer_factory.hpp:114] Creating layer conv1
I1226 14:16:42.253104 98928 net.cpp:178] Creating Layer conv1
I1226 14:16:42.253151 98928 net.cpp:612] conv1 <- data
I1226 14:16:42.253197 98928 net.cpp:586] conv1 -> conv1
I1226 14:16:42.259727 93418 net.cpp:228] Setting up label_data_1_split
I1226 14:16:42.259834 93418 net.cpp:235] Top shape: 128 (128)
I1226 14:16:42.259865 93418 net.cpp:235] Top shape: 128 (128)
I1226 14:16:42.259886 93418 net.cpp:243] Memory required for data: 79150080
I1226 14:16:42.259920 93418 layer_factory.hpp:114] Creating layer conv1
I1226 14:16:42.260020 93418 net.cpp:178] Creating Layer conv1
I1226 14:16:42.260056 93418 net.cpp:612] conv1 <- data
I1226 14:16:42.260099 93418 net.cpp:586] conv1 -> conv1
I1226 14:16:42.351372 91497 net.cpp:228] Setting up conv1
I1226 14:16:42.351871 91497 net.cpp:235] Top shape: 128 96 55 55 (37171200)
I1226 14:16:42.351953 91497 net.cpp:243] Memory required for data: 227834880
I1226 14:16:42.352126 91497 layer_factory.hpp:114] Creating layer relu1
I1226 14:16:42.352250 91497 net.cpp:178] Creating Layer relu1
I1226 14:16:42.352407 91497 net.cpp:612] relu1 <- conv1
I1226 14:16:42.352458 91497 net.cpp:573] relu1 -> conv1 (in-place)
I1226 14:16:42.352582 91497 net.cpp:228] Setting up relu1
I1226 14:16:42.352648 91497 net.cpp:235] Top shape: 128 96 55 55 (37171200)
I1226 14:16:42.352676 91497 net.cpp:243] Memory required for data: 376519680
I1226 14:16:42.352743 91497 layer_factory.hpp:114] Creating layer norm1
I1226 14:16:42.352815 91497 net.cpp:178] Creating Layer norm1
I1226 14:16:42.352869 91497 net.cpp:612] norm1 <- conv1
I1226 14:16:42.352938 91497 net.cpp:586] norm1 -> norm1
I1226 14:16:42.353080 91497 net.cpp:228] Setting up norm1
I1226 14:16:42.353164 91497 net.cpp:235] Top shape: 128 96 55 55 (37171200)
I1226 14:16:42.353193 91497 net.cpp:243] Memory required for data: 525204480
I1226 14:16:42.353413 91497 layer_factory.hpp:114] Creating layer pool1
I1226 14:16:42.353482 91497 net.cpp:178] Creating Layer pool1
I1226 14:16:42.353518 91497 net.cpp:612] pool1 <- norm1
I1226 14:16:42.353570 91497 net.cpp:586] pool1 -> pool1
I1226 14:16:42.354086 91497 net.cpp:228] Setting up pool1
I1226 14:16:42.354255 91497 net.cpp:235] Top shape: 128 96 27 27 (8957952)
I1226 14:16:42.354306 91497 net.cpp:243] Memory required for data: 561036288
I1226 14:16:42.354367 91497 layer_factory.hpp:114] Creating layer conv2
I1226 14:16:42.354475 91497 net.cpp:178] Creating Layer conv2
I1226 14:16:42.354518 91497 net.cpp:612] conv2 <- pool1
I1226 14:16:42.354579 91497 net.cpp:586] conv2 -> conv2
I1226 14:16:42.363843 93636 net.cpp:228] Setting up conv2
I1226 14:16:42.363981 93636 net.cpp:235] Top shape: 128 256 27 27 (23887872)
I1226 14:16:42.364006 93636 net.cpp:243] Memory required for data: 656587776
I1226 14:16:42.364104 93636 layer_factory.hpp:114] Creating layer relu2
I1226 14:16:42.364183 93636 net.cpp:178] Creating Layer relu2
I1226 14:16:42.364215 93636 net.cpp:612] relu2 <- conv2
I1226 14:16:42.364261 93636 net.cpp:573] relu2 -> conv2 (in-place)
I1226 14:16:42.364353 93636 net.cpp:228] Setting up relu2
I1226 14:16:42.364398 93636 net.cpp:235] Top shape: 128 256 27 27 (23887872)
I1226 14:16:42.364423 93636 net.cpp:243] Memory required for data: 752139264
I1226 14:16:42.364451 93636 layer_factory.hpp:114] Creating layer norm2
I1226 14:16:42.364498 93636 net.cpp:178] Creating Layer norm2
I1226 14:16:42.364529 93636 net.cpp:612] norm2 <- conv2
I1226 14:16:42.364578 93636 net.cpp:586] norm2 -> norm2
I1226 14:16:42.364660 93636 net.cpp:228] Setting up norm2
I1226 14:16:42.364729 93636 net.cpp:235] Top shape: 128 256 27 27 (23887872)
I1226 14:16:42.364756 93636 net.cpp:243] Memory required for data: 847690752
I1226 14:16:42.364784 93636 layer_factory.hpp:114] Creating layer pool2
I1226 14:16:42.364840 93636 net.cpp:178] Creating Layer pool2
I1226 14:16:42.364872 93636 net.cpp:612] pool2 <- norm2
I1226 14:16:42.364907 93636 net.cpp:586] pool2 -> pool2
I1226 14:16:42.364985 93636 net.cpp:228] Setting up pool2
I1226 14:16:42.365025 93636 net.cpp:235] Top shape: 128 256 13 13 (5537792)
I1226 14:16:42.365197 93636 net.cpp:243] Memory required for data: 869841920
I1226 14:16:42.365229 93636 layer_factory.hpp:114] Creating layer conv3
I1226 14:16:42.365314 93636 net.cpp:178] Creating Layer conv3
I1226 14:16:42.365346 93636 net.cpp:612] conv3 <- pool2
I1226 14:16:42.365393 93636 net.cpp:586] conv3 -> conv3
I1226 14:16:42.403511 91408 net.cpp:228] Setting up conv1
I1226 14:16:42.403640 91408 net.cpp:235] Top shape: 128 96 55 55 (37171200)
I1226 14:16:42.403667 91408 net.cpp:243] Memory required for data: 227834880
I1226 14:16:42.403779 91408 layer_factory.hpp:114] Creating layer relu1
I1226 14:16:42.403857 91408 net.cpp:178] Creating Layer relu1
I1226 14:16:42.403895 91408 net.cpp:612] relu1 <- conv1
I1226 14:16:42.403939 91408 net.cpp:573] relu1 -> conv1 (in-place)
I1226 14:16:42.404044 91408 net.cpp:228] Setting up relu1
I1226 14:16:42.404104 91408 net.cpp:235] Top shape: 128 96 55 55 (37171200)
I1226 14:16:42.404129 91408 net.cpp:243] Memory required for data: 376519680
I1226 14:16:42.404163 91408 layer_factory.hpp:114] Creating layer norm1
I1226 14:16:42.404230 91408 net.cpp:178] Creating Layer norm1
I1226 14:16:42.404256 91408 net.cpp:612] norm1 <- conv1
I1226 14:16:42.404291 91408 net.cpp:586] norm1 -> norm1
I1226 14:16:42.404417 91408 net.cpp:228] Setting up norm1
I1226 14:16:42.404469 91408 net.cpp:235] Top shape: 128 96 55 55 (37171200)
I1226 14:16:42.404492 91408 net.cpp:243] Memory required for data: 525204480
I1226 14:16:42.404520 91408 layer_factory.hpp:114] Creating layer pool1
I1226 14:16:42.404577 91408 net.cpp:178] Creating Layer pool1
I1226 14:16:42.404602 91408 net.cpp:612] pool1 <- norm1
I1226 14:16:42.404659 91408 net.cpp:586] pool1 -> pool1
I1226 14:16:42.404759 91408 net.cpp:228] Setting up pool1
I1226 14:16:42.404803 91408 net.cpp:235] Top shape: 128 96 27 27 (8957952)
I1226 14:16:42.404825 91408 net.cpp:243] Memory required for data: 561036288
I1226 14:16:42.404856 91408 layer_factory.hpp:114] Creating layer conv2
I1226 14:16:42.404937 91408 net.cpp:178] Creating Layer conv2
I1226 14:16:42.404971 91408 net.cpp:612] conv2 <- pool1
I1226 14:16:42.405009 91408 net.cpp:586] conv2 -> conv2
I1226 14:16:42.397267 97044 net.cpp:228] Setting up data
I1226 14:16:42.397397 97044 net.cpp:235] Top shape: 128 3 227 227 (19787136)
I1226 14:16:42.397450 97044 net.cpp:235] Top shape: 128 (128)
I1226 14:16:42.397483 97044 net.cpp:243] Memory required for data: 79149056
I1226 14:16:42.397526 97044 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 14:16:42.397635 97044 net.cpp:178] Creating Layer label_data_1_split
I1226 14:16:42.397812 97044 net.cpp:612] label_data_1_split <- label
I1226 14:16:42.397881 97044 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 14:16:42.397948 97044 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 14:16:42.412935 92503 net.cpp:228] Setting up conv1
I1226 14:16:42.413049 92503 net.cpp:235] Top shape: 128 96 55 55 (37171200)
I1226 14:16:42.413079 92503 net.cpp:243] Memory required for data: 227834880
I1226 14:16:42.413223 92503 layer_factory.hpp:114] Creating layer relu1
I1226 14:16:42.413414 92503 net.cpp:178] Creating Layer relu1
I1226 14:16:42.413463 92503 net.cpp:612] relu1 <- conv1
I1226 14:16:42.413506 92503 net.cpp:573] relu1 -> conv1 (in-place)
I1226 14:16:42.413619 92503 net.cpp:228] Setting up relu1
I1226 14:16:42.413672 92503 net.cpp:235] Top shape: 128 96 55 55 (37171200)
I1226 14:16:42.413697 92503 net.cpp:243] Memory required for data: 376519680
I1226 14:16:42.413728 92503 layer_factory.hpp:114] Creating layer norm1
I1226 14:16:42.413800 92503 net.cpp:178] Creating Layer norm1
I1226 14:16:42.413836 92503 net.cpp:612] norm1 <- conv1
I1226 14:16:42.413873 92503 net.cpp:586] norm1 -> norm1
I1226 14:16:42.413967 92503 net.cpp:228] Setting up norm1
I1226 14:16:42.414026 92503 net.cpp:235] Top shape: 128 96 55 55 (37171200)
I1226 14:16:42.414049 92503 net.cpp:243] Memory required for data: 525204480
I1226 14:16:42.414078 92503 layer_factory.hpp:114] Creating layer pool1
I1226 14:16:42.414129 92503 net.cpp:178] Creating Layer pool1
I1226 14:16:42.414325 92503 net.cpp:612] pool1 <- norm1
I1226 14:16:42.414391 92503 net.cpp:586] pool1 -> pool1
I1226 14:16:42.414510 92503 net.cpp:228] Setting up pool1
I1226 14:16:42.414559 92503 net.cpp:235] Top shape: 128 96 27 27 (8957952)
I1226 14:16:42.414590 92503 net.cpp:243] Memory required for data: 561036288
I1226 14:16:42.414618 92503 layer_factory.hpp:114] Creating layer conv2
I1226 14:16:42.414700 92503 net.cpp:178] Creating Layer conv2
I1226 14:16:42.414737 92503 net.cpp:612] conv2 <- pool1
I1226 14:16:42.414777 92503 net.cpp:586] conv2 -> conv2
I1226 14:16:42.425161 97044 net.cpp:228] Setting up label_data_1_split
I1226 14:16:42.425317 97044 net.cpp:235] Top shape: 128 (128)
I1226 14:16:42.425364 97044 net.cpp:235] Top shape: 128 (128)
I1226 14:16:42.425390 97044 net.cpp:243] Memory required for data: 79150080
I1226 14:16:42.425431 97044 layer_factory.hpp:114] Creating layer conv1
I1226 14:16:42.425542 97044 net.cpp:178] Creating Layer conv1
I1226 14:16:42.425592 97044 net.cpp:612] conv1 <- data
I1226 14:16:42.425644 97044 net.cpp:586] conv1 -> conv1
I1226 14:16:42.433751 97473 net.cpp:228] Setting up data
I1226 14:16:42.433878 97473 net.cpp:235] Top shape: 128 3 227 227 (19787136)
I1226 14:16:42.433925 97473 net.cpp:235] Top shape: 128 (128)
I1226 14:16:42.433954 97473 net.cpp:243] Memory required for data: 79149056
I1226 14:16:42.434010 97473 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 14:16:42.434109 97473 net.cpp:178] Creating Layer label_data_1_split
I1226 14:16:42.434257 97473 net.cpp:612] label_data_1_split <- label
I1226 14:16:42.434311 97473 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 14:16:42.434370 97473 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 14:16:42.447515 95408 net.cpp:228] Setting up data
I1226 14:16:42.447641 95408 net.cpp:235] Top shape: 128 3 227 227 (19787136)
I1226 14:16:42.447687 95408 net.cpp:235] Top shape: 128 (128)
I1226 14:16:42.447715 95408 net.cpp:243] Memory required for data: 79149056
I1226 14:16:42.447770 95408 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 14:16:42.447901 95408 net.cpp:178] Creating Layer label_data_1_split
I1226 14:16:42.448056 95408 net.cpp:612] label_data_1_split <- label
I1226 14:16:42.448117 95408 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 14:16:42.448189 95408 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 14:16:42.474040 95408 net.cpp:228] Setting up label_data_1_split
I1226 14:16:42.474154 95408 net.cpp:235] Top shape: 128 (128)
I1226 14:16:42.474194 95408 net.cpp:235] Top shape: 128 (128)
I1226 14:16:42.474218 95408 net.cpp:243] Memory required for data: 79150080
I1226 14:16:42.474256 95408 layer_factory.hpp:114] Creating layer conv1
I1226 14:16:42.474367 95408 net.cpp:178] Creating Layer conv1
I1226 14:16:42.474409 95408 net.cpp:612] conv1 <- data
I1226 14:16:42.474464 95408 net.cpp:586] conv1 -> conv1
I1226 14:16:42.485772 98928 net.cpp:228] Setting up conv1
I1226 14:16:42.485890 98928 net.cpp:235] Top shape: 128 96 55 55 (37171200)
I1226 14:16:42.485924 98928 net.cpp:243] Memory required for data: 227834880
I1226 14:16:42.486052 98928 layer_factory.hpp:114] Creating layer relu1
I1226 14:16:42.486203 98928 net.cpp:178] Creating Layer relu1
I1226 14:16:42.486261 98928 net.cpp:612] relu1 <- conv1
I1226 14:16:42.473072 97473 net.cpp:228] Setting up label_data_1_split
I1226 14:16:42.486310 98928 net.cpp:573] relu1 -> conv1 (in-place)
I1226 14:16:42.473189 97473 net.cpp:235] Top shape: 128 (128)
I1226 14:16:42.473227 97473 net.cpp:235] Top shape: 128 (128)
I1226 14:16:42.486439 98928 net.cpp:228] Setting up relu1
I1226 14:16:42.473253 97473 net.cpp:243] Memory required for data: 79150080
I1226 14:16:42.486500 98928 net.cpp:235] Top shape: 128 96 55 55 (37171200)
I1226 14:16:42.473289 97473 layer_factory.hpp:114] Creating layer conv1
I1226 14:16:42.486524 98928 net.cpp:243] Memory required for data: 376519680
I1226 14:16:42.486557 98928 layer_factory.hpp:114] Creating layer norm1
I1226 14:16:42.473412 97473 net.cpp:178] Creating Layer conv1
I1226 14:16:42.486639 98928 net.cpp:178] Creating Layer norm1
I1226 14:16:42.473455 97473 net.cpp:612] conv1 <- data
I1226 14:16:42.486682 98928 net.cpp:612] norm1 <- conv1
I1226 14:16:42.486723 98928 net.cpp:586] norm1 -> norm1
I1226 14:16:42.473503 97473 net.cpp:586] conv1 -> conv1
I1226 14:16:42.487231 98928 net.cpp:228] Setting up norm1
I1226 14:16:42.487411 98928 net.cpp:235] Top shape: 128 96 55 55 (37171200)
I1226 14:16:42.487478 98928 net.cpp:243] Memory required for data: 525204480
I1226 14:16:42.487520 98928 layer_factory.hpp:114] Creating layer pool1
I1226 14:16:42.487624 98928 net.cpp:178] Creating Layer pool1
I1226 14:16:42.487669 98928 net.cpp:612] pool1 <- norm1
I1226 14:16:42.487733 98928 net.cpp:586] pool1 -> pool1
I1226 14:16:42.487880 98928 net.cpp:228] Setting up pool1
I1226 14:16:42.487937 98928 net.cpp:235] Top shape: 128 96 27 27 (8957952)
I1226 14:16:42.487962 98928 net.cpp:243] Memory required for data: 561036288
I1226 14:16:42.488020 98928 layer_factory.hpp:114] Creating layer conv2
I1226 14:16:42.488147 98928 net.cpp:178] Creating Layer conv2
I1226 14:16:42.488198 98928 net.cpp:612] conv2 <- pool1
I1226 14:16:42.488261 98928 net.cpp:586] conv2 -> conv2
I1226 14:16:42.502997 93418 net.cpp:228] Setting up conv1
I1226 14:16:42.503114 93418 net.cpp:235] Top shape: 128 96 55 55 (37171200)
I1226 14:16:42.503142 93418 net.cpp:243] Memory required for data: 227834880
I1226 14:16:42.503262 93418 layer_factory.hpp:114] Creating layer relu1
I1226 14:16:42.503357 93418 net.cpp:178] Creating Layer relu1
I1226 14:16:42.503425 93418 net.cpp:612] relu1 <- conv1
I1226 14:16:42.503473 93418 net.cpp:573] relu1 -> conv1 (in-place)
I1226 14:16:42.503585 93418 net.cpp:228] Setting up relu1
I1226 14:16:42.503629 93418 net.cpp:235] Top shape: 128 96 55 55 (37171200)
I1226 14:16:42.503654 93418 net.cpp:243] Memory required for data: 376519680
I1226 14:16:42.503684 93418 layer_factory.hpp:114] Creating layer norm1
I1226 14:16:42.503746 93418 net.cpp:178] Creating Layer norm1
I1226 14:16:42.503774 93418 net.cpp:612] norm1 <- conv1
I1226 14:16:42.503832 93418 net.cpp:586] norm1 -> norm1
I1226 14:16:42.503937 93418 net.cpp:228] Setting up norm1
I1226 14:16:42.504106 93418 net.cpp:235] Top shape: 128 96 55 55 (37171200)
I1226 14:16:42.504130 93418 net.cpp:243] Memory required for data: 525204480
I1226 14:16:42.504161 93418 layer_factory.hpp:114] Creating layer pool1
I1226 14:16:42.504220 93418 net.cpp:178] Creating Layer pool1
I1226 14:16:42.504248 93418 net.cpp:612] pool1 <- norm1
I1226 14:16:42.504298 93418 net.cpp:586] pool1 -> pool1
I1226 14:16:42.504438 93418 net.cpp:228] Setting up pool1
I1226 14:16:42.504482 93418 net.cpp:235] Top shape: 128 96 27 27 (8957952)
I1226 14:16:42.504506 93418 net.cpp:243] Memory required for data: 561036288
I1226 14:16:42.504535 93418 layer_factory.hpp:114] Creating layer conv2
I1226 14:16:42.504621 93418 net.cpp:178] Creating Layer conv2
I1226 14:16:42.504806 93418 net.cpp:612] conv2 <- pool1
I1226 14:16:42.504865 93418 net.cpp:586] conv2 -> conv2
I1226 14:16:42.590739 93636 net.cpp:228] Setting up conv3
I1226 14:16:42.590848 93636 net.cpp:235] Top shape: 128 384 13 13 (8306688)
I1226 14:16:42.590875 93636 net.cpp:243] Memory required for data: 903068672
I1226 14:16:42.590947 93636 layer_factory.hpp:114] Creating layer relu3
I1226 14:16:42.591012 93636 net.cpp:178] Creating Layer relu3
I1226 14:16:42.591043 93636 net.cpp:612] relu3 <- conv3
I1226 14:16:42.591092 93636 net.cpp:573] relu3 -> conv3 (in-place)
I1226 14:16:42.591163 93636 net.cpp:228] Setting up relu3
I1226 14:16:42.591204 93636 net.cpp:235] Top shape: 128 384 13 13 (8306688)
I1226 14:16:42.591233 93636 net.cpp:243] Memory required for data: 936295424
I1226 14:16:42.591259 93636 layer_factory.hpp:114] Creating layer conv4
I1226 14:16:42.591325 93636 net.cpp:178] Creating Layer conv4
I1226 14:16:42.591356 93636 net.cpp:612] conv4 <- conv3
I1226 14:16:42.591394 93636 net.cpp:586] conv4 -> conv4
I1226 14:16:42.745807 91497 net.cpp:228] Setting up conv2
I1226 14:16:42.745920 91497 net.cpp:235] Top shape: 128 256 27 27 (23887872)
I1226 14:16:42.745949 91497 net.cpp:243] Memory required for data: 656587776
I1226 14:16:42.746026 91497 layer_factory.hpp:114] Creating layer relu2
I1226 14:16:42.746114 91497 net.cpp:178] Creating Layer relu2
I1226 14:16:42.746158 91497 net.cpp:612] relu2 <- conv2
I1226 14:16:42.746206 91497 net.cpp:573] relu2 -> conv2 (in-place)
I1226 14:16:42.746326 91497 net.cpp:228] Setting up relu2
I1226 14:16:42.746371 91497 net.cpp:235] Top shape: 128 256 27 27 (23887872)
I1226 14:16:42.746395 91497 net.cpp:243] Memory required for data: 752139264
I1226 14:16:42.746425 91497 layer_factory.hpp:114] Creating layer norm2
I1226 14:16:42.746484 91497 net.cpp:178] Creating Layer norm2
I1226 14:16:42.746520 91497 net.cpp:612] norm2 <- conv2
I1226 14:16:42.746563 91497 net.cpp:586] norm2 -> norm2
I1226 14:16:42.746659 91497 net.cpp:228] Setting up norm2
I1226 14:16:42.746733 91497 net.cpp:235] Top shape: 128 256 27 27 (23887872)
I1226 14:16:42.746773 91497 net.cpp:243] Memory required for data: 847690752
I1226 14:16:42.746803 91497 layer_factory.hpp:114] Creating layer pool2
I1226 14:16:42.746866 91497 net.cpp:178] Creating Layer pool2
I1226 14:16:42.746903 91497 net.cpp:612] pool2 <- norm2
I1226 14:16:42.746948 91497 net.cpp:586] pool2 -> pool2
I1226 14:16:42.747036 91497 net.cpp:228] Setting up pool2
I1226 14:16:42.747174 91497 net.cpp:235] Top shape: 128 256 13 13 (5537792)
I1226 14:16:42.747201 91497 net.cpp:243] Memory required for data: 869841920
I1226 14:16:42.747231 91497 layer_factory.hpp:114] Creating layer conv3
I1226 14:16:42.747319 91497 net.cpp:178] Creating Layer conv3
I1226 14:16:42.747356 91497 net.cpp:612] conv3 <- pool2
I1226 14:16:42.747407 91497 net.cpp:586] conv3 -> conv3
I1226 14:16:42.791002 91408 net.cpp:228] Setting up conv2
I1226 14:16:42.791115 91408 net.cpp:235] Top shape: 128 256 27 27 (23887872)
I1226 14:16:42.791144 91408 net.cpp:243] Memory required for data: 656587776
I1226 14:16:42.791219 91408 layer_factory.hpp:114] Creating layer relu2
I1226 14:16:42.791329 91408 net.cpp:178] Creating Layer relu2
I1226 14:16:42.791370 91408 net.cpp:612] relu2 <- conv2
I1226 14:16:42.791422 91408 net.cpp:573] relu2 -> conv2 (in-place)
I1226 14:16:42.791527 91408 net.cpp:228] Setting up relu2
I1226 14:16:42.791570 91408 net.cpp:235] Top shape: 128 256 27 27 (23887872)
I1226 14:16:42.791594 91408 net.cpp:243] Memory required for data: 752139264
I1226 14:16:42.791621 91408 layer_factory.hpp:114] Creating layer norm2
I1226 14:16:42.791671 91408 net.cpp:178] Creating Layer norm2
I1226 14:16:42.791718 91408 net.cpp:612] norm2 <- conv2
I1226 14:16:42.791757 91408 net.cpp:586] norm2 -> norm2
I1226 14:16:42.791854 91408 net.cpp:228] Setting up norm2
I1226 14:16:42.791909 91408 net.cpp:235] Top shape: 128 256 27 27 (23887872)
I1226 14:16:42.791936 91408 net.cpp:243] Memory required for data: 847690752
I1226 14:16:42.791965 91408 layer_factory.hpp:114] Creating layer pool2
I1226 14:16:42.792021 91408 net.cpp:178] Creating Layer pool2
I1226 14:16:42.792055 91408 net.cpp:612] pool2 <- norm2
I1226 14:16:42.792090 91408 net.cpp:586] pool2 -> pool2
I1226 14:16:42.792181 91408 net.cpp:228] Setting up pool2
I1226 14:16:42.792347 91408 net.cpp:235] Top shape: 128 256 13 13 (5537792)
I1226 14:16:42.792379 91408 net.cpp:243] Memory required for data: 869841920
I1226 14:16:42.792410 91408 layer_factory.hpp:114] Creating layer conv3
I1226 14:16:42.792492 91408 net.cpp:178] Creating Layer conv3
I1226 14:16:42.792529 91408 net.cpp:612] conv3 <- pool2
I1226 14:16:42.792572 91408 net.cpp:586] conv3 -> conv3
I1226 14:16:42.788892 93636 net.cpp:228] Setting up conv4
I1226 14:16:42.789033 93636 net.cpp:235] Top shape: 128 384 13 13 (8306688)
I1226 14:16:42.789062 93636 net.cpp:243] Memory required for data: 969522176
I1226 14:16:42.789146 93636 layer_factory.hpp:114] Creating layer relu4
I1226 14:16:42.789211 93636 net.cpp:178] Creating Layer relu4
I1226 14:16:42.789250 93636 net.cpp:612] relu4 <- conv4
I1226 14:16:42.789294 93636 net.cpp:573] relu4 -> conv4 (in-place)
I1226 14:16:42.789371 93636 net.cpp:228] Setting up relu4
I1226 14:16:42.789407 93636 net.cpp:235] Top shape: 128 384 13 13 (8306688)
I1226 14:16:42.789432 93636 net.cpp:243] Memory required for data: 1002748928
I1226 14:16:42.789461 93636 layer_factory.hpp:114] Creating layer conv5
I1226 14:16:42.789539 93636 net.cpp:178] Creating Layer conv5
I1226 14:16:42.789577 93636 net.cpp:612] conv5 <- conv4
I1226 14:16:42.789618 93636 net.cpp:586] conv5 -> conv5
I1226 14:16:42.790782 92503 net.cpp:228] Setting up conv2
I1226 14:16:42.791193 92503 net.cpp:235] Top shape: 128 256 27 27 (23887872)
I1226 14:16:42.791242 92503 net.cpp:243] Memory required for data: 656587776
I1226 14:16:42.791493 92503 layer_factory.hpp:114] Creating layer relu2
I1226 14:16:42.791632 92503 net.cpp:178] Creating Layer relu2
I1226 14:16:42.791749 92503 net.cpp:612] relu2 <- conv2
I1226 14:16:42.791803 92503 net.cpp:573] relu2 -> conv2 (in-place)
I1226 14:16:42.791909 92503 net.cpp:228] Setting up relu2
I1226 14:16:42.791996 92503 net.cpp:235] Top shape: 128 256 27 27 (23887872)
I1226 14:16:42.792026 92503 net.cpp:243] Memory required for data: 752139264
I1226 14:16:42.792057 92503 layer_factory.hpp:114] Creating layer norm2
I1226 14:16:42.792215 92503 net.cpp:178] Creating Layer norm2
I1226 14:16:42.792246 92503 net.cpp:612] norm2 <- conv2
I1226 14:16:42.792285 92503 net.cpp:586] norm2 -> norm2
I1226 14:16:42.792779 92503 net.cpp:228] Setting up norm2
I1226 14:16:42.792928 92503 net.cpp:235] Top shape: 128 256 27 27 (23887872)
I1226 14:16:42.792992 92503 net.cpp:243] Memory required for data: 847690752
I1226 14:16:42.793153 92503 layer_factory.hpp:114] Creating layer pool2
I1226 14:16:42.793236 92503 net.cpp:178] Creating Layer pool2
I1226 14:16:42.793279 92503 net.cpp:612] pool2 <- norm2
I1226 14:16:42.793391 92503 net.cpp:586] pool2 -> pool2
I1226 14:16:42.793529 92503 net.cpp:228] Setting up pool2
I1226 14:16:42.794013 92503 net.cpp:235] Top shape: 128 256 13 13 (5537792)
I1226 14:16:42.794093 92503 net.cpp:243] Memory required for data: 869841920
I1226 14:16:42.794134 92503 layer_factory.hpp:114] Creating layer conv3
I1226 14:16:42.794275 92503 net.cpp:178] Creating Layer conv3
I1226 14:16:42.794374 92503 net.cpp:612] conv3 <- pool2
I1226 14:16:42.794461 92503 net.cpp:586] conv3 -> conv3
I1226 14:16:42.856415 98928 net.cpp:228] Setting up conv2
I1226 14:16:42.856530 98928 net.cpp:235] Top shape: 128 256 27 27 (23887872)
I1226 14:16:42.856565 98928 net.cpp:243] Memory required for data: 656587776
I1226 14:16:42.856647 98928 layer_factory.hpp:114] Creating layer relu2
I1226 14:16:42.856748 98928 net.cpp:178] Creating Layer relu2
I1226 14:16:42.856828 98928 net.cpp:612] relu2 <- conv2
I1226 14:16:42.857019 98928 net.cpp:573] relu2 -> conv2 (in-place)
I1226 14:16:42.857290 98928 net.cpp:228] Setting up relu2
I1226 14:16:42.857409 98928 net.cpp:235] Top shape: 128 256 27 27 (23887872)
I1226 14:16:42.857439 98928 net.cpp:243] Memory required for data: 752139264
I1226 14:16:42.857578 98928 layer_factory.hpp:114] Creating layer norm2
I1226 14:16:42.857642 98928 net.cpp:178] Creating Layer norm2
I1226 14:16:42.857684 98928 net.cpp:612] norm2 <- conv2
I1226 14:16:42.857736 98928 net.cpp:586] norm2 -> norm2
I1226 14:16:42.858214 98928 net.cpp:228] Setting up norm2
I1226 14:16:42.858388 98928 net.cpp:235] Top shape: 128 256 27 27 (23887872)
I1226 14:16:42.858469 98928 net.cpp:243] Memory required for data: 847690752
I1226 14:16:42.858542 98928 layer_factory.hpp:114] Creating layer pool2
I1226 14:16:42.858636 98928 net.cpp:178] Creating Layer pool2
I1226 14:16:42.858680 98928 net.cpp:612] pool2 <- norm2
I1226 14:16:42.858724 98928 net.cpp:586] pool2 -> pool2
I1226 14:16:42.858836 98928 net.cpp:228] Setting up pool2
I1226 14:16:42.858986 98928 net.cpp:235] Top shape: 128 256 13 13 (5537792)
I1226 14:16:42.859016 98928 net.cpp:243] Memory required for data: 869841920
I1226 14:16:42.859050 98928 layer_factory.hpp:114] Creating layer conv3
I1226 14:16:42.859176 98928 net.cpp:178] Creating Layer conv3
I1226 14:16:42.859248 98928 net.cpp:612] conv3 <- pool2
I1226 14:16:42.859308 98928 net.cpp:586] conv3 -> conv3
I1226 14:16:42.936168 93636 net.cpp:228] Setting up conv5
I1226 14:16:42.936274 93636 net.cpp:235] Top shape: 128 256 13 13 (5537792)
I1226 14:16:42.936297 93636 net.cpp:243] Memory required for data: 1024900096
I1226 14:16:42.936395 93636 layer_factory.hpp:114] Creating layer relu5
I1226 14:16:42.936460 93636 net.cpp:178] Creating Layer relu5
I1226 14:16:42.936491 93636 net.cpp:612] relu5 <- conv5
I1226 14:16:42.936545 93636 net.cpp:573] relu5 -> conv5 (in-place)
I1226 14:16:42.936645 93636 net.cpp:228] Setting up relu5
I1226 14:16:42.936712 93636 net.cpp:235] Top shape: 128 256 13 13 (5537792)
I1226 14:16:42.936738 93636 net.cpp:243] Memory required for data: 1047051264
I1226 14:16:42.936767 93636 layer_factory.hpp:114] Creating layer pool5
I1226 14:16:42.936826 93636 net.cpp:178] Creating Layer pool5
I1226 14:16:42.936856 93636 net.cpp:612] pool5 <- conv5
I1226 14:16:42.936913 93636 net.cpp:586] pool5 -> pool5
I1226 14:16:42.936997 93636 net.cpp:228] Setting up pool5
I1226 14:16:42.937039 93636 net.cpp:235] Top shape: 128 256 6 6 (1179648)
I1226 14:16:42.937069 93636 net.cpp:243] Memory required for data: 1051769856
I1226 14:16:42.937111 93636 layer_factory.hpp:114] Creating layer fc6
I1226 14:16:42.937186 93636 net.cpp:178] Creating Layer fc6
I1226 14:16:42.937222 93636 net.cpp:612] fc6 <- pool5
I1226 14:16:42.937270 93636 net.cpp:586] fc6 -> fc6
I1226 14:16:42.950186 93418 net.cpp:228] Setting up conv2
I1226 14:16:42.950294 93418 net.cpp:235] Top shape: 128 256 27 27 (23887872)
I1226 14:16:42.950320 93418 net.cpp:243] Memory required for data: 656587776
I1226 14:16:42.950413 93418 layer_factory.hpp:114] Creating layer relu2
I1226 14:16:42.950479 93418 net.cpp:178] Creating Layer relu2
I1226 14:16:42.950510 93418 net.cpp:612] relu2 <- conv2
I1226 14:16:42.950551 93418 net.cpp:573] relu2 -> conv2 (in-place)
I1226 14:16:42.950624 93418 net.cpp:228] Setting up relu2
I1226 14:16:42.950659 93418 net.cpp:235] Top shape: 128 256 27 27 (23887872)
I1226 14:16:42.950683 93418 net.cpp:243] Memory required for data: 752139264
I1226 14:16:42.950711 93418 layer_factory.hpp:114] Creating layer norm2
I1226 14:16:42.950758 93418 net.cpp:178] Creating Layer norm2
I1226 14:16:42.950785 93418 net.cpp:612] norm2 <- conv2
I1226 14:16:42.950822 93418 net.cpp:586] norm2 -> norm2
I1226 14:16:42.950891 93418 net.cpp:228] Setting up norm2
I1226 14:16:42.950937 93418 net.cpp:235] Top shape: 128 256 27 27 (23887872)
I1226 14:16:42.950960 93418 net.cpp:243] Memory required for data: 847690752
I1226 14:16:42.950986 93418 layer_factory.hpp:114] Creating layer pool2
I1226 14:16:42.951028 93418 net.cpp:178] Creating Layer pool2
I1226 14:16:42.951051 93418 net.cpp:612] pool2 <- norm2
I1226 14:16:42.951083 93418 net.cpp:586] pool2 -> pool2
I1226 14:16:42.951143 93418 net.cpp:228] Setting up pool2
I1226 14:16:42.951278 93418 net.cpp:235] Top shape: 128 256 13 13 (5537792)
I1226 14:16:42.951305 93418 net.cpp:243] Memory required for data: 869841920
I1226 14:16:42.951333 93418 layer_factory.hpp:114] Creating layer conv3
I1226 14:16:42.951429 93418 net.cpp:178] Creating Layer conv3
I1226 14:16:42.951465 93418 net.cpp:612] conv3 <- pool2
I1226 14:16:42.951510 93418 net.cpp:586] conv3 -> conv3
I1226 14:16:43.092139 97044 net.cpp:228] Setting up conv1
I1226 14:16:43.092321 97044 net.cpp:235] Top shape: 128 96 55 55 (37171200)
I1226 14:16:43.092720 97044 net.cpp:243] Memory required for data: 227834880
I1226 14:16:43.093029 97044 layer_factory.hpp:114] Creating layer relu1
I1226 14:16:43.093155 97044 net.cpp:178] Creating Layer relu1
I1226 14:16:43.093276 97044 net.cpp:612] relu1 <- conv1
I1226 14:16:43.093320 97044 net.cpp:573] relu1 -> conv1 (in-place)
I1226 14:16:43.093441 97044 net.cpp:228] Setting up relu1
I1226 14:16:43.093505 97044 net.cpp:235] Top shape: 128 96 55 55 (37171200)
I1226 14:16:43.093534 97044 net.cpp:243] Memory required for data: 376519680
I1226 14:16:43.093744 97044 layer_factory.hpp:114] Creating layer norm1
I1226 14:16:43.093901 97044 net.cpp:178] Creating Layer norm1
I1226 14:16:43.093978 97044 net.cpp:612] norm1 <- conv1
I1226 14:16:43.094389 97044 net.cpp:586] norm1 -> norm1
I1226 14:16:43.094635 97044 net.cpp:228] Setting up norm1
I1226 14:16:43.094703 97044 net.cpp:235] Top shape: 128 96 55 55 (37171200)
I1226 14:16:43.094732 97044 net.cpp:243] Memory required for data: 525204480
I1226 14:16:43.094764 97044 layer_factory.hpp:114] Creating layer pool1
I1226 14:16:43.094861 97044 net.cpp:178] Creating Layer pool1
I1226 14:16:43.094899 97044 net.cpp:612] pool1 <- norm1
I1226 14:16:43.094952 97044 net.cpp:586] pool1 -> pool1
I1226 14:16:43.095110 97044 net.cpp:228] Setting up pool1
I1226 14:16:43.095181 97044 net.cpp:235] Top shape: 128 96 27 27 (8957952)
I1226 14:16:43.095230 97044 net.cpp:243] Memory required for data: 561036288
I1226 14:16:43.095270 97044 layer_factory.hpp:114] Creating layer conv2
I1226 14:16:43.095403 97044 net.cpp:178] Creating Layer conv2
I1226 14:16:43.095443 97044 net.cpp:612] conv2 <- pool1
I1226 14:16:43.096035 97044 net.cpp:586] conv2 -> conv2
I1226 14:16:43.135105 91497 net.cpp:228] Setting up conv3
I1226 14:16:43.135246 91497 net.cpp:235] Top shape: 128 384 13 13 (8306688)
I1226 14:16:43.135275 91497 net.cpp:243] Memory required for data: 903068672
I1226 14:16:43.135393 91497 layer_factory.hpp:114] Creating layer relu3
I1226 14:16:43.135526 91497 net.cpp:178] Creating Layer relu3
I1226 14:16:43.135604 91497 net.cpp:612] relu3 <- conv3
I1226 14:16:43.135740 91497 net.cpp:573] relu3 -> conv3 (in-place)
I1226 14:16:43.135850 91497 net.cpp:228] Setting up relu3
I1226 14:16:43.135905 91497 net.cpp:235] Top shape: 128 384 13 13 (8306688)
I1226 14:16:43.135929 91497 net.cpp:243] Memory required for data: 936295424
I1226 14:16:43.135960 91497 layer_factory.hpp:114] Creating layer conv4
I1226 14:16:43.136085 91497 net.cpp:178] Creating Layer conv4
I1226 14:16:43.136203 91497 net.cpp:612] conv4 <- conv3
I1226 14:16:43.136301 91497 net.cpp:586] conv4 -> conv4
I1226 14:16:43.175942 92503 net.cpp:228] Setting up conv3
I1226 14:16:43.176070 92503 net.cpp:235] Top shape: 128 384 13 13 (8306688)
I1226 14:16:43.176100 92503 net.cpp:243] Memory required for data: 903068672
I1226 14:16:43.176177 92503 layer_factory.hpp:114] Creating layer relu3
I1226 14:16:43.176334 92503 net.cpp:178] Creating Layer relu3
I1226 14:16:43.176705 92503 net.cpp:612] relu3 <- conv3
I1226 14:16:43.176765 92503 net.cpp:573] relu3 -> conv3 (in-place)
I1226 14:16:43.176914 92503 net.cpp:228] Setting up relu3
I1226 14:16:43.176996 92503 net.cpp:235] Top shape: 128 384 13 13 (8306688)
I1226 14:16:43.177023 92503 net.cpp:243] Memory required for data: 936295424
I1226 14:16:43.177054 92503 layer_factory.hpp:114] Creating layer conv4
I1226 14:16:43.177224 92503 net.cpp:178] Creating Layer conv4
I1226 14:16:43.177260 92503 net.cpp:612] conv4 <- conv3
I1226 14:16:43.177342 92503 net.cpp:586] conv4 -> conv4
I1226 14:16:43.207700 91408 net.cpp:228] Setting up conv3
I1226 14:16:43.207840 91408 net.cpp:235] Top shape: 128 384 13 13 (8306688)
I1226 14:16:43.207873 91408 net.cpp:243] Memory required for data: 903068672
I1226 14:16:43.207952 91408 layer_factory.hpp:114] Creating layer relu3
I1226 14:16:43.208086 91408 net.cpp:178] Creating Layer relu3
I1226 14:16:43.208128 91408 net.cpp:612] relu3 <- conv3
I1226 14:16:43.208170 91408 net.cpp:573] relu3 -> conv3 (in-place)
I1226 14:16:43.208267 91408 net.cpp:228] Setting up relu3
I1226 14:16:43.208344 91408 net.cpp:235] Top shape: 128 384 13 13 (8306688)
I1226 14:16:43.208381 91408 net.cpp:243] Memory required for data: 936295424
I1226 14:16:43.208438 91408 layer_factory.hpp:114] Creating layer conv4
I1226 14:16:43.208526 91408 net.cpp:178] Creating Layer conv4
I1226 14:16:43.208559 91408 net.cpp:612] conv4 <- conv3
I1226 14:16:43.208607 91408 net.cpp:586] conv4 -> conv4
I1226 14:16:43.191431 97473 net.cpp:228] Setting up conv1
I1226 14:16:43.191548 97473 net.cpp:235] Top shape: 128 96 55 55 (37171200)
I1226 14:16:43.191582 97473 net.cpp:243] Memory required for data: 227834880
I1226 14:16:43.191700 97473 layer_factory.hpp:114] Creating layer relu1
I1226 14:16:43.191880 97473 net.cpp:178] Creating Layer relu1
I1226 14:16:43.191932 97473 net.cpp:612] relu1 <- conv1
I1226 14:16:43.191978 97473 net.cpp:573] relu1 -> conv1 (in-place)
I1226 14:16:43.192111 97473 net.cpp:228] Setting up relu1
I1226 14:16:43.192163 97473 net.cpp:235] Top shape: 128 96 55 55 (37171200)
I1226 14:16:43.192190 97473 net.cpp:243] Memory required for data: 376519680
I1226 14:16:43.192222 97473 layer_factory.hpp:114] Creating layer norm1
I1226 14:16:43.192299 97473 net.cpp:178] Creating Layer norm1
I1226 14:16:43.192337 97473 net.cpp:612] norm1 <- conv1
I1226 14:16:43.192378 97473 net.cpp:586] norm1 -> norm1
I1226 14:16:43.192473 97473 net.cpp:228] Setting up norm1
I1226 14:16:43.192519 97473 net.cpp:235] Top shape: 128 96 55 55 (37171200)
I1226 14:16:43.192543 97473 net.cpp:243] Memory required for data: 525204480
I1226 14:16:43.192574 97473 layer_factory.hpp:114] Creating layer pool1
I1226 14:16:43.192638 97473 net.cpp:178] Creating Layer pool1
I1226 14:16:43.192673 97473 net.cpp:612] pool1 <- norm1
I1226 14:16:43.210199 95408 net.cpp:228] Setting up conv1
I1226 14:16:43.210320 95408 net.cpp:235] Top shape: 128 96 55 55 (37171200)
I1226 14:16:43.210350 95408 net.cpp:243] Memory required for data: 227834880
I1226 14:16:43.210472 95408 layer_factory.hpp:114] Creating layer relu1
I1226 14:16:43.210628 95408 net.cpp:178] Creating Layer relu1
I1226 14:16:43.210676 95408 net.cpp:612] relu1 <- conv1
I1226 14:16:43.210729 95408 net.cpp:573] relu1 -> conv1 (in-place)
I1226 14:16:43.210880 95408 net.cpp:228] Setting up relu1
I1226 14:16:43.210940 95408 net.cpp:235] Top shape: 128 96 55 55 (37171200)
I1226 14:16:43.210968 95408 net.cpp:243] Memory required for data: 376519680
I1226 14:16:43.211001 95408 layer_factory.hpp:114] Creating layer norm1
I1226 14:16:43.211071 95408 net.cpp:178] Creating Layer norm1
I1226 14:16:43.211112 95408 net.cpp:612] norm1 <- conv1
I1226 14:16:43.211161 95408 net.cpp:586] norm1 -> norm1
I1226 14:16:43.211262 95408 net.cpp:228] Setting up norm1
I1226 14:16:43.211310 95408 net.cpp:235] Top shape: 128 96 55 55 (37171200)
I1226 14:16:43.211335 95408 net.cpp:243] Memory required for data: 525204480
I1226 14:16:43.211364 95408 layer_factory.hpp:114] Creating layer pool1
I1226 14:16:43.211431 95408 net.cpp:178] Creating Layer pool1
I1226 14:16:43.211467 95408 net.cpp:612] pool1 <- norm1
I1226 14:16:43.211505 95408 net.cpp:586] pool1 -> pool1
I1226 14:16:43.211603 95408 net.cpp:228] Setting up pool1
I1226 14:16:43.211649 95408 net.cpp:235] Top shape: 128 96 27 27 (8957952)
I1226 14:16:43.211680 95408 net.cpp:243] Memory required for data: 561036288
I1226 14:16:43.211709 95408 layer_factory.hpp:114] Creating layer conv2
I1226 14:16:43.192713 97473 net.cpp:586] pool1 -> pool1
I1226 14:16:43.205983 97473 net.cpp:228] Setting up pool1
I1226 14:16:43.206060 97473 net.cpp:235] Top shape: 128 96 27 27 (8957952)
I1226 14:16:43.206087 97473 net.cpp:243] Memory required for data: 561036288
I1226 14:16:43.206120 97473 layer_factory.hpp:114] Creating layer conv2
I1226 14:16:43.206223 97473 net.cpp:178] Creating Layer conv2
I1226 14:16:43.206261 97473 net.cpp:612] conv2 <- pool1
I1226 14:16:43.206307 97473 net.cpp:586] conv2 -> conv2
I1226 14:16:43.211808 95408 net.cpp:178] Creating Layer conv2
I1226 14:16:43.224959 95408 net.cpp:612] conv2 <- pool1
I1226 14:16:43.225049 95408 net.cpp:586] conv2 -> conv2
I1226 14:16:43.253497 98928 net.cpp:228] Setting up conv3
I1226 14:16:43.255121 98928 net.cpp:235] Top shape: 128 384 13 13 (8306688)
I1226 14:16:43.255200 98928 net.cpp:243] Memory required for data: 903068672
I1226 14:16:43.255333 98928 layer_factory.hpp:114] Creating layer relu3
I1226 14:16:43.255451 98928 net.cpp:178] Creating Layer relu3
I1226 14:16:43.255570 98928 net.cpp:612] relu3 <- conv3
I1226 14:16:43.255625 98928 net.cpp:573] relu3 -> conv3 (in-place)
I1226 14:16:43.255738 98928 net.cpp:228] Setting up relu3
I1226 14:16:43.255786 98928 net.cpp:235] Top shape: 128 384 13 13 (8306688)
I1226 14:16:43.255810 98928 net.cpp:243] Memory required for data: 936295424
I1226 14:16:43.255844 98928 layer_factory.hpp:114] Creating layer conv4
I1226 14:16:43.255939 98928 net.cpp:178] Creating Layer conv4
I1226 14:16:43.255978 98928 net.cpp:612] conv4 <- conv3
I1226 14:16:43.256041 98928 net.cpp:586] conv4 -> conv4
I1226 14:16:43.412889 93418 net.cpp:228] Setting up conv3
I1226 14:16:43.413017 93418 net.cpp:235] Top shape: 128 384 13 13 (8306688)
I1226 14:16:43.413054 93418 net.cpp:243] Memory required for data: 903068672
I1226 14:16:43.413142 93418 layer_factory.hpp:114] Creating layer relu3
I1226 14:16:43.413218 93418 net.cpp:178] Creating Layer relu3
I1226 14:16:43.413254 93418 net.cpp:612] relu3 <- conv3
I1226 14:16:43.413326 93418 net.cpp:573] relu3 -> conv3 (in-place)
I1226 14:16:43.413483 93418 net.cpp:228] Setting up relu3
I1226 14:16:43.413552 93418 net.cpp:235] Top shape: 128 384 13 13 (8306688)
I1226 14:16:43.413583 93418 net.cpp:243] Memory required for data: 936295424
I1226 14:16:43.413625 93418 layer_factory.hpp:114] Creating layer conv4
I1226 14:16:43.413713 93418 net.cpp:178] Creating Layer conv4
I1226 14:16:43.413785 93418 net.cpp:612] conv4 <- conv3
I1226 14:16:43.413866 93418 net.cpp:586] conv4 -> conv4
I1226 14:16:43.461974 91497 net.cpp:228] Setting up conv4
I1226 14:16:43.462111 91497 net.cpp:235] Top shape: 128 384 13 13 (8306688)
I1226 14:16:43.462144 91497 net.cpp:243] Memory required for data: 969522176
I1226 14:16:43.462206 91497 layer_factory.hpp:114] Creating layer relu4
I1226 14:16:43.462278 91497 net.cpp:178] Creating Layer relu4
I1226 14:16:43.462452 91497 net.cpp:612] relu4 <- conv4
I1226 14:16:43.462504 91497 net.cpp:573] relu4 -> conv4 (in-place)
I1226 14:16:43.462612 91497 net.cpp:228] Setting up relu4
I1226 14:16:43.463207 91497 net.cpp:235] Top shape: 128 384 13 13 (8306688)
I1226 14:16:43.463239 91497 net.cpp:243] Memory required for data: 1002748928
I1226 14:16:43.463289 91497 layer_factory.hpp:114] Creating layer conv5
I1226 14:16:43.463464 91497 net.cpp:178] Creating Layer conv5
I1226 14:16:43.463518 91497 net.cpp:612] conv5 <- conv4
I1226 14:16:43.463587 91497 net.cpp:586] conv5 -> conv5
I1226 14:16:43.505234 92503 net.cpp:228] Setting up conv4
I1226 14:16:43.505358 92503 net.cpp:235] Top shape: 128 384 13 13 (8306688)
I1226 14:16:43.505386 92503 net.cpp:243] Memory required for data: 969522176
I1226 14:16:43.505446 92503 layer_factory.hpp:114] Creating layer relu4
I1226 14:16:43.505511 92503 net.cpp:178] Creating Layer relu4
I1226 14:16:43.505691 92503 net.cpp:612] relu4 <- conv4
I1226 14:16:43.505846 92503 net.cpp:573] relu4 -> conv4 (in-place)
I1226 14:16:43.506032 92503 net.cpp:228] Setting up relu4
I1226 14:16:43.506131 92503 net.cpp:235] Top shape: 128 384 13 13 (8306688)
I1226 14:16:43.506235 92503 net.cpp:243] Memory required for data: 1002748928
I1226 14:16:43.506265 92503 layer_factory.hpp:114] Creating layer conv5
I1226 14:16:43.519438 92503 net.cpp:178] Creating Layer conv5
I1226 14:16:43.519531 92503 net.cpp:612] conv5 <- conv4
I1226 14:16:43.519604 92503 net.cpp:586] conv5 -> conv5
I1226 14:16:43.589583 91408 net.cpp:228] Setting up conv4
I1226 14:16:43.589711 91408 net.cpp:235] Top shape: 128 384 13 13 (8306688)
I1226 14:16:43.589738 91408 net.cpp:243] Memory required for data: 969522176
I1226 14:16:43.589797 91408 layer_factory.hpp:114] Creating layer relu4
I1226 14:16:43.589898 91408 net.cpp:178] Creating Layer relu4
I1226 14:16:43.590003 91408 net.cpp:612] relu4 <- conv4
I1226 14:16:43.590042 91408 net.cpp:573] relu4 -> conv4 (in-place)
I1226 14:16:43.590164 91408 net.cpp:228] Setting up relu4
I1226 14:16:43.590204 91408 net.cpp:235] Top shape: 128 384 13 13 (8306688)
I1226 14:16:43.590340 91408 net.cpp:243] Memory required for data: 1002748928
I1226 14:16:43.590369 91408 layer_factory.hpp:114] Creating layer conv5
I1226 14:16:43.590454 91408 net.cpp:178] Creating Layer conv5
I1226 14:16:43.590486 91408 net.cpp:612] conv5 <- conv4
I1226 14:16:43.590531 91408 net.cpp:586] conv5 -> conv5
I1226 14:16:43.616317 98928 net.cpp:228] Setting up conv4
I1226 14:16:43.616493 98928 net.cpp:235] Top shape: 128 384 13 13 (8306688)
I1226 14:16:43.616552 98928 net.cpp:243] Memory required for data: 969522176
I1226 14:16:43.616696 98928 layer_factory.hpp:114] Creating layer relu4
I1226 14:16:43.616791 98928 net.cpp:178] Creating Layer relu4
I1226 14:16:43.616928 98928 net.cpp:612] relu4 <- conv4
I1226 14:16:43.617020 98928 net.cpp:573] relu4 -> conv4 (in-place)
I1226 14:16:43.617172 98928 net.cpp:228] Setting up relu4
I1226 14:16:43.617295 98928 net.cpp:235] Top shape: 128 384 13 13 (8306688)
I1226 14:16:43.617331 98928 net.cpp:243] Memory required for data: 1002748928
I1226 14:16:43.617362 98928 layer_factory.hpp:114] Creating layer conv5
I1226 14:16:43.617483 98928 net.cpp:178] Creating Layer conv5
I1226 14:16:43.617521 98928 net.cpp:612] conv5 <- conv4
I1226 14:16:43.617563 98928 net.cpp:586] conv5 -> conv5
I1226 14:16:43.726641 91497 net.cpp:228] Setting up conv5
I1226 14:16:43.726801 91497 net.cpp:235] Top shape: 128 256 13 13 (5537792)
I1226 14:16:43.726827 91497 net.cpp:243] Memory required for data: 1024900096
I1226 14:16:43.726928 91497 layer_factory.hpp:114] Creating layer relu5
I1226 14:16:43.727051 91497 net.cpp:178] Creating Layer relu5
I1226 14:16:43.727082 91497 net.cpp:612] relu5 <- conv5
I1226 14:16:43.727149 91497 net.cpp:573] relu5 -> conv5 (in-place)
I1226 14:16:43.727246 91497 net.cpp:228] Setting up relu5
I1226 14:16:43.727285 91497 net.cpp:235] Top shape: 128 256 13 13 (5537792)
I1226 14:16:43.727308 91497 net.cpp:243] Memory required for data: 1047051264
I1226 14:16:43.727344 91497 layer_factory.hpp:114] Creating layer pool5
I1226 14:16:43.727393 91497 net.cpp:178] Creating Layer pool5
I1226 14:16:43.727417 91497 net.cpp:612] pool5 <- conv5
I1226 14:16:43.727452 91497 net.cpp:586] pool5 -> pool5
I1226 14:16:43.727568 91497 net.cpp:228] Setting up pool5
I1226 14:16:43.727648 91497 net.cpp:235] Top shape: 128 256 6 6 (1179648)
I1226 14:16:43.727671 91497 net.cpp:243] Memory required for data: 1051769856
I1226 14:16:43.727697 91497 layer_factory.hpp:114] Creating layer fc6
I1226 14:16:43.727807 91497 net.cpp:178] Creating Layer fc6
I1226 14:16:43.727843 91497 net.cpp:612] fc6 <- pool5
I1226 14:16:43.727890 91497 net.cpp:586] fc6 -> fc6
I1226 14:16:43.765926 92503 net.cpp:228] Setting up conv5
I1226 14:16:43.766049 92503 net.cpp:235] Top shape: 128 256 13 13 (5537792)
I1226 14:16:43.766075 92503 net.cpp:243] Memory required for data: 1024900096
I1226 14:16:43.766146 92503 layer_factory.hpp:114] Creating layer relu5
I1226 14:16:43.766216 92503 net.cpp:178] Creating Layer relu5
I1226 14:16:43.766247 92503 net.cpp:612] relu5 <- conv5
I1226 14:16:43.766330 92503 net.cpp:573] relu5 -> conv5 (in-place)
I1226 14:16:43.766424 92503 net.cpp:228] Setting up relu5
I1226 14:16:43.766481 92503 net.cpp:235] Top shape: 128 256 13 13 (5537792)
I1226 14:16:43.766505 92503 net.cpp:243] Memory required for data: 1047051264
I1226 14:16:43.766532 92503 layer_factory.hpp:114] Creating layer pool5
I1226 14:16:43.766589 92503 net.cpp:178] Creating Layer pool5
I1226 14:16:43.766619 92503 net.cpp:612] pool5 <- conv5
I1226 14:16:43.766655 92503 net.cpp:586] pool5 -> pool5
I1226 14:16:43.766741 92503 net.cpp:228] Setting up pool5
I1226 14:16:43.766783 92503 net.cpp:235] Top shape: 128 256 6 6 (1179648)
I1226 14:16:43.766805 92503 net.cpp:243] Memory required for data: 1051769856
I1226 14:16:43.766831 92503 layer_factory.hpp:114] Creating layer fc6
I1226 14:16:43.766894 92503 net.cpp:178] Creating Layer fc6
I1226 14:16:43.766927 92503 net.cpp:612] fc6 <- pool5
I1226 14:16:43.766964 92503 net.cpp:586] fc6 -> fc6
I1226 14:16:43.821960 93418 net.cpp:228] Setting up conv4
I1226 14:16:43.822073 93418 net.cpp:235] Top shape: 128 384 13 13 (8306688)
I1226 14:16:43.822099 93418 net.cpp:243] Memory required for data: 969522176
I1226 14:16:43.822157 93418 layer_factory.hpp:114] Creating layer relu4
I1226 14:16:43.822217 93418 net.cpp:178] Creating Layer relu4
I1226 14:16:43.822252 93418 net.cpp:612] relu4 <- conv4
I1226 14:16:43.822314 93418 net.cpp:573] relu4 -> conv4 (in-place)
I1226 14:16:43.835563 93418 net.cpp:228] Setting up relu4
I1226 14:16:43.836196 93418 net.cpp:235] Top shape: 128 384 13 13 (8306688)
I1226 14:16:43.836256 93418 net.cpp:243] Memory required for data: 1002748928
I1226 14:16:43.836293 93418 layer_factory.hpp:114] Creating layer conv5
I1226 14:16:43.836418 93418 net.cpp:178] Creating Layer conv5
I1226 14:16:43.836464 93418 net.cpp:612] conv5 <- conv4
I1226 14:16:43.836518 93418 net.cpp:586] conv5 -> conv5
I1226 14:16:43.873164 91408 net.cpp:228] Setting up conv5
I1226 14:16:43.873270 91408 net.cpp:235] Top shape: 128 256 13 13 (5537792)
I1226 14:16:43.873317 91408 net.cpp:243] Memory required for data: 1024900096
I1226 14:16:43.873411 91408 layer_factory.hpp:114] Creating layer relu5
I1226 14:16:43.873783 91408 net.cpp:178] Creating Layer relu5
I1226 14:16:43.873842 91408 net.cpp:612] relu5 <- conv5
I1226 14:16:43.873883 91408 net.cpp:573] relu5 -> conv5 (in-place)
I1226 14:16:43.874013 91408 net.cpp:228] Setting up relu5
I1226 14:16:43.874068 91408 net.cpp:235] Top shape: 128 256 13 13 (5537792)
I1226 14:16:43.874092 91408 net.cpp:243] Memory required for data: 1047051264
I1226 14:16:43.874122 91408 layer_factory.hpp:114] Creating layer pool5
I1226 14:16:43.874209 91408 net.cpp:178] Creating Layer pool5
I1226 14:16:43.874239 91408 net.cpp:612] pool5 <- conv5
I1226 14:16:43.874276 91408 net.cpp:586] pool5 -> pool5
I1226 14:16:43.874506 91408 net.cpp:228] Setting up pool5
I1226 14:16:43.874611 91408 net.cpp:235] Top shape: 128 256 6 6 (1179648)
I1226 14:16:43.874636 91408 net.cpp:243] Memory required for data: 1051769856
I1226 14:16:43.874673 91408 layer_factory.hpp:114] Creating layer fc6
I1226 14:16:43.874760 91408 net.cpp:178] Creating Layer fc6
I1226 14:16:43.874861 91408 net.cpp:612] fc6 <- pool5
I1226 14:16:43.874939 91408 net.cpp:586] fc6 -> fc6
I1226 14:16:43.876142 98928 net.cpp:228] Setting up conv5
I1226 14:16:43.876260 98928 net.cpp:235] Top shape: 128 256 13 13 (5537792)
I1226 14:16:43.876287 98928 net.cpp:243] Memory required for data: 1024900096
I1226 14:16:43.876391 98928 layer_factory.hpp:114] Creating layer relu5
I1226 14:16:43.876533 98928 net.cpp:178] Creating Layer relu5
I1226 14:16:43.876631 98928 net.cpp:612] relu5 <- conv5
I1226 14:16:43.876675 98928 net.cpp:573] relu5 -> conv5 (in-place)
I1226 14:16:43.876804 98928 net.cpp:228] Setting up relu5
I1226 14:16:43.876859 98928 net.cpp:235] Top shape: 128 256 13 13 (5537792)
I1226 14:16:43.876900 98928 net.cpp:243] Memory required for data: 1047051264
I1226 14:16:43.876936 98928 layer_factory.hpp:114] Creating layer pool5
I1226 14:16:43.877002 98928 net.cpp:178] Creating Layer pool5
I1226 14:16:43.877035 98928 net.cpp:612] pool5 <- conv5
I1226 14:16:43.877104 98928 net.cpp:586] pool5 -> pool5
I1226 14:16:43.877249 98928 net.cpp:228] Setting up pool5
I1226 14:16:43.877317 98928 net.cpp:235] Top shape: 128 256 6 6 (1179648)
I1226 14:16:43.877344 98928 net.cpp:243] Memory required for data: 1051769856
I1226 14:16:43.877379 98928 layer_factory.hpp:114] Creating layer fc6
I1226 14:16:43.877467 98928 net.cpp:178] Creating Layer fc6
I1226 14:16:43.877517 98928 net.cpp:612] fc6 <- pool5
I1226 14:16:43.877568 98928 net.cpp:586] fc6 -> fc6
I1226 14:16:44.120260 93418 net.cpp:228] Setting up conv5
I1226 14:16:44.120398 93418 net.cpp:235] Top shape: 128 256 13 13 (5537792)
I1226 14:16:44.120427 93418 net.cpp:243] Memory required for data: 1024900096
I1226 14:16:44.120506 93418 layer_factory.hpp:114] Creating layer relu5
I1226 14:16:44.120568 93418 net.cpp:178] Creating Layer relu5
I1226 14:16:44.120604 93418 net.cpp:612] relu5 <- conv5
I1226 14:16:44.120666 93418 net.cpp:573] relu5 -> conv5 (in-place)
I1226 14:16:44.120779 93418 net.cpp:228] Setting up relu5
I1226 14:16:44.120827 93418 net.cpp:235] Top shape: 128 256 13 13 (5537792)
I1226 14:16:44.120853 93418 net.cpp:243] Memory required for data: 1047051264
I1226 14:16:44.120887 93418 layer_factory.hpp:114] Creating layer pool5
I1226 14:16:44.120942 93418 net.cpp:178] Creating Layer pool5
I1226 14:16:44.120972 93418 net.cpp:612] pool5 <- conv5
I1226 14:16:44.121013 93418 net.cpp:586] pool5 -> pool5
I1226 14:16:44.121112 93418 net.cpp:228] Setting up pool5
I1226 14:16:44.121170 93418 net.cpp:235] Top shape: 128 256 6 6 (1179648)
I1226 14:16:44.121194 93418 net.cpp:243] Memory required for data: 1051769856
I1226 14:16:44.121225 93418 layer_factory.hpp:114] Creating layer fc6
I1226 14:16:44.121284 93418 net.cpp:178] Creating Layer fc6
I1226 14:16:44.121312 93418 net.cpp:612] fc6 <- pool5
I1226 14:16:44.121366 93418 net.cpp:586] fc6 -> fc6
I1226 14:16:44.607697 97044 net.cpp:228] Setting up conv2
I1226 14:16:44.608134 97044 net.cpp:235] Top shape: 128 256 27 27 (23887872)
I1226 14:16:44.608258 97044 net.cpp:243] Memory required for data: 656587776
I1226 14:16:44.608342 97044 layer_factory.hpp:114] Creating layer relu2
I1226 14:16:44.608433 97044 net.cpp:178] Creating Layer relu2
I1226 14:16:44.608572 97044 net.cpp:612] relu2 <- conv2
I1226 14:16:44.608661 97044 net.cpp:573] relu2 -> conv2 (in-place)
I1226 14:16:44.608822 97044 net.cpp:228] Setting up relu2
I1226 14:16:44.609504 97044 net.cpp:235] Top shape: 128 256 27 27 (23887872)
I1226 14:16:44.609644 97044 net.cpp:243] Memory required for data: 752139264
I1226 14:16:44.609706 97044 layer_factory.hpp:114] Creating layer norm2
I1226 14:16:44.609890 97044 net.cpp:178] Creating Layer norm2
I1226 14:16:44.610028 97044 net.cpp:612] norm2 <- conv2
I1226 14:16:44.610071 97044 net.cpp:586] norm2 -> norm2
I1226 14:16:44.610579 97044 net.cpp:228] Setting up norm2
I1226 14:16:44.610662 97044 net.cpp:235] Top shape: 128 256 27 27 (23887872)
I1226 14:16:44.610689 97044 net.cpp:243] Memory required for data: 847690752
I1226 14:16:44.610749 97044 layer_factory.hpp:114] Creating layer pool2
I1226 14:16:44.610875 97044 net.cpp:178] Creating Layer pool2
I1226 14:16:44.610935 97044 net.cpp:612] pool2 <- norm2
I1226 14:16:44.611016 97044 net.cpp:586] pool2 -> pool2
I1226 14:16:44.611129 97044 net.cpp:228] Setting up pool2
I1226 14:16:44.611661 97044 net.cpp:235] Top shape: 128 256 13 13 (5537792)
I1226 14:16:44.611708 97044 net.cpp:243] Memory required for data: 869841920
I1226 14:16:44.611768 97044 layer_factory.hpp:114] Creating layer conv3
I1226 14:16:44.611937 97044 net.cpp:178] Creating Layer conv3
I1226 14:16:44.611999 97044 net.cpp:612] conv3 <- pool2
I1226 14:16:44.612071 97044 net.cpp:586] conv3 -> conv3
I1226 14:16:44.801951 97473 net.cpp:228] Setting up conv2
I1226 14:16:44.802064 97473 net.cpp:235] Top shape: 128 256 27 27 (23887872)
I1226 14:16:44.802094 97473 net.cpp:243] Memory required for data: 656587776
I1226 14:16:44.802170 97473 layer_factory.hpp:114] Creating layer relu2
I1226 14:16:44.802366 97473 net.cpp:178] Creating Layer relu2
I1226 14:16:44.802413 97473 net.cpp:612] relu2 <- conv2
I1226 14:16:44.802477 97473 net.cpp:573] relu2 -> conv2 (in-place)
I1226 14:16:44.802589 97473 net.cpp:228] Setting up relu2
I1226 14:16:44.802652 97473 net.cpp:235] Top shape: 128 256 27 27 (23887872)
I1226 14:16:44.802680 97473 net.cpp:243] Memory required for data: 752139264
I1226 14:16:44.802709 97473 layer_factory.hpp:114] Creating layer norm2
I1226 14:16:44.802789 97473 net.cpp:178] Creating Layer norm2
I1226 14:16:44.802829 97473 net.cpp:612] norm2 <- conv2
I1226 14:16:44.802870 97473 net.cpp:586] norm2 -> norm2
I1226 14:16:44.802974 97473 net.cpp:228] Setting up norm2
I1226 14:16:44.803037 97473 net.cpp:235] Top shape: 128 256 27 27 (23887872)
I1226 14:16:44.803063 97473 net.cpp:243] Memory required for data: 847690752
I1226 14:16:44.803093 97473 layer_factory.hpp:114] Creating layer pool2
I1226 14:16:44.803148 97473 net.cpp:178] Creating Layer pool2
I1226 14:16:44.803182 97473 net.cpp:612] pool2 <- norm2
I1226 14:16:44.803222 97473 net.cpp:586] pool2 -> pool2
I1226 14:16:44.803318 97473 net.cpp:228] Setting up pool2
I1226 14:16:44.803462 97473 net.cpp:235] Top shape: 128 256 13 13 (5537792)
I1226 14:16:44.803489 97473 net.cpp:243] Memory required for data: 869841920
I1226 14:16:44.803521 97473 layer_factory.hpp:114] Creating layer conv3
I1226 14:16:44.803611 97473 net.cpp:178] Creating Layer conv3
I1226 14:16:44.803652 97473 net.cpp:612] conv3 <- pool2
I1226 14:16:44.803695 97473 net.cpp:586] conv3 -> conv3
I1226 14:16:44.863411 95408 net.cpp:228] Setting up conv2
I1226 14:16:44.863524 95408 net.cpp:235] Top shape: 128 256 27 27 (23887872)
I1226 14:16:44.863556 95408 net.cpp:243] Memory required for data: 656587776
I1226 14:16:44.863631 95408 layer_factory.hpp:114] Creating layer relu2
I1226 14:16:44.863699 95408 net.cpp:178] Creating Layer relu2
I1226 14:16:44.863737 95408 net.cpp:612] relu2 <- conv2
I1226 14:16:44.863803 95408 net.cpp:573] relu2 -> conv2 (in-place)
I1226 14:16:44.866240 95408 net.cpp:228] Setting up relu2
I1226 14:16:44.866322 95408 net.cpp:235] Top shape: 128 256 27 27 (23887872)
I1226 14:16:44.866350 95408 net.cpp:243] Memory required for data: 752139264
I1226 14:16:44.866386 95408 layer_factory.hpp:114] Creating layer norm2
I1226 14:16:44.866466 95408 net.cpp:178] Creating Layer norm2
I1226 14:16:44.866503 95408 net.cpp:612] norm2 <- conv2
I1226 14:16:44.866552 95408 net.cpp:586] norm2 -> norm2
I1226 14:16:44.866647 95408 net.cpp:228] Setting up norm2
I1226 14:16:44.866696 95408 net.cpp:235] Top shape: 128 256 27 27 (23887872)
I1226 14:16:44.866720 95408 net.cpp:243] Memory required for data: 847690752
I1226 14:16:44.866755 95408 layer_factory.hpp:114] Creating layer pool2
I1226 14:16:44.866816 95408 net.cpp:178] Creating Layer pool2
I1226 14:16:44.866868 95408 net.cpp:612] pool2 <- norm2
I1226 14:16:44.866914 95408 net.cpp:586] pool2 -> pool2
I1226 14:16:44.867004 95408 net.cpp:228] Setting up pool2
I1226 14:16:44.867152 95408 net.cpp:235] Top shape: 128 256 13 13 (5537792)
I1226 14:16:44.867184 95408 net.cpp:243] Memory required for data: 869841920
I1226 14:16:44.867216 95408 layer_factory.hpp:114] Creating layer conv3
I1226 14:16:44.867308 95408 net.cpp:178] Creating Layer conv3
I1226 14:16:44.867344 95408 net.cpp:612] conv3 <- pool2
I1226 14:16:44.867395 95408 net.cpp:586] conv3 -> conv3
I1226 14:16:46.057963 95408 net.cpp:228] Setting up conv3
I1226 14:16:46.058101 95408 net.cpp:235] Top shape: 128 384 13 13 (8306688)
I1226 14:16:46.058133 95408 net.cpp:243] Memory required for data: 903068672
I1226 14:16:46.058210 95408 layer_factory.hpp:114] Creating layer relu3
I1226 14:16:46.058357 95408 net.cpp:178] Creating Layer relu3
I1226 14:16:46.058390 95408 net.cpp:612] relu3 <- conv3
I1226 14:16:46.058437 95408 net.cpp:573] relu3 -> conv3 (in-place)
I1226 14:16:46.058544 95408 net.cpp:228] Setting up relu3
I1226 14:16:46.058591 95408 net.cpp:235] Top shape: 128 384 13 13 (8306688)
I1226 14:16:46.058632 95408 net.cpp:243] Memory required for data: 936295424
I1226 14:16:46.058671 95408 layer_factory.hpp:114] Creating layer conv4
I1226 14:16:46.058759 95408 net.cpp:178] Creating Layer conv4
I1226 14:16:46.058799 95408 net.cpp:612] conv4 <- conv3
I1226 14:16:46.058876 95408 net.cpp:586] conv4 -> conv4
I1226 14:16:46.282860 97473 net.cpp:228] Setting up conv3
I1226 14:16:46.282991 97473 net.cpp:235] Top shape: 128 384 13 13 (8306688)
I1226 14:16:46.283025 97473 net.cpp:243] Memory required for data: 903068672
I1226 14:16:46.283107 97473 layer_factory.hpp:114] Creating layer relu3
I1226 14:16:46.283260 97473 net.cpp:178] Creating Layer relu3
I1226 14:16:46.283313 97473 net.cpp:612] relu3 <- conv3
I1226 14:16:46.283356 97473 net.cpp:573] relu3 -> conv3 (in-place)
I1226 14:16:46.283473 97473 net.cpp:228] Setting up relu3
I1226 14:16:46.283525 97473 net.cpp:235] Top shape: 128 384 13 13 (8306688)
I1226 14:16:46.283551 97473 net.cpp:243] Memory required for data: 936295424
I1226 14:16:46.283582 97473 layer_factory.hpp:114] Creating layer conv4
I1226 14:16:46.283691 97473 net.cpp:178] Creating Layer conv4
I1226 14:16:46.283757 97473 net.cpp:612] conv4 <- conv3
I1226 14:16:46.283831 97473 net.cpp:586] conv4 -> conv4
I1226 14:16:46.531476 97044 net.cpp:228] Setting up conv3
I1226 14:16:46.531592 97044 net.cpp:235] Top shape: 128 384 13 13 (8306688)
I1226 14:16:46.531625 97044 net.cpp:243] Memory required for data: 903068672
I1226 14:16:46.531703 97044 layer_factory.hpp:114] Creating layer relu3
I1226 14:16:46.531929 97044 net.cpp:178] Creating Layer relu3
I1226 14:16:46.531968 97044 net.cpp:612] relu3 <- conv3
I1226 14:16:46.532101 97044 net.cpp:573] relu3 -> conv3 (in-place)
I1226 14:16:46.532208 97044 net.cpp:228] Setting up relu3
I1226 14:16:46.532299 97044 net.cpp:235] Top shape: 128 384 13 13 (8306688)
I1226 14:16:46.532326 97044 net.cpp:243] Memory required for data: 936295424
I1226 14:16:46.532358 97044 layer_factory.hpp:114] Creating layer conv4
I1226 14:16:46.532450 97044 net.cpp:178] Creating Layer conv4
I1226 14:16:46.532495 97044 net.cpp:612] conv4 <- conv3
I1226 14:16:46.532551 97044 net.cpp:586] conv4 -> conv4
I1226 14:16:46.944581 95408 net.cpp:228] Setting up conv4
I1226 14:16:46.944701 95408 net.cpp:235] Top shape: 128 384 13 13 (8306688)
I1226 14:16:46.944732 95408 net.cpp:243] Memory required for data: 969522176
I1226 14:16:46.944797 95408 layer_factory.hpp:114] Creating layer relu4
I1226 14:16:46.944888 95408 net.cpp:178] Creating Layer relu4
I1226 14:16:46.944933 95408 net.cpp:612] relu4 <- conv4
I1226 14:16:46.944988 95408 net.cpp:573] relu4 -> conv4 (in-place)
I1226 14:16:46.945070 95408 net.cpp:228] Setting up relu4
I1226 14:16:46.945122 95408 net.cpp:235] Top shape: 128 384 13 13 (8306688)
I1226 14:16:46.945147 95408 net.cpp:243] Memory required for data: 1002748928
I1226 14:16:46.945178 95408 layer_factory.hpp:114] Creating layer conv5
I1226 14:16:46.945250 95408 net.cpp:178] Creating Layer conv5
I1226 14:16:46.945291 95408 net.cpp:612] conv5 <- conv4
I1226 14:16:46.945336 95408 net.cpp:586] conv5 -> conv5
I1226 14:16:47.169402 97473 net.cpp:228] Setting up conv4
I1226 14:16:47.169540 97473 net.cpp:235] Top shape: 128 384 13 13 (8306688)
I1226 14:16:47.169576 97473 net.cpp:243] Memory required for data: 969522176
I1226 14:16:47.169642 97473 layer_factory.hpp:114] Creating layer relu4
I1226 14:16:47.169715 97473 net.cpp:178] Creating Layer relu4
I1226 14:16:47.169777 97473 net.cpp:612] relu4 <- conv4
I1226 14:16:47.169826 97473 net.cpp:573] relu4 -> conv4 (in-place)
I1226 14:16:47.169940 97473 net.cpp:228] Setting up relu4
I1226 14:16:47.169996 97473 net.cpp:235] Top shape: 128 384 13 13 (8306688)
I1226 14:16:47.170022 97473 net.cpp:243] Memory required for data: 1002748928
I1226 14:16:47.170052 97473 layer_factory.hpp:114] Creating layer conv5
I1226 14:16:47.170148 97473 net.cpp:178] Creating Layer conv5
I1226 14:16:47.170187 97473 net.cpp:612] conv5 <- conv4
I1226 14:16:47.170236 97473 net.cpp:586] conv5 -> conv5
I1226 14:16:47.578361 95408 net.cpp:228] Setting up conv5
I1226 14:16:47.578481 95408 net.cpp:235] Top shape: 128 256 13 13 (5537792)
I1226 14:16:47.578512 95408 net.cpp:243] Memory required for data: 1024900096
I1226 14:16:47.578618 95408 layer_factory.hpp:114] Creating layer relu5
I1226 14:16:47.578739 95408 net.cpp:178] Creating Layer relu5
I1226 14:16:47.578776 95408 net.cpp:612] relu5 <- conv5
I1226 14:16:47.578857 95408 net.cpp:573] relu5 -> conv5 (in-place)
I1226 14:16:47.578977 95408 net.cpp:228] Setting up relu5
I1226 14:16:47.579043 95408 net.cpp:235] Top shape: 128 256 13 13 (5537792)
I1226 14:16:47.579079 95408 net.cpp:243] Memory required for data: 1047051264
I1226 14:16:47.579113 95408 layer_factory.hpp:114] Creating layer pool5
I1226 14:16:47.579181 95408 net.cpp:178] Creating Layer pool5
I1226 14:16:47.579221 95408 net.cpp:612] pool5 <- conv5
I1226 14:16:47.579264 95408 net.cpp:586] pool5 -> pool5
I1226 14:16:47.579376 95408 net.cpp:228] Setting up pool5
I1226 14:16:47.579427 95408 net.cpp:235] Top shape: 128 256 6 6 (1179648)
I1226 14:16:47.579452 95408 net.cpp:243] Memory required for data: 1051769856
I1226 14:16:47.579481 95408 layer_factory.hpp:114] Creating layer fc6
I1226 14:16:47.579541 95408 net.cpp:178] Creating Layer fc6
I1226 14:16:47.579578 95408 net.cpp:612] fc6 <- pool5
I1226 14:16:47.579632 95408 net.cpp:586] fc6 -> fc6
I1226 14:16:47.805703 97473 net.cpp:228] Setting up conv5
I1226 14:16:47.805846 97473 net.cpp:235] Top shape: 128 256 13 13 (5537792)
I1226 14:16:47.805882 97473 net.cpp:243] Memory required for data: 1024900096
I1226 14:16:47.805989 97473 layer_factory.hpp:114] Creating layer relu5
I1226 14:16:47.806071 97473 net.cpp:178] Creating Layer relu5
I1226 14:16:47.806109 97473 net.cpp:612] relu5 <- conv5
I1226 14:16:47.806162 97473 net.cpp:573] relu5 -> conv5 (in-place)
I1226 14:16:47.806279 97473 net.cpp:228] Setting up relu5
I1226 14:16:47.806334 97473 net.cpp:235] Top shape: 128 256 13 13 (5537792)
I1226 14:16:47.806360 97473 net.cpp:243] Memory required for data: 1047051264
I1226 14:16:47.806391 97473 layer_factory.hpp:114] Creating layer pool5
I1226 14:16:47.806468 97473 net.cpp:178] Creating Layer pool5
I1226 14:16:47.806506 97473 net.cpp:612] pool5 <- conv5
I1226 14:16:47.806548 97473 net.cpp:586] pool5 -> pool5
I1226 14:16:47.806653 97473 net.cpp:228] Setting up pool5
I1226 14:16:47.806715 97473 net.cpp:235] Top shape: 128 256 6 6 (1179648)
I1226 14:16:47.806769 97473 net.cpp:243] Memory required for data: 1051769856
I1226 14:16:47.806802 97473 layer_factory.hpp:114] Creating layer fc6
I1226 14:16:47.806864 97473 net.cpp:178] Creating Layer fc6
I1226 14:16:47.806902 97473 net.cpp:612] fc6 <- pool5
I1226 14:16:47.806960 97473 net.cpp:586] fc6 -> fc6
I1226 14:16:47.946012 97044 net.cpp:228] Setting up conv4
I1226 14:16:47.946130 97044 net.cpp:235] Top shape: 128 384 13 13 (8306688)
I1226 14:16:47.946161 97044 net.cpp:243] Memory required for data: 969522176
I1226 14:16:47.946225 97044 layer_factory.hpp:114] Creating layer relu4
I1226 14:16:47.946406 97044 net.cpp:178] Creating Layer relu4
I1226 14:16:47.946455 97044 net.cpp:612] relu4 <- conv4
I1226 14:16:47.946498 97044 net.cpp:573] relu4 -> conv4 (in-place)
I1226 14:16:47.946602 97044 net.cpp:228] Setting up relu4
I1226 14:16:47.946653 97044 net.cpp:235] Top shape: 128 384 13 13 (8306688)
I1226 14:16:47.946681 97044 net.cpp:243] Memory required for data: 1002748928
I1226 14:16:47.946710 97044 layer_factory.hpp:114] Creating layer conv5
I1226 14:16:47.946830 97044 net.cpp:178] Creating Layer conv5
I1226 14:16:47.946876 97044 net.cpp:612] conv5 <- conv4
I1226 14:16:47.946933 97044 net.cpp:586] conv5 -> conv5
I1226 14:16:48.014863 93636 net.cpp:228] Setting up fc6
I1226 14:16:48.014997 93636 net.cpp:235] Top shape: 128 4096 (524288)
I1226 14:16:48.015027 93636 net.cpp:243] Memory required for data: 1053867008
I1226 14:16:48.015087 93636 layer_factory.hpp:114] Creating layer relu6
I1226 14:16:48.015164 93636 net.cpp:178] Creating Layer relu6
I1226 14:16:48.015282 93636 net.cpp:612] relu6 <- fc6
I1226 14:16:48.015321 93636 net.cpp:573] relu6 -> fc6 (in-place)
I1226 14:16:48.015424 93636 net.cpp:228] Setting up relu6
I1226 14:16:48.015471 93636 net.cpp:235] Top shape: 128 4096 (524288)
I1226 14:16:48.015594 93636 net.cpp:243] Memory required for data: 1055964160
I1226 14:16:48.015630 93636 layer_factory.hpp:114] Creating layer drop6
I1226 14:16:48.015681 93636 net.cpp:178] Creating Layer drop6
I1226 14:16:48.015746 93636 net.cpp:612] drop6 <- fc6
I1226 14:16:48.015782 93636 net.cpp:573] drop6 -> fc6 (in-place)
I1226 14:16:48.015844 93636 net.cpp:228] Setting up drop6
I1226 14:16:48.015884 93636 net.cpp:235] Top shape: 128 4096 (524288)
I1226 14:16:48.015908 93636 net.cpp:243] Memory required for data: 1058061312
I1226 14:16:48.015935 93636 layer_factory.hpp:114] Creating layer fc7
I1226 14:16:48.015995 93636 net.cpp:178] Creating Layer fc7
I1226 14:16:48.016033 93636 net.cpp:612] fc7 <- fc6
I1226 14:16:48.016094 93636 net.cpp:586] fc7 -> fc7
I1226 14:16:48.581626 97044 net.cpp:228] Setting up conv5
I1226 14:16:48.581745 97044 net.cpp:235] Top shape: 128 256 13 13 (5537792)
I1226 14:16:48.581779 97044 net.cpp:243] Memory required for data: 1024900096
I1226 14:16:48.581912 97044 layer_factory.hpp:114] Creating layer relu5
I1226 14:16:48.582005 97044 net.cpp:178] Creating Layer relu5
I1226 14:16:48.582051 97044 net.cpp:612] relu5 <- conv5
I1226 14:16:48.582103 97044 net.cpp:573] relu5 -> conv5 (in-place)
I1226 14:16:48.582201 97044 net.cpp:228] Setting up relu5
I1226 14:16:48.582247 97044 net.cpp:235] Top shape: 128 256 13 13 (5537792)
I1226 14:16:48.582273 97044 net.cpp:243] Memory required for data: 1047051264
I1226 14:16:48.582304 97044 layer_factory.hpp:114] Creating layer pool5
I1226 14:16:48.582366 97044 net.cpp:178] Creating Layer pool5
I1226 14:16:48.582418 97044 net.cpp:612] pool5 <- conv5
I1226 14:16:48.582460 97044 net.cpp:586] pool5 -> pool5
I1226 14:16:48.582557 97044 net.cpp:228] Setting up pool5
I1226 14:16:48.582607 97044 net.cpp:235] Top shape: 128 256 6 6 (1179648)
I1226 14:16:48.582631 97044 net.cpp:243] Memory required for data: 1051769856
I1226 14:16:48.582660 97044 layer_factory.hpp:114] Creating layer fc6
I1226 14:16:48.582720 97044 net.cpp:178] Creating Layer fc6
I1226 14:16:48.582754 97044 net.cpp:612] fc6 <- pool5
I1226 14:16:48.582834 97044 net.cpp:586] fc6 -> fc6
I1226 14:16:49.295042 93418 net.cpp:228] Setting up fc6
I1226 14:16:49.295161 93418 net.cpp:235] Top shape: 128 4096 (524288)
I1226 14:16:49.295189 93418 net.cpp:243] Memory required for data: 1053867008
I1226 14:16:49.295248 93418 layer_factory.hpp:114] Creating layer relu6
I1226 14:16:49.295321 93418 net.cpp:178] Creating Layer relu6
I1226 14:16:49.295361 93418 net.cpp:612] relu6 <- fc6
I1226 14:16:49.295442 93418 net.cpp:573] relu6 -> fc6 (in-place)
I1226 14:16:49.295553 93418 net.cpp:228] Setting up relu6
I1226 14:16:49.295716 93418 net.cpp:235] Top shape: 128 4096 (524288)
I1226 14:16:49.295752 93418 net.cpp:243] Memory required for data: 1055964160
I1226 14:16:49.295789 93418 layer_factory.hpp:114] Creating layer drop6
I1226 14:16:49.295850 93418 net.cpp:178] Creating Layer drop6
I1226 14:16:49.295887 93418 net.cpp:612] drop6 <- fc6
I1226 14:16:49.295930 93418 net.cpp:573] drop6 -> fc6 (in-place)
I1226 14:16:49.295999 93418 net.cpp:228] Setting up drop6
I1226 14:16:49.296041 93418 net.cpp:235] Top shape: 128 4096 (524288)
I1226 14:16:49.296069 93418 net.cpp:243] Memory required for data: 1058061312
I1226 14:16:49.296102 93418 layer_factory.hpp:114] Creating layer fc7
I1226 14:16:49.296167 93418 net.cpp:178] Creating Layer fc7
I1226 14:16:49.296200 93418 net.cpp:612] fc7 <- fc6
I1226 14:16:49.296247 93418 net.cpp:586] fc7 -> fc7
I1226 14:16:50.287760 93636 net.cpp:228] Setting up fc7
I1226 14:16:50.287870 93636 net.cpp:235] Top shape: 128 4096 (524288)
I1226 14:16:50.287899 93636 net.cpp:243] Memory required for data: 1060158464
I1226 14:16:50.287979 93636 layer_factory.hpp:114] Creating layer relu7
I1226 14:16:50.288067 93636 net.cpp:178] Creating Layer relu7
I1226 14:16:50.288110 93636 net.cpp:612] relu7 <- fc7
I1226 14:16:50.288149 93636 net.cpp:573] relu7 -> fc7 (in-place)
I1226 14:16:50.288234 93636 net.cpp:228] Setting up relu7
I1226 14:16:50.288280 93636 net.cpp:235] Top shape: 128 4096 (524288)
I1226 14:16:50.288305 93636 net.cpp:243] Memory required for data: 1062255616
I1226 14:16:50.288332 93636 layer_factory.hpp:114] Creating layer drop7
I1226 14:16:50.288379 93636 net.cpp:178] Creating Layer drop7
I1226 14:16:50.288408 93636 net.cpp:612] drop7 <- fc7
I1226 14:16:50.288463 93636 net.cpp:573] drop7 -> fc7 (in-place)
I1226 14:16:50.288516 93636 net.cpp:228] Setting up drop7
I1226 14:16:50.288556 93636 net.cpp:235] Top shape: 128 4096 (524288)
I1226 14:16:50.288579 93636 net.cpp:243] Memory required for data: 1064352768
I1226 14:16:50.288605 93636 layer_factory.hpp:114] Creating layer fc8
I1226 14:16:50.288663 93636 net.cpp:178] Creating Layer fc8
I1226 14:16:50.288717 93636 net.cpp:612] fc8 <- fc7
I1226 14:16:50.288758 93636 net.cpp:586] fc8 -> fc8
I1226 14:16:50.447870 91408 net.cpp:228] Setting up fc6
I1226 14:16:50.447981 91408 net.cpp:235] Top shape: 128 4096 (524288)
I1226 14:16:50.448014 91408 net.cpp:243] Memory required for data: 1053867008
I1226 14:16:50.448073 91408 layer_factory.hpp:114] Creating layer relu6
I1226 14:16:50.448155 91408 net.cpp:178] Creating Layer relu6
I1226 14:16:50.448287 91408 net.cpp:612] relu6 <- fc6
I1226 14:16:50.448355 91408 net.cpp:573] relu6 -> fc6 (in-place)
I1226 14:16:50.448451 91408 net.cpp:228] Setting up relu6
I1226 14:16:50.448607 91408 net.cpp:235] Top shape: 128 4096 (524288)
I1226 14:16:50.448637 91408 net.cpp:243] Memory required for data: 1055964160
I1226 14:16:50.448669 91408 layer_factory.hpp:114] Creating layer drop6
I1226 14:16:50.448737 91408 net.cpp:178] Creating Layer drop6
I1226 14:16:50.448779 91408 net.cpp:612] drop6 <- fc6
I1226 14:16:50.448822 91408 net.cpp:573] drop6 -> fc6 (in-place)
I1226 14:16:50.448886 91408 net.cpp:228] Setting up drop6
I1226 14:16:50.448925 91408 net.cpp:235] Top shape: 128 4096 (524288)
I1226 14:16:50.449054 91408 net.cpp:243] Memory required for data: 1058061312
I1226 14:16:50.449082 91408 layer_factory.hpp:114] Creating layer fc7
I1226 14:16:50.449142 91408 net.cpp:178] Creating Layer fc7
I1226 14:16:50.449177 91408 net.cpp:612] fc7 <- fc6
I1226 14:16:50.449218 91408 net.cpp:586] fc7 -> fc7
I1226 14:16:50.478808 91497 net.cpp:228] Setting up fc6
I1226 14:16:50.478919 91497 net.cpp:235] Top shape: 128 4096 (524288)
I1226 14:16:50.478945 91497 net.cpp:243] Memory required for data: 1053867008
I1226 14:16:50.479029 91497 layer_factory.hpp:114] Creating layer relu6
I1226 14:16:50.479107 91497 net.cpp:178] Creating Layer relu6
I1226 14:16:50.479146 91497 net.cpp:612] relu6 <- fc6
I1226 14:16:50.479185 91497 net.cpp:573] relu6 -> fc6 (in-place)
I1226 14:16:50.479284 91497 net.cpp:228] Setting up relu6
I1226 14:16:50.479419 91497 net.cpp:235] Top shape: 128 4096 (524288)
I1226 14:16:50.479450 91497 net.cpp:243] Memory required for data: 1055964160
I1226 14:16:50.479482 91497 layer_factory.hpp:114] Creating layer drop6
I1226 14:16:50.479543 91497 net.cpp:178] Creating Layer drop6
I1226 14:16:50.479571 91497 net.cpp:612] drop6 <- fc6
I1226 14:16:50.479607 91497 net.cpp:573] drop6 -> fc6 (in-place)
I1226 14:16:50.479671 91497 net.cpp:228] Setting up drop6
I1226 14:16:50.479732 91497 net.cpp:235] Top shape: 128 4096 (524288)
I1226 14:16:50.479760 91497 net.cpp:243] Memory required for data: 1058061312
I1226 14:16:50.479799 91497 layer_factory.hpp:114] Creating layer fc7
I1226 14:16:50.479858 91497 net.cpp:178] Creating Layer fc7
I1226 14:16:50.479883 91497 net.cpp:612] fc7 <- fc6
I1226 14:16:50.479930 91497 net.cpp:586] fc7 -> fc7
I1226 14:16:50.505254 92503 net.cpp:228] Setting up fc6
I1226 14:16:50.505389 92503 net.cpp:235] Top shape: 128 4096 (524288)
I1226 14:16:50.505419 92503 net.cpp:243] Memory required for data: 1053867008
I1226 14:16:50.505476 92503 layer_factory.hpp:114] Creating layer relu6
I1226 14:16:50.505548 92503 net.cpp:178] Creating Layer relu6
I1226 14:16:50.505583 92503 net.cpp:612] relu6 <- fc6
I1226 14:16:50.505625 92503 net.cpp:573] relu6 -> fc6 (in-place)
I1226 14:16:50.505717 92503 net.cpp:228] Setting up relu6
I1226 14:16:50.505892 92503 net.cpp:235] Top shape: 128 4096 (524288)
I1226 14:16:50.505921 92503 net.cpp:243] Memory required for data: 1055964160
I1226 14:16:50.505951 92503 layer_factory.hpp:114] Creating layer drop6
I1226 14:16:50.506014 92503 net.cpp:178] Creating Layer drop6
I1226 14:16:50.506042 92503 net.cpp:612] drop6 <- fc6
I1226 14:16:50.506078 92503 net.cpp:573] drop6 -> fc6 (in-place)
I1226 14:16:50.506175 92503 net.cpp:228] Setting up drop6
I1226 14:16:50.506224 92503 net.cpp:235] Top shape: 128 4096 (524288)
I1226 14:16:50.506248 92503 net.cpp:243] Memory required for data: 1058061312
I1226 14:16:50.506275 92503 layer_factory.hpp:114] Creating layer fc7
I1226 14:16:50.506373 92503 net.cpp:178] Creating Layer fc7
I1226 14:16:50.506409 92503 net.cpp:612] fc7 <- fc6
I1226 14:16:50.506448 92503 net.cpp:586] fc7 -> fc7
I1226 14:16:50.593550 98928 net.cpp:228] Setting up fc6
I1226 14:16:50.593660 98928 net.cpp:235] Top shape: 128 4096 (524288)
I1226 14:16:50.593689 98928 net.cpp:243] Memory required for data: 1053867008
I1226 14:16:50.593775 98928 layer_factory.hpp:114] Creating layer relu6
I1226 14:16:50.593859 98928 net.cpp:178] Creating Layer relu6
I1226 14:16:50.593895 98928 net.cpp:612] relu6 <- fc6
I1226 14:16:50.593935 98928 net.cpp:573] relu6 -> fc6 (in-place)
I1226 14:16:50.594036 98928 net.cpp:228] Setting up relu6
I1226 14:16:50.594219 98928 net.cpp:235] Top shape: 128 4096 (524288)
I1226 14:16:50.594252 98928 net.cpp:243] Memory required for data: 1055964160
I1226 14:16:50.594287 98928 layer_factory.hpp:114] Creating layer drop6
I1226 14:16:50.594354 98928 net.cpp:178] Creating Layer drop6
I1226 14:16:50.594384 98928 net.cpp:612] drop6 <- fc6
I1226 14:16:50.594420 98928 net.cpp:573] drop6 -> fc6 (in-place)
I1226 14:16:50.594485 98928 net.cpp:228] Setting up drop6
I1226 14:16:50.594527 98928 net.cpp:235] Top shape: 128 4096 (524288)
I1226 14:16:50.594552 98928 net.cpp:243] Memory required for data: 1058061312
I1226 14:16:50.594583 98928 layer_factory.hpp:114] Creating layer fc7
I1226 14:16:50.594650 98928 net.cpp:178] Creating Layer fc7
I1226 14:16:50.594683 98928 net.cpp:612] fc7 <- fc6
I1226 14:16:50.594728 98928 net.cpp:586] fc7 -> fc7
I1226 14:16:50.835952 93636 net.cpp:228] Setting up fc8
I1226 14:16:50.836061 93636 net.cpp:235] Top shape: 128 1000 (128000)
I1226 14:16:50.836091 93636 net.cpp:243] Memory required for data: 1064864768
I1226 14:16:50.836148 93636 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 14:16:50.836241 93636 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 14:16:50.836355 93636 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 14:16:50.836393 93636 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 14:16:50.836442 93636 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 14:16:50.836530 93636 net.cpp:228] Setting up fc8_fc8_0_split
I1226 14:16:50.836576 93636 net.cpp:235] Top shape: 128 1000 (128000)
I1226 14:16:50.836607 93636 net.cpp:235] Top shape: 128 1000 (128000)
I1226 14:16:50.836630 93636 net.cpp:243] Memory required for data: 1065888768
I1226 14:16:50.836658 93636 layer_factory.hpp:114] Creating layer accuracy
I1226 14:16:50.836735 93636 net.cpp:178] Creating Layer accuracy
I1226 14:16:50.836772 93636 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 14:16:50.836802 93636 net.cpp:612] accuracy <- label_data_1_split_0
I1226 14:16:50.836834 93636 net.cpp:586] accuracy -> accuracy
I1226 14:16:50.836880 93636 net.cpp:228] Setting up accuracy
I1226 14:16:50.836920 93636 net.cpp:235] Top shape: (1)
I1226 14:16:50.836949 93636 net.cpp:243] Memory required for data: 1065888772
I1226 14:16:50.836977 93636 layer_factory.hpp:114] Creating layer loss
I1226 14:16:50.837019 93636 net.cpp:178] Creating Layer loss
I1226 14:16:50.837044 93636 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 14:16:50.837070 93636 net.cpp:612] loss <- label_data_1_split_1
I1226 14:16:50.837116 93636 net.cpp:586] loss -> loss
I1226 14:16:50.837177 93636 layer_factory.hpp:114] Creating layer loss
I1226 14:16:50.863306 93636 net.cpp:228] Setting up loss
I1226 14:16:50.863423 93636 net.cpp:235] Top shape: (1)
I1226 14:16:50.863595 93636 net.cpp:238]     with loss weight 1
I1226 14:16:50.863802 93636 net.cpp:243] Memory required for data: 1065888776
I1226 14:16:50.863857 93636 net.cpp:305] loss needs backward computation.
I1226 14:16:50.863901 93636 net.cpp:307] accuracy does not need backward computation.
I1226 14:16:50.863955 93636 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 14:16:50.863989 93636 net.cpp:305] fc8 needs backward computation.
I1226 14:16:50.864024 93636 net.cpp:305] drop7 needs backward computation.
I1226 14:16:50.864063 93636 net.cpp:305] relu7 needs backward computation.
I1226 14:16:50.864104 93636 net.cpp:305] fc7 needs backward computation.
I1226 14:16:50.864145 93636 net.cpp:305] drop6 needs backward computation.
I1226 14:16:50.864183 93636 net.cpp:305] relu6 needs backward computation.
I1226 14:16:50.864215 93636 net.cpp:305] fc6 needs backward computation.
I1226 14:16:50.864253 93636 net.cpp:305] pool5 needs backward computation.
I1226 14:16:50.864287 93636 net.cpp:305] relu5 needs backward computation.
I1226 14:16:50.864328 93636 net.cpp:305] conv5 needs backward computation.
I1226 14:16:50.864362 93636 net.cpp:305] relu4 needs backward computation.
I1226 14:16:50.864399 93636 net.cpp:305] conv4 needs backward computation.
I1226 14:16:50.864437 93636 net.cpp:305] relu3 needs backward computation.
I1226 14:16:50.864473 93636 net.cpp:305] conv3 needs backward computation.
I1226 14:16:50.864507 93636 net.cpp:305] pool2 needs backward computation.
I1226 14:16:50.864548 93636 net.cpp:305] norm2 needs backward computation.
I1226 14:16:50.864588 93636 net.cpp:305] relu2 needs backward computation.
I1226 14:16:50.864620 93636 net.cpp:305] conv2 needs backward computation.
I1226 14:16:50.864653 93636 net.cpp:305] pool1 needs backward computation.
I1226 14:16:50.864711 93636 net.cpp:305] norm1 needs backward computation.
I1226 14:16:50.864748 93636 net.cpp:305] relu1 needs backward computation.
I1226 14:16:50.864778 93636 net.cpp:305] conv1 needs backward computation.
I1226 14:16:50.864810 93636 net.cpp:307] label_data_1_split does not need backward computation.
I1226 14:16:50.864843 93636 net.cpp:307] data does not need backward computation.
I1226 14:16:50.864872 93636 net.cpp:349] This network produces output accuracy
I1226 14:16:50.864907 93636 net.cpp:349] This network produces output loss
I1226 14:16:50.865005 93636 net.cpp:363] Network initialization done.
I1226 14:16:50.865559 93636 solver.cpp:107] Solver scaffolding done.
I1226 14:16:50.865799 93636 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 14:16:51.595155 93418 net.cpp:228] Setting up fc7
I1226 14:16:51.595265 93418 net.cpp:235] Top shape: 128 4096 (524288)
I1226 14:16:51.595291 93418 net.cpp:243] Memory required for data: 1060158464
I1226 14:16:51.595348 93418 layer_factory.hpp:114] Creating layer relu7
I1226 14:16:51.595438 93418 net.cpp:178] Creating Layer relu7
I1226 14:16:51.595482 93418 net.cpp:612] relu7 <- fc7
I1226 14:16:51.595525 93418 net.cpp:573] relu7 -> fc7 (in-place)
I1226 14:16:51.595633 93418 net.cpp:228] Setting up relu7
I1226 14:16:51.595684 93418 net.cpp:235] Top shape: 128 4096 (524288)
I1226 14:16:51.595710 93418 net.cpp:243] Memory required for data: 1062255616
I1226 14:16:51.595743 93418 layer_factory.hpp:114] Creating layer drop7
I1226 14:16:51.595816 93418 net.cpp:178] Creating Layer drop7
I1226 14:16:51.595849 93418 net.cpp:612] drop7 <- fc7
I1226 14:16:51.595890 93418 net.cpp:573] drop7 -> fc7 (in-place)
I1226 14:16:51.595973 93418 net.cpp:228] Setting up drop7
I1226 14:16:51.596011 93418 net.cpp:235] Top shape: 128 4096 (524288)
I1226 14:16:51.596035 93418 net.cpp:243] Memory required for data: 1064352768
I1226 14:16:51.596065 93418 layer_factory.hpp:114] Creating layer fc8
I1226 14:16:51.596123 93418 net.cpp:178] Creating Layer fc8
I1226 14:16:51.596153 93418 net.cpp:612] fc8 <- fc7
I1226 14:16:51.596194 93418 net.cpp:586] fc8 -> fc8
I1226 14:16:52.165596 93418 net.cpp:228] Setting up fc8
I1226 14:16:52.165710 93418 net.cpp:235] Top shape: 128 1000 (128000)
I1226 14:16:52.165740 93418 net.cpp:243] Memory required for data: 1064864768
I1226 14:16:52.165796 93418 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 14:16:52.165865 93418 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 14:16:52.165901 93418 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 14:16:52.165966 93418 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 14:16:52.166028 93418 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 14:16:52.166129 93418 net.cpp:228] Setting up fc8_fc8_0_split
I1226 14:16:52.166180 93418 net.cpp:235] Top shape: 128 1000 (128000)
I1226 14:16:52.166215 93418 net.cpp:235] Top shape: 128 1000 (128000)
I1226 14:16:52.166240 93418 net.cpp:243] Memory required for data: 1065888768
I1226 14:16:52.166272 93418 layer_factory.hpp:114] Creating layer accuracy
I1226 14:16:52.166339 93418 net.cpp:178] Creating Layer accuracy
I1226 14:16:52.166390 93418 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 14:16:52.166435 93418 net.cpp:612] accuracy <- label_data_1_split_0
I1226 14:16:52.166478 93418 net.cpp:586] accuracy -> accuracy
I1226 14:16:52.166534 93418 net.cpp:228] Setting up accuracy
I1226 14:16:52.166575 93418 net.cpp:235] Top shape: (1)
I1226 14:16:52.166601 93418 net.cpp:243] Memory required for data: 1065888772
I1226 14:16:52.166633 93418 layer_factory.hpp:114] Creating layer loss
I1226 14:16:52.166702 93418 net.cpp:178] Creating Layer loss
I1226 14:16:52.166734 93418 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 14:16:52.166766 93418 net.cpp:612] loss <- label_data_1_split_1
I1226 14:16:52.166806 93418 net.cpp:586] loss -> loss
I1226 14:16:52.166872 93418 layer_factory.hpp:114] Creating layer loss
I1226 14:16:52.187068 93418 net.cpp:228] Setting up loss
I1226 14:16:52.187247 93418 net.cpp:235] Top shape: (1)
I1226 14:16:52.187274 93418 net.cpp:238]     with loss weight 1
I1226 14:16:52.187402 93418 net.cpp:243] Memory required for data: 1065888776
I1226 14:16:52.187438 93418 net.cpp:305] loss needs backward computation.
I1226 14:16:52.187470 93418 net.cpp:307] accuracy does not need backward computation.
I1226 14:16:52.187499 93418 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 14:16:52.187525 93418 net.cpp:305] fc8 needs backward computation.
I1226 14:16:52.187549 93418 net.cpp:305] drop7 needs backward computation.
I1226 14:16:52.187572 93418 net.cpp:305] relu7 needs backward computation.
I1226 14:16:52.187594 93418 net.cpp:305] fc7 needs backward computation.
I1226 14:16:52.187618 93418 net.cpp:305] drop6 needs backward computation.
I1226 14:16:52.187643 93418 net.cpp:305] relu6 needs backward computation.
I1226 14:16:52.187664 93418 net.cpp:305] fc6 needs backward computation.
I1226 14:16:52.187688 93418 net.cpp:305] pool5 needs backward computation.
I1226 14:16:52.187712 93418 net.cpp:305] relu5 needs backward computation.
I1226 14:16:52.187736 93418 net.cpp:305] conv5 needs backward computation.
I1226 14:16:52.187759 93418 net.cpp:305] relu4 needs backward computation.
I1226 14:16:52.187783 93418 net.cpp:305] conv4 needs backward computation.
I1226 14:16:52.187808 93418 net.cpp:305] relu3 needs backward computation.
I1226 14:16:52.187829 93418 net.cpp:305] conv3 needs backward computation.
I1226 14:16:52.187854 93418 net.cpp:305] pool2 needs backward computation.
I1226 14:16:52.187878 93418 net.cpp:305] norm2 needs backward computation.
I1226 14:16:52.187903 93418 net.cpp:305] relu2 needs backward computation.
I1226 14:16:52.187927 93418 net.cpp:305] conv2 needs backward computation.
I1226 14:16:52.187950 93418 net.cpp:305] pool1 needs backward computation.
I1226 14:16:52.187974 93418 net.cpp:305] norm1 needs backward computation.
I1226 14:16:52.187999 93418 net.cpp:305] relu1 needs backward computation.
I1226 14:16:52.188021 93418 net.cpp:305] conv1 needs backward computation.
I1226 14:16:52.188046 93418 net.cpp:307] label_data_1_split does not need backward computation.
I1226 14:16:52.188072 93418 net.cpp:307] data does not need backward computation.
I1226 14:16:52.188093 93418 net.cpp:349] This network produces output accuracy
I1226 14:16:52.188120 93418 net.cpp:349] This network produces output loss
I1226 14:16:52.188194 93418 net.cpp:363] Network initialization done.
I1226 14:16:52.188626 93418 solver.cpp:107] Solver scaffolding done.
I1226 14:16:52.188783 93418 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 14:16:52.729979 91408 net.cpp:228] Setting up fc7
I1226 14:16:52.730109 91408 net.cpp:235] Top shape: 128 4096 (524288)
I1226 14:16:52.730140 91408 net.cpp:243] Memory required for data: 1060158464
I1226 14:16:52.730198 91408 layer_factory.hpp:114] Creating layer relu7
I1226 14:16:52.730383 91408 net.cpp:178] Creating Layer relu7
I1226 14:16:52.730482 91408 net.cpp:612] relu7 <- fc7
I1226 14:16:52.730598 91408 net.cpp:573] relu7 -> fc7 (in-place)
I1226 14:16:52.730717 91408 net.cpp:228] Setting up relu7
I1226 14:16:52.730775 91408 net.cpp:235] Top shape: 128 4096 (524288)
I1226 14:16:52.730800 91408 net.cpp:243] Memory required for data: 1062255616
I1226 14:16:52.730832 91408 layer_factory.hpp:114] Creating layer drop7
I1226 14:16:52.730901 91408 net.cpp:178] Creating Layer drop7
I1226 14:16:52.730927 91408 net.cpp:612] drop7 <- fc7
I1226 14:16:52.730962 91408 net.cpp:573] drop7 -> fc7 (in-place)
I1226 14:16:52.731088 91408 net.cpp:228] Setting up drop7
I1226 14:16:52.731124 91408 net.cpp:235] Top shape: 128 4096 (524288)
I1226 14:16:52.731147 91408 net.cpp:243] Memory required for data: 1064352768
I1226 14:16:52.731178 91408 layer_factory.hpp:114] Creating layer fc8
I1226 14:16:52.731248 91408 net.cpp:178] Creating Layer fc8
I1226 14:16:52.731289 91408 net.cpp:612] fc8 <- fc7
I1226 14:16:52.731354 91408 net.cpp:586] fc8 -> fc8
I1226 14:16:52.745993 91497 net.cpp:228] Setting up fc7
I1226 14:16:52.746104 91497 net.cpp:235] Top shape: 128 4096 (524288)
I1226 14:16:52.746134 91497 net.cpp:243] Memory required for data: 1060158464
I1226 14:16:52.746196 91497 layer_factory.hpp:114] Creating layer relu7
I1226 14:16:52.746273 91497 net.cpp:178] Creating Layer relu7
I1226 14:16:52.746305 91497 net.cpp:612] relu7 <- fc7
I1226 14:16:52.746346 91497 net.cpp:573] relu7 -> fc7 (in-place)
I1226 14:16:52.746465 91497 net.cpp:228] Setting up relu7
I1226 14:16:52.746520 91497 net.cpp:235] Top shape: 128 4096 (524288)
I1226 14:16:52.746546 91497 net.cpp:243] Memory required for data: 1062255616
I1226 14:16:52.746577 91497 layer_factory.hpp:114] Creating layer drop7
I1226 14:16:52.746630 91497 net.cpp:178] Creating Layer drop7
I1226 14:16:52.746660 91497 net.cpp:612] drop7 <- fc7
I1226 14:16:52.746726 91497 net.cpp:573] drop7 -> fc7 (in-place)
I1226 14:16:52.746783 91497 net.cpp:228] Setting up drop7
I1226 14:16:52.746821 91497 net.cpp:235] Top shape: 128 4096 (524288)
I1226 14:16:52.746858 91497 net.cpp:243] Memory required for data: 1064352768
I1226 14:16:52.746888 91497 layer_factory.hpp:114] Creating layer fc8
I1226 14:16:52.746949 91497 net.cpp:178] Creating Layer fc8
I1226 14:16:52.746978 91497 net.cpp:612] fc8 <- fc7
I1226 14:16:52.747022 91497 net.cpp:586] fc8 -> fc8
I1226 14:16:52.763401 92503 net.cpp:228] Setting up fc7
I1226 14:16:52.763514 92503 net.cpp:235] Top shape: 128 4096 (524288)
I1226 14:16:52.763545 92503 net.cpp:243] Memory required for data: 1060158464
I1226 14:16:52.763602 92503 layer_factory.hpp:114] Creating layer relu7
I1226 14:16:52.763681 92503 net.cpp:178] Creating Layer relu7
I1226 14:16:52.763808 92503 net.cpp:612] relu7 <- fc7
I1226 14:16:52.763845 92503 net.cpp:573] relu7 -> fc7 (in-place)
I1226 14:16:52.763937 92503 net.cpp:228] Setting up relu7
I1226 14:16:52.763986 92503 net.cpp:235] Top shape: 128 4096 (524288)
I1226 14:16:52.764010 92503 net.cpp:243] Memory required for data: 1062255616
I1226 14:16:52.764040 92503 layer_factory.hpp:114] Creating layer drop7
I1226 14:16:52.764091 92503 net.cpp:178] Creating Layer drop7
I1226 14:16:52.764117 92503 net.cpp:612] drop7 <- fc7
I1226 14:16:52.764161 92503 net.cpp:573] drop7 -> fc7 (in-place)
I1226 14:16:52.764211 92503 net.cpp:228] Setting up drop7
I1226 14:16:52.764255 92503 net.cpp:235] Top shape: 128 4096 (524288)
I1226 14:16:52.764278 92503 net.cpp:243] Memory required for data: 1064352768
I1226 14:16:52.764325 92503 layer_factory.hpp:114] Creating layer fc8
I1226 14:16:52.764382 92503 net.cpp:178] Creating Layer fc8
I1226 14:16:52.764416 92503 net.cpp:612] fc8 <- fc7
I1226 14:16:52.764456 92503 net.cpp:586] fc8 -> fc8
I1226 14:16:52.864197 98928 net.cpp:228] Setting up fc7
I1226 14:16:52.864310 98928 net.cpp:235] Top shape: 128 4096 (524288)
I1226 14:16:52.864337 98928 net.cpp:243] Memory required for data: 1060158464
I1226 14:16:52.864423 98928 layer_factory.hpp:114] Creating layer relu7
I1226 14:16:52.864517 98928 net.cpp:178] Creating Layer relu7
I1226 14:16:52.864565 98928 net.cpp:612] relu7 <- fc7
I1226 14:16:52.864605 98928 net.cpp:573] relu7 -> fc7 (in-place)
I1226 14:16:52.864696 98928 net.cpp:228] Setting up relu7
I1226 14:16:52.864745 98928 net.cpp:235] Top shape: 128 4096 (524288)
I1226 14:16:52.864769 98928 net.cpp:243] Memory required for data: 1062255616
I1226 14:16:52.864799 98928 layer_factory.hpp:114] Creating layer drop7
I1226 14:16:52.864837 98928 net.cpp:178] Creating Layer drop7
I1226 14:16:52.864864 98928 net.cpp:612] drop7 <- fc7
I1226 14:16:52.864912 98928 net.cpp:573] drop7 -> fc7 (in-place)
I1226 14:16:52.864959 98928 net.cpp:228] Setting up drop7
I1226 14:16:52.864989 98928 net.cpp:235] Top shape: 128 4096 (524288)
I1226 14:16:52.865021 98928 net.cpp:243] Memory required for data: 1064352768
I1226 14:16:52.865054 98928 layer_factory.hpp:114] Creating layer fc8
I1226 14:16:52.865139 98928 net.cpp:178] Creating Layer fc8
I1226 14:16:52.865170 98928 net.cpp:612] fc8 <- fc7
I1226 14:16:52.865211 98928 net.cpp:586] fc8 -> fc8
I1226 14:16:53.285995 91408 net.cpp:228] Setting up fc8
I1226 14:16:53.286106 91408 net.cpp:235] Top shape: 128 1000 (128000)
I1226 14:16:53.286134 91408 net.cpp:243] Memory required for data: 1064864768
I1226 14:16:53.286191 91408 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 14:16:53.286336 91408 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 14:16:53.286427 91408 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 14:16:53.286468 91408 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 14:16:53.286519 91408 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 14:16:53.286633 91408 net.cpp:228] Setting up fc8_fc8_0_split
I1226 14:16:53.286700 91408 net.cpp:235] Top shape: 128 1000 (128000)
I1226 14:16:53.286731 91408 net.cpp:235] Top shape: 128 1000 (128000)
I1226 14:16:53.286752 91408 net.cpp:243] Memory required for data: 1065888768
I1226 14:16:53.286887 91408 layer_factory.hpp:114] Creating layer accuracy
I1226 14:16:53.286943 91408 net.cpp:178] Creating Layer accuracy
I1226 14:16:53.286969 91408 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 14:16:53.287000 91408 net.cpp:612] accuracy <- label_data_1_split_0
I1226 14:16:53.287040 91408 net.cpp:586] accuracy -> accuracy
I1226 14:16:53.287098 91408 net.cpp:228] Setting up accuracy
I1226 14:16:53.287139 91408 net.cpp:235] Top shape: (1)
I1226 14:16:53.287161 91408 net.cpp:243] Memory required for data: 1065888772
I1226 14:16:53.287187 91408 layer_factory.hpp:114] Creating layer loss
I1226 14:16:53.287230 91408 net.cpp:178] Creating Layer loss
I1226 14:16:53.287255 91408 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 14:16:53.287281 91408 net.cpp:612] loss <- label_data_1_split_1
I1226 14:16:53.287376 91408 net.cpp:586] loss -> loss
I1226 14:16:53.287458 91408 layer_factory.hpp:114] Creating layer loss
I1226 14:16:53.317767 91408 net.cpp:228] Setting up loss
I1226 14:16:53.317972 91408 net.cpp:235] Top shape: (1)
I1226 14:16:53.318019 91408 net.cpp:238]     with loss weight 1
I1226 14:16:53.318346 91408 net.cpp:243] Memory required for data: 1065888776
I1226 14:16:53.318397 91408 net.cpp:305] loss needs backward computation.
I1226 14:16:53.318449 91408 net.cpp:307] accuracy does not need backward computation.
I1226 14:16:53.318486 91408 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 14:16:53.318517 91408 net.cpp:305] fc8 needs backward computation.
I1226 14:16:53.318547 91408 net.cpp:305] drop7 needs backward computation.
I1226 14:16:53.318578 91408 net.cpp:305] relu7 needs backward computation.
I1226 14:16:53.318608 91408 net.cpp:305] fc7 needs backward computation.
I1226 14:16:53.318637 91408 net.cpp:305] drop6 needs backward computation.
I1226 14:16:53.318666 91408 net.cpp:305] relu6 needs backward computation.
I1226 14:16:53.318706 91408 net.cpp:305] fc6 needs backward computation.
I1226 14:16:53.318737 91408 net.cpp:305] pool5 needs backward computation.
I1226 14:16:53.318774 91408 net.cpp:305] relu5 needs backward computation.
I1226 14:16:53.318812 91408 net.cpp:305] conv5 needs backward computation.
I1226 14:16:53.318845 91408 net.cpp:305] relu4 needs backward computation.
I1226 14:16:53.318873 91408 net.cpp:305] conv4 needs backward computation.
I1226 14:16:53.318902 91408 net.cpp:305] relu3 needs backward computation.
I1226 14:16:53.318931 91408 net.cpp:305] conv3 needs backward computation.
I1226 14:16:53.318972 91408 net.cpp:305] pool2 needs backward computation.
I1226 14:16:53.319005 91408 net.cpp:305] norm2 needs backward computation.
I1226 14:16:53.319036 91408 net.cpp:305] relu2 needs backward computation.
I1226 14:16:53.319075 91408 net.cpp:305] conv2 needs backward computation.
I1226 14:16:53.319105 91408 net.cpp:305] pool1 needs backward computation.
I1226 14:16:53.319144 91408 net.cpp:305] norm1 needs backward computation.
I1226 14:16:53.319175 91408 net.cpp:305] relu1 needs backward computation.
I1226 14:16:53.319205 91408 net.cpp:305] conv1 needs backward computation.
I1226 14:16:53.319244 91408 net.cpp:307] label_data_1_split does not need backward computation.
I1226 14:16:53.319277 91408 net.cpp:307] data does not need backward computation.
I1226 14:16:53.319326 91408 net.cpp:349] This network produces output accuracy
I1226 14:16:53.319375 91408 net.cpp:349] This network produces output loss
I1226 14:16:53.319478 91408 net.cpp:363] Network initialization done.
I1226 14:16:53.319952 91408 solver.cpp:107] Solver scaffolding done.
I1226 14:16:53.320181 91408 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 14:16:53.314934 91497 net.cpp:228] Setting up fc8
I1226 14:16:53.315047 91497 net.cpp:235] Top shape: 128 1000 (128000)
I1226 14:16:53.315074 91497 net.cpp:243] Memory required for data: 1064864768
I1226 14:16:53.315130 91497 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 14:16:53.315207 91497 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 14:16:53.315325 91497 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 14:16:53.315388 91497 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 14:16:53.315443 91497 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 14:16:53.315536 91497 net.cpp:228] Setting up fc8_fc8_0_split
I1226 14:16:53.315578 91497 net.cpp:235] Top shape: 128 1000 (128000)
I1226 14:16:53.315616 91497 net.cpp:235] Top shape: 128 1000 (128000)
I1226 14:16:53.315649 91497 net.cpp:243] Memory required for data: 1065888768
I1226 14:16:53.315680 91497 layer_factory.hpp:114] Creating layer accuracy
I1226 14:16:53.315758 91497 net.cpp:178] Creating Layer accuracy
I1226 14:16:53.315789 91497 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 14:16:53.315829 91497 net.cpp:612] accuracy <- label_data_1_split_0
I1226 14:16:53.315878 91497 net.cpp:586] accuracy -> accuracy
I1226 14:16:53.315933 91497 net.cpp:228] Setting up accuracy
I1226 14:16:53.315974 91497 net.cpp:235] Top shape: (1)
I1226 14:16:53.315999 91497 net.cpp:243] Memory required for data: 1065888772
I1226 14:16:53.316026 91497 layer_factory.hpp:114] Creating layer loss
I1226 14:16:53.316071 91497 net.cpp:178] Creating Layer loss
I1226 14:16:53.316102 91497 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 14:16:53.316131 91497 net.cpp:612] loss <- label_data_1_split_1
I1226 14:16:53.316164 91497 net.cpp:586] loss -> loss
I1226 14:16:53.316226 91497 layer_factory.hpp:114] Creating layer loss
I1226 14:16:53.319422 92503 net.cpp:228] Setting up fc8
I1226 14:16:53.319533 92503 net.cpp:235] Top shape: 128 1000 (128000)
I1226 14:16:53.319561 92503 net.cpp:243] Memory required for data: 1064864768
I1226 14:16:53.319643 92503 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 14:16:53.319717 92503 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 14:16:53.319756 92503 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 14:16:53.319811 92503 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 14:16:53.319864 92503 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 14:16:53.319950 92503 net.cpp:228] Setting up fc8_fc8_0_split
I1226 14:16:53.319989 92503 net.cpp:235] Top shape: 128 1000 (128000)
I1226 14:16:53.320025 92503 net.cpp:235] Top shape: 128 1000 (128000)
I1226 14:16:53.320051 92503 net.cpp:243] Memory required for data: 1065888768
I1226 14:16:53.320080 92503 layer_factory.hpp:114] Creating layer accuracy
I1226 14:16:53.320129 92503 net.cpp:178] Creating Layer accuracy
I1226 14:16:53.320163 92503 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 14:16:53.320199 92503 net.cpp:612] accuracy <- label_data_1_split_0
I1226 14:16:53.320232 92503 net.cpp:586] accuracy -> accuracy
I1226 14:16:53.320272 92503 net.cpp:228] Setting up accuracy
I1226 14:16:53.320330 92503 net.cpp:235] Top shape: (1)
I1226 14:16:53.320358 92503 net.cpp:243] Memory required for data: 1065888772
I1226 14:16:53.320385 92503 layer_factory.hpp:114] Creating layer loss
I1226 14:16:53.320442 92503 net.cpp:178] Creating Layer loss
I1226 14:16:53.320483 92503 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 14:16:53.320519 92503 net.cpp:612] loss <- label_data_1_split_1
I1226 14:16:53.320552 92503 net.cpp:586] loss -> loss
I1226 14:16:53.320613 92503 layer_factory.hpp:114] Creating layer loss
I1226 14:16:53.344432 91497 net.cpp:228] Setting up loss
I1226 14:16:53.344643 91497 net.cpp:235] Top shape: (1)
I1226 14:16:53.344696 91497 net.cpp:238]     with loss weight 1
I1226 14:16:53.344890 91497 net.cpp:243] Memory required for data: 1065888776
I1226 14:16:53.344950 91497 net.cpp:305] loss needs backward computation.
I1226 14:16:53.345005 91497 net.cpp:307] accuracy does not need backward computation.
I1226 14:16:53.345052 91497 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 14:16:53.345094 91497 net.cpp:305] fc8 needs backward computation.
I1226 14:16:53.345139 91497 net.cpp:305] drop7 needs backward computation.
I1226 14:16:53.345176 91497 net.cpp:305] relu7 needs backward computation.
I1226 14:16:53.345212 91497 net.cpp:305] fc7 needs backward computation.
I1226 14:16:53.345243 91497 net.cpp:305] drop6 needs backward computation.
I1226 14:16:53.345274 91497 net.cpp:305] relu6 needs backward computation.
I1226 14:16:53.345306 91497 net.cpp:305] fc6 needs backward computation.
I1226 14:16:53.345346 91497 net.cpp:305] pool5 needs backward computation.
I1226 14:16:53.345384 91497 net.cpp:305] relu5 needs backward computation.
I1226 14:16:53.345425 91497 net.cpp:305] conv5 needs backward computation.
I1226 14:16:53.345463 91497 net.cpp:305] relu4 needs backward computation.
I1226 14:16:53.345499 91497 net.cpp:305] conv4 needs backward computation.
I1226 14:16:53.345537 91497 net.cpp:305] relu3 needs backward computation.
I1226 14:16:53.345566 91497 net.cpp:305] conv3 needs backward computation.
I1226 14:16:53.345597 91497 net.cpp:305] pool2 needs backward computation.
I1226 14:16:53.345628 91497 net.cpp:305] norm2 needs backward computation.
I1226 14:16:53.345664 91497 net.cpp:305] relu2 needs backward computation.
I1226 14:16:53.345692 91497 net.cpp:305] conv2 needs backward computation.
I1226 14:16:53.345748 91497 net.cpp:305] pool1 needs backward computation.
I1226 14:16:53.345790 91497 net.cpp:305] norm1 needs backward computation.
I1226 14:16:53.345829 91497 net.cpp:305] relu1 needs backward computation.
I1226 14:16:53.345857 91497 net.cpp:305] conv1 needs backward computation.
I1226 14:16:53.345890 91497 net.cpp:307] label_data_1_split does not need backward computation.
I1226 14:16:53.345924 91497 net.cpp:307] data does not need backward computation.
I1226 14:16:53.345957 91497 net.cpp:349] This network produces output accuracy
I1226 14:16:53.345994 91497 net.cpp:349] This network produces output loss
I1226 14:16:53.346099 91497 net.cpp:363] Network initialization done.
I1226 14:16:53.346552 91497 solver.cpp:107] Solver scaffolding done.
I1226 14:16:53.346792 91497 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 14:16:53.353744 92503 net.cpp:228] Setting up loss
I1226 14:16:53.353971 92503 net.cpp:235] Top shape: (1)
I1226 14:16:53.354020 92503 net.cpp:238]     with loss weight 1
I1226 14:16:53.354188 92503 net.cpp:243] Memory required for data: 1065888776
I1226 14:16:53.354246 92503 net.cpp:305] loss needs backward computation.
I1226 14:16:53.354321 92503 net.cpp:307] accuracy does not need backward computation.
I1226 14:16:53.354364 92503 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 14:16:53.354398 92503 net.cpp:305] fc8 needs backward computation.
I1226 14:16:53.354442 92503 net.cpp:305] drop7 needs backward computation.
I1226 14:16:53.354647 92503 net.cpp:305] relu7 needs backward computation.
I1226 14:16:53.354677 92503 net.cpp:305] fc7 needs backward computation.
I1226 14:16:53.354710 92503 net.cpp:305] drop6 needs backward computation.
I1226 14:16:53.354764 92503 net.cpp:305] relu6 needs backward computation.
I1226 14:16:53.354799 92503 net.cpp:305] fc6 needs backward computation.
I1226 14:16:53.355020 92503 net.cpp:305] pool5 needs backward computation.
I1226 14:16:53.355053 92503 net.cpp:305] relu5 needs backward computation.
I1226 14:16:53.355106 92503 net.cpp:305] conv5 needs backward computation.
I1226 14:16:53.355176 92503 net.cpp:305] relu4 needs backward computation.
I1226 14:16:53.355214 92503 net.cpp:305] conv4 needs backward computation.
I1226 14:16:53.355244 92503 net.cpp:305] relu3 needs backward computation.
I1226 14:16:53.355273 92503 net.cpp:305] conv3 needs backward computation.
I1226 14:16:53.355324 92503 net.cpp:305] pool2 needs backward computation.
I1226 14:16:53.355355 92503 net.cpp:305] norm2 needs backward computation.
I1226 14:16:53.355410 92503 net.cpp:305] relu2 needs backward computation.
I1226 14:16:53.355446 92503 net.cpp:305] conv2 needs backward computation.
I1226 14:16:53.355478 92503 net.cpp:305] pool1 needs backward computation.
I1226 14:16:53.355507 92503 net.cpp:305] norm1 needs backward computation.
I1226 14:16:53.355553 92503 net.cpp:305] relu1 needs backward computation.
I1226 14:16:53.355590 92503 net.cpp:305] conv1 needs backward computation.
I1226 14:16:53.355623 92503 net.cpp:307] label_data_1_split does not need backward computation.
I1226 14:16:53.355655 92503 net.cpp:307] data does not need backward computation.
I1226 14:16:53.355685 92503 net.cpp:349] This network produces output accuracy
I1226 14:16:53.355721 92503 net.cpp:349] This network produces output loss
I1226 14:16:53.355823 92503 net.cpp:363] Network initialization done.
I1226 14:16:53.356364 92503 solver.cpp:107] Solver scaffolding done.
I1226 14:16:53.356621 92503 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 14:16:53.429579 98928 net.cpp:228] Setting up fc8
I1226 14:16:53.429708 98928 net.cpp:235] Top shape: 128 1000 (128000)
I1226 14:16:53.429740 98928 net.cpp:243] Memory required for data: 1064864768
I1226 14:16:53.429828 98928 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 14:16:53.429901 98928 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 14:16:53.429934 98928 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 14:16:53.429983 98928 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 14:16:53.430030 98928 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 14:16:53.430158 98928 net.cpp:228] Setting up fc8_fc8_0_split
I1226 14:16:53.430219 98928 net.cpp:235] Top shape: 128 1000 (128000)
I1226 14:16:53.430276 98928 net.cpp:235] Top shape: 128 1000 (128000)
I1226 14:16:53.430327 98928 net.cpp:243] Memory required for data: 1065888768
I1226 14:16:53.430368 98928 layer_factory.hpp:114] Creating layer accuracy
I1226 14:16:53.430429 98928 net.cpp:178] Creating Layer accuracy
I1226 14:16:53.430455 98928 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 14:16:53.430485 98928 net.cpp:612] accuracy <- label_data_1_split_0
I1226 14:16:53.430519 98928 net.cpp:586] accuracy -> accuracy
I1226 14:16:53.430663 98928 net.cpp:228] Setting up accuracy
I1226 14:16:53.430701 98928 net.cpp:235] Top shape: (1)
I1226 14:16:53.430722 98928 net.cpp:243] Memory required for data: 1065888772
I1226 14:16:53.430750 98928 layer_factory.hpp:114] Creating layer loss
I1226 14:16:53.430817 98928 net.cpp:178] Creating Layer loss
I1226 14:16:53.430852 98928 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 14:16:53.430881 98928 net.cpp:612] loss <- label_data_1_split_1
I1226 14:16:53.430915 98928 net.cpp:586] loss -> loss
I1226 14:16:53.430982 98928 layer_factory.hpp:114] Creating layer loss
I1226 14:16:53.470269 98928 net.cpp:228] Setting up loss
I1226 14:16:53.470490 98928 net.cpp:235] Top shape: (1)
I1226 14:16:53.470546 98928 net.cpp:238]     with loss weight 1
I1226 14:16:53.470695 98928 net.cpp:243] Memory required for data: 1065888776
I1226 14:16:53.470746 98928 net.cpp:305] loss needs backward computation.
I1226 14:16:53.470800 98928 net.cpp:307] accuracy does not need backward computation.
I1226 14:16:53.470847 98928 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 14:16:53.470890 98928 net.cpp:305] fc8 needs backward computation.
I1226 14:16:53.470930 98928 net.cpp:305] drop7 needs backward computation.
I1226 14:16:53.470962 98928 net.cpp:305] relu7 needs backward computation.
I1226 14:16:53.470999 98928 net.cpp:305] fc7 needs backward computation.
I1226 14:16:53.471030 98928 net.cpp:305] drop6 needs backward computation.
I1226 14:16:53.471087 98928 net.cpp:305] relu6 needs backward computation.
I1226 14:16:53.471119 98928 net.cpp:305] fc6 needs backward computation.
I1226 14:16:53.471150 98928 net.cpp:305] pool5 needs backward computation.
I1226 14:16:53.471192 98928 net.cpp:305] relu5 needs backward computation.
I1226 14:16:53.471230 98928 net.cpp:305] conv5 needs backward computation.
I1226 14:16:53.471261 98928 net.cpp:305] relu4 needs backward computation.
I1226 14:16:53.471288 98928 net.cpp:305] conv4 needs backward computation.
I1226 14:16:53.471319 98928 net.cpp:305] relu3 needs backward computation.
I1226 14:16:53.471350 98928 net.cpp:305] conv3 needs backward computation.
I1226 14:16:53.471388 98928 net.cpp:305] pool2 needs backward computation.
I1226 14:16:53.471427 98928 net.cpp:305] norm2 needs backward computation.
I1226 14:16:53.471457 98928 net.cpp:305] relu2 needs backward computation.
I1226 14:16:53.471495 98928 net.cpp:305] conv2 needs backward computation.
I1226 14:16:53.471536 98928 net.cpp:305] pool1 needs backward computation.
I1226 14:16:53.471567 98928 net.cpp:305] norm1 needs backward computation.
I1226 14:16:53.471604 98928 net.cpp:305] relu1 needs backward computation.
I1226 14:16:53.471642 98928 net.cpp:305] conv1 needs backward computation.
I1226 14:16:53.471681 98928 net.cpp:307] label_data_1_split does not need backward computation.
I1226 14:16:53.471719 98928 net.cpp:307] data does not need backward computation.
I1226 14:16:53.471748 98928 net.cpp:349] This network produces output accuracy
I1226 14:16:53.471783 98928 net.cpp:349] This network produces output loss
I1226 14:16:53.471884 98928 net.cpp:363] Network initialization done.
I1226 14:16:53.472368 98928 solver.cpp:107] Solver scaffolding done.
I1226 14:16:53.472586 98928 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 14:16:53.874456 93418 caffe.cpp:376] Configuring multinode setup
I1226 14:16:53.875783 93418 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 14:16:54.937470 93636 caffe.cpp:376] Configuring multinode setup
I1226 14:16:54.947064 93636 caffe.cpp:386] Starting parameter server in mpi environment
I1226 14:16:56.573477 92503 caffe.cpp:376] Configuring multinode setup
I1226 14:16:56.575636 92503 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 14:16:56.582511 91497 caffe.cpp:376] Configuring multinode setup
I1226 14:16:56.583919 91497 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 14:16:56.694471 98928 caffe.cpp:376] Configuring multinode setup
I1226 14:16:56.695832 98928 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 14:16:57.340623 91408 caffe.cpp:376] Configuring multinode setup
I1226 14:16:57.342092 91408 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 14:17:16.985034 95408 net.cpp:228] Setting up fc6
I1226 14:17:16.985297 95408 net.cpp:235] Top shape: 128 4096 (524288)
I1226 14:17:16.985340 95408 net.cpp:243] Memory required for data: 1053867008
I1226 14:17:16.985407 95408 layer_factory.hpp:114] Creating layer relu6
I1226 14:17:16.985491 95408 net.cpp:178] Creating Layer relu6
I1226 14:17:16.985535 95408 net.cpp:612] relu6 <- fc6
I1226 14:17:16.985599 95408 net.cpp:573] relu6 -> fc6 (in-place)
I1226 14:17:16.985693 95408 net.cpp:228] Setting up relu6
I1226 14:17:16.985749 95408 net.cpp:235] Top shape: 128 4096 (524288)
I1226 14:17:16.985783 95408 net.cpp:243] Memory required for data: 1055964160
I1226 14:17:16.985848 95408 layer_factory.hpp:114] Creating layer drop6
I1226 14:17:16.985908 95408 net.cpp:178] Creating Layer drop6
I1226 14:17:16.985936 95408 net.cpp:612] drop6 <- fc6
I1226 14:17:16.985985 95408 net.cpp:573] drop6 -> fc6 (in-place)
I1226 14:17:16.986040 95408 net.cpp:228] Setting up drop6
I1226 14:17:16.986076 95408 net.cpp:235] Top shape: 128 4096 (524288)
I1226 14:17:16.986100 95408 net.cpp:243] Memory required for data: 1058061312
I1226 14:17:16.986129 95408 layer_factory.hpp:114] Creating layer fc7
I1226 14:17:16.986182 95408 net.cpp:178] Creating Layer fc7
I1226 14:17:16.986209 95408 net.cpp:612] fc7 <- fc6
I1226 14:17:16.986245 95408 net.cpp:586] fc7 -> fc7
I1226 14:17:17.222532 97473 net.cpp:228] Setting up fc6
I1226 14:17:17.222821 97473 net.cpp:235] Top shape: 128 4096 (524288)
I1226 14:17:17.222869 97473 net.cpp:243] Memory required for data: 1053867008
I1226 14:17:17.222934 97473 layer_factory.hpp:114] Creating layer relu6
I1226 14:17:17.223094 97473 net.cpp:178] Creating Layer relu6
I1226 14:17:17.223145 97473 net.cpp:612] relu6 <- fc6
I1226 14:17:17.223211 97473 net.cpp:573] relu6 -> fc6 (in-place)
I1226 14:17:17.223318 97473 net.cpp:228] Setting up relu6
I1226 14:17:17.223372 97473 net.cpp:235] Top shape: 128 4096 (524288)
I1226 14:17:17.223407 97473 net.cpp:243] Memory required for data: 1055964160
I1226 14:17:17.223448 97473 layer_factory.hpp:114] Creating layer drop6
I1226 14:17:17.223508 97473 net.cpp:178] Creating Layer drop6
I1226 14:17:17.223546 97473 net.cpp:612] drop6 <- fc6
I1226 14:17:17.223587 97473 net.cpp:573] drop6 -> fc6 (in-place)
I1226 14:17:17.223644 97473 net.cpp:228] Setting up drop6
I1226 14:17:17.223686 97473 net.cpp:235] Top shape: 128 4096 (524288)
I1226 14:17:17.223712 97473 net.cpp:243] Memory required for data: 1058061312
I1226 14:17:17.223770 97473 layer_factory.hpp:114] Creating layer fc7
I1226 14:17:17.223860 97473 net.cpp:178] Creating Layer fc7
I1226 14:17:17.223903 97473 net.cpp:612] fc7 <- fc6
I1226 14:17:17.223948 97473 net.cpp:586] fc7 -> fc7
I1226 14:17:18.003244 97044 net.cpp:228] Setting up fc6
I1226 14:17:18.003496 97044 net.cpp:235] Top shape: 128 4096 (524288)
I1226 14:17:18.003541 97044 net.cpp:243] Memory required for data: 1053867008
I1226 14:17:18.003609 97044 layer_factory.hpp:114] Creating layer relu6
I1226 14:17:18.003703 97044 net.cpp:178] Creating Layer relu6
I1226 14:17:18.003819 97044 net.cpp:612] relu6 <- fc6
I1226 14:17:18.003871 97044 net.cpp:573] relu6 -> fc6 (in-place)
I1226 14:17:18.003964 97044 net.cpp:228] Setting up relu6
I1226 14:17:18.004011 97044 net.cpp:235] Top shape: 128 4096 (524288)
I1226 14:17:18.004041 97044 net.cpp:243] Memory required for data: 1055964160
I1226 14:17:18.004076 97044 layer_factory.hpp:114] Creating layer drop6
I1226 14:17:18.004158 97044 net.cpp:178] Creating Layer drop6
I1226 14:17:18.004210 97044 net.cpp:612] drop6 <- fc6
I1226 14:17:18.004287 97044 net.cpp:573] drop6 -> fc6 (in-place)
I1226 14:17:18.004371 97044 net.cpp:228] Setting up drop6
I1226 14:17:18.004441 97044 net.cpp:235] Top shape: 128 4096 (524288)
I1226 14:17:18.004477 97044 net.cpp:243] Memory required for data: 1058061312
I1226 14:17:18.004518 97044 layer_factory.hpp:114] Creating layer fc7
I1226 14:17:18.004590 97044 net.cpp:178] Creating Layer fc7
I1226 14:17:18.004624 97044 net.cpp:612] fc7 <- fc6
I1226 14:17:18.004672 97044 net.cpp:586] fc7 -> fc7
I1226 14:17:30.056569 95408 net.cpp:228] Setting up fc7
I1226 14:17:30.056689 95408 net.cpp:235] Top shape: 128 4096 (524288)
I1226 14:17:30.056726 95408 net.cpp:243] Memory required for data: 1060158464
I1226 14:17:30.056792 95408 layer_factory.hpp:114] Creating layer relu7
I1226 14:17:30.056982 95408 net.cpp:178] Creating Layer relu7
I1226 14:17:30.057023 95408 net.cpp:612] relu7 <- fc7
I1226 14:17:30.057063 95408 net.cpp:573] relu7 -> fc7 (in-place)
I1226 14:17:30.057159 95408 net.cpp:228] Setting up relu7
I1226 14:17:30.057222 95408 net.cpp:235] Top shape: 128 4096 (524288)
I1226 14:17:30.057250 95408 net.cpp:243] Memory required for data: 1062255616
I1226 14:17:30.057284 95408 layer_factory.hpp:114] Creating layer drop7
I1226 14:17:30.057337 95408 net.cpp:178] Creating Layer drop7
I1226 14:17:30.057375 95408 net.cpp:612] drop7 <- fc7
I1226 14:17:30.057415 95408 net.cpp:573] drop7 -> fc7 (in-place)
I1226 14:17:30.057467 95408 net.cpp:228] Setting up drop7
I1226 14:17:30.057513 95408 net.cpp:235] Top shape: 128 4096 (524288)
I1226 14:17:30.057549 95408 net.cpp:243] Memory required for data: 1064352768
I1226 14:17:30.057588 95408 layer_factory.hpp:114] Creating layer fc8
I1226 14:17:30.057654 95408 net.cpp:178] Creating Layer fc8
I1226 14:17:30.057690 95408 net.cpp:612] fc8 <- fc7
I1226 14:17:30.057745 95408 net.cpp:586] fc8 -> fc8
I1226 14:17:30.299144 97473 net.cpp:228] Setting up fc7
I1226 14:17:30.299260 97473 net.cpp:235] Top shape: 128 4096 (524288)
I1226 14:17:30.299298 97473 net.cpp:243] Memory required for data: 1060158464
I1226 14:17:30.299365 97473 layer_factory.hpp:114] Creating layer relu7
I1226 14:17:30.299451 97473 net.cpp:178] Creating Layer relu7
I1226 14:17:30.299499 97473 net.cpp:612] relu7 <- fc7
I1226 14:17:30.299542 97473 net.cpp:573] relu7 -> fc7 (in-place)
I1226 14:17:30.299649 97473 net.cpp:228] Setting up relu7
I1226 14:17:30.299702 97473 net.cpp:235] Top shape: 128 4096 (524288)
I1226 14:17:30.299753 97473 net.cpp:243] Memory required for data: 1062255616
I1226 14:17:30.299793 97473 layer_factory.hpp:114] Creating layer drop7
I1226 14:17:30.299846 97473 net.cpp:178] Creating Layer drop7
I1226 14:17:30.299885 97473 net.cpp:612] drop7 <- fc7
I1226 14:17:30.299932 97473 net.cpp:573] drop7 -> fc7 (in-place)
I1226 14:17:30.299981 97473 net.cpp:228] Setting up drop7
I1226 14:17:30.300019 97473 net.cpp:235] Top shape: 128 4096 (524288)
I1226 14:17:30.300051 97473 net.cpp:243] Memory required for data: 1064352768
I1226 14:17:30.300079 97473 layer_factory.hpp:114] Creating layer fc8
I1226 14:17:30.300142 97473 net.cpp:178] Creating Layer fc8
I1226 14:17:30.300176 97473 net.cpp:612] fc8 <- fc7
I1226 14:17:30.300233 97473 net.cpp:586] fc8 -> fc8
I1226 14:17:31.076092 97044 net.cpp:228] Setting up fc7
I1226 14:17:31.076215 97044 net.cpp:235] Top shape: 128 4096 (524288)
I1226 14:17:31.076264 97044 net.cpp:243] Memory required for data: 1060158464
I1226 14:17:31.076354 97044 layer_factory.hpp:114] Creating layer relu7
I1226 14:17:31.076464 97044 net.cpp:178] Creating Layer relu7
I1226 14:17:31.076529 97044 net.cpp:612] relu7 <- fc7
I1226 14:17:31.076581 97044 net.cpp:573] relu7 -> fc7 (in-place)
I1226 14:17:31.076704 97044 net.cpp:228] Setting up relu7
I1226 14:17:31.076774 97044 net.cpp:235] Top shape: 128 4096 (524288)
I1226 14:17:31.076848 97044 net.cpp:243] Memory required for data: 1062255616
I1226 14:17:31.076895 97044 layer_factory.hpp:114] Creating layer drop7
I1226 14:17:31.076953 97044 net.cpp:178] Creating Layer drop7
I1226 14:17:31.076994 97044 net.cpp:612] drop7 <- fc7
I1226 14:17:31.077044 97044 net.cpp:573] drop7 -> fc7 (in-place)
I1226 14:17:31.077112 97044 net.cpp:228] Setting up drop7
I1226 14:17:31.077188 97044 net.cpp:235] Top shape: 128 4096 (524288)
I1226 14:17:31.077227 97044 net.cpp:243] Memory required for data: 1064352768
I1226 14:17:31.077268 97044 layer_factory.hpp:114] Creating layer fc8
I1226 14:17:31.077347 97044 net.cpp:178] Creating Layer fc8
I1226 14:17:31.077389 97044 net.cpp:612] fc8 <- fc7
I1226 14:17:31.077473 97044 net.cpp:586] fc8 -> fc8
I1226 14:17:33.243432 95408 net.cpp:228] Setting up fc8
I1226 14:17:33.243551 95408 net.cpp:235] Top shape: 128 1000 (128000)
I1226 14:17:33.243588 95408 net.cpp:243] Memory required for data: 1064864768
I1226 14:17:33.243654 95408 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 14:17:33.243759 95408 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 14:17:33.243813 95408 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 14:17:33.243887 95408 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 14:17:33.243943 95408 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 14:17:33.244043 95408 net.cpp:228] Setting up fc8_fc8_0_split
I1226 14:17:33.244107 95408 net.cpp:235] Top shape: 128 1000 (128000)
I1226 14:17:33.244143 95408 net.cpp:235] Top shape: 128 1000 (128000)
I1226 14:17:33.244179 95408 net.cpp:243] Memory required for data: 1065888768
I1226 14:17:33.244222 95408 layer_factory.hpp:114] Creating layer accuracy
I1226 14:17:33.244285 95408 net.cpp:178] Creating Layer accuracy
I1226 14:17:33.244318 95408 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 14:17:33.244350 95408 net.cpp:612] accuracy <- label_data_1_split_0
I1226 14:17:33.244411 95408 net.cpp:586] accuracy -> accuracy
I1226 14:17:33.244472 95408 net.cpp:228] Setting up accuracy
I1226 14:17:33.244518 95408 net.cpp:235] Top shape: (1)
I1226 14:17:33.244550 95408 net.cpp:243] Memory required for data: 1065888772
I1226 14:17:33.244580 95408 layer_factory.hpp:114] Creating layer loss
I1226 14:17:33.244735 95408 net.cpp:178] Creating Layer loss
I1226 14:17:33.244776 95408 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 14:17:33.244812 95408 net.cpp:612] loss <- label_data_1_split_1
I1226 14:17:33.244891 95408 net.cpp:586] loss -> loss
I1226 14:17:33.244971 95408 layer_factory.hpp:114] Creating layer loss
I1226 14:17:33.295778 95408 net.cpp:228] Setting up loss
I1226 14:17:33.295933 95408 net.cpp:235] Top shape: (1)
I1226 14:17:33.295982 95408 net.cpp:238]     with loss weight 1
I1226 14:17:33.296140 95408 net.cpp:243] Memory required for data: 1065888776
I1226 14:17:33.296195 95408 net.cpp:305] loss needs backward computation.
I1226 14:17:33.296250 95408 net.cpp:307] accuracy does not need backward computation.
I1226 14:17:33.296321 95408 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 14:17:33.296368 95408 net.cpp:305] fc8 needs backward computation.
I1226 14:17:33.296404 95408 net.cpp:305] drop7 needs backward computation.
I1226 14:17:33.296435 95408 net.cpp:305] relu7 needs backward computation.
I1226 14:17:33.296466 95408 net.cpp:305] fc7 needs backward computation.
I1226 14:17:33.296509 95408 net.cpp:305] drop6 needs backward computation.
I1226 14:17:33.296540 95408 net.cpp:305] relu6 needs backward computation.
I1226 14:17:33.296568 95408 net.cpp:305] fc6 needs backward computation.
I1226 14:17:33.296602 95408 net.cpp:305] pool5 needs backward computation.
I1226 14:17:33.296633 95408 net.cpp:305] relu5 needs backward computation.
I1226 14:17:33.296670 95408 net.cpp:305] conv5 needs backward computation.
I1226 14:17:33.296702 95408 net.cpp:305] relu4 needs backward computation.
I1226 14:17:33.296743 95408 net.cpp:305] conv4 needs backward computation.
I1226 14:17:33.296775 95408 net.cpp:305] relu3 needs backward computation.
I1226 14:17:33.296813 95408 net.cpp:305] conv3 needs backward computation.
I1226 14:17:33.296869 95408 net.cpp:305] pool2 needs backward computation.
I1226 14:17:33.296900 95408 net.cpp:305] norm2 needs backward computation.
I1226 14:17:33.296928 95408 net.cpp:305] relu2 needs backward computation.
I1226 14:17:33.296957 95408 net.cpp:305] conv2 needs backward computation.
I1226 14:17:33.296985 95408 net.cpp:305] pool1 needs backward computation.
I1226 14:17:33.297014 95408 net.cpp:305] norm1 needs backward computation.
I1226 14:17:33.297042 95408 net.cpp:305] relu1 needs backward computation.
I1226 14:17:33.297070 95408 net.cpp:305] conv1 needs backward computation.
I1226 14:17:33.297101 95408 net.cpp:307] label_data_1_split does not need backward computation.
I1226 14:17:33.297132 95408 net.cpp:307] data does not need backward computation.
I1226 14:17:33.297158 95408 net.cpp:349] This network produces output accuracy
I1226 14:17:33.297189 95408 net.cpp:349] This network produces output loss
I1226 14:17:33.297282 95408 net.cpp:363] Network initialization done.
I1226 14:17:33.297847 95408 solver.cpp:107] Solver scaffolding done.
I1226 14:17:33.298064 95408 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 14:17:33.490627 97473 net.cpp:228] Setting up fc8
I1226 14:17:33.490763 97473 net.cpp:235] Top shape: 128 1000 (128000)
I1226 14:17:33.490803 97473 net.cpp:243] Memory required for data: 1064864768
I1226 14:17:33.490867 97473 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 14:17:33.490950 97473 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 14:17:33.490994 97473 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 14:17:33.491072 97473 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 14:17:33.491228 97473 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 14:17:33.491334 97473 net.cpp:228] Setting up fc8_fc8_0_split
I1226 14:17:33.491389 97473 net.cpp:235] Top shape: 128 1000 (128000)
I1226 14:17:33.491421 97473 net.cpp:235] Top shape: 128 1000 (128000)
I1226 14:17:33.491446 97473 net.cpp:243] Memory required for data: 1065888768
I1226 14:17:33.491485 97473 layer_factory.hpp:114] Creating layer accuracy
I1226 14:17:33.491539 97473 net.cpp:178] Creating Layer accuracy
I1226 14:17:33.491574 97473 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 14:17:33.491607 97473 net.cpp:612] accuracy <- label_data_1_split_0
I1226 14:17:33.491658 97473 net.cpp:586] accuracy -> accuracy
I1226 14:17:33.491742 97473 net.cpp:228] Setting up accuracy
I1226 14:17:33.491793 97473 net.cpp:235] Top shape: (1)
I1226 14:17:33.491818 97473 net.cpp:243] Memory required for data: 1065888772
I1226 14:17:33.491848 97473 layer_factory.hpp:114] Creating layer loss
I1226 14:17:33.492017 97473 net.cpp:178] Creating Layer loss
I1226 14:17:33.492058 97473 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 14:17:33.492090 97473 net.cpp:612] loss <- label_data_1_split_1
I1226 14:17:33.492136 97473 net.cpp:586] loss -> loss
I1226 14:17:33.492215 97473 layer_factory.hpp:114] Creating layer loss
I1226 14:17:33.524791 97473 net.cpp:228] Setting up loss
I1226 14:17:33.524901 97473 net.cpp:235] Top shape: (1)
I1226 14:17:33.524948 97473 net.cpp:238]     with loss weight 1
I1226 14:17:33.525105 97473 net.cpp:243] Memory required for data: 1065888776
I1226 14:17:33.525158 97473 net.cpp:305] loss needs backward computation.
I1226 14:17:33.525199 97473 net.cpp:307] accuracy does not need backward computation.
I1226 14:17:33.525244 97473 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 14:17:33.525279 97473 net.cpp:305] fc8 needs backward computation.
I1226 14:17:33.525311 97473 net.cpp:305] drop7 needs backward computation.
I1226 14:17:33.525341 97473 net.cpp:305] relu7 needs backward computation.
I1226 14:17:33.525372 97473 net.cpp:305] fc7 needs backward computation.
I1226 14:17:33.525403 97473 net.cpp:305] drop6 needs backward computation.
I1226 14:17:33.525434 97473 net.cpp:305] relu6 needs backward computation.
I1226 14:17:33.525470 97473 net.cpp:305] fc6 needs backward computation.
I1226 14:17:33.525501 97473 net.cpp:305] pool5 needs backward computation.
I1226 14:17:33.525540 97473 net.cpp:305] relu5 needs backward computation.
I1226 14:17:33.525571 97473 net.cpp:305] conv5 needs backward computation.
I1226 14:17:33.525609 97473 net.cpp:305] relu4 needs backward computation.
I1226 14:17:33.525640 97473 net.cpp:305] conv4 needs backward computation.
I1226 14:17:33.525671 97473 net.cpp:305] relu3 needs backward computation.
I1226 14:17:33.525712 97473 net.cpp:305] conv3 needs backward computation.
I1226 14:17:33.525774 97473 net.cpp:305] pool2 needs backward computation.
I1226 14:17:33.525825 97473 net.cpp:305] norm2 needs backward computation.
I1226 14:17:33.525857 97473 net.cpp:305] relu2 needs backward computation.
I1226 14:17:33.525885 97473 net.cpp:305] conv2 needs backward computation.
I1226 14:17:33.525914 97473 net.cpp:305] pool1 needs backward computation.
I1226 14:17:33.525943 97473 net.cpp:305] norm1 needs backward computation.
I1226 14:17:33.525970 97473 net.cpp:305] relu1 needs backward computation.
I1226 14:17:33.525998 97473 net.cpp:305] conv1 needs backward computation.
I1226 14:17:33.526028 97473 net.cpp:307] label_data_1_split does not need backward computation.
I1226 14:17:33.526059 97473 net.cpp:307] data does not need backward computation.
I1226 14:17:33.526085 97473 net.cpp:349] This network produces output accuracy
I1226 14:17:33.526118 97473 net.cpp:349] This network produces output loss
I1226 14:17:33.526201 97473 net.cpp:363] Network initialization done.
I1226 14:17:33.526783 97473 solver.cpp:107] Solver scaffolding done.
I1226 14:17:33.527026 97473 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 14:17:34.271813 97044 net.cpp:228] Setting up fc8
I1226 14:17:34.271939 97044 net.cpp:235] Top shape: 128 1000 (128000)
I1226 14:17:34.271989 97044 net.cpp:243] Memory required for data: 1064864768
I1226 14:17:34.272075 97044 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 14:17:34.272199 97044 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 14:17:34.272260 97044 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 14:17:34.272318 97044 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 14:17:34.272383 97044 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 14:17:34.272507 97044 net.cpp:228] Setting up fc8_fc8_0_split
I1226 14:17:34.272577 97044 net.cpp:235] Top shape: 128 1000 (128000)
I1226 14:17:34.272625 97044 net.cpp:235] Top shape: 128 1000 (128000)
I1226 14:17:34.272660 97044 net.cpp:243] Memory required for data: 1065888768
I1226 14:17:34.272701 97044 layer_factory.hpp:114] Creating layer accuracy
I1226 14:17:34.272778 97044 net.cpp:178] Creating Layer accuracy
I1226 14:17:34.272850 97044 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 14:17:34.272893 97044 net.cpp:612] accuracy <- label_data_1_split_0
I1226 14:17:34.272969 97044 net.cpp:586] accuracy -> accuracy
I1226 14:17:34.273049 97044 net.cpp:228] Setting up accuracy
I1226 14:17:34.273128 97044 net.cpp:235] Top shape: (1)
I1226 14:17:34.273165 97044 net.cpp:243] Memory required for data: 1065888772
I1226 14:17:34.273208 97044 layer_factory.hpp:114] Creating layer loss
I1226 14:17:34.273398 97044 net.cpp:178] Creating Layer loss
I1226 14:17:34.273447 97044 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 14:17:34.273493 97044 net.cpp:612] loss <- label_data_1_split_1
I1226 14:17:34.273555 97044 net.cpp:586] loss -> loss
I1226 14:17:34.273643 97044 layer_factory.hpp:114] Creating layer loss
I1226 14:17:34.315178 97044 net.cpp:228] Setting up loss
I1226 14:17:34.315295 97044 net.cpp:235] Top shape: (1)
I1226 14:17:34.315340 97044 net.cpp:238]     with loss weight 1
I1226 14:17:34.315609 97044 net.cpp:243] Memory required for data: 1065888776
I1226 14:17:34.315665 97044 net.cpp:305] loss needs backward computation.
I1226 14:17:34.315706 97044 net.cpp:307] accuracy does not need backward computation.
I1226 14:17:34.315743 97044 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 14:17:34.315835 97044 net.cpp:305] fc8 needs backward computation.
I1226 14:17:34.315886 97044 net.cpp:305] drop7 needs backward computation.
I1226 14:17:34.315919 97044 net.cpp:305] relu7 needs backward computation.
I1226 14:17:34.315950 97044 net.cpp:305] fc7 needs backward computation.
I1226 14:17:34.315995 97044 net.cpp:305] drop6 needs backward computation.
I1226 14:17:34.316040 97044 net.cpp:305] relu6 needs backward computation.
I1226 14:17:34.316084 97044 net.cpp:305] fc6 needs backward computation.
I1226 14:17:34.316128 97044 net.cpp:305] pool5 needs backward computation.
I1226 14:17:34.316161 97044 net.cpp:305] relu5 needs backward computation.
I1226 14:17:34.316192 97044 net.cpp:305] conv5 needs backward computation.
I1226 14:17:34.316223 97044 net.cpp:305] relu4 needs backward computation.
I1226 14:17:34.316254 97044 net.cpp:305] conv4 needs backward computation.
I1226 14:17:34.316284 97044 net.cpp:305] relu3 needs backward computation.
I1226 14:17:34.316315 97044 net.cpp:305] conv3 needs backward computation.
I1226 14:17:34.316347 97044 net.cpp:305] pool2 needs backward computation.
I1226 14:17:34.316378 97044 net.cpp:305] norm2 needs backward computation.
I1226 14:17:34.316421 97044 net.cpp:305] relu2 needs backward computation.
I1226 14:17:34.316450 97044 net.cpp:305] conv2 needs backward computation.
I1226 14:17:34.316483 97044 net.cpp:305] pool1 needs backward computation.
I1226 14:17:34.316515 97044 net.cpp:305] norm1 needs backward computation.
I1226 14:17:34.316545 97044 net.cpp:305] relu1 needs backward computation.
I1226 14:17:34.316575 97044 net.cpp:305] conv1 needs backward computation.
I1226 14:17:34.316607 97044 net.cpp:307] label_data_1_split does not need backward computation.
I1226 14:17:34.316649 97044 net.cpp:307] data does not need backward computation.
I1226 14:17:34.316678 97044 net.cpp:349] This network produces output accuracy
I1226 14:17:34.316722 97044 net.cpp:349] This network produces output loss
I1226 14:17:34.316844 97044 net.cpp:363] Network initialization done.
I1226 14:17:34.317303 97044 solver.cpp:107] Solver scaffolding done.
I1226 14:17:34.317525 97044 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 14:17:37.847816 95408 caffe.cpp:376] Configuring multinode setup
I1226 14:17:37.849560 95408 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 14:17:38.073364 97473 caffe.cpp:376] Configuring multinode setup
I1226 14:17:38.075104 97473 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 14:17:38.865702 97044 caffe.cpp:376] Configuring multinode setup
I1226 14:17:38.867429 97044 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 14:17:38.867664 97044 SynchronousNode.cpp:675] [7] [proc 7] solving
I1226 14:17:38.882273 93418 SynchronousNode.cpp:675] [0] [proc 0] solving
I1226 14:17:38.867861 97473 SynchronousNode.cpp:675] [2] [proc 2] solving
I1226 14:17:38.881137 98928 SynchronousNode.cpp:675] [6] [proc 6] solving
I1226 14:17:38.880481 95408 SynchronousNode.cpp:675] [4] [proc 4] solving
I1226 14:17:38.879853 92503 SynchronousNode.cpp:675] [3] [proc 3] solving
I1226 14:17:38.890902 91408 SynchronousNode.cpp:675] [5] [proc 5] solving
I1226 14:17:38.867768 97044 solver.cpp:354] Solving AlexNet
I1226 14:17:38.882563 93418 solver.cpp:354] Solving AlexNet
I1226 14:17:38.882607 93418 solver.cpp:355] Learning Rate Policy: step
I1226 14:17:38.868010 97473 solver.cpp:354] Solving AlexNet
I1226 14:17:38.881434 98928 solver.cpp:354] Solving AlexNet
I1226 14:17:38.880625 95408 solver.cpp:354] Solving AlexNet
I1226 14:17:38.880139 92503 solver.cpp:354] Solving AlexNet
I1226 14:17:38.880187 92503 solver.cpp:355] Learning Rate Policy: step
I1226 14:17:38.891182 91408 solver.cpp:354] Solving AlexNet
I1226 14:17:38.891229 91408 solver.cpp:355] Learning Rate Policy: step
I1226 14:17:38.876655 91497 SynchronousNode.cpp:675] [1] [proc 1] solving
I1226 14:17:38.867835 97044 solver.cpp:355] Learning Rate Policy: step
I1226 14:17:38.868062 97473 solver.cpp:355] Learning Rate Policy: step
I1226 14:17:38.881489 98928 solver.cpp:355] Learning Rate Policy: step
I1226 14:17:38.880674 95408 solver.cpp:355] Learning Rate Policy: step
I1226 14:17:38.876961 91497 solver.cpp:354] Solving AlexNet
I1226 14:17:38.877012 91497 solver.cpp:355] Learning Rate Policy: step
I1226 14:17:38.878036 97115 SynchronousNode.cpp:293] [7] Comm thread started 1 0
I1226 14:17:38.891629 95480 SynchronousNode.cpp:293] [4] Comm thread started 0 1
I1226 14:17:38.879763 97545 SynchronousNode.cpp:293] [2] Comm thread started 0 1
I1226 14:17:38.924904 91480 SynchronousNode.cpp:293] [5] Comm thread started 1 0
I1226 14:17:38.912010 91567 SynchronousNode.cpp:293] [1] Comm thread started 1 0
I1226 14:17:38.923208 99002 SynchronousNode.cpp:293] [6] Comm thread started 0 1
I1226 14:17:38.923769 99002 SynchronousNode.cpp:479] [6] initialized root of cluster with nodes: 9 and the total iter size is: 2
I1226 14:17:38.922499 92574 SynchronousNode.cpp:293] [3] Comm thread started 1 0
I1226 14:17:38.911268 97545 SynchronousNode.cpp:479] [2] initialized root of cluster with nodes: 9 and the total iter size is: 2
I1226 14:17:38.928092 93489 SynchronousNode.cpp:293] [0] Comm thread started 0 1
I1226 14:17:38.928690 93489 SynchronousNode.cpp:479] [0] initialized root of cluster with nodes: 9 and the total iter size is: 2
I1226 14:17:38.929441 95480 SynchronousNode.cpp:479] [4] initialized root of cluster with nodes: 9 and the total iter size is: 2
I1226 14:17:39.439669 98928 MultiSolver.cpp:92] [6] PROFILING END[Forward]
I1226 14:17:39.439759 98928 MultiSolver.cpp:94] [6] PROFILING BEGIN[Backward]
I1226 14:17:39.455992 93418 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 14:17:39.456080 93418 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 14:17:39.974491 93418 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 14:17:39.974563 93418 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 14:17:39.974614 93418 solver.cpp:291] [0] Iteration 1, loss = 3.50342
I1226 14:17:39.974680 93418 solver.cpp:317]     Train net output #0: accuracy = 0.320312
I1226 14:17:39.974740 93418 solver.cpp:317]     Train net output #1: loss = 3.50342 (* 1 = 3.50342 loss)
I1226 14:17:39.974632 91567 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 14:17:39.974748 91567 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 14:17:40.132750 98928 MultiSolver.cpp:108] [6] PROFILING END[Backward]
I1226 14:17:40.133499 98928 MultiSolver.cpp:109] [6] PROFILING BEGIN[WaitingToSync]
I1226 14:17:40.133579 98928 solver.cpp:291] [6] Iteration 1, loss = 3.06696
I1226 14:17:40.133646 98928 solver.cpp:317]     Train net output #0: accuracy = 0.359375
I1226 14:17:40.134094 98928 solver.cpp:317]     Train net output #1: loss = 3.06696 (* 1 = 3.06696 loss)
I1226 14:17:40.157193 91497 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 14:17:40.157300 91497 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 14:17:40.475574 91497 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 14:17:40.475658 91497 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 14:17:40.475745 91497 solver.cpp:291] [1] Iteration 1, loss = 3.20193
I1226 14:17:40.475836 91497 solver.cpp:317]     Train net output #0: accuracy = 0.289062
I1226 14:17:40.475911 91497 solver.cpp:317]     Train net output #1: loss = 3.20193 (* 1 = 3.20193 loss)
I1226 14:17:40.913476 91480 SynchronousNode.cpp:246] [5] PROFILING END[WaitingToSync]
I1226 14:17:40.913564 91480 SynchronousNode.cpp:247] [5] PROFILING BEGIN[Forward]
I1226 14:17:41.083586 97115 SynchronousNode.cpp:246] [7] PROFILING END[WaitingToSync]
I1226 14:17:41.083674 97115 SynchronousNode.cpp:247] [7] PROFILING BEGIN[Forward]
I1226 14:17:41.183086 93418 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 14:17:41.183190 93418 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 14:17:41.190510 92574 SynchronousNode.cpp:246] [3] PROFILING END[WaitingToSync]
I1226 14:17:41.190598 92574 SynchronousNode.cpp:247] [3] PROFILING BEGIN[Forward]
I1226 14:17:41.242956 91408 MultiSolver.cpp:92] [5] PROFILING END[Forward]
I1226 14:17:41.243052 91408 MultiSolver.cpp:94] [5] PROFILING BEGIN[Backward]
I1226 14:17:41.371373 92503 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 14:17:41.371454 92503 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 14:17:41.404924 93418 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 14:17:41.404991 93418 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 14:17:41.405046 93418 solver.cpp:291] [0] Iteration 2, loss = 3.49381
I1226 14:17:41.405100 93418 solver.cpp:317]     Train net output #0: accuracy = 0.265625
I1226 14:17:41.405155 93418 solver.cpp:317]     Train net output #1: loss = 3.49381 (* 1 = 3.49381 loss)
I1226 14:17:41.445158 91567 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 14:17:41.445247 91567 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 14:17:41.470762 91497 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 14:17:41.470865 91497 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 14:17:41.608850 91408 MultiSolver.cpp:108] [5] PROFILING END[Backward]
I1226 14:17:41.608939 91408 MultiSolver.cpp:109] [5] PROFILING BEGIN[WaitingToSync]
I1226 14:17:41.608990 91408 solver.cpp:291] [5] Iteration 1, loss = 3.33238
I1226 14:17:41.609077 91408 solver.cpp:317]     Train net output #0: accuracy = 0.28125
I1226 14:17:41.609153 91408 solver.cpp:317]     Train net output #1: loss = 3.33238 (* 1 = 3.33238 loss)
I1226 14:17:41.672729 92503 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 14:17:41.672816 92503 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 14:17:41.672868 92503 solver.cpp:291] [3] Iteration 1, loss = 2.84079
I1226 14:17:41.672957 92503 solver.cpp:317]     Train net output #0: accuracy = 0.445312
I1226 14:17:41.673040 92503 solver.cpp:317]     Train net output #1: loss = 2.84079 (* 1 = 2.84079 loss)
I1226 14:17:41.749330 95408 MultiSolver.cpp:92] [4] PROFILING END[Forward]
I1226 14:17:41.749446 95408 MultiSolver.cpp:94] [4] PROFILING BEGIN[Backward]
I1226 14:17:41.746836 91497 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 14:17:41.746958 91497 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 14:17:41.747011 91497 solver.cpp:291] [1] Iteration 2, loss = 3.53407
I1226 14:17:41.747084 91497 solver.cpp:317]     Train net output #0: accuracy = 0.242188
I1226 14:17:41.747167 91497 solver.cpp:317]     Train net output #1: loss = 3.53407 (* 1 = 3.53407 loss)
I1226 14:17:42.071099 97473 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 14:17:42.071216 97473 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 14:17:42.209635 93418 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 14:17:42.209743 93418 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 14:17:42.280439 97044 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 14:17:42.280562 97044 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 14:17:42.437259 91567 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 14:17:42.437360 91567 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 14:17:42.457178 93418 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 14:17:42.457257 93418 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 14:17:42.457295 93418 solver.cpp:291] [0] Iteration 3, loss = 3.14405
I1226 14:17:42.457350 93418 solver.cpp:317]     Train net output #0: accuracy = 0.28125
I1226 14:17:42.457427 93418 solver.cpp:317]     Train net output #1: loss = 3.14405 (* 1 = 3.14405 loss)
I1226 14:17:42.461787 91497 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 14:17:42.461930 91497 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 14:17:42.868341 91497 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 14:17:42.868432 91497 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 14:17:42.868515 91497 solver.cpp:291] [1] Iteration 3, loss = 3.62657
I1226 14:17:42.868589 91497 solver.cpp:317]     Train net output #0: accuracy = 0.257812
I1226 14:17:42.868674 91497 solver.cpp:317]     Train net output #1: loss = 3.62657 (* 1 = 3.62657 loss)
I1226 14:17:43.268599 93418 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 14:17:43.268687 93418 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 14:17:43.437261 91567 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 14:17:43.437356 91567 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 14:17:43.468760 91497 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 14:17:43.468858 91497 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 14:17:43.528926 93418 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 14:17:43.529001 93418 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 14:17:43.529033 93418 solver.cpp:291] [0] Iteration 4, loss = 3.3824
I1226 14:17:43.529088 93418 solver.cpp:317]     Train net output #0: accuracy = 0.242188
I1226 14:17:43.529139 93418 solver.cpp:317]     Train net output #1: loss = 3.3824 (* 1 = 3.3824 loss)
I1226 14:17:43.878728 91497 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 14:17:43.878835 91497 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 14:17:43.878885 91497 solver.cpp:291] [1] Iteration 4, loss = 3.03431
I1226 14:17:43.879333 91497 solver.cpp:317]     Train net output #0: accuracy = 0.320312
I1226 14:17:43.879624 91497 solver.cpp:317]     Train net output #1: loss = 3.03431 (* 1 = 3.03431 loss)
I1226 14:17:44.164626 97044 MultiSolver.cpp:108] [7] PROFILING END[Backward]
I1226 14:17:44.164700 97044 MultiSolver.cpp:109] [7] PROFILING BEGIN[WaitingToSync]
I1226 14:17:44.164746 97044 solver.cpp:291] [7] Iteration 1, loss = 3.38077
I1226 14:17:44.164949 97044 solver.cpp:317]     Train net output #0: accuracy = 0.28125
I1226 14:17:44.165029 97044 solver.cpp:317]     Train net output #1: loss = 3.38077 (* 1 = 3.38077 loss)
I1226 14:17:44.233254 93418 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 14:17:44.233351 93418 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 14:17:44.294085 98928 MultiSolver.cpp:92] [6] PROFILING END[Forward]
I1226 14:17:44.294190 98928 MultiSolver.cpp:94] [6] PROFILING BEGIN[Backward]
I1226 14:17:44.300228 97115 SynchronousNode.cpp:246] [7] PROFILING END[WaitingToSync]
I1226 14:17:44.300309 97115 SynchronousNode.cpp:247] [7] PROFILING BEGIN[Forward]
I1226 14:17:44.377130 93418 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 14:17:44.377202 93418 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 14:17:44.377261 93418 solver.cpp:291] [0] Iteration 5, loss = 3.50264
I1226 14:17:44.377316 93418 solver.cpp:317]     Train net output #0: accuracy = 0.265625
I1226 14:17:44.377367 93418 solver.cpp:317]     Train net output #1: loss = 3.50264 (* 1 = 3.50264 loss)
I1226 14:17:44.430860 91567 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 14:17:44.430946 91567 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 14:17:44.464426 98928 MultiSolver.cpp:108] [6] PROFILING END[Backward]
I1226 14:17:44.464529 98928 MultiSolver.cpp:109] [6] PROFILING BEGIN[WaitingToSync]
I1226 14:17:44.464568 98928 solver.cpp:291] [6] Iteration 2, loss = 3.19998
I1226 14:17:44.464634 98928 solver.cpp:317]     Train net output #0: accuracy = 0.335938
I1226 14:17:44.464699 98928 solver.cpp:317]     Train net output #1: loss = 3.19998 (* 1 = 3.19998 loss)
I1226 14:17:44.472939 91497 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 14:17:44.473016 91497 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 14:17:44.894297 91497 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 14:17:44.894399 91497 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 14:17:44.894747 91497 solver.cpp:291] [1] Iteration 5, loss = 3.74813
I1226 14:17:44.894846 91497 solver.cpp:317]     Train net output #0: accuracy = 0.273438
I1226 14:17:44.894947 91497 solver.cpp:317]     Train net output #1: loss = 3.74813 (* 1 = 3.74813 loss)
I1226 14:17:45.206441 93418 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 14:17:45.206535 93418 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 14:17:45.351881 93418 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 14:17:45.351954 93418 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 14:17:45.352017 93418 solver.cpp:291] [0] Iteration 6, loss = 3.57451
I1226 14:17:45.352072 93418 solver.cpp:317]     Train net output #0: accuracy = 0.296875
I1226 14:17:45.352121 93418 solver.cpp:317]     Train net output #1: loss = 3.57451 (* 1 = 3.57451 loss)
I1226 14:17:45.438531 91567 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 14:17:45.438614 91567 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 14:17:45.452965 95408 MultiSolver.cpp:108] [4] PROFILING END[Backward]
I1226 14:17:45.453047 95408 MultiSolver.cpp:109] [4] PROFILING BEGIN[WaitingToSync]
I1226 14:17:45.453093 95408 solver.cpp:291] [4] Iteration 1, loss = 3.34232
I1226 14:17:45.453251 95408 solver.cpp:317]     Train net output #0: accuracy = 0.265625
I1226 14:17:45.453323 95408 solver.cpp:317]     Train net output #1: loss = 3.34232 (* 1 = 3.34232 loss)
I1226 14:17:45.496520 91497 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 14:17:45.496608 91497 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 14:17:45.734648 97044 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 14:17:45.734760 97044 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 14:17:45.932435 97473 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 14:17:45.932528 97473 MultiSolver.cpp:109] [2] PROFILING BEGIN[WaitingToSync]
I1226 14:17:45.932571 97473 solver.cpp:291] [2] Iteration 1, loss = 3.37075
I1226 14:17:45.932762 97473 solver.cpp:317]     Train net output #0: accuracy = 0.296875
I1226 14:17:45.932844 97473 solver.cpp:317]     Train net output #1: loss = 3.37075 (* 1 = 3.37075 loss)
I1226 14:17:45.993397 91497 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 14:17:45.993485 91497 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 14:17:45.993538 91497 solver.cpp:291] [1] Iteration 6, loss = 3.21805
I1226 14:17:45.993610 91497 solver.cpp:317]     Train net output #0: accuracy = 0.328125
I1226 14:17:45.993687 91497 solver.cpp:317]     Train net output #1: loss = 3.21805 (* 1 = 3.21805 loss)
I1226 14:17:46.437963 93418 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 14:17:46.438071 93418 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 14:17:46.582742 93418 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 14:17:46.582815 93418 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 14:17:46.582936 93418 solver.cpp:291] [0] Iteration 7, loss = 3.06173
I1226 14:17:46.582993 93418 solver.cpp:317]     Train net output #0: accuracy = 0.320312
I1226 14:17:46.583041 93418 solver.cpp:317]     Train net output #1: loss = 3.06173 (* 1 = 3.06173 loss)
I1226 14:17:46.627836 91567 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 14:17:46.627918 91567 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 14:17:46.687304 91497 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 14:17:46.687382 91497 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 14:17:47.135943 91497 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 14:17:47.136023 91497 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 14:17:47.136070 91497 solver.cpp:291] [1] Iteration 7, loss = 3.31183
I1226 14:17:47.136132 91497 solver.cpp:317]     Train net output #0: accuracy = 0.304688
I1226 14:17:47.136193 91497 solver.cpp:317]     Train net output #1: loss = 3.31183 (* 1 = 3.31183 loss)
I1226 14:17:47.496193 93418 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 14:17:47.496294 93418 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 14:17:47.551944 97044 MultiSolver.cpp:108] [7] PROFILING END[Backward]
I1226 14:17:47.552023 97044 MultiSolver.cpp:109] [7] PROFILING BEGIN[WaitingToSync]
I1226 14:17:47.552114 97044 solver.cpp:291] [7] Iteration 2, loss = 3.35656
I1226 14:17:47.552187 97044 solver.cpp:317]     Train net output #0: accuracy = 0.273438
I1226 14:17:47.552266 97044 solver.cpp:317]     Train net output #1: loss = 3.35656 (* 1 = 3.35656 loss)
I1226 14:17:47.645236 93418 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 14:17:47.645323 93418 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 14:17:47.645366 93418 solver.cpp:291] [0] Iteration 8, loss = 3.55791
I1226 14:17:47.645470 93418 solver.cpp:317]     Train net output #0: accuracy = 0.273438
I1226 14:17:47.645581 93418 solver.cpp:317]     Train net output #1: loss = 3.55791 (* 1 = 3.55791 loss)
I1226 14:17:47.680971 98928 MultiSolver.cpp:92] [6] PROFILING END[Forward]
I1226 14:17:47.681087 98928 MultiSolver.cpp:94] [6] PROFILING BEGIN[Backward]
I1226 14:17:47.675740 97115 SynchronousNode.cpp:246] [7] PROFILING END[WaitingToSync]
I1226 14:17:47.675846 97115 SynchronousNode.cpp:247] [7] PROFILING BEGIN[Forward]
I1226 14:17:47.686533 91567 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 14:17:47.686619 91567 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 14:17:47.722378 91497 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 14:17:47.722519 91497 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 14:17:47.851016 98928 MultiSolver.cpp:108] [6] PROFILING END[Backward]
I1226 14:17:47.851128 98928 MultiSolver.cpp:109] [6] PROFILING BEGIN[WaitingToSync]
I1226 14:17:47.851172 98928 solver.cpp:291] [6] Iteration 3, loss = 3.24896
I1226 14:17:47.851235 98928 solver.cpp:317]     Train net output #0: accuracy = 0.273438
I1226 14:17:47.851313 98928 solver.cpp:317]     Train net output #1: loss = 3.24896 (* 1 = 3.24896 loss)
I1226 14:17:48.143503 91497 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 14:17:48.143656 91497 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 14:17:48.143929 91497 solver.cpp:291] [1] Iteration 8, loss = 3.41654
I1226 14:17:48.144132 91497 solver.cpp:317]     Train net output #0: accuracy = 0.28125
I1226 14:17:48.144220 91497 solver.cpp:317]     Train net output #1: loss = 3.41654 (* 1 = 3.41654 loss)
I1226 14:17:48.538179 95408 MultiSolver.cpp:92] [4] PROFILING END[Forward]
I1226 14:17:48.539286 95408 MultiSolver.cpp:94] [4] PROFILING BEGIN[Backward]
I1226 14:17:48.584169 93418 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 14:17:48.584270 93418 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 14:17:48.693117 97473 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 14:17:48.697497 97473 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 14:17:48.730079 93418 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 14:17:48.730173 93418 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 14:17:48.730208 93418 solver.cpp:291] [0] Iteration 9, loss = 3.74733
I1226 14:17:48.730262 93418 solver.cpp:317]     Train net output #0: accuracy = 0.234375
I1226 14:17:48.730314 93418 solver.cpp:317]     Train net output #1: loss = 3.74733 (* 1 = 3.74733 loss)
I1226 14:17:48.824455 91567 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 14:17:48.824533 91567 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 14:17:48.847556 91497 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 14:17:48.847659 91497 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 14:17:49.292898 91497 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 14:17:49.292986 91497 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 14:17:49.293038 91497 solver.cpp:291] [1] Iteration 9, loss = 3.11615
I1226 14:17:49.293114 91497 solver.cpp:317]     Train net output #0: accuracy = 0.351562
I1226 14:17:49.293189 91497 solver.cpp:317]     Train net output #1: loss = 3.11615 (* 1 = 3.11615 loss)
I1226 14:17:49.295045 97044 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 14:17:49.296510 97044 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 14:17:49.623982 93418 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 14:17:49.624084 93418 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 14:17:49.677253 91480 SynchronousNode.cpp:246] [5] PROFILING END[WaitingToSync]
I1226 14:17:49.677362 91480 SynchronousNode.cpp:247] [5] PROFILING BEGIN[Forward]
I1226 14:17:49.700788 91408 MultiSolver.cpp:92] [5] PROFILING END[Forward]
I1226 14:17:49.700909 91408 MultiSolver.cpp:94] [5] PROFILING BEGIN[Backward]
I1226 14:17:49.768491 93418 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 14:17:49.768563 93418 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 14:17:49.768596 93418 solver.cpp:291] [0] Iteration 10, loss = 2.98189
I1226 14:17:49.768649 93418 solver.cpp:317]     Train net output #0: accuracy = 0.351562
I1226 14:17:49.768698 93418 solver.cpp:317]     Train net output #1: loss = 2.98189 (* 1 = 2.98189 loss)
I1226 14:17:49.786947 92574 SynchronousNode.cpp:246] [3] PROFILING END[WaitingToSync]
I1226 14:17:49.787034 92574 SynchronousNode.cpp:247] [3] PROFILING BEGIN[Forward]
I1226 14:17:49.809094 92503 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 14:17:49.809216 92503 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 14:17:49.829283 91567 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 14:17:49.829370 91567 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 14:17:49.859683 91497 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 14:17:49.859855 91497 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 14:17:49.938916 91408 MultiSolver.cpp:108] [5] PROFILING END[Backward]
I1226 14:17:49.939002 91408 MultiSolver.cpp:109] [5] PROFILING BEGIN[WaitingToSync]
I1226 14:17:49.939054 91408 solver.cpp:291] [5] Iteration 2, loss = 3.0963
I1226 14:17:49.939127 91408 solver.cpp:317]     Train net output #0: accuracy = 0.328125
I1226 14:17:49.939203 91408 solver.cpp:317]     Train net output #1: loss = 3.0963 (* 1 = 3.0963 loss)
I1226 14:17:50.034752 92503 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 14:17:50.034860 92503 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 14:17:50.034916 92503 solver.cpp:291] [3] Iteration 2, loss = 3.17131
I1226 14:17:50.034989 92503 solver.cpp:317]     Train net output #0: accuracy = 0.304688
I1226 14:17:50.035066 92503 solver.cpp:317]     Train net output #1: loss = 3.17131 (* 1 = 3.17131 loss)
I1226 14:17:50.261672 91497 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 14:17:50.261801 91497 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 14:17:50.261852 91497 solver.cpp:291] [1] Iteration 10, loss = 3.37299
I1226 14:17:50.261922 91497 solver.cpp:317]     Train net output #0: accuracy = 0.257812
I1226 14:17:50.274814 91497 solver.cpp:317]     Train net output #1: loss = 3.37299 (* 1 = 3.37299 loss)
I1226 14:17:50.676244 93418 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 14:17:50.676343 93418 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 14:17:50.821516 93418 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 14:17:50.821595 93418 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 14:17:50.821630 93418 solver.cpp:291] [0] Iteration 11, loss = 3.18246
I1226 14:17:50.821684 93418 solver.cpp:317]     Train net output #0: accuracy = 0.234375
I1226 14:17:50.821735 93418 solver.cpp:317]     Train net output #1: loss = 3.18246 (* 1 = 3.18246 loss)
I1226 14:17:50.908841 91567 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 14:17:50.908922 91567 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 14:17:50.932034 91497 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 14:17:50.932234 91497 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 14:17:50.948910 95408 MultiSolver.cpp:108] [4] PROFILING END[Backward]
I1226 14:17:50.948999 95408 MultiSolver.cpp:109] [4] PROFILING BEGIN[WaitingToSync]
I1226 14:17:50.949045 95408 solver.cpp:291] [4] Iteration 2, loss = 3.5267
I1226 14:17:50.949116 95408 solver.cpp:317]     Train net output #0: accuracy = 0.242188
I1226 14:17:50.949192 95408 solver.cpp:317]     Train net output #1: loss = 3.5267 (* 1 = 3.5267 loss)
I1226 14:17:51.053921 97473 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 14:17:51.054020 97473 MultiSolver.cpp:109] [2] PROFILING BEGIN[WaitingToSync]
I1226 14:17:51.054067 97473 solver.cpp:291] [2] Iteration 2, loss = 3.23915
I1226 14:17:51.054147 97473 solver.cpp:317]     Train net output #0: accuracy = 0.320312
I1226 14:17:51.054231 97473 solver.cpp:317]     Train net output #1: loss = 3.23915 (* 1 = 3.23915 loss)
I1226 14:17:51.140342 97044 MultiSolver.cpp:108] [7] PROFILING END[Backward]
I1226 14:17:51.140424 97044 MultiSolver.cpp:109] [7] PROFILING BEGIN[WaitingToSync]
I1226 14:17:51.140508 97044 solver.cpp:291] [7] Iteration 3, loss = 2.90813
I1226 14:17:51.140573 97044 solver.cpp:317]     Train net output #0: accuracy = 0.328125
I1226 14:17:51.140648 97044 solver.cpp:317]     Train net output #1: loss = 2.90813 (* 1 = 2.90813 loss)
I1226 14:17:51.268863 98928 MultiSolver.cpp:92] [6] PROFILING END[Forward]
I1226 14:17:51.268957 98928 MultiSolver.cpp:94] [6] PROFILING BEGIN[Backward]
I1226 14:17:51.273576 97115 SynchronousNode.cpp:246] [7] PROFILING END[WaitingToSync]
I1226 14:17:51.273661 97115 SynchronousNode.cpp:247] [7] PROFILING BEGIN[Forward]
I1226 14:17:51.397157 91497 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 14:17:51.397676 91497 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 14:17:51.398092 91497 solver.cpp:291] [1] Iteration 11, loss = 3.24099
I1226 14:17:51.398180 91497 solver.cpp:317]     Train net output #0: accuracy = 0.289062
I1226 14:17:51.398262 91497 solver.cpp:317]     Train net output #1: loss = 3.24099 (* 1 = 3.24099 loss)
I1226 14:17:51.437888 98928 MultiSolver.cpp:108] [6] PROFILING END[Backward]
I1226 14:17:51.437970 98928 MultiSolver.cpp:109] [6] PROFILING BEGIN[WaitingToSync]
I1226 14:17:51.438014 98928 solver.cpp:291] [6] Iteration 4, loss = 3.2514
I1226 14:17:51.438107 98928 solver.cpp:317]     Train net output #0: accuracy = 0.289062
I1226 14:17:51.438182 98928 solver.cpp:317]     Train net output #1: loss = 3.2514 (* 1 = 3.2514 loss)
I1226 14:17:51.699159 93418 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 14:17:51.699256 93418 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 14:17:51.842629 93418 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 14:17:51.842702 93418 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 14:17:51.842736 93418 solver.cpp:291] [0] Iteration 12, loss = 3.57909
I1226 14:17:51.842789 93418 solver.cpp:317]     Train net output #0: accuracy = 0.265625
I1226 14:17:51.842839 93418 solver.cpp:317]     Train net output #1: loss = 3.57909 (* 1 = 3.57909 loss)
I1226 14:17:51.895570 91567 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 14:17:51.895653 91567 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 14:17:51.919505 91497 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 14:17:51.919608 91497 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 14:17:52.351320 91497 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 14:17:52.351433 91497 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 14:17:52.351671 91497 solver.cpp:291] [1] Iteration 12, loss = 3.3578
I1226 14:17:52.351805 91497 solver.cpp:317]     Train net output #0: accuracy = 0.296875
I1226 14:17:52.352368 91497 solver.cpp:317]     Train net output #1: loss = 3.3578 (* 1 = 3.3578 loss)
I1226 14:17:52.726752 93418 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 14:17:52.726848 93418 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 14:17:52.714509 97044 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 14:17:52.714625 97044 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 14:17:52.871481 93418 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 14:17:52.871553 93418 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 14:17:52.871587 93418 solver.cpp:291] [0] Iteration 13, loss = 3.62536
I1226 14:17:52.871640 93418 solver.cpp:317]     Train net output #0: accuracy = 0.265625
I1226 14:17:52.871690 93418 solver.cpp:317]     Train net output #1: loss = 3.62536 (* 1 = 3.62536 loss)
I1226 14:17:52.909518 91567 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 14:17:52.909601 91567 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 14:17:52.935508 91497 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 14:17:52.935611 91497 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 14:17:53.348104 91497 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 14:17:53.348187 91497 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 14:17:53.348877 91497 solver.cpp:291] [1] Iteration 13, loss = 3.21502
I1226 14:17:53.348999 91497 solver.cpp:317]     Train net output #0: accuracy = 0.335938
I1226 14:17:53.349314 91497 solver.cpp:317]     Train net output #1: loss = 3.21502 (* 1 = 3.21502 loss)
I1226 14:17:53.866868 93418 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 14:17:53.866979 93418 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 14:17:53.890808 97473 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 14:17:53.890926 97473 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 14:17:53.910221 95408 MultiSolver.cpp:92] [4] PROFILING END[Forward]
I1226 14:17:53.910336 95408 MultiSolver.cpp:94] [4] PROFILING BEGIN[Backward]
I1226 14:17:54.010344 93418 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 14:17:54.010457 93418 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 14:17:54.010571 93418 solver.cpp:291] [0] Iteration 14, loss = 3.24391
I1226 14:17:54.010627 93418 solver.cpp:317]     Train net output #0: accuracy = 0.28125
I1226 14:17:54.010679 93418 solver.cpp:317]     Train net output #1: loss = 3.24391 (* 1 = 3.24391 loss)
I1226 14:17:54.008026 91567 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 14:17:54.008121 91567 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 14:17:54.032424 91497 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 14:17:54.032531 91497 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 14:17:54.417613 91480 SynchronousNode.cpp:246] [5] PROFILING END[WaitingToSync]
I1226 14:17:54.417703 91480 SynchronousNode.cpp:247] [5] PROFILING BEGIN[Forward]
I1226 14:17:54.408433 92574 SynchronousNode.cpp:246] [3] PROFILING END[WaitingToSync]
I1226 14:17:54.408519 92574 SynchronousNode.cpp:247] [3] PROFILING BEGIN[Forward]
I1226 14:17:54.439680 91408 MultiSolver.cpp:92] [5] PROFILING END[Forward]
I1226 14:17:54.439790 91408 MultiSolver.cpp:94] [5] PROFILING BEGIN[Backward]
I1226 14:17:54.434238 92503 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 14:17:54.434388 92503 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 14:17:54.470875 91497 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 14:17:54.470962 91497 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 14:17:54.471014 91497 solver.cpp:291] [1] Iteration 14, loss = 3.28253
I1226 14:17:54.471089 91497 solver.cpp:317]     Train net output #0: accuracy = 0.328125
I1226 14:17:54.471165 91497 solver.cpp:317]     Train net output #1: loss = 3.28253 (* 1 = 3.28253 loss)
I1226 14:17:54.489733 97044 MultiSolver.cpp:108] [7] PROFILING END[Backward]
I1226 14:17:54.489848 97044 MultiSolver.cpp:109] [7] PROFILING BEGIN[WaitingToSync]
I1226 14:17:54.489892 97044 solver.cpp:291] [7] Iteration 4, loss = 3.21879
I1226 14:17:54.489954 97044 solver.cpp:317]     Train net output #0: accuracy = 0.28125
I1226 14:17:54.490167 97044 solver.cpp:317]     Train net output #1: loss = 3.21879 (* 1 = 3.21879 loss)
I1226 14:17:54.677106 91408 MultiSolver.cpp:108] [5] PROFILING END[Backward]
I1226 14:17:54.677196 91408 MultiSolver.cpp:109] [5] PROFILING BEGIN[WaitingToSync]
I1226 14:17:54.677245 91408 solver.cpp:291] [5] Iteration 3, loss = 3.06942
I1226 14:17:54.677357 91408 solver.cpp:317]     Train net output #0: accuracy = 0.351562
I1226 14:17:54.677435 91408 solver.cpp:317]     Train net output #1: loss = 3.06942 (* 1 = 3.06942 loss)
I1226 14:17:54.673038 92503 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 14:17:54.673128 92503 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 14:17:54.673183 92503 solver.cpp:291] [3] Iteration 3, loss = 3.08766
I1226 14:17:54.673259 92503 solver.cpp:317]     Train net output #0: accuracy = 0.335938
I1226 14:17:54.673363 92503 solver.cpp:317]     Train net output #1: loss = 3.08766 (* 1 = 3.08766 loss)
I1226 14:17:54.739464 98928 MultiSolver.cpp:92] [6] PROFILING END[Forward]
I1226 14:17:54.739573 98928 MultiSolver.cpp:94] [6] PROFILING BEGIN[Backward]
I1226 14:17:54.764140 97115 SynchronousNode.cpp:246] [7] PROFILING END[WaitingToSync]
I1226 14:17:54.764228 97115 SynchronousNode.cpp:247] [7] PROFILING BEGIN[Forward]
I1226 14:17:54.798151 93418 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 14:17:54.798247 93418 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 14:17:54.896524 98928 MultiSolver.cpp:108] [6] PROFILING END[Backward]
I1226 14:17:54.896605 98928 MultiSolver.cpp:109] [6] PROFILING BEGIN[WaitingToSync]
I1226 14:17:54.896647 98928 solver.cpp:291] [6] Iteration 5, loss = 3.36605
I1226 14:17:54.896713 98928 solver.cpp:317]     Train net output #0: accuracy = 0.289062
I1226 14:17:54.896778 98928 solver.cpp:317]     Train net output #1: loss = 3.36605 (* 1 = 3.36605 loss)
I1226 14:17:54.940654 93418 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 14:17:54.940727 93418 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 14:17:54.940762 93418 solver.cpp:291] [0] Iteration 15, loss = 3.15745
I1226 14:17:54.940815 93418 solver.cpp:317]     Train net output #0: accuracy = 0.375
I1226 14:17:54.940865 93418 solver.cpp:317]     Train net output #1: loss = 3.15745 (* 1 = 3.15745 loss)
I1226 14:17:55.015219 91567 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 14:17:55.015308 91567 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 14:17:55.048249 91497 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 14:17:55.048357 91497 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 14:17:55.463575 91497 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 14:17:55.463677 91497 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 14:17:55.463770 91497 solver.cpp:291] [1] Iteration 15, loss = 3.25234
I1226 14:17:55.463865 91497 solver.cpp:317]     Train net output #0: accuracy = 0.289062
I1226 14:17:55.476840 91497 solver.cpp:317]     Train net output #1: loss = 3.25234 (* 1 = 3.25234 loss)
I1226 14:17:55.756054 95408 MultiSolver.cpp:108] [4] PROFILING END[Backward]
I1226 14:17:55.756139 95408 MultiSolver.cpp:109] [4] PROFILING BEGIN[WaitingToSync]
I1226 14:17:55.756184 95408 solver.cpp:291] [4] Iteration 3, loss = 2.95265
I1226 14:17:55.756258 95408 solver.cpp:317]     Train net output #0: accuracy = 0.351562
I1226 14:17:55.756333 95408 solver.cpp:317]     Train net output #1: loss = 2.95265 (* 1 = 2.95265 loss)
I1226 14:17:55.778841 97473 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 14:17:55.778928 97473 MultiSolver.cpp:109] [2] PROFILING BEGIN[WaitingToSync]
I1226 14:17:55.778971 97473 solver.cpp:291] [2] Iteration 3, loss = 3.11756
I1226 14:17:55.779038 97473 solver.cpp:317]     Train net output #0: accuracy = 0.328125
I1226 14:17:55.779115 97473 solver.cpp:317]     Train net output #1: loss = 3.11756 (* 1 = 3.11756 loss)
I1226 14:17:55.806428 93418 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 14:17:55.806527 93418 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 14:17:55.948698 93418 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 14:17:55.948771 93418 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 14:17:55.948809 93418 solver.cpp:291] [0] Iteration 16, loss = 3.35073
I1226 14:17:55.948863 93418 solver.cpp:317]     Train net output #0: accuracy = 0.3125
I1226 14:17:55.948915 93418 solver.cpp:317]     Train net output #1: loss = 3.35073 (* 1 = 3.35073 loss)
I1226 14:17:56.042135 91567 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 14:17:56.042218 91567 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 14:17:56.077344 91497 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 14:17:56.077455 91497 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 14:17:56.105595 97044 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 14:17:56.105711 97044 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 14:17:56.462589 91497 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 14:17:56.462677 91497 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 14:17:56.462759 91497 solver.cpp:291] [1] Iteration 16, loss = 3.17741
I1226 14:17:56.462837 91497 solver.cpp:317]     Train net output #0: accuracy = 0.3125
I1226 14:17:56.462923 91497 solver.cpp:317]     Train net output #1: loss = 3.17741 (* 1 = 3.17741 loss)
I1226 14:17:56.899499 93418 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 14:17:56.899603 93418 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 14:17:57.043606 93418 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 14:17:57.043680 93418 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 14:17:57.043715 93418 solver.cpp:291] [0] Iteration 17, loss = 3.23326
I1226 14:17:57.043771 93418 solver.cpp:317]     Train net output #0: accuracy = 0.320312
I1226 14:17:57.043822 93418 solver.cpp:317]     Train net output #1: loss = 3.23326 (* 1 = 3.23326 loss)
I1226 14:17:57.137832 91567 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 14:17:57.138003 91567 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 14:17:57.170184 91497 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 14:17:57.170264 91497 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 14:17:57.555238 91497 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 14:17:57.555317 91497 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 14:17:57.555364 91497 solver.cpp:291] [1] Iteration 17, loss = 3.34161
I1226 14:17:57.555426 91497 solver.cpp:317]     Train net output #0: accuracy = 0.296875
I1226 14:17:57.555487 91497 solver.cpp:317]     Train net output #1: loss = 3.34161 (* 1 = 3.34161 loss)
I1226 14:17:57.816864 97473 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 14:17:57.816983 97473 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 14:17:57.882516 97044 MultiSolver.cpp:108] [7] PROFILING END[Backward]
I1226 14:17:57.882596 97044 MultiSolver.cpp:109] [7] PROFILING BEGIN[WaitingToSync]
I1226 14:17:57.882637 97044 solver.cpp:291] [7] Iteration 5, loss = 3.22162
I1226 14:17:57.882701 97044 solver.cpp:317]     Train net output #0: accuracy = 0.328125
I1226 14:17:57.882768 97044 solver.cpp:317]     Train net output #1: loss = 3.22162 (* 1 = 3.22162 loss)
I1226 14:17:57.927196 93418 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 14:17:57.927299 93418 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 14:17:57.943028 95408 MultiSolver.cpp:92] [4] PROFILING END[Forward]
I1226 14:17:57.943145 95408 MultiSolver.cpp:94] [4] PROFILING BEGIN[Backward]
I1226 14:17:58.009583 98928 MultiSolver.cpp:92] [6] PROFILING END[Forward]
I1226 14:17:58.009681 98928 MultiSolver.cpp:94] [6] PROFILING BEGIN[Backward]
I1226 14:17:58.070828 93418 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 14:17:58.070904 93418 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 14:17:58.070938 93418 solver.cpp:291] [0] Iteration 18, loss = 3.25699
I1226 14:17:58.070993 93418 solver.cpp:317]     Train net output #0: accuracy = 0.296875
I1226 14:17:58.071043 93418 solver.cpp:317]     Train net output #1: loss = 3.25699 (* 1 = 3.25699 loss)
I1226 14:17:58.087071 97115 SynchronousNode.cpp:246] [7] PROFILING END[WaitingToSync]
I1226 14:17:58.087157 97115 SynchronousNode.cpp:247] [7] PROFILING BEGIN[Forward]
I1226 14:17:58.162081 91567 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 14:17:58.162165 91567 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 14:17:58.181283 98928 MultiSolver.cpp:108] [6] PROFILING END[Backward]
I1226 14:17:58.181365 98928 MultiSolver.cpp:109] [6] PROFILING BEGIN[WaitingToSync]
I1226 14:17:58.181408 98928 solver.cpp:291] [6] Iteration 6, loss = 3.23811
I1226 14:17:58.181475 98928 solver.cpp:317]     Train net output #0: accuracy = 0.351562
I1226 14:17:58.181541 98928 solver.cpp:317]     Train net output #1: loss = 3.23811 (* 1 = 3.23811 loss)
I1226 14:17:58.189841 91497 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 14:17:58.189975 91497 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 14:17:58.375437 92574 SynchronousNode.cpp:246] [3] PROFILING END[WaitingToSync]
I1226 14:17:58.375526 92574 SynchronousNode.cpp:247] [3] PROFILING BEGIN[Forward]
I1226 14:17:58.402632 92503 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 14:17:58.402760 92503 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 14:17:58.531929 91480 SynchronousNode.cpp:246] [5] PROFILING END[WaitingToSync]
I1226 14:17:58.532011 91480 SynchronousNode.cpp:247] [5] PROFILING BEGIN[Forward]
I1226 14:17:58.557879 91408 MultiSolver.cpp:92] [5] PROFILING END[Forward]
I1226 14:17:58.558002 91408 MultiSolver.cpp:94] [5] PROFILING BEGIN[Backward]
I1226 14:17:58.572434 91497 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 14:17:58.572540 91497 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 14:17:58.572594 91497 solver.cpp:291] [1] Iteration 18, loss = 3.79699
I1226 14:17:58.572890 91497 solver.cpp:317]     Train net output #0: accuracy = 0.203125
I1226 14:17:58.573129 91497 solver.cpp:317]     Train net output #1: loss = 3.79699 (* 1 = 3.79699 loss)
I1226 14:17:58.634536 92503 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 14:17:58.634624 92503 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 14:17:58.634673 92503 solver.cpp:291] [3] Iteration 4, loss = 3.36962
I1226 14:17:58.634749 92503 solver.cpp:317]     Train net output #0: accuracy = 0.3125
I1226 14:17:58.634837 92503 solver.cpp:317]     Train net output #1: loss = 3.36962 (* 1 = 3.36962 loss)
I1226 14:17:58.795982 91408 MultiSolver.cpp:108] [5] PROFILING END[Backward]
I1226 14:17:58.796073 91408 MultiSolver.cpp:109] [5] PROFILING BEGIN[WaitingToSync]
I1226 14:17:58.796123 91408 solver.cpp:291] [5] Iteration 4, loss = 2.96321
I1226 14:17:58.796197 91408 solver.cpp:317]     Train net output #0: accuracy = 0.328125
I1226 14:17:58.796273 91408 solver.cpp:317]     Train net output #1: loss = 2.96321 (* 1 = 2.96321 loss)
I1226 14:17:58.925397 93418 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 14:17:58.925501 93418 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 14:17:59.067347 93418 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 14:17:59.067451 93418 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 14:17:59.067486 93418 solver.cpp:291] [0] Iteration 19, loss = 3.05277
I1226 14:17:59.067540 93418 solver.cpp:317]     Train net output #0: accuracy = 0.28125
I1226 14:17:59.067591 93418 solver.cpp:317]     Train net output #1: loss = 3.05277 (* 1 = 3.05277 loss)
I1226 14:17:59.188796 91567 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 14:17:59.188880 91567 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 14:17:59.225455 91497 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 14:17:59.225582 91497 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 14:17:59.462146 97044 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 14:17:59.462260 97044 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 14:17:59.594871 97473 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 14:17:59.594959 97473 MultiSolver.cpp:109] [2] PROFILING BEGIN[WaitingToSync]
I1226 14:17:59.595005 97473 solver.cpp:291] [2] Iteration 4, loss = 3.39074
I1226 14:17:59.595075 97473 solver.cpp:317]     Train net output #0: accuracy = 0.304688
I1226 14:17:59.595151 97473 solver.cpp:317]     Train net output #1: loss = 3.39074 (* 1 = 3.39074 loss)
I1226 14:17:59.677822 91497 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 14:17:59.677911 91497 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 14:17:59.677958 91497 solver.cpp:291] [1] Iteration 19, loss = 3.29966
I1226 14:17:59.678035 91497 solver.cpp:317]     Train net output #0: accuracy = 0.273438
I1226 14:17:59.678110 91497 solver.cpp:317]     Train net output #1: loss = 3.29966 (* 1 = 3.29966 loss)
I1226 14:17:59.824559 95408 MultiSolver.cpp:108] [4] PROFILING END[Backward]
I1226 14:17:59.824652 95408 MultiSolver.cpp:109] [4] PROFILING BEGIN[WaitingToSync]
I1226 14:17:59.824699 95408 solver.cpp:291] [4] Iteration 4, loss = 3.33401
I1226 14:17:59.824767 95408 solver.cpp:317]     Train net output #0: accuracy = 0.296875
I1226 14:17:59.825402 95408 solver.cpp:317]     Train net output #1: loss = 3.33401 (* 1 = 3.33401 loss)
I1226 14:17:59.953758 93418 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 14:17:59.953863 93418 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 14:18:00.098610 93418 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 14:18:00.098707 93418 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 14:18:00.098742 93418 solver.cpp:291] [0] Iteration 20, loss = 3.32094
I1226 14:18:00.098796 93418 solver.cpp:317]     Train net output #0: accuracy = 0.25
I1226 14:18:00.098846 93418 solver.cpp:317]     Train net output #1: loss = 3.32094 (* 1 = 3.32094 loss)
I1226 14:18:00.176182 91567 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 14:18:00.176268 91567 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 14:18:00.216245 91497 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 14:18:00.216419 91497 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 14:18:00.644954 91497 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 14:18:00.645046 91497 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 14:18:00.645100 91497 solver.cpp:291] [1] Iteration 20, loss = 3.63227
I1226 14:18:00.645211 91497 solver.cpp:317]     Train net output #0: accuracy = 0.242188
I1226 14:18:00.645432 91497 solver.cpp:317]     Train net output #1: loss = 3.63227 (* 1 = 3.63227 loss)
I1226 14:18:01.015759 93418 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 14:18:01.015857 93418 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 14:18:01.158907 93418 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 14:18:01.158982 93418 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 14:18:01.159097 93418 solver.cpp:291] [0] Iteration 21, loss = 3.34757
I1226 14:18:01.159154 93418 solver.cpp:317]     Train net output #0: accuracy = 0.335938
I1226 14:18:01.159204 93418 solver.cpp:317]     Train net output #1: loss = 3.34757 (* 1 = 3.34757 loss)
I1226 14:18:01.228036 97044 MultiSolver.cpp:108] [7] PROFILING END[Backward]
I1226 14:18:01.228122 97044 MultiSolver.cpp:109] [7] PROFILING BEGIN[WaitingToSync]
I1226 14:18:01.228165 97044 solver.cpp:291] [7] Iteration 6, loss = 3.38632
I1226 14:18:01.228230 97044 solver.cpp:317]     Train net output #0: accuracy = 0.351562
I1226 14:18:01.228298 97044 solver.cpp:317]     Train net output #1: loss = 3.38632 (* 1 = 3.38632 loss)
I1226 14:18:01.275645 91567 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 14:18:01.275771 91567 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 14:18:01.299569 91497 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 14:18:01.299677 91497 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 14:18:01.355232 98928 MultiSolver.cpp:92] [6] PROFILING END[Forward]
I1226 14:18:01.355327 98928 MultiSolver.cpp:94] [6] PROFILING BEGIN[Backward]
I1226 14:18:01.414031 97115 SynchronousNode.cpp:246] [7] PROFILING END[WaitingToSync]
I1226 14:18:01.414113 97115 SynchronousNode.cpp:247] [7] PROFILING BEGIN[Forward]
I1226 14:18:01.524792 98928 MultiSolver.cpp:108] [6] PROFILING END[Backward]
I1226 14:18:01.524874 98928 MultiSolver.cpp:109] [6] PROFILING BEGIN[WaitingToSync]
I1226 14:18:01.525028 98928 solver.cpp:291] [6] Iteration 7, loss = 3.36825
I1226 14:18:01.525123 98928 solver.cpp:317]     Train net output #0: accuracy = 0.265625
I1226 14:18:01.525195 98928 solver.cpp:317]     Train net output #1: loss = 3.36825 (* 1 = 3.36825 loss)
I1226 14:18:01.707825 91497 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 14:18:01.707929 91497 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 14:18:01.707989 91497 solver.cpp:291] [1] Iteration 21, loss = 2.63988
I1226 14:18:01.708072 91497 solver.cpp:317]     Train net output #0: accuracy = 0.398438
I1226 14:18:01.708166 91497 solver.cpp:317]     Train net output #1: loss = 2.63988 (* 1 = 2.63988 loss)
I1226 14:18:02.075744 93418 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 14:18:02.075896 93418 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 14:18:02.219802 93418 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 14:18:02.219878 93418 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 14:18:02.219913 93418 solver.cpp:291] [0] Iteration 22, loss = 3.40841
I1226 14:18:02.219967 93418 solver.cpp:317]     Train net output #0: accuracy = 0.328125
I1226 14:18:02.220019 93418 solver.cpp:317]     Train net output #1: loss = 3.40841 (* 1 = 3.40841 loss)
I1226 14:18:02.246517 97473 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 14:18:02.246635 97473 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 14:18:02.289459 91567 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 14:18:02.289541 91567 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 14:18:02.322202 91497 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 14:18:02.322360 91497 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 14:18:02.509922 95408 MultiSolver.cpp:92] [4] PROFILING END[Forward]
I1226 14:18:02.510131 95408 MultiSolver.cpp:94] [4] PROFILING BEGIN[Backward]
I1226 14:18:02.731875 91497 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 14:18:02.732046 91497 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 14:18:02.732105 91497 solver.cpp:291] [1] Iteration 22, loss = 3.13484
I1226 14:18:02.732178 91497 solver.cpp:317]     Train net output #0: accuracy = 0.367188
I1226 14:18:02.732262 91497 solver.cpp:317]     Train net output #1: loss = 3.13484 (* 1 = 3.13484 loss)
I1226 14:18:02.776430 92574 SynchronousNode.cpp:246] [3] PROFILING END[WaitingToSync]
I1226 14:18:02.776517 92574 SynchronousNode.cpp:247] [3] PROFILING BEGIN[Forward]
I1226 14:18:02.779855 97044 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 14:18:02.779999 97044 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 14:18:02.799700 92503 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 14:18:02.799841 92503 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 14:18:03.024420 92503 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 14:18:03.024503 92503 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 14:18:03.024552 92503 solver.cpp:291] [3] Iteration 5, loss = 3.14021
I1226 14:18:03.024628 92503 solver.cpp:317]     Train net output #0: accuracy = 0.335938
I1226 14:18:03.024703 92503 solver.cpp:317]     Train net output #1: loss = 3.14021 (* 1 = 3.14021 loss)
I1226 14:18:03.064029 91480 SynchronousNode.cpp:246] [5] PROFILING END[WaitingToSync]
I1226 14:18:03.064133 91480 SynchronousNode.cpp:247] [5] PROFILING BEGIN[Forward]
I1226 14:18:03.088109 91408 MultiSolver.cpp:92] [5] PROFILING END[Forward]
I1226 14:18:03.088218 91408 MultiSolver.cpp:94] [5] PROFILING BEGIN[Backward]
I1226 14:18:03.084314 93418 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 14:18:03.084444 93418 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 14:18:03.229187 93418 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 14:18:03.229260 93418 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 14:18:03.229295 93418 solver.cpp:291] [0] Iteration 23, loss = 3.00124
I1226 14:18:03.229348 93418 solver.cpp:317]     Train net output #0: accuracy = 0.320312
I1226 14:18:03.229421 93418 solver.cpp:317]     Train net output #1: loss = 3.00124 (* 1 = 3.00124 loss)
I1226 14:18:03.318404 91408 MultiSolver.cpp:108] [5] PROFILING END[Backward]
I1226 14:18:03.318493 91408 MultiSolver.cpp:109] [5] PROFILING BEGIN[WaitingToSync]
I1226 14:18:03.318543 91408 solver.cpp:291] [5] Iteration 5, loss = 3.20471
I1226 14:18:03.318617 91408 solver.cpp:317]     Train net output #0: accuracy = 0.328125
I1226 14:18:03.318692 91408 solver.cpp:317]     Train net output #1: loss = 3.20471 (* 1 = 3.20471 loss)
I1226 14:18:03.314759 91567 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 14:18:03.314842 91567 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 14:18:03.337602 91497 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 14:18:03.337777 91497 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 14:18:03.729482 91497 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 14:18:03.729946 91497 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 14:18:03.730051 91497 solver.cpp:291] [1] Iteration 23, loss = 3.28183
I1226 14:18:03.730265 91497 solver.cpp:317]     Train net output #0: accuracy = 0.335938
I1226 14:18:03.730347 91497 solver.cpp:317]     Train net output #1: loss = 3.28183 (* 1 = 3.28183 loss)
I1226 14:18:04.076014 97473 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 14:18:04.076179 97473 MultiSolver.cpp:109] [2] PROFILING BEGIN[WaitingToSync]
I1226 14:18:04.076243 97473 solver.cpp:291] [2] Iteration 5, loss = 3.33696
I1226 14:18:04.076321 97473 solver.cpp:317]     Train net output #0: accuracy = 0.304688
I1226 14:18:04.076400 97473 solver.cpp:317]     Train net output #1: loss = 3.33696 (* 1 = 3.33696 loss)
I1226 14:18:04.177094 93418 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 14:18:04.177250 93418 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 14:18:04.281761 95408 MultiSolver.cpp:108] [4] PROFILING END[Backward]
I1226 14:18:04.281867 95408 MultiSolver.cpp:109] [4] PROFILING BEGIN[WaitingToSync]
I1226 14:18:04.281919 95408 solver.cpp:291] [4] Iteration 5, loss = 2.87599
I1226 14:18:04.281990 95408 solver.cpp:317]     Train net output #0: accuracy = 0.375
I1226 14:18:04.282229 95408 solver.cpp:317]     Train net output #1: loss = 2.87599 (* 1 = 2.87599 loss)
I1226 14:18:04.325460 93418 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 14:18:04.325558 93418 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 14:18:04.325593 93418 solver.cpp:291] [0] Iteration 24, loss = 3.62719
I1226 14:18:04.325649 93418 solver.cpp:317]     Train net output #0: accuracy = 0.28125
I1226 14:18:04.325700 93418 solver.cpp:317]     Train net output #1: loss = 3.62719 (* 1 = 3.62719 loss)
I1226 14:18:04.412607 91567 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 14:18:04.412689 91567 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 14:18:04.436370 91497 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 14:18:04.436520 91497 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 14:18:04.546918 97044 MultiSolver.cpp:108] [7] PROFILING END[Backward]
I1226 14:18:04.546999 97044 MultiSolver.cpp:109] [7] PROFILING BEGIN[WaitingToSync]
I1226 14:18:04.547041 97044 solver.cpp:291] [7] Iteration 7, loss = 3.01983
I1226 14:18:04.547106 97044 solver.cpp:317]     Train net output #0: accuracy = 0.335938
I1226 14:18:04.547174 97044 solver.cpp:317]     Train net output #1: loss = 3.01983 (* 1 = 3.01983 loss)
I1226 14:18:04.672140 98928 MultiSolver.cpp:92] [6] PROFILING END[Forward]
I1226 14:18:04.672238 98928 MultiSolver.cpp:94] [6] PROFILING BEGIN[Backward]
I1226 14:18:04.736090 97115 SynchronousNode.cpp:246] [7] PROFILING END[WaitingToSync]
I1226 14:18:04.736176 97115 SynchronousNode.cpp:247] [7] PROFILING BEGIN[Forward]
I1226 14:18:04.840648 98928 MultiSolver.cpp:108] [6] PROFILING END[Backward]
I1226 14:18:04.840735 98928 MultiSolver.cpp:109] [6] PROFILING BEGIN[WaitingToSync]
I1226 14:18:04.840776 98928 solver.cpp:291] [6] Iteration 8, loss = 3.27247
I1226 14:18:04.840842 98928 solver.cpp:317]     Train net output #0: accuracy = 0.273438
I1226 14:18:04.840909 98928 solver.cpp:317]     Train net output #1: loss = 3.27247 (* 1 = 3.27247 loss)
I1226 14:18:04.847429 91497 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 14:18:04.847580 91497 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 14:18:04.847873 91497 solver.cpp:291] [1] Iteration 24, loss = 2.97655
I1226 14:18:04.847955 91497 solver.cpp:317]     Train net output #0: accuracy = 0.34375
I1226 14:18:04.848034 91497 solver.cpp:317]     Train net output #1: loss = 2.97655 (* 1 = 2.97655 loss)
I1226 14:18:05.178908 93418 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 14:18:05.179019 93418 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 14:18:05.322775 93418 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 14:18:05.322850 93418 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 14:18:05.322885 93418 solver.cpp:291] [0] Iteration 25, loss = 3.14167
I1226 14:18:05.322939 93418 solver.cpp:317]     Train net output #0: accuracy = 0.335938
I1226 14:18:05.322990 93418 solver.cpp:317]     Train net output #1: loss = 3.14167 (* 1 = 3.14167 loss)
I1226 14:18:05.408064 91567 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 14:18:05.408146 91567 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 14:18:05.431543 91497 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 14:18:05.431694 91497 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 14:18:05.821439 91497 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 14:18:05.821527 91497 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 14:18:05.821578 91497 solver.cpp:291] [1] Iteration 25, loss = 3.02189
I1226 14:18:05.821650 91497 solver.cpp:317]     Train net output #0: accuracy = 0.390625
I1226 14:18:05.821943 91497 solver.cpp:317]     Train net output #1: loss = 3.02189 (* 1 = 3.02189 loss)
I1226 14:18:06.103021 97044 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 14:18:06.103135 97044 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 14:18:06.214794 93418 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 14:18:06.214944 93418 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 14:18:06.357748 93418 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 14:18:06.357825 93418 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 14:18:06.357861 93418 solver.cpp:291] [0] Iteration 26, loss = 2.92962
I1226 14:18:06.357916 93418 solver.cpp:317]     Train net output #0: accuracy = 0.320312
I1226 14:18:06.357969 93418 solver.cpp:317]     Train net output #1: loss = 2.92962 (* 1 = 2.92962 loss)
I1226 14:18:06.479270 91567 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 14:18:06.479357 91567 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 14:18:06.502620 91497 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 14:18:06.502766 91497 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 14:18:06.615772 97473 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 14:18:06.615883 97473 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 14:18:06.907404 91497 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 14:18:06.907591 91497 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 14:18:06.907833 91497 solver.cpp:291] [1] Iteration 26, loss = 3.40391
I1226 14:18:06.908056 91497 solver.cpp:317]     Train net output #0: accuracy = 0.296875
I1226 14:18:06.908133 91497 solver.cpp:317]     Train net output #1: loss = 3.40391 (* 1 = 3.40391 loss)
I1226 14:18:07.096706 95408 MultiSolver.cpp:92] [4] PROFILING END[Forward]
I1226 14:18:07.096864 95408 MultiSolver.cpp:94] [4] PROFILING BEGIN[Backward]
I1226 14:18:07.164134 92574 SynchronousNode.cpp:246] [3] PROFILING END[WaitingToSync]
I1226 14:18:07.164222 92574 SynchronousNode.cpp:247] [3] PROFILING BEGIN[Forward]
I1226 14:18:07.186108 92503 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 14:18:07.186223 92503 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 14:18:07.333189 93418 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 14:18:07.333290 93418 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 14:18:07.417716 92503 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 14:18:07.417806 92503 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 14:18:07.417855 92503 solver.cpp:291] [3] Iteration 6, loss = 3.56918
I1226 14:18:07.417930 92503 solver.cpp:317]     Train net output #0: accuracy = 0.289062
I1226 14:18:07.418006 92503 solver.cpp:317]     Train net output #1: loss = 3.56918 (* 1 = 3.56918 loss)
I1226 14:18:07.496728 93418 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 14:18:07.496804 93418 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 14:18:07.496839 93418 solver.cpp:291] [0] Iteration 27, loss = 3.60452
I1226 14:18:07.496893 93418 solver.cpp:317]     Train net output #0: accuracy = 0.234375
I1226 14:18:07.496944 93418 solver.cpp:317]     Train net output #1: loss = 3.60452 (* 1 = 3.60452 loss)
I1226 14:18:07.559123 91567 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 14:18:07.559213 91567 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 14:18:07.595247 91480 SynchronousNode.cpp:246] [5] PROFILING END[WaitingToSync]
I1226 14:18:07.595372 91480 SynchronousNode.cpp:247] [5] PROFILING BEGIN[Forward]
I1226 14:18:07.589856 91497 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 14:18:07.589963 91497 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 14:18:07.617738 91408 MultiSolver.cpp:92] [5] PROFILING END[Forward]
I1226 14:18:07.617861 91408 MultiSolver.cpp:94] [5] PROFILING BEGIN[Backward]
I1226 14:18:07.856194 91408 MultiSolver.cpp:108] [5] PROFILING END[Backward]
I1226 14:18:07.856288 91408 MultiSolver.cpp:109] [5] PROFILING BEGIN[WaitingToSync]
I1226 14:18:07.856369 91408 solver.cpp:291] [5] Iteration 6, loss = 3.32077
I1226 14:18:07.856446 91408 solver.cpp:317]     Train net output #0: accuracy = 0.335938
I1226 14:18:07.856520 91408 solver.cpp:317]     Train net output #1: loss = 3.32077 (* 1 = 3.32077 loss)
I1226 14:18:07.874490 97044 MultiSolver.cpp:108] [7] PROFILING END[Backward]
I1226 14:18:07.874573 97044 MultiSolver.cpp:109] [7] PROFILING BEGIN[WaitingToSync]
I1226 14:18:07.874614 97044 solver.cpp:291] [7] Iteration 8, loss = 3.27691
I1226 14:18:07.874680 97044 solver.cpp:317]     Train net output #0: accuracy = 0.375
I1226 14:18:07.874866 97044 solver.cpp:317]     Train net output #1: loss = 3.27691 (* 1 = 3.27691 loss)
I1226 14:18:08.001739 98928 MultiSolver.cpp:92] [6] PROFILING END[Forward]
I1226 14:18:08.001840 98928 MultiSolver.cpp:94] [6] PROFILING BEGIN[Backward]
I1226 14:18:08.016225 91497 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 14:18:08.016310 91497 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 14:18:08.016368 91497 solver.cpp:291] [1] Iteration 27, loss = 3.27061
I1226 14:18:08.016441 91497 solver.cpp:317]     Train net output #0: accuracy = 0.335938
I1226 14:18:08.016681 91497 solver.cpp:317]     Train net output #1: loss = 3.27061 (* 1 = 3.27061 loss)
I1226 14:18:08.075969 97115 SynchronousNode.cpp:246] [7] PROFILING END[WaitingToSync]
I1226 14:18:08.076052 97115 SynchronousNode.cpp:247] [7] PROFILING BEGIN[Forward]
I1226 14:18:08.169672 98928 MultiSolver.cpp:108] [6] PROFILING END[Backward]
I1226 14:18:08.169750 98928 MultiSolver.cpp:109] [6] PROFILING BEGIN[WaitingToSync]
I1226 14:18:08.169796 98928 solver.cpp:291] [6] Iteration 9, loss = 3.29153
I1226 14:18:08.169862 98928 solver.cpp:317]     Train net output #0: accuracy = 0.289062
I1226 14:18:08.169926 98928 solver.cpp:317]     Train net output #1: loss = 3.29153 (* 1 = 3.29153 loss)
I1226 14:18:08.349081 93418 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 14:18:08.349184 93418 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 14:18:08.418030 97473 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 14:18:08.418121 97473 MultiSolver.cpp:109] [2] PROFILING BEGIN[WaitingToSync]
I1226 14:18:08.418162 97473 solver.cpp:291] [2] Iteration 6, loss = 3.12374
I1226 14:18:08.418226 97473 solver.cpp:317]     Train net output #0: accuracy = 0.390625
I1226 14:18:08.418303 97473 solver.cpp:317]     Train net output #1: loss = 3.12374 (* 1 = 3.12374 loss)
I1226 14:18:08.493827 93418 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 14:18:08.493903 93418 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 14:18:08.494021 93418 solver.cpp:291] [0] Iteration 28, loss = 2.74845
I1226 14:18:08.494079 93418 solver.cpp:317]     Train net output #0: accuracy = 0.34375
I1226 14:18:08.494130 93418 solver.cpp:317]     Train net output #1: loss = 2.74845 (* 1 = 2.74845 loss)
I1226 14:18:08.584877 91567 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 14:18:08.584959 91567 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 14:18:08.613456 91497 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 14:18:08.613986 91497 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 14:18:08.866958 95408 MultiSolver.cpp:108] [4] PROFILING END[Backward]
I1226 14:18:08.867048 95408 MultiSolver.cpp:109] [4] PROFILING BEGIN[WaitingToSync]
I1226 14:18:08.867094 95408 solver.cpp:291] [4] Iteration 6, loss = 3.01695
I1226 14:18:08.867168 95408 solver.cpp:317]     Train net output #0: accuracy = 0.359375
I1226 14:18:08.867246 95408 solver.cpp:317]     Train net output #1: loss = 3.01695 (* 1 = 3.01695 loss)
I1226 14:18:09.052883 91497 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 14:18:09.053653 91497 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 14:18:09.053761 91497 solver.cpp:291] [1] Iteration 28, loss = 3.58844
I1226 14:18:09.053874 91497 solver.cpp:317]     Train net output #0: accuracy = 0.289062
I1226 14:18:09.054069 91497 solver.cpp:317]     Train net output #1: loss = 3.58844 (* 1 = 3.58844 loss)
I1226 14:18:09.364686 93418 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 14:18:09.365974 93418 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 14:18:09.449090 97044 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 14:18:09.449213 97044 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 14:18:09.511209 93418 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 14:18:09.511284 93418 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 14:18:09.511342 93418 solver.cpp:291] [0] Iteration 29, loss = 3.15395
I1226 14:18:09.511422 93418 solver.cpp:317]     Train net output #0: accuracy = 0.3125
I1226 14:18:09.511473 93418 solver.cpp:317]     Train net output #1: loss = 3.15395 (* 1 = 3.15395 loss)
I1226 14:18:09.609643 91567 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 14:18:09.609757 91567 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 14:18:09.632812 91497 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 14:18:09.632920 91497 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 14:18:10.062921 91497 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 14:18:10.063005 91497 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 14:18:10.063097 91497 solver.cpp:291] [1] Iteration 29, loss = 3.26644
I1226 14:18:10.063191 91497 solver.cpp:317]     Train net output #0: accuracy = 0.375
I1226 14:18:10.063401 91497 solver.cpp:317]     Train net output #1: loss = 3.26644 (* 1 = 3.26644 loss)
I1226 14:18:10.478960 93418 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 14:18:10.479069 93418 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 14:18:10.620573 93418 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 14:18:10.620647 93418 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 14:18:10.620681 93418 solver.cpp:291] [0] Iteration 30, loss = 3.16364
I1226 14:18:10.620735 93418 solver.cpp:317]     Train net output #0: accuracy = 0.296875
I1226 14:18:10.620785 93418 solver.cpp:317]     Train net output #1: loss = 3.16364 (* 1 = 3.16364 loss)
I1226 14:18:10.723440 91567 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 14:18:10.723525 91567 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 14:18:10.747848 91497 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 14:18:10.748001 91497 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 14:18:11.011993 97473 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 14:18:11.012212 97473 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 14:18:11.190392 91497 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 14:18:11.190523 91497 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 14:18:11.190618 91497 solver.cpp:291] [1] Iteration 30, loss = 3.39163
I1226 14:18:11.190865 91497 solver.cpp:317]     Train net output #0: accuracy = 0.273438
I1226 14:18:11.190948 91497 solver.cpp:317]     Train net output #1: loss = 3.39163 (* 1 = 3.39163 loss)
I1226 14:18:11.224367 97044 MultiSolver.cpp:108] [7] PROFILING END[Backward]
I1226 14:18:11.224452 97044 MultiSolver.cpp:109] [7] PROFILING BEGIN[WaitingToSync]
I1226 14:18:11.224494 97044 solver.cpp:291] [7] Iteration 9, loss = 3.24367
I1226 14:18:11.224557 97044 solver.cpp:317]     Train net output #0: accuracy = 0.3125
I1226 14:18:11.224633 97044 solver.cpp:317]     Train net output #1: loss = 3.24367 (* 1 = 3.24367 loss)
I1226 14:18:11.475543 98928 MultiSolver.cpp:92] [6] PROFILING END[Forward]
I1226 14:18:11.476708 98928 MultiSolver.cpp:94] [6] PROFILING BEGIN[Backward]
I1226 14:18:11.525149 97115 SynchronousNode.cpp:246] [7] PROFILING END[WaitingToSync]
I1226 14:18:11.525229 97115 SynchronousNode.cpp:247] [7] PROFILING BEGIN[Forward]
I1226 14:18:11.560200 92574 SynchronousNode.cpp:246] [3] PROFILING END[WaitingToSync]
I1226 14:18:11.561082 92574 SynchronousNode.cpp:247] [3] PROFILING BEGIN[Forward]
I1226 14:18:11.572207 93418 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 14:18:11.572322 93418 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 14:18:11.595145 92503 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 14:18:11.595262 92503 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 14:18:11.611665 95408 MultiSolver.cpp:92] [4] PROFILING END[Forward]
I1226 14:18:11.611780 95408 MultiSolver.cpp:94] [4] PROFILING BEGIN[Backward]
I1226 14:18:11.624044 98928 MultiSolver.cpp:108] [6] PROFILING END[Backward]
I1226 14:18:11.624173 98928 MultiSolver.cpp:109] [6] PROFILING BEGIN[WaitingToSync]
I1226 14:18:11.624217 98928 solver.cpp:291] [6] Iteration 10, loss = 3.43546
I1226 14:18:11.624284 98928 solver.cpp:317]     Train net output #0: accuracy = 0.273438
I1226 14:18:11.624351 98928 solver.cpp:317]     Train net output #1: loss = 3.43546 (* 1 = 3.43546 loss)
I1226 14:18:11.714048 93418 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 14:18:11.714123 93418 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 14:18:11.714161 93418 solver.cpp:291] [0] Iteration 31, loss = 3.1216
I1226 14:18:11.714215 93418 solver.cpp:317]     Train net output #0: accuracy = 0.289062
I1226 14:18:11.714267 93418 solver.cpp:317]     Train net output #1: loss = 3.1216 (* 1 = 3.1216 loss)
I1226 14:18:11.770891 91567 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 14:18:11.770972 91567 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 14:18:11.815342 91497 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 14:18:11.815428 91497 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 14:18:11.976455 92503 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 14:18:11.976541 92503 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 14:18:11.976591 92503 solver.cpp:291] [3] Iteration 7, loss = 3.08435
I1226 14:18:11.976665 92503 solver.cpp:317]     Train net output #0: accuracy = 0.296875
I1226 14:18:11.976740 92503 solver.cpp:317]     Train net output #1: loss = 3.08435 (* 1 = 3.08435 loss)
I1226 14:18:12.178581 91480 SynchronousNode.cpp:246] [5] PROFILING END[WaitingToSync]
I1226 14:18:12.180769 91480 SynchronousNode.cpp:247] [5] PROFILING BEGIN[Forward]
I1226 14:18:12.226081 91408 MultiSolver.cpp:92] [5] PROFILING END[Forward]
I1226 14:18:12.226197 91408 MultiSolver.cpp:94] [5] PROFILING BEGIN[Backward]
I1226 14:18:12.224977 91497 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 14:18:12.225095 91497 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 14:18:12.225152 91497 solver.cpp:291] [1] Iteration 31, loss = 3.34469
I1226 14:18:12.225265 91497 solver.cpp:317]     Train net output #0: accuracy = 0.328125
I1226 14:18:12.225395 91497 solver.cpp:317]     Train net output #1: loss = 3.34469 (* 1 = 3.34469 loss)
I1226 14:18:12.518844 91408 MultiSolver.cpp:108] [5] PROFILING END[Backward]
I1226 14:18:12.518936 91408 MultiSolver.cpp:109] [5] PROFILING BEGIN[WaitingToSync]
I1226 14:18:12.518985 91408 solver.cpp:291] [5] Iteration 7, loss = 2.99899
I1226 14:18:12.519059 91408 solver.cpp:317]     Train net output #0: accuracy = 0.375
I1226 14:18:12.519135 91408 solver.cpp:317]     Train net output #1: loss = 2.99899 (* 1 = 2.99899 loss)
I1226 14:18:12.570006 93418 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 14:18:12.570152 93418 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 14:18:12.713277 93418 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 14:18:12.713354 93418 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 14:18:12.713418 93418 solver.cpp:291] [0] Iteration 32, loss = 3.14401
I1226 14:18:12.713474 93418 solver.cpp:317]     Train net output #0: accuracy = 0.25
I1226 14:18:12.713524 93418 solver.cpp:317]     Train net output #1: loss = 3.14401 (* 1 = 3.14401 loss)
I1226 14:18:12.794641 91567 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 14:18:12.794752 91567 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 14:18:12.817420 91497 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 14:18:12.817976 91497 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 14:18:12.828615 97044 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 14:18:12.828735 97044 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 14:18:13.067785 97473 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 14:18:13.067862 97473 MultiSolver.cpp:109] [2] PROFILING BEGIN[WaitingToSync]
I1226 14:18:13.067901 97473 solver.cpp:291] [2] Iteration 7, loss = 3.42376
I1226 14:18:13.067966 97473 solver.cpp:317]     Train net output #0: accuracy = 0.304688
I1226 14:18:13.068300 97473 solver.cpp:317]     Train net output #1: loss = 3.42376 (* 1 = 3.42376 loss)
I1226 14:18:13.202493 91497 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 14:18:13.202630 91497 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 14:18:13.202746 91497 solver.cpp:291] [1] Iteration 32, loss = 3.45311
I1226 14:18:13.203097 91497 solver.cpp:317]     Train net output #0: accuracy = 0.296875
I1226 14:18:13.203384 91497 solver.cpp:317]     Train net output #1: loss = 3.45311 (* 1 = 3.45311 loss)
I1226 14:18:13.395916 95408 MultiSolver.cpp:108] [4] PROFILING END[Backward]
I1226 14:18:13.396003 95408 MultiSolver.cpp:109] [4] PROFILING BEGIN[WaitingToSync]
I1226 14:18:13.396049 95408 solver.cpp:291] [4] Iteration 7, loss = 3.37786
I1226 14:18:13.396121 95408 solver.cpp:317]     Train net output #0: accuracy = 0.304688
I1226 14:18:13.396205 95408 solver.cpp:317]     Train net output #1: loss = 3.37786 (* 1 = 3.37786 loss)
I1226 14:18:13.532884 93418 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 14:18:13.532984 93418 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 14:18:13.717795 93418 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 14:18:13.717866 93418 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 14:18:13.717900 93418 solver.cpp:291] [0] Iteration 33, loss = 3.12036
I1226 14:18:13.717954 93418 solver.cpp:317]     Train net output #0: accuracy = 0.28125
I1226 14:18:13.718004 93418 solver.cpp:317]     Train net output #1: loss = 3.12036 (* 1 = 3.12036 loss)
I1226 14:18:13.808681 91567 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 14:18:13.808801 91567 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 14:18:13.831634 91497 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 14:18:13.831815 91497 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 14:18:14.205042 91497 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 14:18:14.205152 91497 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 14:18:14.205227 91497 solver.cpp:291] [1] Iteration 33, loss = 3.10466
I1226 14:18:14.205487 91497 solver.cpp:317]     Train net output #0: accuracy = 0.351562
I1226 14:18:14.205752 91497 solver.cpp:317]     Train net output #1: loss = 3.10466 (* 1 = 3.10466 loss)
I1226 14:18:14.207607 91548 blocking_queue.cpp:87] Waiting for data
I1226 14:18:14.595275 97044 MultiSolver.cpp:108] [7] PROFILING END[Backward]
I1226 14:18:14.595360 97044 MultiSolver.cpp:109] [7] PROFILING BEGIN[WaitingToSync]
I1226 14:18:14.595402 97044 solver.cpp:291] [7] Iteration 10, loss = 3.14517
I1226 14:18:14.595464 97044 solver.cpp:317]     Train net output #0: accuracy = 0.304688
I1226 14:18:14.595532 97044 solver.cpp:317]     Train net output #1: loss = 3.14517 (* 1 = 3.14517 loss)
I1226 14:18:14.626246 93418 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 14:18:14.626344 93418 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 14:18:14.723134 98928 MultiSolver.cpp:92] [6] PROFILING END[Forward]
I1226 14:18:14.723237 98928 MultiSolver.cpp:94] [6] PROFILING BEGIN[Backward]
I1226 14:18:14.785676 97115 SynchronousNode.cpp:246] [7] PROFILING END[WaitingToSync]
I1226 14:18:14.785765 97115 SynchronousNode.cpp:247] [7] PROFILING BEGIN[Forward]
I1226 14:18:14.827391 97473 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 14:18:14.827606 97473 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 14:18:14.839532 91567 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 14:18:14.839612 91567 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 14:18:14.863616 91497 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 14:18:14.863860 91497 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 14:18:14.890596 98928 MultiSolver.cpp:108] [6] PROFILING END[Backward]
I1226 14:18:14.890681 98928 MultiSolver.cpp:109] [6] PROFILING BEGIN[WaitingToSync]
I1226 14:18:14.890727 98928 solver.cpp:291] [6] Iteration 11, loss = 3.25407
I1226 14:18:14.890803 98928 solver.cpp:317]     Train net output #0: accuracy = 0.289062
I1226 14:18:14.890877 98928 solver.cpp:317]     Train net output #1: loss = 3.25407 (* 1 = 3.25407 loss)
I1226 14:18:14.905575 93418 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 14:18:14.905655 93418 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 14:18:14.905689 93418 solver.cpp:291] [0] Iteration 34, loss = 3.47662
I1226 14:18:14.905745 93418 solver.cpp:317]     Train net output #0: accuracy = 0.320312
I1226 14:18:14.905824 93418 solver.cpp:317]     Train net output #1: loss = 3.47662 (* 1 = 3.47662 loss)
I1226 14:18:15.262791 91497 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 14:18:15.262887 91497 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 14:18:15.268144 91497 solver.cpp:291] [1] Iteration 34, loss = 3.2024
I1226 14:18:15.268311 91497 solver.cpp:317]     Train net output #0: accuracy = 0.3125
I1226 14:18:15.268435 91497 solver.cpp:317]     Train net output #1: loss = 3.2024 (* 1 = 3.2024 loss)
I1226 14:18:15.383795 92574 SynchronousNode.cpp:246] [3] PROFILING END[WaitingToSync]
I1226 14:18:15.383878 92574 SynchronousNode.cpp:247] [3] PROFILING BEGIN[Forward]
I1226 14:18:15.405792 92503 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 14:18:15.405918 92503 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 14:18:15.638533 92503 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 14:18:15.638625 92503 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 14:18:15.638682 92503 solver.cpp:291] [3] Iteration 8, loss = 3.17624
I1226 14:18:15.638756 92503 solver.cpp:317]     Train net output #0: accuracy = 0.34375
I1226 14:18:15.638830 92503 solver.cpp:317]     Train net output #1: loss = 3.17624 (* 1 = 3.17624 loss)
I1226 14:18:15.679250 93418 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 14:18:15.679354 93418 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 14:18:15.879747 93418 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 14:18:15.879822 93418 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 14:18:15.879856 93418 solver.cpp:291] [0] Iteration 35, loss = 3.22861
I1226 14:18:15.879910 93418 solver.cpp:317]     Train net output #0: accuracy = 0.289062
I1226 14:18:15.879961 93418 solver.cpp:317]     Train net output #1: loss = 3.22861 (* 1 = 3.22861 loss)
I1226 14:18:15.932942 91567 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 14:18:15.933019 91567 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 14:18:15.966337 91497 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 14:18:15.966447 91497 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 14:18:16.091557 95408 MultiSolver.cpp:92] [4] PROFILING END[Forward]
I1226 14:18:16.091667 95408 MultiSolver.cpp:94] [4] PROFILING BEGIN[Backward]
I1226 14:18:16.084882 97044 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 14:18:16.084990 97044 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 14:18:16.390508 91497 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 14:18:16.390628 91497 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 14:18:16.390724 91497 solver.cpp:291] [1] Iteration 35, loss = 3.15439
I1226 14:18:16.390976 91497 solver.cpp:317]     Train net output #0: accuracy = 0.328125
I1226 14:18:16.391063 91497 solver.cpp:317]     Train net output #1: loss = 3.15439 (* 1 = 3.15439 loss)
I1226 14:18:16.648218 91480 SynchronousNode.cpp:246] [5] PROFILING END[WaitingToSync]
I1226 14:18:16.648332 91480 SynchronousNode.cpp:247] [5] PROFILING BEGIN[Forward]
I1226 14:18:16.670090 91408 MultiSolver.cpp:92] [5] PROFILING END[Forward]
I1226 14:18:16.670207 91408 MultiSolver.cpp:94] [5] PROFILING BEGIN[Backward]
I1226 14:18:16.719702 97473 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 14:18:16.719820 97473 MultiSolver.cpp:109] [2] PROFILING BEGIN[WaitingToSync]
I1226 14:18:16.719859 97473 solver.cpp:291] [2] Iteration 8, loss = 3.12478
I1226 14:18:16.719921 97473 solver.cpp:317]     Train net output #0: accuracy = 0.34375
I1226 14:18:16.719996 97473 solver.cpp:317]     Train net output #1: loss = 3.12478 (* 1 = 3.12478 loss)
I1226 14:18:16.735128 93418 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 14:18:16.735227 93418 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 14:18:16.877001 93418 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 14:18:16.877074 93418 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 14:18:16.877106 93418 solver.cpp:291] [0] Iteration 36, loss = 3.08411
I1226 14:18:16.877159 93418 solver.cpp:317]     Train net output #0: accuracy = 0.351562
I1226 14:18:16.877209 93418 solver.cpp:317]     Train net output #1: loss = 3.08411 (* 1 = 3.08411 loss)
I1226 14:18:16.908332 91408 MultiSolver.cpp:108] [5] PROFILING END[Backward]
I1226 14:18:16.908421 91408 MultiSolver.cpp:109] [5] PROFILING BEGIN[WaitingToSync]
I1226 14:18:16.908473 91408 solver.cpp:291] [5] Iteration 8, loss = 3.5063
I1226 14:18:16.908546 91408 solver.cpp:317]     Train net output #0: accuracy = 0.320312
I1226 14:18:16.908622 91408 solver.cpp:317]     Train net output #1: loss = 3.5063 (* 1 = 3.5063 loss)
I1226 14:18:16.990425 91567 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 14:18:16.990505 91567 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 14:18:17.012651 91497 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 14:18:17.012843 91497 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 14:18:17.478035 91497 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 14:18:17.478139 91497 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 14:18:17.478196 91497 solver.cpp:291] [1] Iteration 36, loss = 3.13735
I1226 14:18:17.478271 91497 solver.cpp:317]     Train net output #0: accuracy = 0.320312
I1226 14:18:17.478736 91497 solver.cpp:317]     Train net output #1: loss = 3.13735 (* 1 = 3.13735 loss)
I1226 14:18:17.784495 93418 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 14:18:17.784591 93418 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 14:18:17.864650 95408 MultiSolver.cpp:108] [4] PROFILING END[Backward]
I1226 14:18:17.864742 95408 MultiSolver.cpp:109] [4] PROFILING BEGIN[WaitingToSync]
I1226 14:18:17.864791 95408 solver.cpp:291] [4] Iteration 8, loss = 3.01293
I1226 14:18:17.864887 95408 solver.cpp:317]     Train net output #0: accuracy = 0.382812
I1226 14:18:17.864966 95408 solver.cpp:317]     Train net output #1: loss = 3.01293 (* 1 = 3.01293 loss)
I1226 14:18:17.854245 97044 MultiSolver.cpp:108] [7] PROFILING END[Backward]
I1226 14:18:17.854326 97044 MultiSolver.cpp:109] [7] PROFILING BEGIN[WaitingToSync]
I1226 14:18:17.854367 97044 solver.cpp:291] [7] Iteration 11, loss = 2.9951
I1226 14:18:17.854429 97044 solver.cpp:317]     Train net output #0: accuracy = 0.34375
I1226 14:18:17.854497 97044 solver.cpp:317]     Train net output #1: loss = 2.9951 (* 1 = 2.9951 loss)
I1226 14:18:17.928647 93418 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 14:18:17.928755 93418 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 14:18:17.928788 93418 solver.cpp:291] [0] Iteration 37, loss = 3.29078
I1226 14:18:17.928843 93418 solver.cpp:317]     Train net output #0: accuracy = 0.28125
I1226 14:18:17.928894 93418 solver.cpp:317]     Train net output #1: loss = 3.29078 (* 1 = 3.29078 loss)
I1226 14:18:18.020433 97115 SynchronousNode.cpp:246] [7] PROFILING END[WaitingToSync]
I1226 14:18:18.020516 97115 SynchronousNode.cpp:247] [7] PROFILING BEGIN[Forward]
I1226 14:18:18.031482 91567 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 14:18:18.031563 91567 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 14:18:18.093430 98928 MultiSolver.cpp:92] [6] PROFILING END[Forward]
I1226 14:18:18.093519 98928 MultiSolver.cpp:94] [6] PROFILING BEGIN[Backward]
I1226 14:18:18.095928 91497 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 14:18:18.096007 91497 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 14:18:18.386471 98928 MultiSolver.cpp:108] [6] PROFILING END[Backward]
I1226 14:18:18.386562 98928 MultiSolver.cpp:109] [6] PROFILING BEGIN[WaitingToSync]
I1226 14:18:18.386606 98928 solver.cpp:291] [6] Iteration 12, loss = 3.09789
I1226 14:18:18.386672 98928 solver.cpp:317]     Train net output #0: accuracy = 0.359375
I1226 14:18:18.386934 98928 solver.cpp:317]     Train net output #1: loss = 3.09789 (* 1 = 3.09789 loss)
I1226 14:18:18.579319 91497 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 14:18:18.579419 91497 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 14:18:18.580265 91497 solver.cpp:291] [1] Iteration 37, loss = 3.50207
I1226 14:18:18.580700 91497 solver.cpp:317]     Train net output #0: accuracy = 0.265625
I1226 14:18:18.581184 91497 solver.cpp:317]     Train net output #1: loss = 3.50207 (* 1 = 3.50207 loss)
I1226 14:18:18.929357 93418 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 14:18:18.929530 93418 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 14:18:19.071233 93418 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 14:18:19.071312 93418 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 14:18:19.071352 93418 solver.cpp:291] [0] Iteration 38, loss = 3.19479
I1226 14:18:19.071436 93418 solver.cpp:317]     Train net output #0: accuracy = 0.351562
I1226 14:18:19.071488 93418 solver.cpp:317]     Train net output #1: loss = 3.19479 (* 1 = 3.19479 loss)
I1226 14:18:19.154196 91567 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 14:18:19.154276 91567 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 14:18:19.306290 91497 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 14:18:19.306413 91497 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 14:18:19.399097 97473 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 14:18:19.400149 97473 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 14:18:19.425725 97044 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 14:18:19.426327 97044 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 14:18:19.725364 91497 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 14:18:19.726119 91497 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 14:18:19.726236 91497 solver.cpp:291] [1] Iteration 38, loss = 3.12273
I1226 14:18:19.726451 91497 solver.cpp:317]     Train net output #0: accuracy = 0.359375
I1226 14:18:19.726536 91497 solver.cpp:317]     Train net output #1: loss = 3.12273 (* 1 = 3.12273 loss)
I1226 14:18:19.948024 92574 SynchronousNode.cpp:246] [3] PROFILING END[WaitingToSync]
I1226 14:18:19.948112 92574 SynchronousNode.cpp:247] [3] PROFILING BEGIN[Forward]
I1226 14:18:19.970379 92503 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 14:18:19.970492 92503 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 14:18:20.084676 93418 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 14:18:20.084779 93418 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 14:18:20.208768 92503 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 14:18:20.208856 92503 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 14:18:20.208906 92503 solver.cpp:291] [3] Iteration 9, loss = 3.31113
I1226 14:18:20.208982 92503 solver.cpp:317]     Train net output #0: accuracy = 0.328125
I1226 14:18:20.209058 92503 solver.cpp:317]     Train net output #1: loss = 3.31113 (* 1 = 3.31113 loss)
I1226 14:18:20.228988 93418 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 14:18:20.229063 93418 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 14:18:20.229096 93418 solver.cpp:291] [0] Iteration 39, loss = 3.42188
I1226 14:18:20.229151 93418 solver.cpp:317]     Train net output #0: accuracy = 0.28125
I1226 14:18:20.229200 93418 solver.cpp:317]     Train net output #1: loss = 3.42188 (* 1 = 3.42188 loss)
I1226 14:18:20.345942 91567 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 14:18:20.346017 91567 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 14:18:20.373759 91497 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 14:18:20.373904 91497 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 14:18:20.604784 95408 MultiSolver.cpp:92] [4] PROFILING END[Forward]
I1226 14:18:20.605726 95408 MultiSolver.cpp:94] [4] PROFILING BEGIN[Backward]
I1226 14:18:20.776295 91497 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 14:18:20.776377 91497 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 14:18:20.776476 91497 solver.cpp:291] [1] Iteration 39, loss = 3.16493
I1226 14:18:20.776754 91497 solver.cpp:317]     Train net output #0: accuracy = 0.398438
I1226 14:18:20.776867 91497 solver.cpp:317]     Train net output #1: loss = 3.16493 (* 1 = 3.16493 loss)
I1226 14:18:21.137872 93418 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 14:18:21.137974 93418 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 14:18:21.162868 91480 SynchronousNode.cpp:246] [5] PROFILING END[WaitingToSync]
I1226 14:18:21.162956 91480 SynchronousNode.cpp:247] [5] PROFILING BEGIN[Forward]
I1226 14:18:21.185055 91408 MultiSolver.cpp:92] [5] PROFILING END[Forward]
I1226 14:18:21.185171 91408 MultiSolver.cpp:94] [5] PROFILING BEGIN[Backward]
I1226 14:18:21.211768 97473 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 14:18:21.211863 97473 MultiSolver.cpp:109] [2] PROFILING BEGIN[WaitingToSync]
I1226 14:18:21.211902 97473 solver.cpp:291] [2] Iteration 9, loss = 3.3386
I1226 14:18:21.211968 97473 solver.cpp:317]     Train net output #0: accuracy = 0.34375
I1226 14:18:21.212045 97473 solver.cpp:317]     Train net output #1: loss = 3.3386 (* 1 = 3.3386 loss)
I1226 14:18:21.279516 93418 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 14:18:21.279593 93418 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 14:18:21.279626 93418 solver.cpp:291] [0] Iteration 40, loss = 3.37365
I1226 14:18:21.279682 93418 solver.cpp:317]     Train net output #0: accuracy = 0.289062
I1226 14:18:21.279732 93418 solver.cpp:317]     Train net output #1: loss = 3.37365 (* 1 = 3.37365 loss)
I1226 14:18:21.382293 91567 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 14:18:21.382376 91567 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 14:18:21.421934 91408 MultiSolver.cpp:108] [5] PROFILING END[Backward]
I1226 14:18:21.422024 91408 MultiSolver.cpp:109] [5] PROFILING BEGIN[WaitingToSync]
I1226 14:18:21.422078 91408 solver.cpp:291] [5] Iteration 9, loss = 3.43154
I1226 14:18:21.422153 91408 solver.cpp:317]     Train net output #0: accuracy = 0.296875
I1226 14:18:21.422229 91408 solver.cpp:317]     Train net output #1: loss = 3.43154 (* 1 = 3.43154 loss)
I1226 14:18:21.430125 91497 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 14:18:21.430244 91497 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 14:18:21.860136 91497 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 14:18:21.860283 91497 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 14:18:21.860535 91497 solver.cpp:291] [1] Iteration 40, loss = 3.24719
I1226 14:18:21.860746 91497 solver.cpp:317]     Train net output #0: accuracy = 0.382812
I1226 14:18:21.860838 91497 solver.cpp:317]     Train net output #1: loss = 3.24719 (* 1 = 3.24719 loss)
I1226 14:18:21.933590 97044 MultiSolver.cpp:108] [7] PROFILING END[Backward]
I1226 14:18:21.933668 97044 MultiSolver.cpp:109] [7] PROFILING BEGIN[WaitingToSync]
I1226 14:18:21.933713 97044 solver.cpp:291] [7] Iteration 12, loss = 3.17004
I1226 14:18:21.933779 97044 solver.cpp:317]     Train net output #0: accuracy = 0.3125
I1226 14:18:21.933857 97044 solver.cpp:317]     Train net output #1: loss = 3.17004 (* 1 = 3.17004 loss)
I1226 14:18:22.001713 97115 SynchronousNode.cpp:246] [7] PROFILING END[WaitingToSync]
I1226 14:18:22.001817 97115 SynchronousNode.cpp:247] [7] PROFILING BEGIN[Forward]
I1226 14:18:22.126973 98928 MultiSolver.cpp:92] [6] PROFILING END[Forward]
I1226 14:18:22.127110 98928 MultiSolver.cpp:94] [6] PROFILING BEGIN[Backward]
I1226 14:18:22.181010 93418 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 14:18:22.181110 93418 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 14:18:22.290727 98928 MultiSolver.cpp:108] [6] PROFILING END[Backward]
I1226 14:18:22.290803 98928 MultiSolver.cpp:109] [6] PROFILING BEGIN[WaitingToSync]
I1226 14:18:22.290841 98928 solver.cpp:291] [6] Iteration 13, loss = 2.82706
I1226 14:18:22.290906 98928 solver.cpp:317]     Train net output #0: accuracy = 0.367188
I1226 14:18:22.290971 98928 solver.cpp:317]     Train net output #1: loss = 2.82706 (* 1 = 2.82706 loss)
I1226 14:18:22.323873 93418 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 14:18:22.323982 93418 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 14:18:22.324054 93418 solver.cpp:291] [0] Iteration 41, loss = 3.1604
I1226 14:18:22.324110 93418 solver.cpp:317]     Train net output #0: accuracy = 0.289062
I1226 14:18:22.324159 93418 solver.cpp:317]     Train net output #1: loss = 3.1604 (* 1 = 3.1604 loss)
I1226 14:18:22.375638 95408 MultiSolver.cpp:108] [4] PROFILING END[Backward]
I1226 14:18:22.375732 95408 MultiSolver.cpp:109] [4] PROFILING BEGIN[WaitingToSync]
I1226 14:18:22.375782 95408 solver.cpp:291] [4] Iteration 9, loss = 2.69011
I1226 14:18:22.375883 95408 solver.cpp:317]     Train net output #0: accuracy = 0.34375
I1226 14:18:22.375972 95408 solver.cpp:317]     Train net output #1: loss = 2.69011 (* 1 = 2.69011 loss)
I1226 14:18:22.417254 91567 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 14:18:22.417356 91567 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 14:18:22.488121 91497 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 14:18:22.488255 91497 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 14:18:22.846829 91497 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 14:18:22.846918 91497 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 14:18:22.846974 91497 solver.cpp:291] [1] Iteration 41, loss = 3.00067
I1226 14:18:22.847049 91497 solver.cpp:317]     Train net output #0: accuracy = 0.335938
I1226 14:18:22.847126 91497 solver.cpp:317]     Train net output #1: loss = 3.00067 (* 1 = 3.00067 loss)
I1226 14:18:23.322130 93418 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 14:18:23.322232 93418 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 14:18:23.463920 93418 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 14:18:23.463994 93418 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 14:18:23.464028 93418 solver.cpp:291] [0] Iteration 42, loss = 3.5873
I1226 14:18:23.464082 93418 solver.cpp:317]     Train net output #0: accuracy = 0.234375
I1226 14:18:23.464131 93418 solver.cpp:317]     Train net output #1: loss = 3.5873 (* 1 = 3.5873 loss)
I1226 14:18:23.461410 97044 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 14:18:23.461530 97044 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 14:18:23.562989 91567 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 14:18:23.563072 91567 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 14:18:23.855877 91497 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 14:18:23.855981 91497 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 14:18:23.946180 97473 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 14:18:23.946297 97473 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 14:18:24.218261 91497 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 14:18:24.218358 91497 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 14:18:24.218451 91497 solver.cpp:291] [1] Iteration 42, loss = 3.53482
I1226 14:18:24.218689 91497 solver.cpp:317]     Train net output #0: accuracy = 0.289062
I1226 14:18:24.218827 91497 solver.cpp:317]     Train net output #1: loss = 3.53482 (* 1 = 3.53482 loss)
I1226 14:18:24.494431 92574 SynchronousNode.cpp:246] [3] PROFILING END[WaitingToSync]
I1226 14:18:24.494514 92574 SynchronousNode.cpp:247] [3] PROFILING BEGIN[Forward]
I1226 14:18:24.516413 92503 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 14:18:24.516525 92503 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 14:18:24.690464 93418 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 14:18:24.690557 93418 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 14:18:24.749634 92503 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 14:18:24.749722 92503 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 14:18:24.749770 92503 solver.cpp:291] [3] Iteration 10, loss = 3.32309
I1226 14:18:24.749847 92503 solver.cpp:317]     Train net output #0: accuracy = 0.304688
I1226 14:18:24.749922 92503 solver.cpp:317]     Train net output #1: loss = 3.32309 (* 1 = 3.32309 loss)
I1226 14:18:24.834913 93418 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 14:18:24.834986 93418 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 14:18:24.835021 93418 solver.cpp:291] [0] Iteration 43, loss = 3.20287
I1226 14:18:24.835077 93418 solver.cpp:317]     Train net output #0: accuracy = 0.289062
I1226 14:18:24.835129 93418 solver.cpp:317]     Train net output #1: loss = 3.20287 (* 1 = 3.20287 loss)
I1226 14:18:24.972383 91567 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 14:18:24.972470 91567 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 14:18:25.106958 95408 MultiSolver.cpp:92] [4] PROFILING END[Forward]
I1226 14:18:25.107130 95408 MultiSolver.cpp:94] [4] PROFILING BEGIN[Backward]
I1226 14:18:25.122073 91497 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 14:18:25.122211 91497 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 14:18:25.233888 97044 MultiSolver.cpp:108] [7] PROFILING END[Backward]
I1226 14:18:25.234010 97044 MultiSolver.cpp:109] [7] PROFILING BEGIN[WaitingToSync]
I1226 14:18:25.234055 97044 solver.cpp:291] [7] Iteration 13, loss = 3.52787
I1226 14:18:25.234118 97044 solver.cpp:317]     Train net output #0: accuracy = 0.257812
I1226 14:18:25.234192 97044 solver.cpp:317]     Train net output #1: loss = 3.52787 (* 1 = 3.52787 loss)
I1226 14:18:25.360378 98928 MultiSolver.cpp:92] [6] PROFILING END[Forward]
I1226 14:18:25.360476 98928 MultiSolver.cpp:94] [6] PROFILING BEGIN[Backward]
I1226 14:18:25.437973 97115 SynchronousNode.cpp:246] [7] PROFILING END[WaitingToSync]
I1226 14:18:25.438053 97115 SynchronousNode.cpp:247] [7] PROFILING BEGIN[Forward]
I1226 14:18:25.531610 98928 MultiSolver.cpp:108] [6] PROFILING END[Backward]
I1226 14:18:25.531692 98928 MultiSolver.cpp:109] [6] PROFILING BEGIN[WaitingToSync]
I1226 14:18:25.531739 98928 solver.cpp:291] [6] Iteration 14, loss = 3.18689
I1226 14:18:25.531805 98928 solver.cpp:317]     Train net output #0: accuracy = 0.34375
I1226 14:18:25.531872 98928 solver.cpp:317]     Train net output #1: loss = 3.18689 (* 1 = 3.18689 loss)
I1226 14:18:25.532999 91497 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 14:18:25.533093 91497 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 14:18:25.533154 91497 solver.cpp:291] [1] Iteration 43, loss = 2.84159
I1226 14:18:25.533229 91497 solver.cpp:317]     Train net output #0: accuracy = 0.4375
I1226 14:18:25.533340 91497 solver.cpp:317]     Train net output #1: loss = 2.84159 (* 1 = 2.84159 loss)
I1226 14:18:25.657440 91480 SynchronousNode.cpp:246] [5] PROFILING END[WaitingToSync]
I1226 14:18:25.657526 91480 SynchronousNode.cpp:247] [5] PROFILING BEGIN[Forward]
I1226 14:18:25.679291 91408 MultiSolver.cpp:92] [5] PROFILING END[Forward]
I1226 14:18:25.679435 91408 MultiSolver.cpp:94] [5] PROFILING BEGIN[Backward]
I1226 14:18:25.826187 97473 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 14:18:25.826267 97473 MultiSolver.cpp:109] [2] PROFILING BEGIN[WaitingToSync]
I1226 14:18:25.826309 97473 solver.cpp:291] [2] Iteration 10, loss = 3.10189
I1226 14:18:25.826372 97473 solver.cpp:317]     Train net output #0: accuracy = 0.34375
I1226 14:18:25.826450 97473 solver.cpp:317]     Train net output #1: loss = 3.10189 (* 1 = 3.10189 loss)
I1226 14:18:25.881909 93418 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 14:18:25.882004 93418 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 14:18:25.915115 91408 MultiSolver.cpp:108] [5] PROFILING END[Backward]
I1226 14:18:25.915201 91408 MultiSolver.cpp:109] [5] PROFILING BEGIN[WaitingToSync]
I1226 14:18:25.915251 91408 solver.cpp:291] [5] Iteration 10, loss = 3.42654
I1226 14:18:25.915360 91408 solver.cpp:317]     Train net output #0: accuracy = 0.320312
I1226 14:18:25.915434 91408 solver.cpp:317]     Train net output #1: loss = 3.42654 (* 1 = 3.42654 loss)
I1226 14:18:26.025712 93418 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 14:18:26.025791 93418 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 14:18:26.025825 93418 solver.cpp:291] [0] Iteration 44, loss = 2.94686
I1226 14:18:26.025878 93418 solver.cpp:317]     Train net output #0: accuracy = 0.328125
I1226 14:18:26.025929 93418 solver.cpp:317]     Train net output #1: loss = 2.94686 (* 1 = 2.94686 loss)
I1226 14:18:26.118604 91567 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 14:18:26.118690 91567 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 14:18:26.511647 91497 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 14:18:26.511828 91497 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 14:18:26.813016 97044 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 14:18:26.813129 97044 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 14:18:26.876117 95408 MultiSolver.cpp:108] [4] PROFILING END[Backward]
I1226 14:18:26.876204 95408 MultiSolver.cpp:109] [4] PROFILING BEGIN[WaitingToSync]
I1226 14:18:26.876250 95408 solver.cpp:291] [4] Iteration 10, loss = 3.36237
I1226 14:18:26.876322 95408 solver.cpp:317]     Train net output #0: accuracy = 0.335938
I1226 14:18:26.876408 95408 solver.cpp:317]     Train net output #1: loss = 3.36237 (* 1 = 3.36237 loss)
I1226 14:18:26.919383 91497 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 14:18:26.919509 91497 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 14:18:26.919580 91497 solver.cpp:291] [1] Iteration 44, loss = 2.98368
I1226 14:18:26.919657 91497 solver.cpp:317]     Train net output #0: accuracy = 0.40625
I1226 14:18:26.920569 91497 solver.cpp:317]     Train net output #1: loss = 2.98368 (* 1 = 2.98368 loss)
I1226 14:18:27.253592 93418 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 14:18:27.253695 93418 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 14:18:27.395081 93418 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 14:18:27.395176 93418 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 14:18:27.395210 93418 solver.cpp:291] [0] Iteration 45, loss = 3.22011
I1226 14:18:27.395287 93418 solver.cpp:317]     Train net output #0: accuracy = 0.3125
I1226 14:18:27.395337 93418 solver.cpp:317]     Train net output #1: loss = 3.22011 (* 1 = 3.22011 loss)
I1226 14:18:27.487298 91567 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 14:18:27.487380 91567 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 14:18:27.518477 91497 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 14:18:27.518584 91497 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 14:18:27.951635 91497 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 14:18:27.951743 91497 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 14:18:27.951838 91497 solver.cpp:291] [1] Iteration 45, loss = 3.38137
I1226 14:18:27.952106 91497 solver.cpp:317]     Train net output #0: accuracy = 0.3125
I1226 14:18:27.952198 91497 solver.cpp:317]     Train net output #1: loss = 3.38137 (* 1 = 3.38137 loss)
I1226 14:18:28.349422 93418 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 14:18:28.349560 93418 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 14:18:28.491837 93418 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 14:18:28.491909 93418 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 14:18:28.491966 93418 solver.cpp:291] [0] Iteration 46, loss = 3.01959
I1226 14:18:28.492020 93418 solver.cpp:317]     Train net output #0: accuracy = 0.28125
I1226 14:18:28.492070 93418 solver.cpp:317]     Train net output #1: loss = 3.01959 (* 1 = 3.01959 loss)
I1226 14:18:28.584064 91567 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 14:18:28.584142 91567 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 14:18:28.588044 97044 MultiSolver.cpp:108] [7] PROFILING END[Backward]
I1226 14:18:28.588127 97044 MultiSolver.cpp:109] [7] PROFILING BEGIN[WaitingToSync]
I1226 14:18:28.588170 97044 solver.cpp:291] [7] Iteration 14, loss = 3.25643
I1226 14:18:28.588233 97044 solver.cpp:317]     Train net output #0: accuracy = 0.351562
I1226 14:18:28.588300 97044 solver.cpp:317]     Train net output #1: loss = 3.25643 (* 1 = 3.25643 loss)
I1226 14:18:28.599763 97473 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 14:18:28.599880 97473 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 14:18:28.615080 91497 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 14:18:28.615260 91497 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 14:18:28.717022 98928 MultiSolver.cpp:92] [6] PROFILING END[Forward]
I1226 14:18:28.717149 98928 MultiSolver.cpp:94] [6] PROFILING BEGIN[Backward]
I1226 14:18:28.778492 97115 SynchronousNode.cpp:246] [7] PROFILING END[WaitingToSync]
I1226 14:18:28.778574 97115 SynchronousNode.cpp:247] [7] PROFILING BEGIN[Forward]
I1226 14:18:28.885324 98928 MultiSolver.cpp:108] [6] PROFILING END[Backward]
I1226 14:18:28.885404 98928 MultiSolver.cpp:109] [6] PROFILING BEGIN[WaitingToSync]
I1226 14:18:28.885448 98928 solver.cpp:291] [6] Iteration 15, loss = 3.12767
I1226 14:18:28.885514 98928 solver.cpp:317]     Train net output #0: accuracy = 0.34375
I1226 14:18:28.885581 98928 solver.cpp:317]     Train net output #1: loss = 3.12767 (* 1 = 3.12767 loss)
I1226 14:18:29.064502 91497 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 14:18:29.064612 91497 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 14:18:29.064690 91497 solver.cpp:291] [1] Iteration 46, loss = 3.38136
I1226 14:18:29.064949 91497 solver.cpp:317]     Train net output #0: accuracy = 0.242188
I1226 14:18:29.065029 91497 solver.cpp:317]     Train net output #1: loss = 3.38136 (* 1 = 3.38136 loss)
I1226 14:18:29.092165 92574 SynchronousNode.cpp:246] [3] PROFILING END[WaitingToSync]
I1226 14:18:29.092249 92574 SynchronousNode.cpp:247] [3] PROFILING BEGIN[Forward]
I1226 14:18:29.114055 92503 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 14:18:29.114171 92503 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 14:18:29.346717 92503 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 14:18:29.346803 92503 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 14:18:29.346853 92503 solver.cpp:291] [3] Iteration 11, loss = 3.57373
I1226 14:18:29.346925 92503 solver.cpp:317]     Train net output #0: accuracy = 0.25
I1226 14:18:29.347000 92503 solver.cpp:317]     Train net output #1: loss = 3.57373 (* 1 = 3.57373 loss)
I1226 14:18:29.391093 93418 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 14:18:29.391192 93418 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 14:18:29.534219 93418 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 14:18:29.534292 93418 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 14:18:29.534325 93418 solver.cpp:291] [0] Iteration 47, loss = 3.1944
I1226 14:18:29.534407 93418 solver.cpp:317]     Train net output #0: accuracy = 0.382812
I1226 14:18:29.534461 93418 solver.cpp:317]     Train net output #1: loss = 3.1944 (* 1 = 3.1944 loss)
I1226 14:18:29.614347 95408 MultiSolver.cpp:92] [4] PROFILING END[Forward]
I1226 14:18:29.614465 95408 MultiSolver.cpp:94] [4] PROFILING BEGIN[Backward]
I1226 14:18:29.621474 91567 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 14:18:29.621552 91567 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 14:18:29.643899 91497 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 14:18:29.644090 91497 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 14:18:30.099153 91497 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 14:18:30.099292 91497 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 14:18:30.099347 91497 solver.cpp:291] [1] Iteration 47, loss = 2.99865
I1226 14:18:30.099642 91497 solver.cpp:317]     Train net output #0: accuracy = 0.296875
I1226 14:18:30.099905 91497 solver.cpp:317]     Train net output #1: loss = 2.99865 (* 1 = 2.99865 loss)
I1226 14:18:30.172173 91480 SynchronousNode.cpp:246] [5] PROFILING END[WaitingToSync]
I1226 14:18:30.172256 91480 SynchronousNode.cpp:247] [5] PROFILING BEGIN[Forward]
I1226 14:18:30.170656 97044 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 14:18:30.170840 97044 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 14:18:30.194195 91408 MultiSolver.cpp:92] [5] PROFILING END[Forward]
I1226 14:18:30.194334 91408 MultiSolver.cpp:94] [5] PROFILING BEGIN[Backward]
I1226 14:18:30.410801 93418 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 14:18:30.410907 93418 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 14:18:30.429481 91408 MultiSolver.cpp:108] [5] PROFILING END[Backward]
I1226 14:18:30.429567 91408 MultiSolver.cpp:109] [5] PROFILING BEGIN[WaitingToSync]
I1226 14:18:30.429616 91408 solver.cpp:291] [5] Iteration 11, loss = 3.17145
I1226 14:18:30.429690 91408 solver.cpp:317]     Train net output #0: accuracy = 0.335938
I1226 14:18:30.429765 91408 solver.cpp:317]     Train net output #1: loss = 3.17145 (* 1 = 3.17145 loss)
I1226 14:18:30.457990 97473 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 14:18:30.458072 97473 MultiSolver.cpp:109] [2] PROFILING BEGIN[WaitingToSync]
I1226 14:18:30.458113 97473 solver.cpp:291] [2] Iteration 11, loss = 3.23453
I1226 14:18:30.458176 97473 solver.cpp:317]     Train net output #0: accuracy = 0.289062
I1226 14:18:30.458251 97473 solver.cpp:317]     Train net output #1: loss = 3.23453 (* 1 = 3.23453 loss)
I1226 14:18:30.555519 93418 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 14:18:30.555593 93418 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 14:18:30.555624 93418 solver.cpp:291] [0] Iteration 48, loss = 3.07112
I1226 14:18:30.555677 93418 solver.cpp:317]     Train net output #0: accuracy = 0.34375
I1226 14:18:30.555727 93418 solver.cpp:317]     Train net output #1: loss = 3.07112 (* 1 = 3.07112 loss)
I1226 14:18:30.641299 91567 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 14:18:30.641386 91567 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 14:18:30.664476 91497 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 14:18:30.664582 91497 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 14:18:31.049594 91497 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 14:18:31.049692 91497 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 14:18:31.049783 91497 solver.cpp:291] [1] Iteration 48, loss = 3.24645
I1226 14:18:31.049899 91497 solver.cpp:317]     Train net output #0: accuracy = 0.320312
I1226 14:18:31.050115 91497 solver.cpp:317]     Train net output #1: loss = 3.24645 (* 1 = 3.24645 loss)
I1226 14:18:31.402506 95408 MultiSolver.cpp:108] [4] PROFILING END[Backward]
I1226 14:18:31.402601 95408 MultiSolver.cpp:109] [4] PROFILING BEGIN[WaitingToSync]
I1226 14:18:31.402648 95408 solver.cpp:291] [4] Iteration 11, loss = 3.24195
I1226 14:18:31.402719 95408 solver.cpp:317]     Train net output #0: accuracy = 0.359375
I1226 14:18:31.402806 95408 solver.cpp:317]     Train net output #1: loss = 3.24195 (* 1 = 3.24195 loss)
I1226 14:18:31.527931 93418 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 14:18:31.528035 93418 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 14:18:31.670893 93418 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 14:18:31.670969 93418 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 14:18:31.671001 93418 solver.cpp:291] [0] Iteration 49, loss = 3.08898
I1226 14:18:31.671056 93418 solver.cpp:317]     Train net output #0: accuracy = 0.328125
I1226 14:18:31.671106 93418 solver.cpp:317]     Train net output #1: loss = 3.08898 (* 1 = 3.08898 loss)
I1226 14:18:31.756597 91567 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 14:18:31.756680 91567 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 14:18:31.783573 91497 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 14:18:31.783768 91497 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 14:18:31.942368 97044 MultiSolver.cpp:108] [7] PROFILING END[Backward]
I1226 14:18:31.942447 97044 MultiSolver.cpp:109] [7] PROFILING BEGIN[WaitingToSync]
I1226 14:18:31.942519 97044 solver.cpp:291] [7] Iteration 15, loss = 2.87662
I1226 14:18:31.942736 97044 solver.cpp:317]     Train net output #0: accuracy = 0.359375
I1226 14:18:31.942852 97044 solver.cpp:317]     Train net output #1: loss = 2.87662 (* 1 = 2.87662 loss)
I1226 14:18:32.126044 97115 SynchronousNode.cpp:246] [7] PROFILING END[WaitingToSync]
I1226 14:18:32.126124 97115 SynchronousNode.cpp:247] [7] PROFILING BEGIN[Forward]
I1226 14:18:32.143005 98928 MultiSolver.cpp:92] [6] PROFILING END[Forward]
I1226 14:18:32.143134 98928 MultiSolver.cpp:94] [6] PROFILING BEGIN[Backward]
I1226 14:18:32.223737 91497 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 14:18:32.223865 91497 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 14:18:32.223958 91497 solver.cpp:291] [1] Iteration 49, loss = 3.12125
I1226 14:18:32.224189 91497 solver.cpp:317]     Train net output #0: accuracy = 0.304688
I1226 14:18:32.224426 91497 solver.cpp:317]     Train net output #1: loss = 3.12125 (* 1 = 3.12125 loss)
I1226 14:18:32.313690 98928 MultiSolver.cpp:108] [6] PROFILING END[Backward]
I1226 14:18:32.313773 98928 MultiSolver.cpp:109] [6] PROFILING BEGIN[WaitingToSync]
I1226 14:18:32.313817 98928 solver.cpp:291] [6] Iteration 16, loss = 3.03472
I1226 14:18:32.313881 98928 solver.cpp:317]     Train net output #0: accuracy = 0.351562
I1226 14:18:32.313947 98928 solver.cpp:317]     Train net output #1: loss = 3.03472 (* 1 = 3.03472 loss)
I1226 14:18:32.608781 93418 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 14:18:32.608886 93418 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 14:18:32.752040 93418 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 14:18:32.752115 93418 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 14:18:32.752148 93418 solver.cpp:291] [0] Iteration 50, loss = 3.38213
I1226 14:18:32.752203 93418 solver.cpp:317]     Train net output #0: accuracy = 0.28125
I1226 14:18:32.752254 93418 solver.cpp:317]     Train net output #1: loss = 3.38213 (* 1 = 3.38213 loss)
I1226 14:18:32.859812 91567 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 14:18:32.859892 91567 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 14:18:32.902454 91497 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 14:18:32.902545 91497 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 14:18:33.149863 97473 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 14:18:33.149981 97473 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 14:18:33.308825 91497 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 14:18:33.308912 91497 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 14:18:33.308990 91497 solver.cpp:291] [1] Iteration 50, loss = 3.01398
I1226 14:18:33.309062 91497 solver.cpp:317]     Train net output #0: accuracy = 0.304688
I1226 14:18:33.309289 91497 solver.cpp:317]     Train net output #1: loss = 3.01398 (* 1 = 3.01398 loss)
I1226 14:18:33.441619 91497 blocking_queue.cpp:87] Waiting for data
I1226 14:18:33.534447 97044 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 14:18:33.534559 97044 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 14:18:33.667596 93418 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 14:18:33.667696 93418 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 14:18:33.678988 92574 SynchronousNode.cpp:246] [3] PROFILING END[WaitingToSync]
I1226 14:18:33.679081 92574 SynchronousNode.cpp:247] [3] PROFILING BEGIN[Forward]
I1226 14:18:33.700994 92503 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 14:18:33.701110 92503 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 14:18:33.813297 93418 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 14:18:33.813403 93418 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 14:18:33.813447 93418 solver.cpp:291] [0] Iteration 51, loss = 3.02733
I1226 14:18:33.813500 93418 solver.cpp:317]     Train net output #0: accuracy = 0.351562
I1226 14:18:33.813551 93418 solver.cpp:317]     Train net output #1: loss = 3.02733 (* 1 = 3.02733 loss)
I1226 14:18:33.928078 92503 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 14:18:33.928165 92503 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 14:18:33.928351 92503 solver.cpp:291] [3] Iteration 12, loss = 3.57636
I1226 14:18:33.928426 92503 solver.cpp:317]     Train net output #0: accuracy = 0.304688
I1226 14:18:33.928503 92503 solver.cpp:317]     Train net output #1: loss = 3.57636 (* 1 = 3.57636 loss)
I1226 14:18:33.938812 91567 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 14:18:33.938889 91567 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 14:18:33.969983 91497 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 14:18:33.970090 91497 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 14:18:34.183400 95408 MultiSolver.cpp:92] [4] PROFILING END[Forward]
I1226 14:18:34.183512 95408 MultiSolver.cpp:94] [4] PROFILING BEGIN[Backward]
I1226 14:18:34.403870 91497 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 14:18:34.403959 91497 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 14:18:34.404011 91497 solver.cpp:291] [1] Iteration 51, loss = 3.07254
I1226 14:18:34.404127 91497 solver.cpp:317]     Train net output #0: accuracy = 0.335938
I1226 14:18:34.404402 91497 solver.cpp:317]     Train net output #1: loss = 3.07254 (* 1 = 3.07254 loss)
I1226 14:18:34.739506 91480 SynchronousNode.cpp:246] [5] PROFILING END[WaitingToSync]
I1226 14:18:34.739586 91480 SynchronousNode.cpp:247] [5] PROFILING BEGIN[Forward]
I1226 14:18:34.765965 91408 MultiSolver.cpp:92] [5] PROFILING END[Forward]
I1226 14:18:34.766088 91408 MultiSolver.cpp:94] [5] PROFILING BEGIN[Backward]
I1226 14:18:34.770896 93418 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 14:18:34.771003 93418 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 14:18:34.915827 93418 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 14:18:34.915906 93418 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 14:18:34.915940 93418 solver.cpp:291] [0] Iteration 52, loss = 2.80451
I1226 14:18:34.915994 93418 solver.cpp:317]     Train net output #0: accuracy = 0.390625
I1226 14:18:34.916044 93418 solver.cpp:317]     Train net output #1: loss = 2.80451 (* 1 = 2.80451 loss)
I1226 14:18:34.921263 97473 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 14:18:34.921350 97473 MultiSolver.cpp:109] [2] PROFILING BEGIN[WaitingToSync]
I1226 14:18:34.921398 97473 solver.cpp:291] [2] Iteration 12, loss = 3.37543
I1226 14:18:34.921471 97473 solver.cpp:317]     Train net output #0: accuracy = 0.257812
I1226 14:18:34.921555 97473 solver.cpp:317]     Train net output #1: loss = 3.37543 (* 1 = 3.37543 loss)
I1226 14:18:35.004513 91408 MultiSolver.cpp:108] [5] PROFILING END[Backward]
I1226 14:18:35.004598 91408 MultiSolver.cpp:109] [5] PROFILING BEGIN[WaitingToSync]
I1226 14:18:35.004751 91408 solver.cpp:291] [5] Iteration 12, loss = 3.46362
I1226 14:18:35.004830 91408 solver.cpp:317]     Train net output #0: accuracy = 0.304688
I1226 14:18:35.004911 91408 solver.cpp:317]     Train net output #1: loss = 3.46362 (* 1 = 3.46362 loss)
I1226 14:18:35.012931 91567 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 14:18:35.013006 91567 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 14:18:35.040789 91497 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 14:18:35.040896 91497 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 14:18:35.344774 97044 MultiSolver.cpp:108] [7] PROFILING END[Backward]
I1226 14:18:35.344880 97044 MultiSolver.cpp:109] [7] PROFILING BEGIN[WaitingToSync]
I1226 14:18:35.344923 97044 solver.cpp:291] [7] Iteration 16, loss = 3.15513
I1226 14:18:35.344985 97044 solver.cpp:317]     Train net output #0: accuracy = 0.367188
I1226 14:18:35.345052 97044 solver.cpp:317]     Train net output #1: loss = 3.15513 (* 1 = 3.15513 loss)
I1226 14:18:35.432400 91497 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 14:18:35.432545 91497 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 14:18:35.432871 91497 solver.cpp:291] [1] Iteration 52, loss = 3.73583
I1226 14:18:35.433094 91497 solver.cpp:317]     Train net output #0: accuracy = 0.171875
I1226 14:18:35.433373 91497 solver.cpp:317]     Train net output #1: loss = 3.73583 (* 1 = 3.73583 loss)
I1226 14:18:35.476397 98928 MultiSolver.cpp:92] [6] PROFILING END[Forward]
I1226 14:18:35.476491 98928 MultiSolver.cpp:94] [6] PROFILING BEGIN[Backward]
I1226 14:18:35.533689 97115 SynchronousNode.cpp:246] [7] PROFILING END[WaitingToSync]
I1226 14:18:35.533774 97115 SynchronousNode.cpp:247] [7] PROFILING BEGIN[Forward]
I1226 14:18:35.642937 98928 MultiSolver.cpp:108] [6] PROFILING END[Backward]
I1226 14:18:35.643019 98928 MultiSolver.cpp:109] [6] PROFILING BEGIN[WaitingToSync]
I1226 14:18:35.643095 98928 solver.cpp:291] [6] Iteration 17, loss = 3.7009
I1226 14:18:35.643164 98928 solver.cpp:317]     Train net output #0: accuracy = 0.234375
I1226 14:18:35.643240 98928 solver.cpp:317]     Train net output #1: loss = 3.7009 (* 1 = 3.7009 loss)
I1226 14:18:35.799934 93418 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 14:18:35.800046 93418 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 14:18:35.944355 93418 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 14:18:35.944502 93418 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 14:18:35.944535 93418 solver.cpp:291] [0] Iteration 53, loss = 3.05222
I1226 14:18:35.944589 93418 solver.cpp:317]     Train net output #0: accuracy = 0.359375
I1226 14:18:35.944640 93418 solver.cpp:317]     Train net output #1: loss = 3.05222 (* 1 = 3.05222 loss)
I1226 14:18:35.966708 95408 MultiSolver.cpp:108] [4] PROFILING END[Backward]
I1226 14:18:35.966805 95408 MultiSolver.cpp:109] [4] PROFILING BEGIN[WaitingToSync]
I1226 14:18:35.966876 95408 solver.cpp:291] [4] Iteration 12, loss = 3.26173
I1226 14:18:35.966948 95408 solver.cpp:317]     Train net output #0: accuracy = 0.328125
I1226 14:18:35.967033 95408 solver.cpp:317]     Train net output #1: loss = 3.26173 (* 1 = 3.26173 loss)
I1226 14:18:36.052665 91567 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 14:18:36.052769 91567 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 14:18:36.178983 91497 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 14:18:36.179096 91497 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 14:18:36.576016 91497 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 14:18:36.576107 91497 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 14:18:36.576169 91497 solver.cpp:291] [1] Iteration 53, loss = 2.88069
I1226 14:18:36.576472 91497 solver.cpp:317]     Train net output #0: accuracy = 0.320312
I1226 14:18:36.576689 91497 solver.cpp:317]     Train net output #1: loss = 2.88069 (* 1 = 2.88069 loss)
I1226 14:18:36.882271 97044 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 14:18:36.882390 97044 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 14:18:36.978471 93418 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 14:18:36.978570 93418 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 14:18:37.124459 93418 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 14:18:37.124555 93418 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 14:18:37.124590 93418 solver.cpp:291] [0] Iteration 54, loss = 3.19751
I1226 14:18:37.124644 93418 solver.cpp:317]     Train net output #0: accuracy = 0.359375
I1226 14:18:37.124693 93418 solver.cpp:317]     Train net output #1: loss = 3.19751 (* 1 = 3.19751 loss)
I1226 14:18:37.210572 91567 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 14:18:37.210659 91567 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 14:18:37.347604 91497 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 14:18:37.347725 91497 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 14:18:37.589505 97473 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 14:18:37.589674 97473 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 14:18:37.739593 91497 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 14:18:37.739675 91497 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 14:18:37.739745 91497 solver.cpp:291] [1] Iteration 54, loss = 3.20601
I1226 14:18:37.739819 91497 solver.cpp:317]     Train net output #0: accuracy = 0.3125
I1226 14:18:37.740077 91497 solver.cpp:317]     Train net output #1: loss = 3.20601 (* 1 = 3.20601 loss)
I1226 14:18:38.129272 92574 SynchronousNode.cpp:246] [3] PROFILING END[WaitingToSync]
I1226 14:18:38.129392 92574 SynchronousNode.cpp:247] [3] PROFILING BEGIN[Forward]
I1226 14:18:38.152453 92503 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 14:18:38.152571 92503 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 14:18:38.236270 93418 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 14:18:38.236436 93418 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 14:18:38.380352 93418 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 14:18:38.380475 93418 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 14:18:38.380511 93418 solver.cpp:291] [0] Iteration 55, loss = 3.09213
I1226 14:18:38.380566 93418 solver.cpp:317]     Train net output #0: accuracy = 0.273438
I1226 14:18:38.380616 93418 solver.cpp:317]     Train net output #1: loss = 3.09213 (* 1 = 3.09213 loss)
I1226 14:18:38.394538 92503 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 14:18:38.394625 92503 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 14:18:38.394683 92503 solver.cpp:291] [3] Iteration 13, loss = 3.28157
I1226 14:18:38.394754 92503 solver.cpp:317]     Train net output #0: accuracy = 0.3125
I1226 14:18:38.394830 92503 solver.cpp:317]     Train net output #1: loss = 3.28157 (* 1 = 3.28157 loss)
I1226 14:18:38.461102 91567 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 14:18:38.461184 91567 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 14:18:38.589356 91497 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 14:18:38.589493 91497 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 14:18:38.653394 97044 MultiSolver.cpp:108] [7] PROFILING END[Backward]
I1226 14:18:38.653479 97044 MultiSolver.cpp:109] [7] PROFILING BEGIN[WaitingToSync]
I1226 14:18:38.653520 97044 solver.cpp:291] [7] Iteration 17, loss = 2.95169
I1226 14:18:38.653584 97044 solver.cpp:317]     Train net output #0: accuracy = 0.34375
I1226 14:18:38.653777 97044 solver.cpp:317]     Train net output #1: loss = 2.95169 (* 1 = 2.95169 loss)
I1226 14:18:38.737156 95408 MultiSolver.cpp:92] [4] PROFILING END[Forward]
I1226 14:18:38.737285 95408 MultiSolver.cpp:94] [4] PROFILING BEGIN[Backward]
I1226 14:18:38.783479 98928 MultiSolver.cpp:92] [6] PROFILING END[Forward]
I1226 14:18:38.783620 98928 MultiSolver.cpp:94] [6] PROFILING BEGIN[Backward]
I1226 14:18:38.873322 97115 SynchronousNode.cpp:246] [7] PROFILING END[WaitingToSync]
I1226 14:18:38.873406 97115 SynchronousNode.cpp:247] [7] PROFILING BEGIN[Forward]
I1226 14:18:38.952596 98928 MultiSolver.cpp:108] [6] PROFILING END[Backward]
I1226 14:18:38.952684 98928 MultiSolver.cpp:109] [6] PROFILING BEGIN[WaitingToSync]
I1226 14:18:38.952724 98928 solver.cpp:291] [6] Iteration 18, loss = 3.33663
I1226 14:18:38.952791 98928 solver.cpp:317]     Train net output #0: accuracy = 0.289062
I1226 14:18:38.952867 98928 solver.cpp:317]     Train net output #1: loss = 3.33663 (* 1 = 3.33663 loss)
I1226 14:18:39.006866 91497 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 14:18:39.007588 91497 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 14:18:39.007966 91497 solver.cpp:291] [1] Iteration 55, loss = 3.16531
I1226 14:18:39.008059 91497 solver.cpp:317]     Train net output #0: accuracy = 0.351562
I1226 14:18:39.008141 91497 solver.cpp:317]     Train net output #1: loss = 3.16531 (* 1 = 3.16531 loss)
I1226 14:18:39.293431 91480 SynchronousNode.cpp:246] [5] PROFILING END[WaitingToSync]
I1226 14:18:39.293514 91480 SynchronousNode.cpp:247] [5] PROFILING BEGIN[Forward]
I1226 14:18:39.315698 91408 MultiSolver.cpp:92] [5] PROFILING END[Forward]
I1226 14:18:39.315809 91408 MultiSolver.cpp:94] [5] PROFILING BEGIN[Backward]
I1226 14:18:39.327869 93418 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 14:18:39.327981 93418 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 14:18:39.397353 97473 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 14:18:39.397450 97473 MultiSolver.cpp:109] [2] PROFILING BEGIN[WaitingToSync]
I1226 14:18:39.397493 97473 solver.cpp:291] [2] Iteration 13, loss = 3.31804
I1226 14:18:39.397557 97473 solver.cpp:317]     Train net output #0: accuracy = 0.242188
I1226 14:18:39.397632 97473 solver.cpp:317]     Train net output #1: loss = 3.31804 (* 1 = 3.31804 loss)
I1226 14:18:39.472694 93418 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 14:18:39.473400 93418 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 14:18:39.473454 93418 solver.cpp:291] [0] Iteration 56, loss = 3.45095
I1226 14:18:39.473513 93418 solver.cpp:317]     Train net output #0: accuracy = 0.28125
I1226 14:18:39.473565 93418 solver.cpp:317]     Train net output #1: loss = 3.45095 (* 1 = 3.45095 loss)
I1226 14:18:39.549624 91408 MultiSolver.cpp:108] [5] PROFILING END[Backward]
I1226 14:18:39.549710 91408 MultiSolver.cpp:109] [5] PROFILING BEGIN[WaitingToSync]
I1226 14:18:39.549758 91408 solver.cpp:291] [5] Iteration 13, loss = 3.1625
I1226 14:18:39.549834 91408 solver.cpp:317]     Train net output #0: accuracy = 0.296875
I1226 14:18:39.549908 91408 solver.cpp:317]     Train net output #1: loss = 3.1625 (* 1 = 3.1625 loss)
I1226 14:18:39.567992 91567 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 14:18:39.568655 91567 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 14:18:39.965443 91497 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 14:18:39.965556 91497 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 14:18:40.200508 97044 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 14:18:40.200628 97044 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 14:18:40.378082 91497 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 14:18:40.378166 91497 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 14:18:40.378217 91497 solver.cpp:291] [1] Iteration 56, loss = 3.36529
I1226 14:18:40.378331 91497 solver.cpp:317]     Train net output #0: accuracy = 0.335938
I1226 14:18:40.378625 91497 solver.cpp:317]     Train net output #1: loss = 3.36529 (* 1 = 3.36529 loss)
I1226 14:18:40.540256 95408 MultiSolver.cpp:108] [4] PROFILING END[Backward]
I1226 14:18:40.540354 95408 MultiSolver.cpp:109] [4] PROFILING BEGIN[WaitingToSync]
I1226 14:18:40.540448 95408 solver.cpp:291] [4] Iteration 13, loss = 3.23245
I1226 14:18:40.540544 95408 solver.cpp:317]     Train net output #0: accuracy = 0.335938
I1226 14:18:40.540633 95408 solver.cpp:317]     Train net output #1: loss = 3.23245 (* 1 = 3.23245 loss)
I1226 14:18:40.769223 93418 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 14:18:40.769327 93418 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 14:18:40.924525 91567 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 14:18:40.924607 91567 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 14:18:41.017830 93418 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 14:18:41.017909 93418 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 14:18:41.017943 93418 solver.cpp:291] [0] Iteration 57, loss = 3.50132
I1226 14:18:41.017998 93418 solver.cpp:317]     Train net output #0: accuracy = 0.273438
I1226 14:18:41.018049 93418 solver.cpp:317]     Train net output #1: loss = 3.50132 (* 1 = 3.50132 loss)
I1226 14:18:41.021289 91497 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 14:18:41.021409 91497 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 14:18:41.404984 91497 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 14:18:41.405071 91497 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 14:18:41.405156 91497 solver.cpp:291] [1] Iteration 57, loss = 3.3953
I1226 14:18:41.405228 91497 solver.cpp:317]     Train net output #0: accuracy = 0.28125
I1226 14:18:41.405309 91497 solver.cpp:317]     Train net output #1: loss = 3.3953 (* 1 = 3.3953 loss)
I1226 14:18:41.843529 93418 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 14:18:41.843628 93418 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 14:18:41.975914 97044 MultiSolver.cpp:108] [7] PROFILING END[Backward]
User defined signal 2
