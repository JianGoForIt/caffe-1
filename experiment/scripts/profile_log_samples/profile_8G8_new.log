Dec 26 13:38:25 2016 96533 3 10.1 NIOS_DEBUG: stdin_fd set to -1
Dec 26 13:38:25 2016 96533 3 10.1 NIOS_DEBUG: fds[0] has a value of -1
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 13:38:28.004863 92321 mpiutil.cpp:166] Process rank 2 from number of 9 processes running on knl-node025
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 13:38:28.007424 91021 mpiutil.cpp:166] Process rank 7 from number of 9 processes running on knl-node076
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 13:38:28.003634 96543 mpiutil.cpp:166] Process rank 0 from number of 9 processes running on knl-node079
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 13:38:28.002135 96918 mpiutil.cpp:166] Process rank 3 from number of 9 processes running on knl-node078
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 13:38:28.009469 93001 mpiutil.cpp:166] Process rank 1 from number of 9 processes running on knl-node051
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 13:38:28.013270 94726 mpiutil.cpp:166] Process rank 8 from number of 9 processes running on knl-node084
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 13:38:28.010651 91865 mpiutil.cpp:166] Process rank 4 from number of 9 processes running on knl-node019
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 13:38:27.997205 122432 mpiutil.cpp:166] Process rank 5 from number of 9 processes running on knl-node010
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 13:38:28.012465 89419 mpiutil.cpp:166] Process rank 6 from number of 9 processes running on knl-node058
I1226 13:38:28.023739 91865 caffe.cpp:316] Use CPU.
I1226 13:38:28.026535 94726 caffe.cpp:316] Use CPU.
I1226 13:38:28.015669 96918 caffe.cpp:316] Use CPU.
I1226 13:38:28.025015 91865 solver.cpp:93] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_5.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 13:38:28.026018 91865 solver.cpp:128] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_5.prototxt
I1226 13:38:28.024185 93001 caffe.cpp:316] Use CPU.
I1226 13:38:28.027799 94726 solver.cpp:93] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_dummy.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 13:38:28.027815 89419 caffe.cpp:316] Use CPU.
I1226 13:38:28.028795 94726 solver.cpp:128] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_dummy.prototxt
I1226 13:38:28.017087 96918 solver.cpp:93] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_4.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 13:38:28.023314 91021 caffe.cpp:316] Use CPU.
I1226 13:38:28.018218 96918 solver.cpp:128] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_4.prototxt
I1226 13:38:28.025364 93001 solver.cpp:93] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_2.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 13:38:28.026337 93001 solver.cpp:128] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_2.prototxt
I1226 13:38:28.028952 89419 solver.cpp:93] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_7.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 13:38:28.029875 89419 solver.cpp:128] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_7.prototxt
I1226 13:38:28.019732 96543 caffe.cpp:316] Use CPU.
I1226 13:38:28.024463 91021 solver.cpp:93] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_8.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 13:38:28.025490 91021 solver.cpp:128] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_8.prototxt
I1226 13:38:28.021189 96543 solver.cpp:93] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_1.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 13:38:28.022300 96543 solver.cpp:128] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_1.prototxt
I1226 13:38:28.023998 92321 caffe.cpp:316] Use CPU.
I1226 13:38:28.025144 92321 solver.cpp:93] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_3.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 13:38:28.025974 92321 solver.cpp:128] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_3.prototxt
I1226 13:38:28.018189 122432 caffe.cpp:316] Use CPU.
I1226 13:38:28.019747 122432 solver.cpp:93] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_6.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 13:38:28.020964 122432 solver.cpp:128] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_6.prototxt
I1226 13:38:28.056953 91865 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 13:38:28.057027 91865 cpu_info.cpp:455] Total number of sockets: 1
I1226 13:38:28.057050 91865 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 13:38:28.057070 91865 cpu_info.cpp:461] Total number of processors: 272
I1226 13:38:28.057090 91865 cpu_info.cpp:464] GPU is used: no
I1226 13:38:28.057111 91865 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 13:38:28.057131 91865 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 13:38:28.056912 91021 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 13:38:28.056988 91021 cpu_info.cpp:455] Total number of sockets: 1
I1226 13:38:28.057046 91021 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 13:38:28.057065 91021 cpu_info.cpp:461] Total number of processors: 272
I1226 13:38:28.057085 91021 cpu_info.cpp:464] GPU is used: no
I1226 13:38:28.057103 91021 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 13:38:28.057152 91021 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 13:38:28.062158 89419 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 13:38:28.062546 94726 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 13:38:28.062275 89419 cpu_info.cpp:455] Total number of sockets: 1
I1226 13:38:28.062340 89419 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 13:38:28.062373 89419 cpu_info.cpp:461] Total number of processors: 272
I1226 13:38:28.062638 94726 cpu_info.cpp:455] Total number of sockets: 1
I1226 13:38:28.062667 94726 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 13:38:28.062690 94726 cpu_info.cpp:461] Total number of processors: 272
I1226 13:38:28.062711 94726 cpu_info.cpp:464] GPU is used: no
I1226 13:38:28.062402 89419 cpu_info.cpp:464] GPU is used: no
I1226 13:38:28.062734 94726 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 13:38:28.062439 89419 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 13:38:28.062466 89419 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 13:38:28.062757 94726 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 13:38:28.059725 93001 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 13:38:28.059805 93001 cpu_info.cpp:455] Total number of sockets: 1
I1226 13:38:28.059828 93001 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 13:38:28.059847 93001 cpu_info.cpp:461] Total number of processors: 272
I1226 13:38:28.059867 93001 cpu_info.cpp:464] GPU is used: no
I1226 13:38:28.059887 93001 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 13:38:28.059906 93001 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 13:38:28.055960 92321 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 13:38:28.056033 92321 cpu_info.cpp:455] Total number of sockets: 1
I1226 13:38:28.056056 92321 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 13:38:28.056077 92321 cpu_info.cpp:461] Total number of processors: 272
I1226 13:38:28.056097 92321 cpu_info.cpp:464] GPU is used: no
I1226 13:38:28.056116 92321 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 13:38:28.056138 92321 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 13:38:28.053542 96918 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 13:38:28.053634 96918 cpu_info.cpp:455] Total number of sockets: 1
I1226 13:38:28.053663 96918 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 13:38:28.053689 96918 cpu_info.cpp:461] Total number of processors: 272
I1226 13:38:28.053712 96918 cpu_info.cpp:464] GPU is used: no
I1226 13:38:28.053772 96918 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 13:38:28.053797 96918 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 13:38:28.057046 96543 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 13:38:28.057137 96543 cpu_info.cpp:455] Total number of sockets: 1
I1226 13:38:28.057163 96543 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 13:38:28.057185 96543 cpu_info.cpp:461] Total number of processors: 272
I1226 13:38:28.057207 96543 cpu_info.cpp:464] GPU is used: no
I1226 13:38:28.057230 96543 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 13:38:28.057255 96543 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 13:38:28.063549 122432 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 13:38:28.063664 122432 cpu_info.cpp:455] Total number of sockets: 1
I1226 13:38:28.063700 122432 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 13:38:28.063730 122432 cpu_info.cpp:461] Total number of processors: 272
I1226 13:38:28.063760 122432 cpu_info.cpp:464] GPU is used: no
I1226 13:38:28.063791 122432 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 13:38:28.063833 122432 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 13:38:28.086011 94726 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 13:38:28.086390 94726 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 13:38:28.088605 94726 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "DummyData"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  dummy_data_param {
    data_filler {
      type: "gaussian"
      std: 1
    }
    data_filler {
      type: "constant"
      value: 0
    }
    shape {
      dim: 256
      dim: 3
      dim: 227
      dim: 227
    }
    shape {
      dim: 256
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 13:38:28.089231 92321 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 13:38:28.088606 91021 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 13:38:28.089574 92321 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 13:38:28.088903 91021 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 13:38:28.090615 91021 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_7"
    batch_size: 256
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 13:38:28.088435 96918 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 13:38:28.091745 92321 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_2"
    batch_size: 256
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 13:38:28.088860 96918 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 13:38:28.091013 96918 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_3"
    batch_size: 256
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 13:38:28.119712 89419 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 13:38:28.120013 89419 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 13:38:28.121572 89419 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_6"
    batch_size: 256
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 13:38:28.113044 96543 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 13:38:28.113404 96543 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 13:38:28.115453 96543 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_0"
    batch_size: 256
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 13:38:28.123775 93001 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 13:38:28.121987 91021 layer_factory.hpp:114] Creating layer data
I1226 13:38:28.124137 93001 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 13:38:28.125859 91865 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 13:38:28.126240 91865 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 13:38:28.126405 93001 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_1"
    batch_size: 256
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 13:38:28.128501 91865 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_4"
    batch_size: 256
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 13:38:28.137013 96543 layer_factory.hpp:114] Creating layer data
I1226 13:38:28.135798 122432 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 13:38:28.136176 122432 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 13:38:28.138483 122432 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_5"
    batch_size: 256
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 13:38:28.147388 92321 layer_factory.hpp:114] Creating layer data
I1226 13:38:28.152426 91021 net.cpp:178] Creating Layer data
I1226 13:38:28.152499 91021 net.cpp:586] data -> data
I1226 13:38:28.152601 91021 net.cpp:586] data -> label
I1226 13:38:28.156510 91865 layer_factory.hpp:114] Creating layer data
I1226 13:38:28.150478 96543 net.cpp:178] Creating Layer data
I1226 13:38:28.150588 96543 net.cpp:586] data -> data
I1226 13:38:28.150691 96543 net.cpp:586] data -> label
I1226 13:38:28.165839 89419 layer_factory.hpp:114] Creating layer data
I1226 13:38:28.169468 91865 net.cpp:178] Creating Layer data
I1226 13:38:28.169554 91865 net.cpp:586] data -> data
I1226 13:38:28.169656 91865 net.cpp:586] data -> label
I1226 13:38:28.162853 122432 layer_factory.hpp:114] Creating layer data
I1226 13:38:28.170469 96546 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_0
I1226 13:38:28.176476 91023 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_7
I1226 13:38:28.182070 89419 net.cpp:178] Creating Layer data
I1226 13:38:28.182183 89419 net.cpp:586] data -> data
I1226 13:38:28.182289 89419 net.cpp:586] data -> label
I1226 13:38:28.178423 96543 data_layer.cpp:80] output data size: 256,3,227,227
I1226 13:38:28.188539 92321 net.cpp:178] Creating Layer data
I1226 13:38:28.188627 92321 net.cpp:586] data -> data
I1226 13:38:28.188730 92321 net.cpp:586] data -> label
I1226 13:38:28.195024 91867 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_4
I1226 13:38:28.187513 122432 net.cpp:178] Creating Layer data
I1226 13:38:28.187623 122432 net.cpp:586] data -> data
I1226 13:38:28.187764 122432 net.cpp:586] data -> label
I1226 13:38:28.203289 91865 data_layer.cpp:80] output data size: 256,3,227,227
I1226 13:38:28.202497 91021 data_layer.cpp:80] output data size: 256,3,227,227
I1226 13:38:28.211113 89421 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_6
I1226 13:38:28.208361 92323 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_2
I1226 13:38:28.217941 93001 layer_factory.hpp:114] Creating layer data
I1226 13:38:28.222190 94726 layer_factory.hpp:114] Creating layer data
I1226 13:38:28.222357 94726 net.cpp:178] Creating Layer data
I1226 13:38:28.222465 94726 net.cpp:586] data -> data
I1226 13:38:28.222597 94726 net.cpp:586] data -> label
I1226 13:38:28.215265 92321 data_layer.cpp:80] output data size: 256,3,227,227
I1226 13:38:28.214529 96918 layer_factory.hpp:114] Creating layer data
I1226 13:38:28.235055 89419 data_layer.cpp:80] output data size: 256,3,227,227
I1226 13:38:28.233595 93001 net.cpp:178] Creating Layer data
I1226 13:38:28.233677 93001 net.cpp:586] data -> data
I1226 13:38:28.233783 93001 net.cpp:586] data -> label
I1226 13:38:28.228543 96918 net.cpp:178] Creating Layer data
I1226 13:38:28.228678 96918 net.cpp:586] data -> data
I1226 13:38:28.228850 96918 net.cpp:586] data -> label
I1226 13:38:28.224690 122434 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_5
I1226 13:38:28.247578 93003 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_1
I1226 13:38:28.235569 122432 data_layer.cpp:80] output data size: 256,3,227,227
I1226 13:38:28.249354 93001 data_layer.cpp:80] output data size: 256,3,227,227
I1226 13:38:28.261333 94726 net.cpp:228] Setting up data
I1226 13:38:28.261471 94726 net.cpp:235] Top shape: 256 3 227 227 (39574272)
I1226 13:38:28.261515 94726 net.cpp:235] Top shape: 256 1 1 1 (256)
I1226 13:38:28.261541 94726 net.cpp:243] Memory required for data: 158298112
I1226 13:38:28.261595 94726 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 13:38:28.250486 96920 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_3
I1226 13:38:28.261689 94726 net.cpp:178] Creating Layer label_data_1_split
I1226 13:38:28.261862 94726 net.cpp:612] label_data_1_split <- label
I1226 13:38:28.261919 94726 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 13:38:28.261986 94726 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 13:38:28.258224 96918 data_layer.cpp:80] output data size: 256,3,227,227
I1226 13:38:28.274499 94726 net.cpp:228] Setting up label_data_1_split
I1226 13:38:28.274613 94726 net.cpp:235] Top shape: 256 1 1 1 (256)
I1226 13:38:28.274648 94726 net.cpp:235] Top shape: 256 1 1 1 (256)
I1226 13:38:28.274673 94726 net.cpp:243] Memory required for data: 158300160
I1226 13:38:28.274709 94726 layer_factory.hpp:114] Creating layer conv1
I1226 13:38:28.274816 94726 net.cpp:178] Creating Layer conv1
I1226 13:38:28.274883 94726 net.cpp:612] conv1 <- data
I1226 13:38:28.274930 94726 net.cpp:586] conv1 -> conv1
I1226 13:38:28.435606 91021 net.cpp:228] Setting up data
I1226 13:38:28.435760 91021 net.cpp:235] Top shape: 256 3 227 227 (39574272)
I1226 13:38:28.435808 91021 net.cpp:235] Top shape: 256 (256)
I1226 13:38:28.435863 91021 net.cpp:243] Memory required for data: 158298112
I1226 13:38:28.435901 91021 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 13:38:28.436019 91021 net.cpp:178] Creating Layer label_data_1_split
I1226 13:38:28.436163 91021 net.cpp:612] label_data_1_split <- label
I1226 13:38:28.436213 91021 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 13:38:28.436298 91021 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 13:38:28.468675 91021 net.cpp:228] Setting up label_data_1_split
I1226 13:38:28.468786 91021 net.cpp:235] Top shape: 256 (256)
I1226 13:38:28.468824 91021 net.cpp:235] Top shape: 256 (256)
I1226 13:38:28.468848 91021 net.cpp:243] Memory required for data: 158300160
I1226 13:38:28.468888 91021 layer_factory.hpp:114] Creating layer conv1
I1226 13:38:28.468989 91021 net.cpp:178] Creating Layer conv1
I1226 13:38:28.469033 91021 net.cpp:612] conv1 <- data
I1226 13:38:28.469079 91021 net.cpp:586] conv1 -> conv1
I1226 13:38:28.480819 91865 net.cpp:228] Setting up data
I1226 13:38:28.480937 91865 net.cpp:235] Top shape: 256 3 227 227 (39574272)
I1226 13:38:28.481351 91865 net.cpp:235] Top shape: 256 (256)
I1226 13:38:28.481391 91865 net.cpp:243] Memory required for data: 158298112
I1226 13:38:28.481431 91865 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 13:38:28.481565 91865 net.cpp:178] Creating Layer label_data_1_split
I1226 13:38:28.481730 91865 net.cpp:612] label_data_1_split <- label
I1226 13:38:28.481783 91865 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 13:38:28.481835 91865 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 13:38:28.486641 92321 net.cpp:228] Setting up data
I1226 13:38:28.486759 92321 net.cpp:235] Top shape: 256 3 227 227 (39574272)
I1226 13:38:28.486799 92321 net.cpp:235] Top shape: 256 (256)
I1226 13:38:28.486824 92321 net.cpp:243] Memory required for data: 158298112
I1226 13:38:28.486860 92321 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 13:38:28.486959 92321 net.cpp:178] Creating Layer label_data_1_split
I1226 13:38:28.487095 92321 net.cpp:612] label_data_1_split <- label
I1226 13:38:28.487145 92321 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 13:38:28.487205 92321 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 13:38:28.500294 89419 net.cpp:228] Setting up data
I1226 13:38:28.500408 89419 net.cpp:235] Top shape: 256 3 227 227 (39574272)
I1226 13:38:28.500444 89419 net.cpp:235] Top shape: 256 (256)
I1226 13:38:28.500468 89419 net.cpp:243] Memory required for data: 158298112
I1226 13:38:28.500505 89419 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 13:38:28.500596 89419 net.cpp:178] Creating Layer label_data_1_split
I1226 13:38:28.500751 89419 net.cpp:612] label_data_1_split <- label
I1226 13:38:28.500804 89419 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 13:38:28.500855 89419 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 13:38:28.503051 91865 net.cpp:228] Setting up label_data_1_split
I1226 13:38:28.503175 91865 net.cpp:235] Top shape: 256 (256)
I1226 13:38:28.503211 91865 net.cpp:235] Top shape: 256 (256)
I1226 13:38:28.503235 91865 net.cpp:243] Memory required for data: 158300160
I1226 13:38:28.503271 91865 layer_factory.hpp:114] Creating layer conv1
I1226 13:38:28.503399 91865 net.cpp:178] Creating Layer conv1
I1226 13:38:28.503463 91865 net.cpp:612] conv1 <- data
I1226 13:38:28.503526 91865 net.cpp:586] conv1 -> conv1
I1226 13:38:28.505863 92321 net.cpp:228] Setting up label_data_1_split
I1226 13:38:28.505971 92321 net.cpp:235] Top shape: 256 (256)
I1226 13:38:28.506008 92321 net.cpp:235] Top shape: 256 (256)
I1226 13:38:28.506032 92321 net.cpp:243] Memory required for data: 158300160
I1226 13:38:28.506068 92321 layer_factory.hpp:114] Creating layer conv1
I1226 13:38:28.506199 92321 net.cpp:178] Creating Layer conv1
I1226 13:38:28.506289 92321 net.cpp:612] conv1 <- data
I1226 13:38:28.506350 92321 net.cpp:586] conv1 -> conv1
I1226 13:38:28.518523 89419 net.cpp:228] Setting up label_data_1_split
I1226 13:38:28.518661 89419 net.cpp:235] Top shape: 256 (256)
I1226 13:38:28.518699 89419 net.cpp:235] Top shape: 256 (256)
I1226 13:38:28.518725 89419 net.cpp:243] Memory required for data: 158300160
I1226 13:38:28.518761 89419 layer_factory.hpp:114] Creating layer conv1
I1226 13:38:28.518873 89419 net.cpp:178] Creating Layer conv1
I1226 13:38:28.518914 89419 net.cpp:612] conv1 <- data
I1226 13:38:28.518959 89419 net.cpp:586] conv1 -> conv1
I1226 13:38:28.569622 93001 net.cpp:228] Setting up data
I1226 13:38:28.569725 93001 net.cpp:235] Top shape: 256 3 227 227 (39574272)
I1226 13:38:28.569758 93001 net.cpp:235] Top shape: 256 (256)
I1226 13:38:28.569809 93001 net.cpp:243] Memory required for data: 158298112
I1226 13:38:28.569845 93001 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 13:38:28.570116 93001 net.cpp:178] Creating Layer label_data_1_split
I1226 13:38:28.570248 93001 net.cpp:612] label_data_1_split <- label
I1226 13:38:28.570293 93001 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 13:38:28.570446 93001 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 13:38:28.601892 93001 net.cpp:228] Setting up label_data_1_split
I1226 13:38:28.601994 93001 net.cpp:235] Top shape: 256 (256)
I1226 13:38:28.602025 93001 net.cpp:235] Top shape: 256 (256)
I1226 13:38:28.602046 93001 net.cpp:243] Memory required for data: 158300160
I1226 13:38:28.602110 93001 layer_factory.hpp:114] Creating layer conv1
I1226 13:38:28.602210 93001 net.cpp:178] Creating Layer conv1
I1226 13:38:28.602246 93001 net.cpp:612] conv1 <- data
I1226 13:38:28.602286 93001 net.cpp:586] conv1 -> conv1
I1226 13:38:28.626463 122432 net.cpp:228] Setting up data
I1226 13:38:28.626679 122432 net.cpp:235] Top shape: 256 3 227 227 (39574272)
I1226 13:38:28.626749 122432 net.cpp:235] Top shape: 256 (256)
I1226 13:38:28.626787 122432 net.cpp:243] Memory required for data: 158298112
I1226 13:38:28.626858 122432 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 13:38:28.627107 122432 net.cpp:178] Creating Layer label_data_1_split
I1226 13:38:28.627173 122432 net.cpp:612] label_data_1_split <- label
I1226 13:38:28.627233 122432 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 13:38:28.627305 122432 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 13:38:28.649461 94726 net.cpp:228] Setting up conv1
I1226 13:38:28.649593 94726 net.cpp:235] Top shape: 256 96 55 55 (74342400)
I1226 13:38:28.649633 94726 net.cpp:243] Memory required for data: 455669760
I1226 13:38:28.649785 94726 layer_factory.hpp:114] Creating layer relu1
I1226 13:38:28.649927 94726 net.cpp:178] Creating Layer relu1
I1226 13:38:28.650010 94726 net.cpp:612] relu1 <- conv1
I1226 13:38:28.650063 94726 net.cpp:573] relu1 -> conv1 (in-place)
I1226 13:38:28.650193 94726 net.cpp:228] Setting up relu1
I1226 13:38:28.650261 94726 net.cpp:235] Top shape: 256 96 55 55 (74342400)
I1226 13:38:28.650290 94726 net.cpp:243] Memory required for data: 753039360
I1226 13:38:28.650326 94726 layer_factory.hpp:114] Creating layer norm1
I1226 13:38:28.650420 94726 net.cpp:178] Creating Layer norm1
I1226 13:38:28.650465 94726 net.cpp:612] norm1 <- conv1
I1226 13:38:28.650507 94726 net.cpp:586] norm1 -> norm1
I1226 13:38:28.650629 94726 net.cpp:228] Setting up norm1
I1226 13:38:28.650693 94726 net.cpp:235] Top shape: 256 96 55 55 (74342400)
I1226 13:38:28.650720 94726 net.cpp:243] Memory required for data: 1050408960
I1226 13:38:28.650753 94726 layer_factory.hpp:114] Creating layer pool1
I1226 13:38:28.650863 94726 net.cpp:178] Creating Layer pool1
I1226 13:38:28.650909 94726 net.cpp:612] pool1 <- norm1
I1226 13:38:28.650952 94726 net.cpp:586] pool1 -> pool1
I1226 13:38:28.651069 94726 net.cpp:228] Setting up pool1
I1226 13:38:28.651125 94726 net.cpp:235] Top shape: 256 96 27 27 (17915904)
I1226 13:38:28.651151 94726 net.cpp:243] Memory required for data: 1122072576
I1226 13:38:28.651182 94726 layer_factory.hpp:114] Creating layer conv2
I1226 13:38:28.651270 94726 net.cpp:178] Creating Layer conv2
I1226 13:38:28.651312 94726 net.cpp:612] conv2 <- pool1
I1226 13:38:28.651371 94726 net.cpp:586] conv2 -> conv2
I1226 13:38:28.653556 122432 net.cpp:228] Setting up label_data_1_split
I1226 13:38:28.653765 122432 net.cpp:235] Top shape: 256 (256)
I1226 13:38:28.653821 122432 net.cpp:235] Top shape: 256 (256)
I1226 13:38:28.653980 122432 net.cpp:243] Memory required for data: 158300160
I1226 13:38:28.654031 122432 layer_factory.hpp:114] Creating layer conv1
I1226 13:38:28.654163 122432 net.cpp:178] Creating Layer conv1
I1226 13:38:28.654217 122432 net.cpp:612] conv1 <- data
I1226 13:38:28.654278 122432 net.cpp:586] conv1 -> conv1
I1226 13:38:28.692898 91021 net.cpp:228] Setting up conv1
I1226 13:38:28.693011 91021 net.cpp:235] Top shape: 256 96 55 55 (74342400)
I1226 13:38:28.693037 91021 net.cpp:243] Memory required for data: 455669760
I1226 13:38:28.693171 91021 layer_factory.hpp:114] Creating layer relu1
I1226 13:38:28.693289 91021 net.cpp:178] Creating Layer relu1
I1226 13:38:28.693332 91021 net.cpp:612] relu1 <- conv1
I1226 13:38:28.693374 91021 net.cpp:573] relu1 -> conv1 (in-place)
I1226 13:38:28.693485 91021 net.cpp:228] Setting up relu1
I1226 13:38:28.693533 91021 net.cpp:235] Top shape: 256 96 55 55 (74342400)
I1226 13:38:28.693557 91021 net.cpp:243] Memory required for data: 753039360
I1226 13:38:28.693586 91021 layer_factory.hpp:114] Creating layer norm1
I1226 13:38:28.693660 91021 net.cpp:178] Creating Layer norm1
I1226 13:38:28.693691 91021 net.cpp:612] norm1 <- conv1
I1226 13:38:28.693727 91021 net.cpp:586] norm1 -> norm1
I1226 13:38:28.693871 91021 net.cpp:228] Setting up norm1
I1226 13:38:28.693913 91021 net.cpp:235] Top shape: 256 96 55 55 (74342400)
I1226 13:38:28.693935 91021 net.cpp:243] Memory required for data: 1050408960
I1226 13:38:28.693963 91021 layer_factory.hpp:114] Creating layer pool1
I1226 13:38:28.694018 91021 net.cpp:178] Creating Layer pool1
I1226 13:38:28.694056 91021 net.cpp:612] pool1 <- norm1
I1226 13:38:28.694098 91021 net.cpp:586] pool1 -> pool1
I1226 13:38:28.694195 91021 net.cpp:228] Setting up pool1
I1226 13:38:28.694231 91021 net.cpp:235] Top shape: 256 96 27 27 (17915904)
I1226 13:38:28.694294 91021 net.cpp:243] Memory required for data: 1122072576
I1226 13:38:28.694324 91021 layer_factory.hpp:114] Creating layer conv2
I1226 13:38:28.694397 91021 net.cpp:178] Creating Layer conv2
I1226 13:38:28.694424 91021 net.cpp:612] conv2 <- pool1
I1226 13:38:28.694464 91021 net.cpp:586] conv2 -> conv2
I1226 13:38:28.748636 89419 net.cpp:228] Setting up conv1
I1226 13:38:28.748754 89419 net.cpp:235] Top shape: 256 96 55 55 (74342400)
I1226 13:38:28.748785 89419 net.cpp:243] Memory required for data: 455669760
I1226 13:38:28.748895 89419 layer_factory.hpp:114] Creating layer relu1
I1226 13:38:28.748986 89419 net.cpp:178] Creating Layer relu1
I1226 13:38:28.749028 89419 net.cpp:612] relu1 <- conv1
I1226 13:38:28.749070 89419 net.cpp:573] relu1 -> conv1 (in-place)
I1226 13:38:28.749176 89419 net.cpp:228] Setting up relu1
I1226 13:38:28.749225 89419 net.cpp:235] Top shape: 256 96 55 55 (74342400)
I1226 13:38:28.749249 89419 net.cpp:243] Memory required for data: 753039360
I1226 13:38:28.749277 89419 layer_factory.hpp:114] Creating layer norm1
I1226 13:38:28.749346 89419 net.cpp:178] Creating Layer norm1
I1226 13:38:28.749379 89419 net.cpp:612] norm1 <- conv1
I1226 13:38:28.749416 89419 net.cpp:586] norm1 -> norm1
I1226 13:38:28.749505 89419 net.cpp:228] Setting up norm1
I1226 13:38:28.749552 89419 net.cpp:235] Top shape: 256 96 55 55 (74342400)
I1226 13:38:28.749575 89419 net.cpp:243] Memory required for data: 1050408960
I1226 13:38:28.749603 89419 layer_factory.hpp:114] Creating layer pool1
I1226 13:38:28.749699 89419 net.cpp:178] Creating Layer pool1
I1226 13:38:28.749742 89419 net.cpp:612] pool1 <- norm1
I1226 13:38:28.749796 89419 net.cpp:586] pool1 -> pool1
I1226 13:38:28.749893 89419 net.cpp:228] Setting up pool1
I1226 13:38:28.749940 89419 net.cpp:235] Top shape: 256 96 27 27 (17915904)
I1226 13:38:28.749969 89419 net.cpp:243] Memory required for data: 1122072576
I1226 13:38:28.749995 89419 layer_factory.hpp:114] Creating layer conv2
I1226 13:38:28.748136 91865 net.cpp:228] Setting up conv1
I1226 13:38:28.750077 89419 net.cpp:178] Creating Layer conv2
I1226 13:38:28.748240 91865 net.cpp:235] Top shape: 256 96 55 55 (74342400)
I1226 13:38:28.748292 91865 net.cpp:243] Memory required for data: 455669760
I1226 13:38:28.750110 89419 net.cpp:612] conv2 <- pool1
I1226 13:38:28.750149 89419 net.cpp:586] conv2 -> conv2
I1226 13:38:28.748457 91865 layer_factory.hpp:114] Creating layer relu1
I1226 13:38:28.748622 91865 net.cpp:178] Creating Layer relu1
I1226 13:38:28.748663 91865 net.cpp:612] relu1 <- conv1
I1226 13:38:28.748723 91865 net.cpp:573] relu1 -> conv1 (in-place)
I1226 13:38:28.748946 91865 net.cpp:228] Setting up relu1
I1226 13:38:28.749351 91865 net.cpp:235] Top shape: 256 96 55 55 (74342400)
I1226 13:38:28.749399 91865 net.cpp:243] Memory required for data: 753039360
I1226 13:38:28.749436 91865 layer_factory.hpp:114] Creating layer norm1
I1226 13:38:28.749505 91865 net.cpp:178] Creating Layer norm1
I1226 13:38:28.749574 91865 net.cpp:612] norm1 <- conv1
I1226 13:38:28.749632 91865 net.cpp:586] norm1 -> norm1
I1226 13:38:28.749764 91865 net.cpp:228] Setting up norm1
I1226 13:38:28.749811 91865 net.cpp:235] Top shape: 256 96 55 55 (74342400)
I1226 13:38:28.749853 91865 net.cpp:243] Memory required for data: 1050408960
I1226 13:38:28.749896 91865 layer_factory.hpp:114] Creating layer pool1
I1226 13:38:28.749964 91865 net.cpp:178] Creating Layer pool1
I1226 13:38:28.749996 91865 net.cpp:612] pool1 <- norm1
I1226 13:38:28.750031 91865 net.cpp:586] pool1 -> pool1
I1226 13:38:28.750125 91865 net.cpp:228] Setting up pool1
I1226 13:38:28.750169 91865 net.cpp:235] Top shape: 256 96 27 27 (17915904)
I1226 13:38:28.750213 91865 net.cpp:243] Memory required for data: 1122072576
I1226 13:38:28.750262 91865 layer_factory.hpp:114] Creating layer conv2
I1226 13:38:28.750365 91865 net.cpp:178] Creating Layer conv2
I1226 13:38:28.750399 91865 net.cpp:612] conv2 <- pool1
I1226 13:38:28.750447 91865 net.cpp:586] conv2 -> conv2
I1226 13:38:28.759552 92321 net.cpp:228] Setting up conv1
I1226 13:38:28.759660 92321 net.cpp:235] Top shape: 256 96 55 55 (74342400)
I1226 13:38:28.759686 92321 net.cpp:243] Memory required for data: 455669760
I1226 13:38:28.759820 92321 layer_factory.hpp:114] Creating layer relu1
I1226 13:38:28.759914 92321 net.cpp:178] Creating Layer relu1
I1226 13:38:28.759955 92321 net.cpp:612] relu1 <- conv1
I1226 13:38:28.759994 92321 net.cpp:573] relu1 -> conv1 (in-place)
I1226 13:38:28.760099 92321 net.cpp:228] Setting up relu1
I1226 13:38:28.760293 92321 net.cpp:235] Top shape: 256 96 55 55 (74342400)
I1226 13:38:28.760318 92321 net.cpp:243] Memory required for data: 753039360
I1226 13:38:28.760349 92321 layer_factory.hpp:114] Creating layer norm1
I1226 13:38:28.760411 92321 net.cpp:178] Creating Layer norm1
I1226 13:38:28.760437 92321 net.cpp:612] norm1 <- conv1
I1226 13:38:28.760511 92321 net.cpp:586] norm1 -> norm1
I1226 13:38:28.760649 92321 net.cpp:228] Setting up norm1
I1226 13:38:28.760782 92321 net.cpp:235] Top shape: 256 96 55 55 (74342400)
I1226 13:38:28.760804 92321 net.cpp:243] Memory required for data: 1050408960
I1226 13:38:28.760835 92321 layer_factory.hpp:114] Creating layer pool1
I1226 13:38:28.760972 92321 net.cpp:178] Creating Layer pool1
I1226 13:38:28.761008 92321 net.cpp:612] pool1 <- norm1
I1226 13:38:28.761044 92321 net.cpp:586] pool1 -> pool1
I1226 13:38:28.761147 92321 net.cpp:228] Setting up pool1
I1226 13:38:28.761188 92321 net.cpp:235] Top shape: 256 96 27 27 (17915904)
I1226 13:38:28.761211 92321 net.cpp:243] Memory required for data: 1122072576
I1226 13:38:28.761237 92321 layer_factory.hpp:114] Creating layer conv2
I1226 13:38:28.761310 92321 net.cpp:178] Creating Layer conv2
I1226 13:38:28.761343 92321 net.cpp:612] conv2 <- pool1
I1226 13:38:28.761381 92321 net.cpp:586] conv2 -> conv2
I1226 13:38:28.845661 93001 net.cpp:228] Setting up conv1
I1226 13:38:28.845770 93001 net.cpp:235] Top shape: 256 96 55 55 (74342400)
I1226 13:38:28.845798 93001 net.cpp:243] Memory required for data: 455669760
I1226 13:38:28.845904 93001 layer_factory.hpp:114] Creating layer relu1
I1226 13:38:28.845979 93001 net.cpp:178] Creating Layer relu1
I1226 13:38:28.846009 93001 net.cpp:612] relu1 <- conv1
I1226 13:38:28.846076 93001 net.cpp:573] relu1 -> conv1 (in-place)
I1226 13:38:28.846179 93001 net.cpp:228] Setting up relu1
I1226 13:38:28.846226 93001 net.cpp:235] Top shape: 256 96 55 55 (74342400)
I1226 13:38:28.846263 93001 net.cpp:243] Memory required for data: 753039360
I1226 13:38:28.846300 93001 layer_factory.hpp:114] Creating layer norm1
I1226 13:38:28.846359 93001 net.cpp:178] Creating Layer norm1
I1226 13:38:28.846410 93001 net.cpp:612] norm1 <- conv1
I1226 13:38:28.846452 93001 net.cpp:586] norm1 -> norm1
I1226 13:38:28.846544 93001 net.cpp:228] Setting up norm1
I1226 13:38:28.846593 93001 net.cpp:235] Top shape: 256 96 55 55 (74342400)
I1226 13:38:28.846617 93001 net.cpp:243] Memory required for data: 1050408960
I1226 13:38:28.846643 93001 layer_factory.hpp:114] Creating layer pool1
I1226 13:38:28.846691 93001 net.cpp:178] Creating Layer pool1
I1226 13:38:28.846726 93001 net.cpp:612] pool1 <- norm1
I1226 13:38:28.846758 93001 net.cpp:586] pool1 -> pool1
I1226 13:38:28.846850 93001 net.cpp:228] Setting up pool1
I1226 13:38:28.846890 93001 net.cpp:235] Top shape: 256 96 27 27 (17915904)
I1226 13:38:28.846912 93001 net.cpp:243] Memory required for data: 1122072576
I1226 13:38:28.846938 93001 layer_factory.hpp:114] Creating layer conv2
I1226 13:38:28.847018 93001 net.cpp:178] Creating Layer conv2
I1226 13:38:28.847050 93001 net.cpp:612] conv2 <- pool1
I1226 13:38:28.847087 93001 net.cpp:586] conv2 -> conv2
I1226 13:38:28.966985 122432 net.cpp:228] Setting up conv1
I1226 13:38:28.967126 122432 net.cpp:235] Top shape: 256 96 55 55 (74342400)
I1226 13:38:28.967165 122432 net.cpp:243] Memory required for data: 455669760
I1226 13:38:28.967306 122432 layer_factory.hpp:114] Creating layer relu1
I1226 13:38:28.967406 122432 net.cpp:178] Creating Layer relu1
I1226 13:38:28.967448 122432 net.cpp:612] relu1 <- conv1
I1226 13:38:28.967519 122432 net.cpp:573] relu1 -> conv1 (in-place)
I1226 13:38:28.967695 122432 net.cpp:228] Setting up relu1
I1226 13:38:28.967777 122432 net.cpp:235] Top shape: 256 96 55 55 (74342400)
I1226 13:38:28.967813 122432 net.cpp:243] Memory required for data: 753039360
I1226 13:38:28.967857 122432 layer_factory.hpp:114] Creating layer norm1
I1226 13:38:28.968128 122432 net.cpp:178] Creating Layer norm1
I1226 13:38:28.968221 122432 net.cpp:612] norm1 <- conv1
I1226 13:38:28.968276 122432 net.cpp:586] norm1 -> norm1
I1226 13:38:28.968410 122432 net.cpp:228] Setting up norm1
I1226 13:38:28.968482 122432 net.cpp:235] Top shape: 256 96 55 55 (74342400)
I1226 13:38:28.968516 122432 net.cpp:243] Memory required for data: 1050408960
I1226 13:38:28.968556 122432 layer_factory.hpp:114] Creating layer pool1
I1226 13:38:28.968780 122432 net.cpp:178] Creating Layer pool1
I1226 13:38:28.968930 122432 net.cpp:612] pool1 <- norm1
I1226 13:38:28.968994 122432 net.cpp:586] pool1 -> pool1
I1226 13:38:28.969138 122432 net.cpp:228] Setting up pool1
I1226 13:38:28.969195 122432 net.cpp:235] Top shape: 256 96 27 27 (17915904)
I1226 13:38:28.969238 122432 net.cpp:243] Memory required for data: 1122072576
I1226 13:38:28.969279 122432 layer_factory.hpp:114] Creating layer conv2
I1226 13:38:28.969380 122432 net.cpp:178] Creating Layer conv2
I1226 13:38:28.969427 122432 net.cpp:612] conv2 <- pool1
I1226 13:38:28.969480 122432 net.cpp:586] conv2 -> conv2
I1226 13:38:28.994473 96543 net.cpp:228] Setting up data
I1226 13:38:28.994619 96543 net.cpp:235] Top shape: 256 3 227 227 (39574272)
I1226 13:38:28.994663 96543 net.cpp:235] Top shape: 256 (256)
I1226 13:38:28.994693 96543 net.cpp:243] Memory required for data: 158298112
I1226 13:38:28.994735 96543 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 13:38:28.994864 96543 net.cpp:178] Creating Layer label_data_1_split
I1226 13:38:28.995038 96543 net.cpp:612] label_data_1_split <- label
I1226 13:38:28.995100 96543 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 13:38:28.995163 96543 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 13:38:29.029268 96543 net.cpp:228] Setting up label_data_1_split
I1226 13:38:29.029383 96543 net.cpp:235] Top shape: 256 (256)
I1226 13:38:29.029422 96543 net.cpp:235] Top shape: 256 (256)
I1226 13:38:29.029449 96543 net.cpp:243] Memory required for data: 158300160
I1226 13:38:29.029489 96543 layer_factory.hpp:114] Creating layer conv1
I1226 13:38:29.029606 96543 net.cpp:178] Creating Layer conv1
I1226 13:38:29.029666 96543 net.cpp:612] conv1 <- data
I1226 13:38:29.029726 96543 net.cpp:586] conv1 -> conv1
I1226 13:38:29.077389 91021 net.cpp:228] Setting up conv2
I1226 13:38:29.077500 91021 net.cpp:235] Top shape: 256 256 27 27 (47775744)
I1226 13:38:29.077527 91021 net.cpp:243] Memory required for data: 1313175552
I1226 13:38:29.077600 91021 layer_factory.hpp:114] Creating layer relu2
I1226 13:38:29.077683 91021 net.cpp:178] Creating Layer relu2
I1226 13:38:29.077721 91021 net.cpp:612] relu2 <- conv2
I1226 13:38:29.077769 91021 net.cpp:573] relu2 -> conv2 (in-place)
I1226 13:38:29.077862 91021 net.cpp:228] Setting up relu2
I1226 13:38:29.077900 91021 net.cpp:235] Top shape: 256 256 27 27 (47775744)
I1226 13:38:29.077924 91021 net.cpp:243] Memory required for data: 1504278528
I1226 13:38:29.077960 91021 layer_factory.hpp:114] Creating layer norm2
I1226 13:38:29.078017 91021 net.cpp:178] Creating Layer norm2
I1226 13:38:29.078045 91021 net.cpp:612] norm2 <- conv2
I1226 13:38:29.078080 91021 net.cpp:586] norm2 -> norm2
I1226 13:38:29.078162 91021 net.cpp:228] Setting up norm2
I1226 13:38:29.078207 91021 net.cpp:235] Top shape: 256 256 27 27 (47775744)
I1226 13:38:29.078233 91021 net.cpp:243] Memory required for data: 1695381504
I1226 13:38:29.082046 91021 layer_factory.hpp:114] Creating layer pool2
I1226 13:38:29.082178 91021 net.cpp:178] Creating Layer pool2
I1226 13:38:29.082219 91021 net.cpp:612] pool2 <- norm2
I1226 13:38:29.082311 91021 net.cpp:586] pool2 -> pool2
I1226 13:38:29.082499 91021 net.cpp:228] Setting up pool2
I1226 13:38:29.082690 91021 net.cpp:235] Top shape: 256 256 13 13 (11075584)
I1226 13:38:29.082722 91021 net.cpp:243] Memory required for data: 1739683840
I1226 13:38:29.082754 91021 layer_factory.hpp:114] Creating layer conv3
I1226 13:38:29.082852 91021 net.cpp:178] Creating Layer conv3
I1226 13:38:29.082902 91021 net.cpp:612] conv3 <- pool2
I1226 13:38:29.082949 91021 net.cpp:586] conv3 -> conv3
I1226 13:38:29.094537 96918 net.cpp:228] Setting up data
I1226 13:38:29.094666 96918 net.cpp:235] Top shape: 256 3 227 227 (39574272)
I1226 13:38:29.094714 96918 net.cpp:235] Top shape: 256 (256)
I1226 13:38:29.094775 96918 net.cpp:243] Memory required for data: 158298112
I1226 13:38:29.094820 96918 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 13:38:29.094998 96918 net.cpp:178] Creating Layer label_data_1_split
I1226 13:38:29.095154 96918 net.cpp:612] label_data_1_split <- label
I1226 13:38:29.095208 96918 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 13:38:29.095270 96918 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 13:38:29.106436 89419 net.cpp:228] Setting up conv2
I1226 13:38:29.106575 89419 net.cpp:235] Top shape: 256 256 27 27 (47775744)
I1226 13:38:29.106665 89419 net.cpp:243] Memory required for data: 1313175552
I1226 13:38:29.106768 89419 layer_factory.hpp:114] Creating layer relu2
I1226 13:38:29.106906 89419 net.cpp:178] Creating Layer relu2
I1226 13:38:29.106962 89419 net.cpp:612] relu2 <- conv2
I1226 13:38:29.107025 89419 net.cpp:573] relu2 -> conv2 (in-place)
I1226 13:38:29.107136 89419 net.cpp:228] Setting up relu2
I1226 13:38:29.107214 89419 net.cpp:235] Top shape: 256 256 27 27 (47775744)
I1226 13:38:29.107244 89419 net.cpp:243] Memory required for data: 1504278528
I1226 13:38:29.107275 89419 layer_factory.hpp:114] Creating layer norm2
I1226 13:38:29.107328 89419 net.cpp:178] Creating Layer norm2
I1226 13:38:29.107355 89419 net.cpp:612] norm2 <- conv2
I1226 13:38:29.107406 89419 net.cpp:586] norm2 -> norm2
I1226 13:38:29.107533 89419 net.cpp:228] Setting up norm2
I1226 13:38:29.107679 89419 net.cpp:235] Top shape: 256 256 27 27 (47775744)
I1226 13:38:29.107712 89419 net.cpp:243] Memory required for data: 1695381504
I1226 13:38:29.107842 89419 layer_factory.hpp:114] Creating layer pool2
I1226 13:38:29.108453 89419 net.cpp:178] Creating Layer pool2
I1226 13:38:29.108513 89419 net.cpp:612] pool2 <- norm2
I1226 13:38:29.108594 89419 net.cpp:586] pool2 -> pool2
I1226 13:38:29.108781 89419 net.cpp:228] Setting up pool2
I1226 13:38:29.108947 89419 net.cpp:235] Top shape: 256 256 13 13 (11075584)
I1226 13:38:29.108976 89419 net.cpp:243] Memory required for data: 1739683840
I1226 13:38:29.109009 89419 layer_factory.hpp:114] Creating layer conv3
I1226 13:38:29.109125 89419 net.cpp:178] Creating Layer conv3
I1226 13:38:29.109166 89419 net.cpp:612] conv3 <- pool2
I1226 13:38:29.109239 89419 net.cpp:586] conv3 -> conv3
I1226 13:38:29.108006 96918 net.cpp:228] Setting up label_data_1_split
I1226 13:38:29.108119 96918 net.cpp:235] Top shape: 256 (256)
I1226 13:38:29.108156 96918 net.cpp:235] Top shape: 256 (256)
I1226 13:38:29.108181 96918 net.cpp:243] Memory required for data: 158300160
I1226 13:38:29.108218 96918 layer_factory.hpp:114] Creating layer conv1
I1226 13:38:29.108331 96918 net.cpp:178] Creating Layer conv1
I1226 13:38:29.108371 96918 net.cpp:612] conv1 <- data
I1226 13:38:29.108419 96918 net.cpp:586] conv1 -> conv1
I1226 13:38:29.125797 91865 net.cpp:228] Setting up conv2
I1226 13:38:29.125903 91865 net.cpp:235] Top shape: 256 256 27 27 (47775744)
I1226 13:38:29.125931 91865 net.cpp:243] Memory required for data: 1313175552
I1226 13:38:29.126040 91865 layer_factory.hpp:114] Creating layer relu2
I1226 13:38:29.126178 91865 net.cpp:178] Creating Layer relu2
I1226 13:38:29.126216 91865 net.cpp:612] relu2 <- conv2
I1226 13:38:29.126281 91865 net.cpp:573] relu2 -> conv2 (in-place)
I1226 13:38:29.126405 91865 net.cpp:228] Setting up relu2
I1226 13:38:29.126448 91865 net.cpp:235] Top shape: 256 256 27 27 (47775744)
I1226 13:38:29.126471 91865 net.cpp:243] Memory required for data: 1504278528
I1226 13:38:29.126498 91865 layer_factory.hpp:114] Creating layer norm2
I1226 13:38:29.126543 91865 net.cpp:178] Creating Layer norm2
I1226 13:38:29.126567 91865 net.cpp:612] norm2 <- conv2
I1226 13:38:29.126888 91865 net.cpp:586] norm2 -> norm2
I1226 13:38:29.127002 91865 net.cpp:228] Setting up norm2
I1226 13:38:29.127051 91865 net.cpp:235] Top shape: 256 256 27 27 (47775744)
I1226 13:38:29.127074 91865 net.cpp:243] Memory required for data: 1695381504
I1226 13:38:29.127149 91865 layer_factory.hpp:114] Creating layer pool2
I1226 13:38:29.127214 91865 net.cpp:178] Creating Layer pool2
I1226 13:38:29.127266 91865 net.cpp:612] pool2 <- norm2
I1226 13:38:29.127302 91865 net.cpp:586] pool2 -> pool2
I1226 13:38:29.127403 91865 net.cpp:228] Setting up pool2
I1226 13:38:29.127547 91865 net.cpp:235] Top shape: 256 256 13 13 (11075584)
I1226 13:38:29.127574 91865 net.cpp:243] Memory required for data: 1739683840
I1226 13:38:29.127604 91865 layer_factory.hpp:114] Creating layer conv3
I1226 13:38:29.127686 91865 net.cpp:178] Creating Layer conv3
I1226 13:38:29.127730 91865 net.cpp:612] conv3 <- pool2
I1226 13:38:29.127769 91865 net.cpp:586] conv3 -> conv3
I1226 13:38:29.130996 92321 net.cpp:228] Setting up conv2
I1226 13:38:29.131099 92321 net.cpp:235] Top shape: 256 256 27 27 (47775744)
I1226 13:38:29.131125 92321 net.cpp:243] Memory required for data: 1313175552
I1226 13:38:29.131197 92321 layer_factory.hpp:114] Creating layer relu2
I1226 13:38:29.131265 92321 net.cpp:178] Creating Layer relu2
I1226 13:38:29.131295 92321 net.cpp:612] relu2 <- conv2
I1226 13:38:29.131350 92321 net.cpp:573] relu2 -> conv2 (in-place)
I1226 13:38:29.131449 92321 net.cpp:228] Setting up relu2
I1226 13:38:29.131548 92321 net.cpp:235] Top shape: 256 256 27 27 (47775744)
I1226 13:38:29.131574 92321 net.cpp:243] Memory required for data: 1504278528
I1226 13:38:29.131603 92321 layer_factory.hpp:114] Creating layer norm2
I1226 13:38:29.132099 92321 net.cpp:178] Creating Layer norm2
I1226 13:38:29.132154 92321 net.cpp:612] norm2 <- conv2
I1226 13:38:29.132246 92321 net.cpp:586] norm2 -> norm2
I1226 13:38:29.132419 92321 net.cpp:228] Setting up norm2
I1226 13:38:29.132504 92321 net.cpp:235] Top shape: 256 256 27 27 (47775744)
I1226 13:38:29.132531 92321 net.cpp:243] Memory required for data: 1695381504
I1226 13:38:29.132573 92321 layer_factory.hpp:114] Creating layer pool2
I1226 13:38:29.132658 92321 net.cpp:178] Creating Layer pool2
I1226 13:38:29.132710 92321 net.cpp:612] pool2 <- norm2
I1226 13:38:29.132745 92321 net.cpp:586] pool2 -> pool2
I1226 13:38:29.132851 92321 net.cpp:228] Setting up pool2
I1226 13:38:29.133004 92321 net.cpp:235] Top shape: 256 256 13 13 (11075584)
I1226 13:38:29.133039 92321 net.cpp:243] Memory required for data: 1739683840
I1226 13:38:29.133077 92321 layer_factory.hpp:114] Creating layer conv3
I1226 13:38:29.133160 92321 net.cpp:178] Creating Layer conv3
I1226 13:38:29.133216 92321 net.cpp:612] conv3 <- pool2
I1226 13:38:29.133256 92321 net.cpp:586] conv3 -> conv3
I1226 13:38:29.308147 93001 net.cpp:228] Setting up conv2
I1226 13:38:29.308256 93001 net.cpp:235] Top shape: 256 256 27 27 (47775744)
I1226 13:38:29.308284 93001 net.cpp:243] Memory required for data: 1313175552
I1226 13:38:29.308353 93001 layer_factory.hpp:114] Creating layer relu2
I1226 13:38:29.308596 93001 net.cpp:178] Creating Layer relu2
I1226 13:38:29.308704 93001 net.cpp:612] relu2 <- conv2
I1226 13:38:29.308744 93001 net.cpp:573] relu2 -> conv2 (in-place)
I1226 13:38:29.308837 93001 net.cpp:228] Setting up relu2
I1226 13:38:29.308879 93001 net.cpp:235] Top shape: 256 256 27 27 (47775744)
I1226 13:38:29.308902 93001 net.cpp:243] Memory required for data: 1504278528
I1226 13:38:29.308928 93001 layer_factory.hpp:114] Creating layer norm2
I1226 13:38:29.308974 93001 net.cpp:178] Creating Layer norm2
I1226 13:38:29.309002 93001 net.cpp:612] norm2 <- conv2
I1226 13:38:29.309036 93001 net.cpp:586] norm2 -> norm2
I1226 13:38:29.309120 93001 net.cpp:228] Setting up norm2
I1226 13:38:29.309161 93001 net.cpp:235] Top shape: 256 256 27 27 (47775744)
I1226 13:38:29.309188 93001 net.cpp:243] Memory required for data: 1695381504
I1226 13:38:29.309213 93001 layer_factory.hpp:114] Creating layer pool2
I1226 13:38:29.309262 93001 net.cpp:178] Creating Layer pool2
I1226 13:38:29.309286 93001 net.cpp:612] pool2 <- norm2
I1226 13:38:29.309322 93001 net.cpp:586] pool2 -> pool2
I1226 13:38:29.309419 93001 net.cpp:228] Setting up pool2
I1226 13:38:29.309548 93001 net.cpp:235] Top shape: 256 256 13 13 (11075584)
I1226 13:38:29.309572 93001 net.cpp:243] Memory required for data: 1739683840
I1226 13:38:29.309598 93001 layer_factory.hpp:114] Creating layer conv3
I1226 13:38:29.309671 93001 net.cpp:178] Creating Layer conv3
I1226 13:38:29.309703 93001 net.cpp:612] conv3 <- pool2
I1226 13:38:29.309741 93001 net.cpp:586] conv3 -> conv3
I1226 13:38:29.465723 94726 net.cpp:228] Setting up conv2
I1226 13:38:29.465873 94726 net.cpp:235] Top shape: 256 256 27 27 (47775744)
I1226 13:38:29.465917 94726 net.cpp:243] Memory required for data: 1313175552
I1226 13:38:29.466011 94726 layer_factory.hpp:114] Creating layer relu2
I1226 13:38:29.466125 94726 net.cpp:178] Creating Layer relu2
I1226 13:38:29.466173 94726 net.cpp:612] relu2 <- conv2
I1226 13:38:29.466220 94726 net.cpp:573] relu2 -> conv2 (in-place)
I1226 13:38:29.466359 94726 net.cpp:228] Setting up relu2
I1226 13:38:29.466430 94726 net.cpp:235] Top shape: 256 256 27 27 (47775744)
I1226 13:38:29.466460 94726 net.cpp:243] Memory required for data: 1504278528
I1226 13:38:29.466496 94726 layer_factory.hpp:114] Creating layer norm2
I1226 13:38:29.466564 94726 net.cpp:178] Creating Layer norm2
I1226 13:38:29.466605 94726 net.cpp:612] norm2 <- conv2
I1226 13:38:29.466648 94726 net.cpp:586] norm2 -> norm2
I1226 13:38:29.466768 94726 net.cpp:228] Setting up norm2
I1226 13:38:29.466856 94726 net.cpp:235] Top shape: 256 256 27 27 (47775744)
I1226 13:38:29.466888 94726 net.cpp:243] Memory required for data: 1695381504
I1226 13:38:29.466922 94726 layer_factory.hpp:114] Creating layer pool2
I1226 13:38:29.467003 94726 net.cpp:178] Creating Layer pool2
I1226 13:38:29.467046 94726 net.cpp:612] pool2 <- norm2
I1226 13:38:29.467100 94726 net.cpp:586] pool2 -> pool2
I1226 13:38:29.467201 94726 net.cpp:228] Setting up pool2
I1226 13:38:29.467259 94726 net.cpp:235] Top shape: 256 256 13 13 (11075584)
I1226 13:38:29.467396 94726 net.cpp:243] Memory required for data: 1739683840
I1226 13:38:29.467437 94726 layer_factory.hpp:114] Creating layer conv3
I1226 13:38:29.467533 94726 net.cpp:178] Creating Layer conv3
I1226 13:38:29.467576 94726 net.cpp:612] conv3 <- pool2
I1226 13:38:29.467623 94726 net.cpp:586] conv3 -> conv3
I1226 13:38:29.490941 91021 net.cpp:228] Setting up conv3
I1226 13:38:29.491055 91021 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 13:38:29.491081 91021 net.cpp:243] Memory required for data: 1806137344
I1226 13:38:29.491153 91021 layer_factory.hpp:114] Creating layer relu3
I1226 13:38:29.491217 91021 net.cpp:178] Creating Layer relu3
I1226 13:38:29.491269 91021 net.cpp:612] relu3 <- conv3
I1226 13:38:29.491309 91021 net.cpp:573] relu3 -> conv3 (in-place)
I1226 13:38:29.491407 91021 net.cpp:228] Setting up relu3
I1226 13:38:29.491446 91021 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 13:38:29.491469 91021 net.cpp:243] Memory required for data: 1872590848
I1226 13:38:29.491497 91021 layer_factory.hpp:114] Creating layer conv4
I1226 13:38:29.491587 91021 net.cpp:178] Creating Layer conv4
I1226 13:38:29.491621 91021 net.cpp:612] conv4 <- conv3
I1226 13:38:29.491660 91021 net.cpp:586] conv4 -> conv4
I1226 13:38:29.481768 122432 net.cpp:228] Setting up conv2
I1226 13:38:29.481904 122432 net.cpp:235] Top shape: 256 256 27 27 (47775744)
I1226 13:38:29.481946 122432 net.cpp:243] Memory required for data: 1313175552
I1226 13:38:29.482046 122432 layer_factory.hpp:114] Creating layer relu2
I1226 13:38:29.482137 122432 net.cpp:178] Creating Layer relu2
I1226 13:38:29.482180 122432 net.cpp:612] relu2 <- conv2
I1226 13:38:29.482285 122432 net.cpp:573] relu2 -> conv2 (in-place)
I1226 13:38:29.482415 122432 net.cpp:228] Setting up relu2
I1226 13:38:29.482494 122432 net.cpp:235] Top shape: 256 256 27 27 (47775744)
I1226 13:38:29.482530 122432 net.cpp:243] Memory required for data: 1504278528
I1226 13:38:29.482574 122432 layer_factory.hpp:114] Creating layer norm2
I1226 13:38:29.482684 122432 net.cpp:178] Creating Layer norm2
I1226 13:38:29.482729 122432 net.cpp:612] norm2 <- conv2
I1226 13:38:29.502750 89419 net.cpp:228] Setting up conv3
I1226 13:38:29.503109 89419 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 13:38:29.503149 89419 net.cpp:243] Memory required for data: 1806137344
I1226 13:38:29.503239 89419 layer_factory.hpp:114] Creating layer relu3
I1226 13:38:29.503341 89419 net.cpp:178] Creating Layer relu3
I1226 13:38:29.503376 89419 net.cpp:612] relu3 <- conv3
I1226 13:38:29.503444 89419 net.cpp:573] relu3 -> conv3 (in-place)
I1226 13:38:29.503554 89419 net.cpp:228] Setting up relu3
I1226 13:38:29.503634 89419 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 13:38:29.503665 89419 net.cpp:243] Memory required for data: 1872590848
I1226 13:38:29.503697 89419 layer_factory.hpp:114] Creating layer conv4
I1226 13:38:29.503782 89419 net.cpp:178] Creating Layer conv4
I1226 13:38:29.503820 89419 net.cpp:612] conv4 <- conv3
I1226 13:38:29.503872 89419 net.cpp:586] conv4 -> conv4
I1226 13:38:29.482805 122432 net.cpp:586] norm2 -> norm2
I1226 13:38:29.489841 122432 net.cpp:228] Setting up norm2
I1226 13:38:29.489943 122432 net.cpp:235] Top shape: 256 256 27 27 (47775744)
I1226 13:38:29.489979 122432 net.cpp:243] Memory required for data: 1695381504
I1226 13:38:29.490025 122432 layer_factory.hpp:114] Creating layer pool2
I1226 13:38:29.490121 122432 net.cpp:178] Creating Layer pool2
I1226 13:38:29.491336 122432 net.cpp:612] pool2 <- norm2
I1226 13:38:29.491487 122432 net.cpp:586] pool2 -> pool2
I1226 13:38:29.491946 122432 net.cpp:228] Setting up pool2
I1226 13:38:29.492041 122432 net.cpp:235] Top shape: 256 256 13 13 (11075584)
I1226 13:38:29.492075 122432 net.cpp:243] Memory required for data: 1739683840
I1226 13:38:29.492120 122432 layer_factory.hpp:114] Creating layer conv3
I1226 13:38:29.493249 122432 net.cpp:178] Creating Layer conv3
I1226 13:38:29.493365 122432 net.cpp:612] conv3 <- pool2
I1226 13:38:29.493448 122432 net.cpp:586] conv3 -> conv3
I1226 13:38:29.511538 91865 net.cpp:228] Setting up conv3
I1226 13:38:29.511664 91865 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 13:38:29.511692 91865 net.cpp:243] Memory required for data: 1806137344
I1226 13:38:29.511792 91865 layer_factory.hpp:114] Creating layer relu3
I1226 13:38:29.511904 91865 net.cpp:178] Creating Layer relu3
I1226 13:38:29.511977 91865 net.cpp:612] relu3 <- conv3
I1226 13:38:29.512033 91865 net.cpp:573] relu3 -> conv3 (in-place)
I1226 13:38:29.512145 91865 net.cpp:228] Setting up relu3
I1226 13:38:29.512635 91865 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 13:38:29.512703 91865 net.cpp:243] Memory required for data: 1872590848
I1226 13:38:29.512749 91865 layer_factory.hpp:114] Creating layer conv4
I1226 13:38:29.512897 91865 net.cpp:178] Creating Layer conv4
I1226 13:38:29.512943 91865 net.cpp:612] conv4 <- conv3
I1226 13:38:29.513025 91865 net.cpp:586] conv4 -> conv4
I1226 13:38:29.550290 92321 net.cpp:228] Setting up conv3
I1226 13:38:29.550909 92321 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 13:38:29.551007 92321 net.cpp:243] Memory required for data: 1806137344
I1226 13:38:29.551126 92321 layer_factory.hpp:114] Creating layer relu3
I1226 13:38:29.551261 92321 net.cpp:178] Creating Layer relu3
I1226 13:38:29.551376 92321 net.cpp:612] relu3 <- conv3
I1226 13:38:29.551436 92321 net.cpp:573] relu3 -> conv3 (in-place)
I1226 13:38:29.551645 92321 net.cpp:228] Setting up relu3
I1226 13:38:29.551750 92321 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 13:38:29.551776 92321 net.cpp:243] Memory required for data: 1872590848
I1226 13:38:29.551807 92321 layer_factory.hpp:114] Creating layer conv4
I1226 13:38:29.551923 92321 net.cpp:178] Creating Layer conv4
I1226 13:38:29.552348 92321 net.cpp:612] conv4 <- conv3
I1226 13:38:29.552466 92321 net.cpp:586] conv4 -> conv4
I1226 13:38:29.666084 96543 net.cpp:228] Setting up conv1
I1226 13:38:29.666242 96543 net.cpp:235] Top shape: 256 96 55 55 (74342400)
I1226 13:38:29.666324 96543 net.cpp:243] Memory required for data: 455669760
I1226 13:38:29.666462 96543 layer_factory.hpp:114] Creating layer relu1
I1226 13:38:29.666623 96543 net.cpp:178] Creating Layer relu1
I1226 13:38:29.666877 96543 net.cpp:612] relu1 <- conv1
I1226 13:38:29.666977 96543 net.cpp:573] relu1 -> conv1 (in-place)
I1226 13:38:29.667209 96543 net.cpp:228] Setting up relu1
I1226 13:38:29.667305 96543 net.cpp:235] Top shape: 256 96 55 55 (74342400)
I1226 13:38:29.667361 96543 net.cpp:243] Memory required for data: 753039360
I1226 13:38:29.667428 96543 layer_factory.hpp:114] Creating layer norm1
I1226 13:38:29.668340 96543 net.cpp:178] Creating Layer norm1
I1226 13:38:29.668432 96543 net.cpp:612] norm1 <- conv1
I1226 13:38:29.668493 96543 net.cpp:586] norm1 -> norm1
I1226 13:38:29.668664 96543 net.cpp:228] Setting up norm1
I1226 13:38:29.668848 96543 net.cpp:235] Top shape: 256 96 55 55 (74342400)
I1226 13:38:29.668915 96543 net.cpp:243] Memory required for data: 1050408960
I1226 13:38:29.668952 96543 layer_factory.hpp:114] Creating layer pool1
I1226 13:38:29.669122 96543 net.cpp:178] Creating Layer pool1
I1226 13:38:29.669188 96543 net.cpp:612] pool1 <- norm1
I1226 13:38:29.669250 96543 net.cpp:586] pool1 -> pool1
I1226 13:38:29.669881 96543 net.cpp:228] Setting up pool1
I1226 13:38:29.669972 96543 net.cpp:235] Top shape: 256 96 27 27 (17915904)
I1226 13:38:29.670001 96543 net.cpp:243] Memory required for data: 1122072576
I1226 13:38:29.670037 96543 layer_factory.hpp:114] Creating layer conv2
I1226 13:38:29.670186 96543 net.cpp:178] Creating Layer conv2
I1226 13:38:29.670348 96543 net.cpp:612] conv2 <- pool1
I1226 13:38:29.670441 96543 net.cpp:586] conv2 -> conv2
I1226 13:38:29.734609 96918 net.cpp:228] Setting up conv1
I1226 13:38:29.735069 96918 net.cpp:235] Top shape: 256 96 55 55 (74342400)
I1226 13:38:29.735112 96918 net.cpp:243] Memory required for data: 455669760
I1226 13:38:29.735307 96918 layer_factory.hpp:114] Creating layer relu1
I1226 13:38:29.735419 96918 net.cpp:178] Creating Layer relu1
I1226 13:38:29.735466 96918 net.cpp:612] relu1 <- conv1
I1226 13:38:29.735507 96918 net.cpp:573] relu1 -> conv1 (in-place)
I1226 13:38:29.735618 96918 net.cpp:228] Setting up relu1
I1226 13:38:29.735671 96918 net.cpp:235] Top shape: 256 96 55 55 (74342400)
I1226 13:38:29.735698 96918 net.cpp:243] Memory required for data: 753039360
I1226 13:38:29.735800 96918 layer_factory.hpp:114] Creating layer norm1
I1226 13:38:29.736003 96918 net.cpp:178] Creating Layer norm1
I1226 13:38:29.736098 96918 net.cpp:612] norm1 <- conv1
I1226 13:38:29.736171 96918 net.cpp:586] norm1 -> norm1
I1226 13:38:29.736845 96918 net.cpp:228] Setting up norm1
I1226 13:38:29.737009 96918 net.cpp:235] Top shape: 256 96 55 55 (74342400)
I1226 13:38:29.737041 96918 net.cpp:243] Memory required for data: 1050408960
I1226 13:38:29.737077 96918 layer_factory.hpp:114] Creating layer pool1
I1226 13:38:29.737174 96918 net.cpp:178] Creating Layer pool1
I1226 13:38:29.737213 96918 net.cpp:612] pool1 <- norm1
I1226 13:38:29.737264 96918 net.cpp:586] pool1 -> pool1
I1226 13:38:29.737437 96918 net.cpp:228] Setting up pool1
I1226 13:38:29.737517 96918 net.cpp:235] Top shape: 256 96 27 27 (17915904)
I1226 13:38:29.737545 96918 net.cpp:243] Memory required for data: 1122072576
I1226 13:38:29.737679 96918 layer_factory.hpp:114] Creating layer conv2
I1226 13:38:29.738168 96918 net.cpp:178] Creating Layer conv2
I1226 13:38:29.738307 96918 net.cpp:612] conv2 <- pool1
I1226 13:38:29.738420 96918 net.cpp:586] conv2 -> conv2
I1226 13:38:29.767129 93001 net.cpp:228] Setting up conv3
I1226 13:38:29.767235 93001 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 13:38:29.767261 93001 net.cpp:243] Memory required for data: 1806137344
I1226 13:38:29.767333 93001 layer_factory.hpp:114] Creating layer relu3
I1226 13:38:29.767416 93001 net.cpp:178] Creating Layer relu3
I1226 13:38:29.767448 93001 net.cpp:612] relu3 <- conv3
I1226 13:38:29.767513 93001 net.cpp:573] relu3 -> conv3 (in-place)
I1226 13:38:29.767599 93001 net.cpp:228] Setting up relu3
I1226 13:38:29.767642 93001 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 13:38:29.767665 93001 net.cpp:243] Memory required for data: 1872590848
I1226 13:38:29.767693 93001 layer_factory.hpp:114] Creating layer conv4
I1226 13:38:29.767770 93001 net.cpp:178] Creating Layer conv4
I1226 13:38:29.767802 93001 net.cpp:612] conv4 <- conv3
I1226 13:38:29.767839 93001 net.cpp:586] conv4 -> conv4
I1226 13:38:29.815943 91021 net.cpp:228] Setting up conv4
I1226 13:38:29.816076 91021 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 13:38:29.816107 91021 net.cpp:243] Memory required for data: 1939044352
I1226 13:38:29.816166 91021 layer_factory.hpp:114] Creating layer relu4
I1226 13:38:29.816349 91021 net.cpp:178] Creating Layer relu4
I1226 13:38:29.816504 91021 net.cpp:612] relu4 <- conv4
I1226 13:38:29.816571 91021 net.cpp:573] relu4 -> conv4 (in-place)
I1226 13:38:29.816737 91021 net.cpp:228] Setting up relu4
I1226 13:38:29.816853 91021 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 13:38:29.816876 91021 net.cpp:243] Memory required for data: 2005497856
I1226 13:38:29.817219 91021 layer_factory.hpp:114] Creating layer conv5
I1226 13:38:29.817380 91021 net.cpp:178] Creating Layer conv5
I1226 13:38:29.817447 91021 net.cpp:612] conv5 <- conv4
I1226 13:38:29.817555 91021 net.cpp:586] conv5 -> conv5
I1226 13:38:29.852463 89419 net.cpp:228] Setting up conv4
I1226 13:38:29.852572 89419 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 13:38:29.852601 89419 net.cpp:243] Memory required for data: 1939044352
I1226 13:38:29.852694 89419 layer_factory.hpp:114] Creating layer relu4
I1226 13:38:29.852754 89419 net.cpp:178] Creating Layer relu4
I1226 13:38:29.852794 89419 net.cpp:612] relu4 <- conv4
I1226 13:38:29.852833 89419 net.cpp:573] relu4 -> conv4 (in-place)
I1226 13:38:29.852944 89419 net.cpp:228] Setting up relu4
I1226 13:38:29.852994 89419 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 13:38:29.853018 89419 net.cpp:243] Memory required for data: 2005497856
I1226 13:38:29.853057 89419 layer_factory.hpp:114] Creating layer conv5
I1226 13:38:29.853145 89419 net.cpp:178] Creating Layer conv5
I1226 13:38:29.853173 89419 net.cpp:612] conv5 <- conv4
I1226 13:38:29.853350 89419 net.cpp:586] conv5 -> conv5
I1226 13:38:29.882028 91865 net.cpp:228] Setting up conv4
I1226 13:38:29.882146 91865 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 13:38:29.882175 91865 net.cpp:243] Memory required for data: 1939044352
I1226 13:38:29.882277 91865 layer_factory.hpp:114] Creating layer relu4
I1226 13:38:29.882444 91865 net.cpp:178] Creating Layer relu4
I1226 13:38:29.882520 91865 net.cpp:612] relu4 <- conv4
I1226 13:38:29.882576 91865 net.cpp:573] relu4 -> conv4 (in-place)
I1226 13:38:29.882692 91865 net.cpp:228] Setting up relu4
I1226 13:38:29.882746 91865 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 13:38:29.882776 91865 net.cpp:243] Memory required for data: 2005497856
I1226 13:38:29.882838 91865 layer_factory.hpp:114] Creating layer conv5
I1226 13:38:29.882956 91865 net.cpp:178] Creating Layer conv5
I1226 13:38:29.882992 91865 net.cpp:612] conv5 <- conv4
I1226 13:38:29.883059 91865 net.cpp:586] conv5 -> conv5
I1226 13:38:29.954509 92321 net.cpp:228] Setting up conv4
I1226 13:38:29.954627 92321 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 13:38:29.954655 92321 net.cpp:243] Memory required for data: 1939044352
I1226 13:38:29.954716 92321 layer_factory.hpp:114] Creating layer relu4
I1226 13:38:29.954804 92321 net.cpp:178] Creating Layer relu4
I1226 13:38:29.954855 92321 net.cpp:612] relu4 <- conv4
I1226 13:38:29.954898 92321 net.cpp:573] relu4 -> conv4 (in-place)
I1226 13:38:29.954995 92321 net.cpp:228] Setting up relu4
I1226 13:38:29.955047 92321 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 13:38:29.955073 92321 net.cpp:243] Memory required for data: 2005497856
I1226 13:38:29.955102 92321 layer_factory.hpp:114] Creating layer conv5
I1226 13:38:29.955181 92321 net.cpp:178] Creating Layer conv5
I1226 13:38:29.955219 92321 net.cpp:612] conv5 <- conv4
I1226 13:38:29.955268 92321 net.cpp:586] conv5 -> conv5
I1226 13:38:30.073390 91021 net.cpp:228] Setting up conv5
I1226 13:38:30.073508 91021 net.cpp:235] Top shape: 256 256 13 13 (11075584)
I1226 13:38:30.073537 91021 net.cpp:243] Memory required for data: 2049800192
I1226 13:38:30.073662 91021 layer_factory.hpp:114] Creating layer relu5
I1226 13:38:30.073760 91021 net.cpp:178] Creating Layer relu5
I1226 13:38:30.073810 91021 net.cpp:612] relu5 <- conv5
I1226 13:38:30.073858 91021 net.cpp:573] relu5 -> conv5 (in-place)
I1226 13:38:30.073968 91021 net.cpp:228] Setting up relu5
I1226 13:38:30.074024 91021 net.cpp:235] Top shape: 256 256 13 13 (11075584)
I1226 13:38:30.074055 91021 net.cpp:243] Memory required for data: 2094102528
I1226 13:38:30.074090 91021 layer_factory.hpp:114] Creating layer pool5
I1226 13:38:30.079649 91021 net.cpp:178] Creating Layer pool5
I1226 13:38:30.079735 91021 net.cpp:612] pool5 <- conv5
I1226 13:38:30.079814 91021 net.cpp:586] pool5 -> pool5
I1226 13:38:30.079977 91021 net.cpp:228] Setting up pool5
I1226 13:38:30.080041 91021 net.cpp:235] Top shape: 256 256 6 6 (2359296)
I1226 13:38:30.080071 91021 net.cpp:243] Memory required for data: 2103539712
I1226 13:38:30.080149 91021 layer_factory.hpp:114] Creating layer fc6
I1226 13:38:30.080459 91021 net.cpp:178] Creating Layer fc6
I1226 13:38:30.080497 91021 net.cpp:612] fc6 <- pool5
I1226 13:38:30.080564 91021 net.cpp:586] fc6 -> fc6
I1226 13:38:30.096045 89419 net.cpp:228] Setting up conv5
I1226 13:38:30.096184 89419 net.cpp:235] Top shape: 256 256 13 13 (11075584)
I1226 13:38:30.096217 89419 net.cpp:243] Memory required for data: 2049800192
I1226 13:38:30.096295 89419 layer_factory.hpp:114] Creating layer relu5
I1226 13:38:30.096441 89419 net.cpp:178] Creating Layer relu5
I1226 13:38:30.096549 89419 net.cpp:612] relu5 <- conv5
I1226 13:38:30.096597 89419 net.cpp:573] relu5 -> conv5 (in-place)
I1226 13:38:30.096738 89419 net.cpp:228] Setting up relu5
I1226 13:38:30.096881 89419 net.cpp:235] Top shape: 256 256 13 13 (11075584)
I1226 13:38:30.096912 89419 net.cpp:243] Memory required for data: 2094102528
I1226 13:38:30.096943 89419 layer_factory.hpp:114] Creating layer pool5
I1226 13:38:30.097043 89419 net.cpp:178] Creating Layer pool5
I1226 13:38:30.097096 89419 net.cpp:612] pool5 <- conv5
I1226 13:38:30.097148 89419 net.cpp:586] pool5 -> pool5
I1226 13:38:30.097244 89419 net.cpp:228] Setting up pool5
I1226 13:38:30.097599 89419 net.cpp:235] Top shape: 256 256 6 6 (2359296)
I1226 13:38:30.097662 89419 net.cpp:243] Memory required for data: 2103539712
I1226 13:38:30.097702 89419 layer_factory.hpp:114] Creating layer fc6
I1226 13:38:30.097779 89419 net.cpp:178] Creating Layer fc6
I1226 13:38:30.097854 89419 net.cpp:612] fc6 <- pool5
I1226 13:38:30.097899 89419 net.cpp:586] fc6 -> fc6
I1226 13:38:30.087934 122432 net.cpp:228] Setting up conv3
I1226 13:38:30.088070 122432 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 13:38:30.088112 122432 net.cpp:243] Memory required for data: 1806137344
I1226 13:38:30.088255 122432 layer_factory.hpp:114] Creating layer relu3
I1226 13:38:30.088345 122432 net.cpp:178] Creating Layer relu3
I1226 13:38:30.088548 122432 net.cpp:612] relu3 <- conv3
I1226 13:38:30.088672 122432 net.cpp:573] relu3 -> conv3 (in-place)
I1226 13:38:30.088874 122432 net.cpp:228] Setting up relu3
I1226 13:38:30.089030 122432 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 13:38:30.089066 122432 net.cpp:243] Memory required for data: 1872590848
I1226 13:38:30.089110 122432 layer_factory.hpp:114] Creating layer conv4
I1226 13:38:30.089244 122432 net.cpp:178] Creating Layer conv4
I1226 13:38:30.089382 122432 net.cpp:612] conv4 <- conv3
I1226 13:38:30.089442 122432 net.cpp:586] conv4 -> conv4
I1226 13:38:30.147946 91865 net.cpp:228] Setting up conv5
I1226 13:38:30.148061 91865 net.cpp:235] Top shape: 256 256 13 13 (11075584)
I1226 13:38:30.148089 91865 net.cpp:243] Memory required for data: 2049800192
I1226 13:38:30.148211 91865 layer_factory.hpp:114] Creating layer relu5
I1226 13:38:30.148303 91865 net.cpp:178] Creating Layer relu5
I1226 13:38:30.148380 91865 net.cpp:612] relu5 <- conv5
I1226 13:38:30.148428 91865 net.cpp:573] relu5 -> conv5 (in-place)
I1226 13:38:30.148541 91865 net.cpp:228] Setting up relu5
I1226 13:38:30.148591 91865 net.cpp:235] Top shape: 256 256 13 13 (11075584)
I1226 13:38:30.148622 91865 net.cpp:243] Memory required for data: 2094102528
I1226 13:38:30.148656 91865 layer_factory.hpp:114] Creating layer pool5
I1226 13:38:30.148725 91865 net.cpp:178] Creating Layer pool5
I1226 13:38:30.148757 91865 net.cpp:612] pool5 <- conv5
I1226 13:38:30.148802 91865 net.cpp:586] pool5 -> pool5
I1226 13:38:30.148918 91865 net.cpp:228] Setting up pool5
I1226 13:38:30.148975 91865 net.cpp:235] Top shape: 256 256 6 6 (2359296)
I1226 13:38:30.149004 91865 net.cpp:243] Memory required for data: 2103539712
I1226 13:38:30.149036 91865 layer_factory.hpp:114] Creating layer fc6
I1226 13:38:30.149116 91865 net.cpp:178] Creating Layer fc6
I1226 13:38:30.149149 91865 net.cpp:612] fc6 <- pool5
I1226 13:38:30.149207 91865 net.cpp:586] fc6 -> fc6
I1226 13:38:30.182699 93001 net.cpp:228] Setting up conv4
I1226 13:38:30.182806 93001 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 13:38:30.182832 93001 net.cpp:243] Memory required for data: 1939044352
I1226 13:38:30.182890 93001 layer_factory.hpp:114] Creating layer relu4
I1226 13:38:30.182971 93001 net.cpp:178] Creating Layer relu4
I1226 13:38:30.183003 93001 net.cpp:612] relu4 <- conv4
I1226 13:38:30.183048 93001 net.cpp:573] relu4 -> conv4 (in-place)
I1226 13:38:30.183156 93001 net.cpp:228] Setting up relu4
I1226 13:38:30.183198 93001 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 13:38:30.183226 93001 net.cpp:243] Memory required for data: 2005497856
I1226 13:38:30.183254 93001 layer_factory.hpp:114] Creating layer conv5
I1226 13:38:30.183333 93001 net.cpp:178] Creating Layer conv5
I1226 13:38:30.183368 93001 net.cpp:612] conv5 <- conv4
I1226 13:38:30.183434 93001 net.cpp:586] conv5 -> conv5
I1226 13:38:30.223055 92321 net.cpp:228] Setting up conv5
I1226 13:38:30.223172 92321 net.cpp:235] Top shape: 256 256 13 13 (11075584)
I1226 13:38:30.223201 92321 net.cpp:243] Memory required for data: 2049800192
I1226 13:38:30.223305 92321 layer_factory.hpp:114] Creating layer relu5
I1226 13:38:30.223373 92321 net.cpp:178] Creating Layer relu5
I1226 13:38:30.223404 92321 net.cpp:612] relu5 <- conv5
I1226 13:38:30.223493 92321 net.cpp:573] relu5 -> conv5 (in-place)
I1226 13:38:30.223601 92321 net.cpp:228] Setting up relu5
I1226 13:38:30.223737 92321 net.cpp:235] Top shape: 256 256 13 13 (11075584)
I1226 13:38:30.223780 92321 net.cpp:243] Memory required for data: 2094102528
I1226 13:38:30.223814 92321 layer_factory.hpp:114] Creating layer pool5
I1226 13:38:30.223956 92321 net.cpp:178] Creating Layer pool5
I1226 13:38:30.223985 92321 net.cpp:612] pool5 <- conv5
I1226 13:38:30.224032 92321 net.cpp:586] pool5 -> pool5
I1226 13:38:30.224129 92321 net.cpp:228] Setting up pool5
I1226 13:38:30.224179 92321 net.cpp:235] Top shape: 256 256 6 6 (2359296)
I1226 13:38:30.224202 92321 net.cpp:243] Memory required for data: 2103539712
I1226 13:38:30.224230 92321 layer_factory.hpp:114] Creating layer fc6
I1226 13:38:30.224304 92321 net.cpp:178] Creating Layer fc6
I1226 13:38:30.224342 92321 net.cpp:612] fc6 <- pool5
I1226 13:38:30.224381 92321 net.cpp:586] fc6 -> fc6
I1226 13:38:30.470875 93001 net.cpp:228] Setting up conv5
I1226 13:38:30.470980 93001 net.cpp:235] Top shape: 256 256 13 13 (11075584)
I1226 13:38:30.471004 93001 net.cpp:243] Memory required for data: 2049800192
I1226 13:38:30.471122 93001 layer_factory.hpp:114] Creating layer relu5
I1226 13:38:30.471192 93001 net.cpp:178] Creating Layer relu5
I1226 13:38:30.471222 93001 net.cpp:612] relu5 <- conv5
I1226 13:38:30.471269 93001 net.cpp:573] relu5 -> conv5 (in-place)
I1226 13:38:30.471503 93001 net.cpp:228] Setting up relu5
I1226 13:38:30.471549 93001 net.cpp:235] Top shape: 256 256 13 13 (11075584)
I1226 13:38:30.471571 93001 net.cpp:243] Memory required for data: 2094102528
I1226 13:38:30.471598 93001 layer_factory.hpp:114] Creating layer pool5
I1226 13:38:30.471652 93001 net.cpp:178] Creating Layer pool5
I1226 13:38:30.471683 93001 net.cpp:612] pool5 <- conv5
I1226 13:38:30.471726 93001 net.cpp:586] pool5 -> pool5
I1226 13:38:30.471809 93001 net.cpp:228] Setting up pool5
I1226 13:38:30.471848 93001 net.cpp:235] Top shape: 256 256 6 6 (2359296)
I1226 13:38:30.471870 93001 net.cpp:243] Memory required for data: 2103539712
I1226 13:38:30.471895 93001 layer_factory.hpp:114] Creating layer fc6
I1226 13:38:30.471977 93001 net.cpp:178] Creating Layer fc6
I1226 13:38:30.472036 93001 net.cpp:612] fc6 <- pool5
I1226 13:38:30.472080 93001 net.cpp:586] fc6 -> fc6
I1226 13:38:30.509789 94726 net.cpp:228] Setting up conv3
I1226 13:38:30.509927 94726 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 13:38:30.509966 94726 net.cpp:243] Memory required for data: 1806137344
I1226 13:38:30.510052 94726 layer_factory.hpp:114] Creating layer relu3
I1226 13:38:30.510143 94726 net.cpp:178] Creating Layer relu3
I1226 13:38:30.510185 94726 net.cpp:612] relu3 <- conv3
I1226 13:38:30.510231 94726 net.cpp:573] relu3 -> conv3 (in-place)
I1226 13:38:30.510341 94726 net.cpp:228] Setting up relu3
I1226 13:38:30.510406 94726 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 13:38:30.510435 94726 net.cpp:243] Memory required for data: 1872590848
I1226 13:38:30.510468 94726 layer_factory.hpp:114] Creating layer conv4
I1226 13:38:30.510586 94726 net.cpp:178] Creating Layer conv4
I1226 13:38:30.510632 94726 net.cpp:612] conv4 <- conv3
I1226 13:38:30.510699 94726 net.cpp:586] conv4 -> conv4
I1226 13:38:30.560523 122432 net.cpp:228] Setting up conv4
I1226 13:38:30.561094 122432 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 13:38:30.561149 122432 net.cpp:243] Memory required for data: 1939044352
I1226 13:38:30.561259 122432 layer_factory.hpp:114] Creating layer relu4
I1226 13:38:30.561517 122432 net.cpp:178] Creating Layer relu4
I1226 13:38:30.561581 122432 net.cpp:612] relu4 <- conv4
I1226 13:38:30.561807 122432 net.cpp:573] relu4 -> conv4 (in-place)
I1226 13:38:30.562084 122432 net.cpp:228] Setting up relu4
I1226 13:38:30.562182 122432 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 13:38:30.562218 122432 net.cpp:243] Memory required for data: 2005497856
I1226 13:38:30.562263 122432 layer_factory.hpp:114] Creating layer conv5
I1226 13:38:30.562407 122432 net.cpp:178] Creating Layer conv5
I1226 13:38:30.562471 122432 net.cpp:612] conv5 <- conv4
I1226 13:38:30.562530 122432 net.cpp:586] conv5 -> conv5
I1226 13:38:30.959695 122432 net.cpp:228] Setting up conv5
I1226 13:38:30.960522 122432 net.cpp:235] Top shape: 256 256 13 13 (11075584)
I1226 13:38:30.960619 122432 net.cpp:243] Memory required for data: 2049800192
I1226 13:38:30.960857 122432 layer_factory.hpp:114] Creating layer relu5
I1226 13:38:30.961104 122432 net.cpp:178] Creating Layer relu5
I1226 13:38:30.961159 122432 net.cpp:612] relu5 <- conv5
I1226 13:38:30.961242 122432 net.cpp:573] relu5 -> conv5 (in-place)
I1226 13:38:30.961411 122432 net.cpp:228] Setting up relu5
I1226 13:38:30.961568 122432 net.cpp:235] Top shape: 256 256 13 13 (11075584)
I1226 13:38:30.961603 122432 net.cpp:243] Memory required for data: 2094102528
I1226 13:38:30.962127 122432 layer_factory.hpp:114] Creating layer pool5
I1226 13:38:30.962229 122432 net.cpp:178] Creating Layer pool5
I1226 13:38:30.962286 122432 net.cpp:612] pool5 <- conv5
I1226 13:38:30.962388 122432 net.cpp:586] pool5 -> pool5
I1226 13:38:30.962594 122432 net.cpp:228] Setting up pool5
I1226 13:38:30.962730 122432 net.cpp:235] Top shape: 256 256 6 6 (2359296)
I1226 13:38:30.962767 122432 net.cpp:243] Memory required for data: 2103539712
I1226 13:38:30.963042 122432 layer_factory.hpp:114] Creating layer fc6
I1226 13:38:30.963176 122432 net.cpp:178] Creating Layer fc6
I1226 13:38:30.963240 122432 net.cpp:612] fc6 <- pool5
I1226 13:38:30.963335 122432 net.cpp:586] fc6 -> fc6
I1226 13:38:31.246794 96918 net.cpp:228] Setting up conv2
I1226 13:38:31.246912 96918 net.cpp:235] Top shape: 256 256 27 27 (47775744)
I1226 13:38:31.246942 96918 net.cpp:243] Memory required for data: 1313175552
I1226 13:38:31.247018 96918 layer_factory.hpp:114] Creating layer relu2
I1226 13:38:31.247174 96918 net.cpp:178] Creating Layer relu2
I1226 13:38:31.247215 96918 net.cpp:612] relu2 <- conv2
I1226 13:38:31.247283 96918 net.cpp:573] relu2 -> conv2 (in-place)
I1226 13:38:31.247385 96918 net.cpp:228] Setting up relu2
I1226 13:38:31.247434 96918 net.cpp:235] Top shape: 256 256 27 27 (47775744)
I1226 13:38:31.247459 96918 net.cpp:243] Memory required for data: 1504278528
I1226 13:38:31.247488 96918 layer_factory.hpp:114] Creating layer norm2
I1226 13:38:31.247547 96918 net.cpp:178] Creating Layer norm2
I1226 13:38:31.247582 96918 net.cpp:612] norm2 <- conv2
I1226 13:38:31.247630 96918 net.cpp:586] norm2 -> norm2
I1226 13:38:31.247767 96918 net.cpp:228] Setting up norm2
I1226 13:38:31.247824 96918 net.cpp:235] Top shape: 256 256 27 27 (47775744)
I1226 13:38:31.247853 96918 net.cpp:243] Memory required for data: 1695381504
I1226 13:38:31.247884 96918 layer_factory.hpp:114] Creating layer pool2
I1226 13:38:31.247946 96918 net.cpp:178] Creating Layer pool2
I1226 13:38:31.247974 96918 net.cpp:612] pool2 <- norm2
I1226 13:38:31.248008 96918 net.cpp:586] pool2 -> pool2
I1226 13:38:31.248111 96918 net.cpp:228] Setting up pool2
I1226 13:38:31.248287 96918 net.cpp:235] Top shape: 256 256 13 13 (11075584)
I1226 13:38:31.248318 96918 net.cpp:243] Memory required for data: 1739683840
I1226 13:38:31.248378 96918 layer_factory.hpp:114] Creating layer conv3
I1226 13:38:31.248524 96918 net.cpp:178] Creating Layer conv3
I1226 13:38:31.248610 96918 net.cpp:612] conv3 <- pool2
I1226 13:38:31.248756 96918 net.cpp:586] conv3 -> conv3
I1226 13:38:31.259279 96543 net.cpp:228] Setting up conv2
I1226 13:38:31.259394 96543 net.cpp:235] Top shape: 256 256 27 27 (47775744)
I1226 13:38:31.259425 96543 net.cpp:243] Memory required for data: 1313175552
I1226 13:38:31.259505 96543 layer_factory.hpp:114] Creating layer relu2
I1226 13:38:31.259577 96543 net.cpp:178] Creating Layer relu2
I1226 13:38:31.259618 96543 net.cpp:612] relu2 <- conv2
I1226 13:38:31.259685 96543 net.cpp:573] relu2 -> conv2 (in-place)
I1226 13:38:31.259819 96543 net.cpp:228] Setting up relu2
I1226 13:38:31.259896 96543 net.cpp:235] Top shape: 256 256 27 27 (47775744)
I1226 13:38:31.259927 96543 net.cpp:243] Memory required for data: 1504278528
I1226 13:38:31.259961 96543 layer_factory.hpp:114] Creating layer norm2
I1226 13:38:31.260023 96543 net.cpp:178] Creating Layer norm2
I1226 13:38:31.260056 96543 net.cpp:612] norm2 <- conv2
I1226 13:38:31.260098 96543 net.cpp:586] norm2 -> norm2
I1226 13:38:31.260215 96543 net.cpp:228] Setting up norm2
I1226 13:38:31.260282 96543 net.cpp:235] Top shape: 256 256 27 27 (47775744)
I1226 13:38:31.260310 96543 net.cpp:243] Memory required for data: 1695381504
I1226 13:38:31.260342 96543 layer_factory.hpp:114] Creating layer pool2
I1226 13:38:31.260404 96543 net.cpp:178] Creating Layer pool2
I1226 13:38:31.260438 96543 net.cpp:612] pool2 <- norm2
I1226 13:38:31.260478 96543 net.cpp:586] pool2 -> pool2
I1226 13:38:31.260574 96543 net.cpp:228] Setting up pool2
I1226 13:38:31.260726 96543 net.cpp:235] Top shape: 256 256 13 13 (11075584)
I1226 13:38:31.260759 96543 net.cpp:243] Memory required for data: 1739683840
I1226 13:38:31.260818 96543 layer_factory.hpp:114] Creating layer conv3
I1226 13:38:31.260917 96543 net.cpp:178] Creating Layer conv3
I1226 13:38:31.260951 96543 net.cpp:612] conv3 <- pool2
I1226 13:38:31.261011 96543 net.cpp:586] conv3 -> conv3
I1226 13:38:31.398377 94726 net.cpp:228] Setting up conv4
I1226 13:38:31.398499 94726 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 13:38:31.398535 94726 net.cpp:243] Memory required for data: 1939044352
I1226 13:38:31.398599 94726 layer_factory.hpp:114] Creating layer relu4
I1226 13:38:31.398696 94726 net.cpp:178] Creating Layer relu4
I1226 13:38:31.398751 94726 net.cpp:612] relu4 <- conv4
I1226 13:38:31.398794 94726 net.cpp:573] relu4 -> conv4 (in-place)
I1226 13:38:31.398936 94726 net.cpp:228] Setting up relu4
I1226 13:38:31.399003 94726 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 13:38:31.399029 94726 net.cpp:243] Memory required for data: 2005497856
I1226 13:38:31.399065 94726 layer_factory.hpp:114] Creating layer conv5
I1226 13:38:31.399170 94726 net.cpp:178] Creating Layer conv5
I1226 13:38:31.399214 94726 net.cpp:612] conv5 <- conv4
I1226 13:38:31.399261 94726 net.cpp:586] conv5 -> conv5
I1226 13:38:32.032069 94726 net.cpp:228] Setting up conv5
I1226 13:38:32.032193 94726 net.cpp:235] Top shape: 256 256 13 13 (11075584)
I1226 13:38:32.032230 94726 net.cpp:243] Memory required for data: 2049800192
I1226 13:38:32.032340 94726 layer_factory.hpp:114] Creating layer relu5
I1226 13:38:32.032428 94726 net.cpp:178] Creating Layer relu5
I1226 13:38:32.032467 94726 net.cpp:612] relu5 <- conv5
I1226 13:38:32.032527 94726 net.cpp:573] relu5 -> conv5 (in-place)
I1226 13:38:32.032652 94726 net.cpp:228] Setting up relu5
I1226 13:38:32.032716 94726 net.cpp:235] Top shape: 256 256 13 13 (11075584)
I1226 13:38:32.032742 94726 net.cpp:243] Memory required for data: 2094102528
I1226 13:38:32.032774 94726 layer_factory.hpp:114] Creating layer pool5
I1226 13:38:32.032874 94726 net.cpp:178] Creating Layer pool5
I1226 13:38:32.032928 94726 net.cpp:612] pool5 <- conv5
I1226 13:38:32.032977 94726 net.cpp:586] pool5 -> pool5
I1226 13:38:32.033107 94726 net.cpp:228] Setting up pool5
I1226 13:38:32.033167 94726 net.cpp:235] Top shape: 256 256 6 6 (2359296)
I1226 13:38:32.033193 94726 net.cpp:243] Memory required for data: 2103539712
I1226 13:38:32.033223 94726 layer_factory.hpp:114] Creating layer fc6
I1226 13:38:32.033300 94726 net.cpp:178] Creating Layer fc6
I1226 13:38:32.033341 94726 net.cpp:612] fc6 <- pool5
I1226 13:38:32.033398 94726 net.cpp:586] fc6 -> fc6
I1226 13:38:33.170423 96918 net.cpp:228] Setting up conv3
I1226 13:38:33.170536 96918 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 13:38:33.170569 96918 net.cpp:243] Memory required for data: 1806137344
I1226 13:38:33.170650 96918 layer_factory.hpp:114] Creating layer relu3
I1226 13:38:33.170822 96918 net.cpp:178] Creating Layer relu3
I1226 13:38:33.170894 96918 net.cpp:612] relu3 <- conv3
I1226 13:38:33.170971 96918 net.cpp:573] relu3 -> conv3 (in-place)
I1226 13:38:33.171090 96918 net.cpp:228] Setting up relu3
I1226 13:38:33.171208 96918 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 13:38:33.171241 96918 net.cpp:243] Memory required for data: 1872590848
I1226 13:38:33.171303 96918 layer_factory.hpp:114] Creating layer conv4
I1226 13:38:33.171409 96918 net.cpp:178] Creating Layer conv4
I1226 13:38:33.171537 96918 net.cpp:612] conv4 <- conv3
I1226 13:38:33.171615 96918 net.cpp:586] conv4 -> conv4
I1226 13:38:33.199127 96543 net.cpp:228] Setting up conv3
I1226 13:38:33.199631 96543 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 13:38:33.199723 96543 net.cpp:243] Memory required for data: 1806137344
I1226 13:38:33.199931 96543 layer_factory.hpp:114] Creating layer relu3
I1226 13:38:33.200073 96543 net.cpp:178] Creating Layer relu3
I1226 13:38:33.200115 96543 net.cpp:612] relu3 <- conv3
I1226 13:38:33.200160 96543 net.cpp:573] relu3 -> conv3 (in-place)
I1226 13:38:33.200315 96543 net.cpp:228] Setting up relu3
I1226 13:38:33.200408 96543 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 13:38:33.200438 96543 net.cpp:243] Memory required for data: 1872590848
I1226 13:38:33.200472 96543 layer_factory.hpp:114] Creating layer conv4
I1226 13:38:33.200613 96543 net.cpp:178] Creating Layer conv4
I1226 13:38:33.200665 96543 net.cpp:612] conv4 <- conv3
I1226 13:38:33.200732 96543 net.cpp:586] conv4 -> conv4
I1226 13:38:34.760692 96918 net.cpp:228] Setting up conv4
I1226 13:38:34.760848 96918 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 13:38:34.760881 96918 net.cpp:243] Memory required for data: 1939044352
I1226 13:38:34.760944 96918 layer_factory.hpp:114] Creating layer relu4
I1226 13:38:34.761041 96918 net.cpp:178] Creating Layer relu4
I1226 13:38:34.761104 96918 net.cpp:612] relu4 <- conv4
I1226 13:38:34.761363 96918 net.cpp:573] relu4 -> conv4 (in-place)
I1226 13:38:34.761524 96918 net.cpp:228] Setting up relu4
I1226 13:38:34.761656 96918 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 13:38:34.761708 96918 net.cpp:243] Memory required for data: 2005497856
I1226 13:38:34.761826 96918 layer_factory.hpp:114] Creating layer conv5
I1226 13:38:34.762468 96918 net.cpp:178] Creating Layer conv5
I1226 13:38:34.762570 96918 net.cpp:612] conv5 <- conv4
I1226 13:38:34.762632 96918 net.cpp:586] conv5 -> conv5
I1226 13:38:34.802847 96543 net.cpp:228] Setting up conv4
I1226 13:38:34.802958 96543 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 13:38:34.802989 96543 net.cpp:243] Memory required for data: 1939044352
I1226 13:38:34.803053 96543 layer_factory.hpp:114] Creating layer relu4
I1226 13:38:34.803139 96543 net.cpp:178] Creating Layer relu4
I1226 13:38:34.803184 96543 net.cpp:612] relu4 <- conv4
I1226 13:38:34.803234 96543 net.cpp:573] relu4 -> conv4 (in-place)
I1226 13:38:34.803347 96543 net.cpp:228] Setting up relu4
I1226 13:38:34.803418 96543 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 13:38:34.803447 96543 net.cpp:243] Memory required for data: 2005497856
I1226 13:38:34.815939 96543 layer_factory.hpp:114] Creating layer conv5
I1226 13:38:34.816076 96543 net.cpp:178] Creating Layer conv5
I1226 13:38:34.816123 96543 net.cpp:612] conv5 <- conv4
I1226 13:38:34.816180 96543 net.cpp:586] conv5 -> conv5
I1226 13:38:35.927429 96918 net.cpp:228] Setting up conv5
I1226 13:38:35.927541 96918 net.cpp:235] Top shape: 256 256 13 13 (11075584)
I1226 13:38:35.927573 96918 net.cpp:243] Memory required for data: 2049800192
I1226 13:38:35.927683 96918 layer_factory.hpp:114] Creating layer relu5
I1226 13:38:35.927824 96918 net.cpp:178] Creating Layer relu5
I1226 13:38:35.927863 96918 net.cpp:612] relu5 <- conv5
I1226 13:38:35.927906 96918 net.cpp:573] relu5 -> conv5 (in-place)
I1226 13:38:35.928006 96918 net.cpp:228] Setting up relu5
I1226 13:38:35.928063 96918 net.cpp:235] Top shape: 256 256 13 13 (11075584)
I1226 13:38:35.928089 96918 net.cpp:243] Memory required for data: 2094102528
I1226 13:38:35.928119 96918 layer_factory.hpp:114] Creating layer pool5
I1226 13:38:35.928195 96918 net.cpp:178] Creating Layer pool5
I1226 13:38:35.928232 96918 net.cpp:612] pool5 <- conv5
I1226 13:38:35.928274 96918 net.cpp:586] pool5 -> pool5
I1226 13:38:35.928380 96918 net.cpp:228] Setting up pool5
I1226 13:38:35.928432 96918 net.cpp:235] Top shape: 256 256 6 6 (2359296)
I1226 13:38:35.928464 96918 net.cpp:243] Memory required for data: 2103539712
I1226 13:38:35.928504 96918 layer_factory.hpp:114] Creating layer fc6
I1226 13:38:35.928566 96918 net.cpp:178] Creating Layer fc6
I1226 13:38:35.928602 96918 net.cpp:612] fc6 <- pool5
I1226 13:38:35.928644 96918 net.cpp:586] fc6 -> fc6
I1226 13:38:35.997275 93001 net.cpp:228] Setting up fc6
I1226 13:38:35.997406 93001 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 13:38:35.997438 93001 net.cpp:243] Memory required for data: 2107734016
I1226 13:38:35.997493 93001 layer_factory.hpp:114] Creating layer relu6
I1226 13:38:35.997557 93001 net.cpp:178] Creating Layer relu6
I1226 13:38:35.997596 93001 net.cpp:612] relu6 <- fc6
I1226 13:38:35.997653 93001 net.cpp:573] relu6 -> fc6 (in-place)
I1226 13:38:35.997843 93001 net.cpp:228] Setting up relu6
I1226 13:38:35.997963 93001 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 13:38:35.997998 93001 net.cpp:243] Memory required for data: 2111928320
I1226 13:38:35.998028 93001 layer_factory.hpp:114] Creating layer drop6
I1226 13:38:35.998083 93001 net.cpp:178] Creating Layer drop6
I1226 13:38:35.998117 93001 net.cpp:612] drop6 <- fc6
I1226 13:38:35.998154 93001 net.cpp:573] drop6 -> fc6 (in-place)
I1226 13:38:35.998210 93001 net.cpp:228] Setting up drop6
I1226 13:38:35.998250 93001 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 13:38:35.998271 93001 net.cpp:243] Memory required for data: 2116122624
I1226 13:38:35.998298 93001 layer_factory.hpp:114] Creating layer fc7
I1226 13:38:35.998354 93001 net.cpp:178] Creating Layer fc7
I1226 13:38:35.998406 93001 net.cpp:612] fc7 <- fc6
I1226 13:38:35.998472 93001 net.cpp:586] fc7 -> fc7
I1226 13:38:35.992348 96543 net.cpp:228] Setting up conv5
I1226 13:38:35.992478 96543 net.cpp:235] Top shape: 256 256 13 13 (11075584)
I1226 13:38:35.992513 96543 net.cpp:243] Memory required for data: 2049800192
I1226 13:38:35.992596 96543 layer_factory.hpp:114] Creating layer relu5
I1226 13:38:35.992664 96543 net.cpp:178] Creating Layer relu5
I1226 13:38:35.992702 96543 net.cpp:612] relu5 <- conv5
I1226 13:38:35.992745 96543 net.cpp:573] relu5 -> conv5 (in-place)
I1226 13:38:35.992885 96543 net.cpp:228] Setting up relu5
I1226 13:38:35.992943 96543 net.cpp:235] Top shape: 256 256 13 13 (11075584)
I1226 13:38:35.992972 96543 net.cpp:243] Memory required for data: 2094102528
I1226 13:38:35.993007 96543 layer_factory.hpp:114] Creating layer pool5
I1226 13:38:35.993083 96543 net.cpp:178] Creating Layer pool5
I1226 13:38:35.993129 96543 net.cpp:612] pool5 <- conv5
I1226 13:38:35.993175 96543 net.cpp:586] pool5 -> pool5
I1226 13:38:35.993285 96543 net.cpp:228] Setting up pool5
I1226 13:38:35.993347 96543 net.cpp:235] Top shape: 256 256 6 6 (2359296)
I1226 13:38:35.993376 96543 net.cpp:243] Memory required for data: 2103539712
I1226 13:38:35.993409 96543 layer_factory.hpp:114] Creating layer fc6
I1226 13:38:35.993474 96543 net.cpp:178] Creating Layer fc6
I1226 13:38:35.993505 96543 net.cpp:612] fc6 <- pool5
I1226 13:38:35.993548 96543 net.cpp:586] fc6 -> fc6
I1226 13:38:38.269430 93001 net.cpp:228] Setting up fc7
I1226 13:38:38.269534 93001 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 13:38:38.269560 93001 net.cpp:243] Memory required for data: 2120316928
I1226 13:38:38.269641 93001 layer_factory.hpp:114] Creating layer relu7
I1226 13:38:38.269718 93001 net.cpp:178] Creating Layer relu7
I1226 13:38:38.269757 93001 net.cpp:612] relu7 <- fc7
I1226 13:38:38.269809 93001 net.cpp:573] relu7 -> fc7 (in-place)
I1226 13:38:38.269901 93001 net.cpp:228] Setting up relu7
I1226 13:38:38.269948 93001 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 13:38:38.269973 93001 net.cpp:243] Memory required for data: 2124511232
I1226 13:38:38.270006 93001 layer_factory.hpp:114] Creating layer drop7
I1226 13:38:38.270053 93001 net.cpp:178] Creating Layer drop7
I1226 13:38:38.270078 93001 net.cpp:612] drop7 <- fc7
I1226 13:38:38.270112 93001 net.cpp:573] drop7 -> fc7 (in-place)
I1226 13:38:38.270153 93001 net.cpp:228] Setting up drop7
I1226 13:38:38.270185 93001 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 13:38:38.270215 93001 net.cpp:243] Memory required for data: 2128705536
I1226 13:38:38.270242 93001 layer_factory.hpp:114] Creating layer fc8
I1226 13:38:38.270309 93001 net.cpp:178] Creating Layer fc8
I1226 13:38:38.270344 93001 net.cpp:612] fc8 <- fc7
I1226 13:38:38.270402 93001 net.cpp:586] fc8 -> fc8
I1226 13:38:38.779127 91021 net.cpp:228] Setting up fc6
I1226 13:38:38.779271 91021 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 13:38:38.779501 91021 net.cpp:243] Memory required for data: 2107734016
I1226 13:38:38.779594 91021 layer_factory.hpp:114] Creating layer relu6
I1226 13:38:38.779711 91021 net.cpp:178] Creating Layer relu6
I1226 13:38:38.779820 91021 net.cpp:612] relu6 <- fc6
I1226 13:38:38.779949 91021 net.cpp:573] relu6 -> fc6 (in-place)
I1226 13:38:38.780190 91021 net.cpp:228] Setting up relu6
I1226 13:38:38.780274 91021 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 13:38:38.780304 91021 net.cpp:243] Memory required for data: 2111928320
I1226 13:38:38.780338 91021 layer_factory.hpp:114] Creating layer drop6
I1226 13:38:38.780397 91021 net.cpp:178] Creating Layer drop6
I1226 13:38:38.780426 91021 net.cpp:612] drop6 <- fc6
I1226 13:38:38.780463 91021 net.cpp:573] drop6 -> fc6 (in-place)
I1226 13:38:38.780527 91021 net.cpp:228] Setting up drop6
I1226 13:38:38.780565 91021 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 13:38:38.780588 91021 net.cpp:243] Memory required for data: 2116122624
I1226 13:38:38.780623 91021 layer_factory.hpp:114] Creating layer fc7
I1226 13:38:38.780681 91021 net.cpp:178] Creating Layer fc7
I1226 13:38:38.780707 91021 net.cpp:612] fc7 <- fc6
I1226 13:38:38.780769 91021 net.cpp:586] fc7 -> fc7
I1226 13:38:38.789572 91865 net.cpp:228] Setting up fc6
I1226 13:38:38.789757 91865 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 13:38:38.789795 91865 net.cpp:243] Memory required for data: 2107734016
I1226 13:38:38.789923 91865 layer_factory.hpp:114] Creating layer relu6
I1226 13:38:38.790169 91865 net.cpp:178] Creating Layer relu6
I1226 13:38:38.790354 91865 net.cpp:612] relu6 <- fc6
I1226 13:38:38.790402 91865 net.cpp:573] relu6 -> fc6 (in-place)
I1226 13:38:38.790645 91865 net.cpp:228] Setting up relu6
I1226 13:38:38.790843 91865 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 13:38:38.791059 91865 net.cpp:243] Memory required for data: 2111928320
I1226 13:38:38.791100 91865 layer_factory.hpp:114] Creating layer drop6
I1226 13:38:38.791158 91865 net.cpp:178] Creating Layer drop6
I1226 13:38:38.791190 91865 net.cpp:612] drop6 <- fc6
I1226 13:38:38.791267 91865 net.cpp:573] drop6 -> fc6 (in-place)
I1226 13:38:38.791357 91865 net.cpp:228] Setting up drop6
I1226 13:38:38.791402 91865 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 13:38:38.791446 91865 net.cpp:243] Memory required for data: 2116122624
I1226 13:38:38.791503 91865 layer_factory.hpp:114] Creating layer fc7
I1226 13:38:38.791571 91865 net.cpp:178] Creating Layer fc7
I1226 13:38:38.791601 91865 net.cpp:612] fc7 <- fc6
I1226 13:38:38.791663 91865 net.cpp:586] fc7 -> fc7
I1226 13:38:38.828928 93001 net.cpp:228] Setting up fc8
I1226 13:38:38.829040 93001 net.cpp:235] Top shape: 256 1000 (256000)
I1226 13:38:38.829066 93001 net.cpp:243] Memory required for data: 2129729536
I1226 13:38:38.829118 93001 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 13:38:38.829212 93001 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 13:38:38.829252 93001 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 13:38:38.829291 93001 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 13:38:38.829341 93001 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 13:38:38.829532 93001 net.cpp:228] Setting up fc8_fc8_0_split
I1226 13:38:38.829594 93001 net.cpp:235] Top shape: 256 1000 (256000)
I1226 13:38:38.829625 93001 net.cpp:235] Top shape: 256 1000 (256000)
I1226 13:38:38.829648 93001 net.cpp:243] Memory required for data: 2131777536
I1226 13:38:38.829676 93001 layer_factory.hpp:114] Creating layer accuracy
I1226 13:38:38.829735 93001 net.cpp:178] Creating Layer accuracy
I1226 13:38:38.829768 93001 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 13:38:38.829799 93001 net.cpp:612] accuracy <- label_data_1_split_0
I1226 13:38:38.829834 93001 net.cpp:586] accuracy -> accuracy
I1226 13:38:38.829887 93001 net.cpp:228] Setting up accuracy
I1226 13:38:38.829926 93001 net.cpp:235] Top shape: (1)
I1226 13:38:38.829947 93001 net.cpp:243] Memory required for data: 2131777540
I1226 13:38:38.829973 93001 layer_factory.hpp:114] Creating layer loss
I1226 13:38:38.830018 93001 net.cpp:178] Creating Layer loss
I1226 13:38:38.830049 93001 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 13:38:38.830078 93001 net.cpp:612] loss <- label_data_1_split_1
I1226 13:38:38.830124 93001 net.cpp:586] loss -> loss
I1226 13:38:38.830183 93001 layer_factory.hpp:114] Creating layer loss
I1226 13:38:38.864447 93001 net.cpp:228] Setting up loss
I1226 13:38:38.864645 93001 net.cpp:235] Top shape: (1)
I1226 13:38:38.864691 93001 net.cpp:238]     with loss weight 1
I1226 13:38:38.864843 93001 net.cpp:243] Memory required for data: 2131777544
I1226 13:38:38.864892 93001 net.cpp:305] loss needs backward computation.
I1226 13:38:38.864933 93001 net.cpp:307] accuracy does not need backward computation.
I1226 13:38:38.864969 93001 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 13:38:38.865002 93001 net.cpp:305] fc8 needs backward computation.
I1226 13:38:38.865046 93001 net.cpp:305] drop7 needs backward computation.
I1226 13:38:38.865077 93001 net.cpp:305] relu7 needs backward computation.
I1226 13:38:38.865118 93001 net.cpp:305] fc7 needs backward computation.
I1226 13:38:38.865149 93001 net.cpp:305] drop6 needs backward computation.
I1226 13:38:38.865187 93001 net.cpp:305] relu6 needs backward computation.
I1226 13:38:38.865216 93001 net.cpp:305] fc6 needs backward computation.
I1226 13:38:38.865252 93001 net.cpp:305] pool5 needs backward computation.
I1226 13:38:38.865291 93001 net.cpp:305] relu5 needs backward computation.
I1226 13:38:38.865329 93001 net.cpp:305] conv5 needs backward computation.
I1226 13:38:38.865368 93001 net.cpp:305] relu4 needs backward computation.
I1226 13:38:38.865422 93001 net.cpp:305] conv4 needs backward computation.
I1226 13:38:38.865450 93001 net.cpp:305] relu3 needs backward computation.
I1226 13:38:38.865478 93001 net.cpp:305] conv3 needs backward computation.
I1226 13:38:38.865506 93001 net.cpp:305] pool2 needs backward computation.
I1226 13:38:38.865535 93001 net.cpp:305] norm2 needs backward computation.
I1226 13:38:38.865562 93001 net.cpp:305] relu2 needs backward computation.
I1226 13:38:38.865589 93001 net.cpp:305] conv2 needs backward computation.
I1226 13:38:38.865617 93001 net.cpp:305] pool1 needs backward computation.
I1226 13:38:38.865645 93001 net.cpp:305] norm1 needs backward computation.
I1226 13:38:38.865672 93001 net.cpp:305] relu1 needs backward computation.
I1226 13:38:38.865701 93001 net.cpp:305] conv1 needs backward computation.
I1226 13:38:38.865732 93001 net.cpp:307] label_data_1_split does not need backward computation.
I1226 13:38:38.865762 93001 net.cpp:307] data does not need backward computation.
I1226 13:38:38.865787 93001 net.cpp:349] This network produces output accuracy
I1226 13:38:38.865818 93001 net.cpp:349] This network produces output loss
I1226 13:38:38.865914 93001 net.cpp:363] Network initialization done.
I1226 13:38:38.866475 93001 solver.cpp:107] Solver scaffolding done.
I1226 13:38:38.866686 93001 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 13:38:38.986922 89419 net.cpp:228] Setting up fc6
I1226 13:38:38.987032 89419 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 13:38:38.987062 89419 net.cpp:243] Memory required for data: 2107734016
I1226 13:38:38.987121 89419 layer_factory.hpp:114] Creating layer relu6
I1226 13:38:38.987215 89419 net.cpp:178] Creating Layer relu6
I1226 13:38:38.987383 89419 net.cpp:612] relu6 <- fc6
I1226 13:38:38.987447 89419 net.cpp:573] relu6 -> fc6 (in-place)
I1226 13:38:38.987700 89419 net.cpp:228] Setting up relu6
I1226 13:38:38.988076 89419 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 13:38:38.988137 89419 net.cpp:243] Memory required for data: 2111928320
I1226 13:38:38.988173 89419 layer_factory.hpp:114] Creating layer drop6
I1226 13:38:38.988226 89419 net.cpp:178] Creating Layer drop6
I1226 13:38:38.988268 89419 net.cpp:612] drop6 <- fc6
I1226 13:38:38.988366 89419 net.cpp:573] drop6 -> fc6 (in-place)
I1226 13:38:38.988456 89419 net.cpp:228] Setting up drop6
I1226 13:38:38.988498 89419 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 13:38:38.988533 89419 net.cpp:243] Memory required for data: 2116122624
I1226 13:38:38.988571 89419 layer_factory.hpp:114] Creating layer fc7
I1226 13:38:38.988656 89419 net.cpp:178] Creating Layer fc7
I1226 13:38:38.988694 89419 net.cpp:612] fc7 <- fc6
I1226 13:38:38.988734 89419 net.cpp:586] fc7 -> fc7
I1226 13:38:39.093515 92321 net.cpp:228] Setting up fc6
I1226 13:38:39.093626 92321 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 13:38:39.093654 92321 net.cpp:243] Memory required for data: 2107734016
I1226 13:38:39.093709 92321 layer_factory.hpp:114] Creating layer relu6
I1226 13:38:39.093788 92321 net.cpp:178] Creating Layer relu6
I1226 13:38:39.093904 92321 net.cpp:612] relu6 <- fc6
I1226 13:38:39.093942 92321 net.cpp:573] relu6 -> fc6 (in-place)
I1226 13:38:39.094141 92321 net.cpp:228] Setting up relu6
I1226 13:38:39.094199 92321 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 13:38:39.094224 92321 net.cpp:243] Memory required for data: 2111928320
I1226 13:38:39.094254 92321 layer_factory.hpp:114] Creating layer drop6
I1226 13:38:39.094317 92321 net.cpp:178] Creating Layer drop6
I1226 13:38:39.094346 92321 net.cpp:612] drop6 <- fc6
I1226 13:38:39.094380 92321 net.cpp:573] drop6 -> fc6 (in-place)
I1226 13:38:39.094444 92321 net.cpp:228] Setting up drop6
I1226 13:38:39.094499 92321 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 13:38:39.094528 92321 net.cpp:243] Memory required for data: 2116122624
I1226 13:38:39.094555 92321 layer_factory.hpp:114] Creating layer fc7
I1226 13:38:39.094614 92321 net.cpp:178] Creating Layer fc7
I1226 13:38:39.094650 92321 net.cpp:612] fc7 <- fc6
I1226 13:38:39.094686 92321 net.cpp:586] fc7 -> fc7
I1226 13:38:40.506326 93001 caffe.cpp:376] Configuring multinode setup
I1226 13:38:40.507761 93001 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 13:38:41.315610 91865 net.cpp:228] Setting up fc7
I1226 13:38:41.315732 91865 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 13:38:41.315767 91865 net.cpp:243] Memory required for data: 2120316928
I1226 13:38:41.315850 91865 layer_factory.hpp:114] Creating layer relu7
I1226 13:38:41.315938 91865 net.cpp:178] Creating Layer relu7
I1226 13:38:41.315989 91865 net.cpp:612] relu7 <- fc7
I1226 13:38:41.316040 91865 net.cpp:573] relu7 -> fc7 (in-place)
I1226 13:38:41.316145 91865 net.cpp:228] Setting up relu7
I1226 13:38:41.316195 91865 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 13:38:41.316226 91865 net.cpp:243] Memory required for data: 2124511232
I1226 13:38:41.316273 91865 layer_factory.hpp:114] Creating layer drop7
I1226 13:38:41.316359 91865 net.cpp:178] Creating Layer drop7
I1226 13:38:41.316398 91865 net.cpp:612] drop7 <- fc7
I1226 13:38:41.316448 91865 net.cpp:573] drop7 -> fc7 (in-place)
I1226 13:38:41.316512 91865 net.cpp:228] Setting up drop7
I1226 13:38:41.316563 91865 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 13:38:41.316592 91865 net.cpp:243] Memory required for data: 2128705536
I1226 13:38:41.316654 91865 layer_factory.hpp:114] Creating layer fc8
I1226 13:38:41.316733 91865 net.cpp:178] Creating Layer fc8
I1226 13:38:41.316771 91865 net.cpp:612] fc8 <- fc7
I1226 13:38:41.316825 91865 net.cpp:586] fc8 -> fc8
I1226 13:38:41.353859 92321 net.cpp:228] Setting up fc7
I1226 13:38:41.353973 92321 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 13:38:41.354001 92321 net.cpp:243] Memory required for data: 2120316928
I1226 13:38:41.354085 92321 layer_factory.hpp:114] Creating layer relu7
I1226 13:38:41.354178 92321 net.cpp:178] Creating Layer relu7
I1226 13:38:41.354223 92321 net.cpp:612] relu7 <- fc7
I1226 13:38:41.354261 92321 net.cpp:573] relu7 -> fc7 (in-place)
I1226 13:38:41.354351 92321 net.cpp:228] Setting up relu7
I1226 13:38:41.354408 92321 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 13:38:41.354431 92321 net.cpp:243] Memory required for data: 2124511232
I1226 13:38:41.354461 92321 layer_factory.hpp:114] Creating layer drop7
I1226 13:38:41.354529 92321 net.cpp:178] Creating Layer drop7
I1226 13:38:41.354562 92321 net.cpp:612] drop7 <- fc7
I1226 13:38:41.354598 92321 net.cpp:573] drop7 -> fc7 (in-place)
I1226 13:38:41.354642 92321 net.cpp:228] Setting up drop7
I1226 13:38:41.354681 92321 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 13:38:41.354706 92321 net.cpp:243] Memory required for data: 2128705536
I1226 13:38:41.354734 92321 layer_factory.hpp:114] Creating layer fc8
I1226 13:38:41.354811 92321 net.cpp:178] Creating Layer fc8
I1226 13:38:41.354887 92321 net.cpp:612] fc8 <- fc7
I1226 13:38:41.354926 92321 net.cpp:586] fc8 -> fc8
I1226 13:38:41.383402 91021 net.cpp:228] Setting up fc7
I1226 13:38:41.383512 91021 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 13:38:41.383539 91021 net.cpp:243] Memory required for data: 2120316928
I1226 13:38:41.383620 91021 layer_factory.hpp:114] Creating layer relu7
I1226 13:38:41.383697 91021 net.cpp:178] Creating Layer relu7
I1226 13:38:41.383736 91021 net.cpp:612] relu7 <- fc7
I1226 13:38:41.383774 91021 net.cpp:573] relu7 -> fc7 (in-place)
I1226 13:38:41.383864 91021 net.cpp:228] Setting up relu7
I1226 13:38:41.383903 91021 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 13:38:41.383926 91021 net.cpp:243] Memory required for data: 2124511232
I1226 13:38:41.383960 91021 layer_factory.hpp:114] Creating layer drop7
I1226 13:38:41.384006 91021 net.cpp:178] Creating Layer drop7
I1226 13:38:41.384032 91021 net.cpp:612] drop7 <- fc7
I1226 13:38:41.384068 91021 net.cpp:573] drop7 -> fc7 (in-place)
I1226 13:38:41.384109 91021 net.cpp:228] Setting up drop7
I1226 13:38:41.384140 91021 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 13:38:41.384168 91021 net.cpp:243] Memory required for data: 2128705536
I1226 13:38:41.384199 91021 layer_factory.hpp:114] Creating layer fc8
I1226 13:38:41.384290 91021 net.cpp:178] Creating Layer fc8
I1226 13:38:41.384320 91021 net.cpp:612] fc8 <- fc7
I1226 13:38:41.384357 91021 net.cpp:586] fc8 -> fc8
I1226 13:38:41.495031 89419 net.cpp:228] Setting up fc7
I1226 13:38:41.495143 89419 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 13:38:41.495172 89419 net.cpp:243] Memory required for data: 2120316928
I1226 13:38:41.495229 89419 layer_factory.hpp:114] Creating layer relu7
I1226 13:38:41.495304 89419 net.cpp:178] Creating Layer relu7
I1226 13:38:41.495349 89419 net.cpp:612] relu7 <- fc7
I1226 13:38:41.495388 89419 net.cpp:573] relu7 -> fc7 (in-place)
I1226 13:38:41.495477 89419 net.cpp:228] Setting up relu7
I1226 13:38:41.495553 89419 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 13:38:41.495578 89419 net.cpp:243] Memory required for data: 2124511232
I1226 13:38:41.495607 89419 layer_factory.hpp:114] Creating layer drop7
I1226 13:38:41.495678 89419 net.cpp:178] Creating Layer drop7
I1226 13:38:41.495717 89419 net.cpp:612] drop7 <- fc7
I1226 13:38:41.495750 89419 net.cpp:573] drop7 -> fc7 (in-place)
I1226 13:38:41.495795 89419 net.cpp:228] Setting up drop7
I1226 13:38:41.495836 89419 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 13:38:41.495858 89419 net.cpp:243] Memory required for data: 2128705536
I1226 13:38:41.495893 89419 layer_factory.hpp:114] Creating layer fc8
I1226 13:38:41.495957 89419 net.cpp:178] Creating Layer fc8
I1226 13:38:41.495991 89419 net.cpp:612] fc8 <- fc7
I1226 13:38:41.496028 89419 net.cpp:586] fc8 -> fc8
I1226 13:38:41.885094 91865 net.cpp:228] Setting up fc8
I1226 13:38:41.885215 91865 net.cpp:235] Top shape: 256 1000 (256000)
I1226 13:38:41.885249 91865 net.cpp:243] Memory required for data: 2129729536
I1226 13:38:41.885354 91865 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 13:38:41.885438 91865 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 13:38:41.885490 91865 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 13:38:41.885567 91865 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 13:38:41.885640 91865 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 13:38:41.885771 91865 net.cpp:228] Setting up fc8_fc8_0_split
I1226 13:38:41.885849 91865 net.cpp:235] Top shape: 256 1000 (256000)
I1226 13:38:41.885890 91865 net.cpp:235] Top shape: 256 1000 (256000)
I1226 13:38:41.885921 91865 net.cpp:243] Memory required for data: 2131777536
I1226 13:38:41.885960 91865 layer_factory.hpp:114] Creating layer accuracy
I1226 13:38:41.886037 91865 net.cpp:178] Creating Layer accuracy
I1226 13:38:41.886091 91865 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 13:38:41.886135 91865 net.cpp:612] accuracy <- label_data_1_split_0
I1226 13:38:41.886196 91865 net.cpp:586] accuracy -> accuracy
I1226 13:38:41.886258 91865 net.cpp:228] Setting up accuracy
I1226 13:38:41.886337 91865 net.cpp:235] Top shape: (1)
I1226 13:38:41.886373 91865 net.cpp:243] Memory required for data: 2131777540
I1226 13:38:41.886411 91865 layer_factory.hpp:114] Creating layer loss
I1226 13:38:41.886471 91865 net.cpp:178] Creating Layer loss
I1226 13:38:41.886507 91865 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 13:38:41.886548 91865 net.cpp:612] loss <- label_data_1_split_1
I1226 13:38:41.886617 91865 net.cpp:586] loss -> loss
I1226 13:38:41.886714 91865 layer_factory.hpp:114] Creating layer loss
I1226 13:38:41.916949 91865 net.cpp:228] Setting up loss
I1226 13:38:41.917151 91865 net.cpp:235] Top shape: (1)
I1226 13:38:41.917204 91865 net.cpp:238]     with loss weight 1
I1226 13:38:41.917402 91865 net.cpp:243] Memory required for data: 2131777544
I1226 13:38:41.917454 91865 net.cpp:305] loss needs backward computation.
I1226 13:38:41.917510 91865 net.cpp:307] accuracy does not need backward computation.
I1226 13:38:41.917554 91865 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 13:38:41.917594 91865 net.cpp:305] fc8 needs backward computation.
I1226 13:38:41.917625 91865 net.cpp:305] drop7 needs backward computation.
I1226 13:38:41.917654 91865 net.cpp:305] relu7 needs backward computation.
I1226 13:38:41.917685 91865 net.cpp:305] fc7 needs backward computation.
I1226 13:38:41.917717 91865 net.cpp:305] drop6 needs backward computation.
I1226 13:38:41.917747 91865 net.cpp:305] relu6 needs backward computation.
I1226 13:38:41.917776 91865 net.cpp:305] fc6 needs backward computation.
I1226 13:38:41.917812 91865 net.cpp:305] pool5 needs backward computation.
I1226 13:38:41.917848 91865 net.cpp:305] relu5 needs backward computation.
I1226 13:38:41.917886 91865 net.cpp:305] conv5 needs backward computation.
I1226 13:38:41.917917 91865 net.cpp:305] relu4 needs backward computation.
I1226 13:38:41.917955 91865 net.cpp:305] conv4 needs backward computation.
I1226 13:38:41.917985 91865 net.cpp:305] relu3 needs backward computation.
I1226 13:38:41.912282 92321 net.cpp:228] Setting up fc8
I1226 13:38:41.918015 91865 net.cpp:305] conv3 needs backward computation.
I1226 13:38:41.912396 92321 net.cpp:235] Top shape: 256 1000 (256000)
I1226 13:38:41.912425 92321 net.cpp:243] Memory required for data: 2129729536
I1226 13:38:41.918046 91865 net.cpp:305] pool2 needs backward computation.
I1226 13:38:41.912508 92321 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 13:38:41.912576 92321 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 13:38:41.918084 91865 net.cpp:305] norm2 needs backward computation.
I1226 13:38:41.912608 92321 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 13:38:41.918126 91865 net.cpp:305] relu2 needs backward computation.
I1226 13:38:41.912662 92321 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 13:38:41.912708 92321 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 13:38:41.918156 91865 net.cpp:305] conv2 needs backward computation.
I1226 13:38:41.912803 92321 net.cpp:228] Setting up fc8_fc8_0_split
I1226 13:38:41.918192 91865 net.cpp:305] pool1 needs backward computation.
I1226 13:38:41.912858 92321 net.cpp:235] Top shape: 256 1000 (256000)
I1226 13:38:41.918228 91865 net.cpp:305] norm1 needs backward computation.
I1226 13:38:41.912889 92321 net.cpp:235] Top shape: 256 1000 (256000)
I1226 13:38:41.912910 92321 net.cpp:243] Memory required for data: 2131777536
I1226 13:38:41.912938 92321 layer_factory.hpp:114] Creating layer accuracy
I1226 13:38:41.912989 92321 net.cpp:178] Creating Layer accuracy
I1226 13:38:41.918258 91865 net.cpp:305] relu1 needs backward computation.
I1226 13:38:41.913017 92321 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 13:38:41.918298 91865 net.cpp:305] conv1 needs backward computation.
I1226 13:38:41.913053 92321 net.cpp:612] accuracy <- label_data_1_split_0
I1226 13:38:41.918350 91865 net.cpp:307] label_data_1_split does not need backward computation.
I1226 13:38:41.913105 92321 net.cpp:586] accuracy -> accuracy
I1226 13:38:41.913151 92321 net.cpp:228] Setting up accuracy
I1226 13:38:41.918391 91865 net.cpp:307] data does not need backward computation.
I1226 13:38:41.913185 92321 net.cpp:235] Top shape: (1)
I1226 13:38:41.918427 91865 net.cpp:349] This network produces output accuracy
I1226 13:38:41.913208 92321 net.cpp:243] Memory required for data: 2131777540
I1226 13:38:41.913235 92321 layer_factory.hpp:114] Creating layer loss
I1226 13:38:41.918464 91865 net.cpp:349] This network produces output loss
I1226 13:38:41.913280 92321 net.cpp:178] Creating Layer loss
I1226 13:38:41.918581 91865 net.cpp:363] Network initialization done.
I1226 13:38:41.913316 92321 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 13:38:41.913342 92321 net.cpp:612] loss <- label_data_1_split_1
I1226 13:38:41.919037 91865 solver.cpp:107] Solver scaffolding done.
I1226 13:38:41.913374 92321 net.cpp:586] loss -> loss
I1226 13:38:41.919253 91865 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 13:38:41.913440 92321 layer_factory.hpp:114] Creating layer loss
I1226 13:38:41.940178 92321 net.cpp:228] Setting up loss
I1226 13:38:41.940379 92321 net.cpp:235] Top shape: (1)
I1226 13:38:41.940424 92321 net.cpp:238]     with loss weight 1
I1226 13:38:41.940594 92321 net.cpp:243] Memory required for data: 2131777544
I1226 13:38:41.943331 91021 net.cpp:228] Setting up fc8
I1226 13:38:41.940798 92321 net.cpp:305] loss needs backward computation.
I1226 13:38:41.943442 91021 net.cpp:235] Top shape: 256 1000 (256000)
I1226 13:38:41.943466 91021 net.cpp:243] Memory required for data: 2129729536
I1226 13:38:41.940843 92321 net.cpp:307] accuracy does not need backward computation.
I1226 13:38:41.943547 91021 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 13:38:41.940877 92321 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 13:38:41.943629 91021 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 13:38:41.940910 92321 net.cpp:305] fc8 needs backward computation.
I1226 13:38:41.943670 91021 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 13:38:41.940953 92321 net.cpp:305] drop7 needs backward computation.
I1226 13:38:41.943708 91021 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 13:38:41.940994 92321 net.cpp:305] relu7 needs backward computation.
I1226 13:38:41.941023 92321 net.cpp:305] fc7 needs backward computation.
I1226 13:38:41.943850 91021 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 13:38:41.941056 92321 net.cpp:305] drop6 needs backward computation.
I1226 13:38:41.943933 91021 net.cpp:228] Setting up fc8_fc8_0_split
I1226 13:38:41.941097 92321 net.cpp:305] relu6 needs backward computation.
I1226 13:38:41.943979 91021 net.cpp:235] Top shape: 256 1000 (256000)
I1226 13:38:41.941128 92321 net.cpp:305] fc6 needs backward computation.
I1226 13:38:41.944018 91021 net.cpp:235] Top shape: 256 1000 (256000)
I1226 13:38:41.941159 92321 net.cpp:305] pool5 needs backward computation.
I1226 13:38:41.944041 91021 net.cpp:243] Memory required for data: 2131777536
I1226 13:38:41.941196 92321 net.cpp:305] relu5 needs backward computation.
I1226 13:38:41.944074 91021 layer_factory.hpp:114] Creating layer accuracy
I1226 13:38:41.941234 92321 net.cpp:305] conv5 needs backward computation.
I1226 13:38:41.944124 91021 net.cpp:178] Creating Layer accuracy
I1226 13:38:41.941265 92321 net.cpp:305] relu4 needs backward computation.
I1226 13:38:41.944155 91021 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 13:38:41.944185 91021 net.cpp:612] accuracy <- label_data_1_split_0
I1226 13:38:41.944219 91021 net.cpp:586] accuracy -> accuracy
I1226 13:38:41.944285 91021 net.cpp:228] Setting up accuracy
I1226 13:38:41.941299 92321 net.cpp:305] conv4 needs backward computation.
I1226 13:38:41.944321 91021 net.cpp:235] Top shape: (1)
I1226 13:38:41.941330 92321 net.cpp:305] relu3 needs backward computation.
I1226 13:38:41.944355 91021 net.cpp:243] Memory required for data: 2131777540
I1226 13:38:41.941366 92321 net.cpp:305] conv3 needs backward computation.
I1226 13:38:41.944383 91021 layer_factory.hpp:114] Creating layer loss
I1226 13:38:41.944427 91021 net.cpp:178] Creating Layer loss
I1226 13:38:41.944453 91021 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 13:38:41.941398 92321 net.cpp:305] pool2 needs backward computation.
I1226 13:38:41.944483 91021 net.cpp:612] loss <- label_data_1_split_1
I1226 13:38:41.941431 92321 net.cpp:305] norm2 needs backward computation.
I1226 13:38:41.944545 91021 net.cpp:586] loss -> loss
I1226 13:38:41.941465 92321 net.cpp:305] relu2 needs backward computation.
I1226 13:38:41.941519 92321 net.cpp:305] conv2 needs backward computation.
I1226 13:38:41.944610 91021 layer_factory.hpp:114] Creating layer loss
I1226 13:38:41.941555 92321 net.cpp:305] pool1 needs backward computation.
I1226 13:38:41.941587 92321 net.cpp:305] norm1 needs backward computation.
I1226 13:38:41.941618 92321 net.cpp:305] relu1 needs backward computation.
I1226 13:38:41.941649 92321 net.cpp:305] conv1 needs backward computation.
I1226 13:38:41.941691 92321 net.cpp:307] label_data_1_split does not need backward computation.
I1226 13:38:41.941730 92321 net.cpp:307] data does not need backward computation.
I1226 13:38:41.941766 92321 net.cpp:349] This network produces output accuracy
I1226 13:38:41.941802 92321 net.cpp:349] This network produces output loss
I1226 13:38:41.941903 92321 net.cpp:363] Network initialization done.
I1226 13:38:41.942363 92321 solver.cpp:107] Solver scaffolding done.
I1226 13:38:41.942610 92321 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 13:38:41.971760 91021 net.cpp:228] Setting up loss
I1226 13:38:41.971958 91021 net.cpp:235] Top shape: (1)
I1226 13:38:41.972003 91021 net.cpp:238]     with loss weight 1
I1226 13:38:41.972143 91021 net.cpp:243] Memory required for data: 2131777544
I1226 13:38:41.972192 91021 net.cpp:305] loss needs backward computation.
I1226 13:38:41.972231 91021 net.cpp:307] accuracy does not need backward computation.
I1226 13:38:41.972301 91021 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 13:38:41.972333 91021 net.cpp:305] fc8 needs backward computation.
I1226 13:38:41.972365 91021 net.cpp:305] drop7 needs backward computation.
I1226 13:38:41.972396 91021 net.cpp:305] relu7 needs backward computation.
I1226 13:38:41.972427 91021 net.cpp:305] fc7 needs backward computation.
I1226 13:38:41.972458 91021 net.cpp:305] drop6 needs backward computation.
I1226 13:38:41.972489 91021 net.cpp:305] relu6 needs backward computation.
I1226 13:38:41.972517 91021 net.cpp:305] fc6 needs backward computation.
I1226 13:38:41.972548 91021 net.cpp:305] pool5 needs backward computation.
I1226 13:38:41.972579 91021 net.cpp:305] relu5 needs backward computation.
I1226 13:38:41.972607 91021 net.cpp:305] conv5 needs backward computation.
I1226 13:38:41.972638 91021 net.cpp:305] relu4 needs backward computation.
I1226 13:38:41.972681 91021 net.cpp:305] conv4 needs backward computation.
I1226 13:38:41.972713 91021 net.cpp:305] relu3 needs backward computation.
I1226 13:38:41.972743 91021 net.cpp:305] conv3 needs backward computation.
I1226 13:38:41.972775 91021 net.cpp:305] pool2 needs backward computation.
I1226 13:38:41.972808 91021 net.cpp:305] norm2 needs backward computation.
I1226 13:38:41.972851 91021 net.cpp:305] relu2 needs backward computation.
I1226 13:38:41.972889 91021 net.cpp:305] conv2 needs backward computation.
I1226 13:38:41.972921 91021 net.cpp:305] pool1 needs backward computation.
I1226 13:38:41.972960 91021 net.cpp:305] norm1 needs backward computation.
I1226 13:38:41.972992 91021 net.cpp:305] relu1 needs backward computation.
I1226 13:38:41.973021 91021 net.cpp:305] conv1 needs backward computation.
I1226 13:38:41.973062 91021 net.cpp:307] label_data_1_split does not need backward computation.
I1226 13:38:41.973095 91021 net.cpp:307] data does not need backward computation.
I1226 13:38:41.973129 91021 net.cpp:349] This network produces output accuracy
I1226 13:38:41.973163 91021 net.cpp:349] This network produces output loss
I1226 13:38:41.973273 91021 net.cpp:363] Network initialization done.
I1226 13:38:41.973736 91021 solver.cpp:107] Solver scaffolding done.
I1226 13:38:41.973949 91021 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 13:38:42.060986 89419 net.cpp:228] Setting up fc8
I1226 13:38:42.061101 89419 net.cpp:235] Top shape: 256 1000 (256000)
I1226 13:38:42.061131 89419 net.cpp:243] Memory required for data: 2129729536
I1226 13:38:42.061185 89419 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 13:38:42.061277 89419 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 13:38:42.061322 89419 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 13:38:42.061367 89419 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 13:38:42.061414 89419 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 13:38:42.061501 89419 net.cpp:228] Setting up fc8_fc8_0_split
I1226 13:38:42.061550 89419 net.cpp:235] Top shape: 256 1000 (256000)
I1226 13:38:42.061581 89419 net.cpp:235] Top shape: 256 1000 (256000)
I1226 13:38:42.061604 89419 net.cpp:243] Memory required for data: 2131777536
I1226 13:38:42.061661 89419 layer_factory.hpp:114] Creating layer accuracy
I1226 13:38:42.061713 89419 net.cpp:178] Creating Layer accuracy
I1226 13:38:42.061748 89419 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 13:38:42.061779 89419 net.cpp:612] accuracy <- label_data_1_split_0
I1226 13:38:42.061820 89419 net.cpp:586] accuracy -> accuracy
I1226 13:38:42.061863 89419 net.cpp:228] Setting up accuracy
I1226 13:38:42.061897 89419 net.cpp:235] Top shape: (1)
I1226 13:38:42.061929 89419 net.cpp:243] Memory required for data: 2131777540
I1226 13:38:42.061955 89419 layer_factory.hpp:114] Creating layer loss
I1226 13:38:42.062010 89419 net.cpp:178] Creating Layer loss
I1226 13:38:42.062041 89419 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 13:38:42.062072 89419 net.cpp:612] loss <- label_data_1_split_1
I1226 13:38:42.062108 89419 net.cpp:586] loss -> loss
I1226 13:38:42.062170 89419 layer_factory.hpp:114] Creating layer loss
I1226 13:38:42.090492 89419 net.cpp:228] Setting up loss
I1226 13:38:42.090740 89419 net.cpp:235] Top shape: (1)
I1226 13:38:42.090790 89419 net.cpp:238]     with loss weight 1
I1226 13:38:42.090939 89419 net.cpp:243] Memory required for data: 2131777544
I1226 13:38:42.090989 89419 net.cpp:305] loss needs backward computation.
I1226 13:38:42.091033 89419 net.cpp:307] accuracy does not need backward computation.
I1226 13:38:42.091081 89419 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 13:38:42.091117 89419 net.cpp:305] fc8 needs backward computation.
I1226 13:38:42.091150 89419 net.cpp:305] drop7 needs backward computation.
I1226 13:38:42.091193 89419 net.cpp:305] relu7 needs backward computation.
I1226 13:38:42.091225 89419 net.cpp:305] fc7 needs backward computation.
I1226 13:38:42.091258 89419 net.cpp:305] drop6 needs backward computation.
I1226 13:38:42.091286 89419 net.cpp:305] relu6 needs backward computation.
I1226 13:38:42.091315 89419 net.cpp:305] fc6 needs backward computation.
I1226 13:38:42.091347 89419 net.cpp:305] pool5 needs backward computation.
I1226 13:38:42.091390 89419 net.cpp:305] relu5 needs backward computation.
I1226 13:38:42.091421 89419 net.cpp:305] conv5 needs backward computation.
I1226 13:38:42.091452 89419 net.cpp:305] relu4 needs backward computation.
I1226 13:38:42.091491 89419 net.cpp:305] conv4 needs backward computation.
I1226 13:38:42.091529 89419 net.cpp:305] relu3 needs backward computation.
I1226 13:38:42.091567 89419 net.cpp:305] conv3 needs backward computation.
I1226 13:38:42.091603 89419 net.cpp:305] pool2 needs backward computation.
I1226 13:38:42.091660 89419 net.cpp:305] norm2 needs backward computation.
I1226 13:38:42.091692 89419 net.cpp:305] relu2 needs backward computation.
I1226 13:38:42.091722 89419 net.cpp:305] conv2 needs backward computation.
I1226 13:38:42.091755 89419 net.cpp:305] pool1 needs backward computation.
I1226 13:38:42.091789 89419 net.cpp:305] norm1 needs backward computation.
I1226 13:38:42.091819 89419 net.cpp:305] relu1 needs backward computation.
I1226 13:38:42.091850 89419 net.cpp:305] conv1 needs backward computation.
I1226 13:38:42.091897 89419 net.cpp:307] label_data_1_split does not need backward computation.
I1226 13:38:42.091933 89419 net.cpp:307] data does not need backward computation.
I1226 13:38:42.091961 89419 net.cpp:349] This network produces output accuracy
I1226 13:38:42.091997 89419 net.cpp:349] This network produces output loss
I1226 13:38:42.092100 89419 net.cpp:363] Network initialization done.
I1226 13:38:42.092564 89419 solver.cpp:107] Solver scaffolding done.
I1226 13:38:42.092804 89419 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 13:38:44.366983 122432 net.cpp:228] Setting up fc6
I1226 13:38:44.367125 122432 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 13:38:44.367168 122432 net.cpp:243] Memory required for data: 2107734016
I1226 13:38:44.367249 122432 layer_factory.hpp:114] Creating layer relu6
I1226 13:38:44.367369 122432 net.cpp:178] Creating Layer relu6
I1226 13:38:44.367542 122432 net.cpp:612] relu6 <- fc6
I1226 13:38:44.367609 122432 net.cpp:573] relu6 -> fc6 (in-place)
I1226 13:38:44.367774 122432 net.cpp:228] Setting up relu6
I1226 13:38:44.367837 122432 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 13:38:44.367872 122432 net.cpp:243] Memory required for data: 2111928320
I1226 13:38:44.367915 122432 layer_factory.hpp:114] Creating layer drop6
I1226 13:38:44.367995 122432 net.cpp:178] Creating Layer drop6
I1226 13:38:44.368036 122432 net.cpp:612] drop6 <- fc6
I1226 13:38:44.368086 122432 net.cpp:573] drop6 -> fc6 (in-place)
I1226 13:38:44.368294 122432 net.cpp:228] Setting up drop6
I1226 13:38:44.368347 122432 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 13:38:44.368397 122432 net.cpp:243] Memory required for data: 2116122624
I1226 13:38:44.368438 122432 layer_factory.hpp:114] Creating layer fc7
I1226 13:38:44.368513 122432 net.cpp:178] Creating Layer fc7
I1226 13:38:44.368561 122432 net.cpp:612] fc7 <- fc6
I1226 13:38:44.368660 122432 net.cpp:586] fc7 -> fc7
I1226 13:38:45.173002 91865 caffe.cpp:376] Configuring multinode setup
I1226 13:38:45.174477 91865 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 13:38:45.183760 92321 caffe.cpp:376] Configuring multinode setup
I1226 13:38:45.185176 92321 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 13:38:46.010191 91021 caffe.cpp:376] Configuring multinode setup
I1226 13:38:46.011710 91021 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 13:38:46.134248 89419 caffe.cpp:376] Configuring multinode setup
I1226 13:38:46.135800 89419 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 13:38:47.819587 122432 net.cpp:228] Setting up fc7
I1226 13:38:47.819762 122432 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 13:38:47.819806 122432 net.cpp:243] Memory required for data: 2120316928
I1226 13:38:47.819886 122432 layer_factory.hpp:114] Creating layer relu7
I1226 13:38:47.819985 122432 net.cpp:178] Creating Layer relu7
I1226 13:38:47.820042 122432 net.cpp:612] relu7 <- fc7
I1226 13:38:47.820096 122432 net.cpp:573] relu7 -> fc7 (in-place)
I1226 13:38:47.820219 122432 net.cpp:228] Setting up relu7
I1226 13:38:47.820297 122432 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 13:38:47.820334 122432 net.cpp:243] Memory required for data: 2124511232
I1226 13:38:47.820377 122432 layer_factory.hpp:114] Creating layer drop7
I1226 13:38:47.820456 122432 net.cpp:178] Creating Layer drop7
I1226 13:38:47.820497 122432 net.cpp:612] drop7 <- fc7
I1226 13:38:47.820546 122432 net.cpp:573] drop7 -> fc7 (in-place)
I1226 13:38:47.820611 122432 net.cpp:228] Setting up drop7
I1226 13:38:47.820696 122432 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 13:38:47.820735 122432 net.cpp:243] Memory required for data: 2128705536
I1226 13:38:47.820775 122432 layer_factory.hpp:114] Creating layer fc8
I1226 13:38:47.820852 122432 net.cpp:178] Creating Layer fc8
I1226 13:38:47.820893 122432 net.cpp:612] fc8 <- fc7
I1226 13:38:47.820948 122432 net.cpp:586] fc8 -> fc8
I1226 13:38:48.669313 122432 net.cpp:228] Setting up fc8
I1226 13:38:48.669483 122432 net.cpp:235] Top shape: 256 1000 (256000)
I1226 13:38:48.669528 122432 net.cpp:243] Memory required for data: 2129729536
I1226 13:38:48.669613 122432 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 13:38:48.669872 122432 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 13:38:48.669924 122432 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 13:38:48.670037 122432 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 13:38:48.670248 122432 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 13:38:48.670472 122432 net.cpp:228] Setting up fc8_fc8_0_split
I1226 13:38:48.670547 122432 net.cpp:235] Top shape: 256 1000 (256000)
I1226 13:38:48.670594 122432 net.cpp:235] Top shape: 256 1000 (256000)
I1226 13:38:48.670629 122432 net.cpp:243] Memory required for data: 2131777536
I1226 13:38:48.670708 122432 layer_factory.hpp:114] Creating layer accuracy
I1226 13:38:48.670797 122432 net.cpp:178] Creating Layer accuracy
I1226 13:38:48.670837 122432 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 13:38:48.670881 122432 net.cpp:612] accuracy <- label_data_1_split_0
I1226 13:38:48.670949 122432 net.cpp:586] accuracy -> accuracy
I1226 13:38:48.671016 122432 net.cpp:228] Setting up accuracy
I1226 13:38:48.671164 122432 net.cpp:235] Top shape: (1)
I1226 13:38:48.671210 122432 net.cpp:243] Memory required for data: 2131777540
I1226 13:38:48.671252 122432 layer_factory.hpp:114] Creating layer loss
I1226 13:38:48.671314 122432 net.cpp:178] Creating Layer loss
I1226 13:38:48.671360 122432 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 13:38:48.671402 122432 net.cpp:612] loss <- label_data_1_split_1
I1226 13:38:48.671576 122432 net.cpp:586] loss -> loss
I1226 13:38:48.671715 122432 layer_factory.hpp:114] Creating layer loss
I1226 13:38:48.710597 122432 net.cpp:228] Setting up loss
I1226 13:38:48.710777 122432 net.cpp:235] Top shape: (1)
I1226 13:38:48.710839 122432 net.cpp:238]     with loss weight 1
I1226 13:38:48.711021 122432 net.cpp:243] Memory required for data: 2131777544
I1226 13:38:48.711089 122432 net.cpp:305] loss needs backward computation.
I1226 13:38:48.711148 122432 net.cpp:307] accuracy does not need backward computation.
I1226 13:38:48.711199 122432 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 13:38:48.711244 122432 net.cpp:305] fc8 needs backward computation.
I1226 13:38:48.711292 122432 net.cpp:305] drop7 needs backward computation.
I1226 13:38:48.711336 122432 net.cpp:305] relu7 needs backward computation.
I1226 13:38:48.711392 122432 net.cpp:305] fc7 needs backward computation.
I1226 13:38:48.711441 122432 net.cpp:305] drop6 needs backward computation.
I1226 13:38:48.711484 122432 net.cpp:305] relu6 needs backward computation.
I1226 13:38:48.711529 122432 net.cpp:305] fc6 needs backward computation.
I1226 13:38:48.711572 122432 net.cpp:305] pool5 needs backward computation.
I1226 13:38:48.711616 122432 net.cpp:305] relu5 needs backward computation.
I1226 13:38:48.711695 122432 net.cpp:305] conv5 needs backward computation.
I1226 13:38:48.711740 122432 net.cpp:305] relu4 needs backward computation.
I1226 13:38:48.711781 122432 net.cpp:305] conv4 needs backward computation.
I1226 13:38:48.711823 122432 net.cpp:305] relu3 needs backward computation.
I1226 13:38:48.711864 122432 net.cpp:305] conv3 needs backward computation.
I1226 13:38:48.711906 122432 net.cpp:305] pool2 needs backward computation.
I1226 13:38:48.711948 122432 net.cpp:305] norm2 needs backward computation.
I1226 13:38:48.711990 122432 net.cpp:305] relu2 needs backward computation.
I1226 13:38:48.712030 122432 net.cpp:305] conv2 needs backward computation.
I1226 13:38:48.712071 122432 net.cpp:305] pool1 needs backward computation.
I1226 13:38:48.712113 122432 net.cpp:305] norm1 needs backward computation.
I1226 13:38:48.712153 122432 net.cpp:305] relu1 needs backward computation.
I1226 13:38:48.712193 122432 net.cpp:305] conv1 needs backward computation.
I1226 13:38:48.712237 122432 net.cpp:307] label_data_1_split does not need backward computation.
I1226 13:38:48.712280 122432 net.cpp:307] data does not need backward computation.
I1226 13:38:48.712318 122432 net.cpp:349] This network produces output accuracy
I1226 13:38:48.712364 122432 net.cpp:349] This network produces output loss
I1226 13:38:48.712529 122432 net.cpp:363] Network initialization done.
I1226 13:38:48.713191 122432 solver.cpp:107] Solver scaffolding done.
I1226 13:38:48.713503 122432 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 13:38:53.617197 122432 caffe.cpp:376] Configuring multinode setup
I1226 13:38:53.619329 122432 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 13:39:01.417141 94726 net.cpp:228] Setting up fc6
I1226 13:39:01.418068 94726 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 13:39:01.418123 94726 net.cpp:243] Memory required for data: 2107734016
I1226 13:39:01.418314 94726 layer_factory.hpp:114] Creating layer relu6
I1226 13:39:01.418408 94726 net.cpp:178] Creating Layer relu6
I1226 13:39:01.418460 94726 net.cpp:612] relu6 <- fc6
I1226 13:39:01.418506 94726 net.cpp:573] relu6 -> fc6 (in-place)
I1226 13:39:01.418612 94726 net.cpp:228] Setting up relu6
I1226 13:39:01.418671 94726 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 13:39:01.418699 94726 net.cpp:243] Memory required for data: 2111928320
I1226 13:39:01.418732 94726 layer_factory.hpp:114] Creating layer drop6
I1226 13:39:01.418788 94726 net.cpp:178] Creating Layer drop6
I1226 13:39:01.418855 94726 net.cpp:612] drop6 <- fc6
I1226 13:39:01.418905 94726 net.cpp:573] drop6 -> fc6 (in-place)
I1226 13:39:01.418974 94726 net.cpp:228] Setting up drop6
I1226 13:39:01.419028 94726 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 13:39:01.419054 94726 net.cpp:243] Memory required for data: 2116122624
I1226 13:39:01.419085 94726 layer_factory.hpp:114] Creating layer fc7
I1226 13:39:01.419143 94726 net.cpp:178] Creating Layer fc7
I1226 13:39:01.419180 94726 net.cpp:612] fc7 <- fc6
I1226 13:39:01.419241 94726 net.cpp:586] fc7 -> fc7
I1226 13:39:07.109690 96918 net.cpp:228] Setting up fc6
I1226 13:39:07.109980 96918 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 13:39:07.110028 96918 net.cpp:243] Memory required for data: 2107734016
I1226 13:39:07.110095 96918 layer_factory.hpp:114] Creating layer relu6
I1226 13:39:07.110268 96918 net.cpp:178] Creating Layer relu6
I1226 13:39:07.110322 96918 net.cpp:612] relu6 <- fc6
I1226 13:39:07.110369 96918 net.cpp:573] relu6 -> fc6 (in-place)
I1226 13:39:07.110473 96918 net.cpp:228] Setting up relu6
I1226 13:39:07.110535 96918 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 13:39:07.110563 96918 net.cpp:243] Memory required for data: 2111928320
I1226 13:39:07.110594 96918 layer_factory.hpp:114] Creating layer drop6
I1226 13:39:07.110676 96918 net.cpp:178] Creating Layer drop6
I1226 13:39:07.110740 96918 net.cpp:612] drop6 <- fc6
I1226 13:39:07.110790 96918 net.cpp:573] drop6 -> fc6 (in-place)
I1226 13:39:07.110862 96918 net.cpp:228] Setting up drop6
I1226 13:39:07.110915 96918 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 13:39:07.110941 96918 net.cpp:243] Memory required for data: 2116122624
I1226 13:39:07.110971 96918 layer_factory.hpp:114] Creating layer fc7
I1226 13:39:07.111035 96918 net.cpp:178] Creating Layer fc7
I1226 13:39:07.111073 96918 net.cpp:612] fc7 <- fc6
I1226 13:39:07.111115 96918 net.cpp:586] fc7 -> fc7
I1226 13:39:07.299311 96543 net.cpp:228] Setting up fc6
I1226 13:39:07.299567 96543 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 13:39:07.299617 96543 net.cpp:243] Memory required for data: 2107734016
I1226 13:39:07.299695 96543 layer_factory.hpp:114] Creating layer relu6
I1226 13:39:07.299782 96543 net.cpp:178] Creating Layer relu6
I1226 13:39:07.299913 96543 net.cpp:612] relu6 <- fc6
I1226 13:39:07.299970 96543 net.cpp:573] relu6 -> fc6 (in-place)
I1226 13:39:07.300086 96543 net.cpp:228] Setting up relu6
I1226 13:39:07.300144 96543 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 13:39:07.300174 96543 net.cpp:243] Memory required for data: 2111928320
I1226 13:39:07.300210 96543 layer_factory.hpp:114] Creating layer drop6
I1226 13:39:07.300287 96543 net.cpp:178] Creating Layer drop6
I1226 13:39:07.300323 96543 net.cpp:612] drop6 <- fc6
I1226 13:39:07.300364 96543 net.cpp:573] drop6 -> fc6 (in-place)
I1226 13:39:07.300432 96543 net.cpp:228] Setting up drop6
I1226 13:39:07.300473 96543 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 13:39:07.300498 96543 net.cpp:243] Memory required for data: 2116122624
I1226 13:39:07.300530 96543 layer_factory.hpp:114] Creating layer fc7
I1226 13:39:07.300595 96543 net.cpp:178] Creating Layer fc7
I1226 13:39:07.300627 96543 net.cpp:612] fc7 <- fc6
I1226 13:39:07.300669 96543 net.cpp:586] fc7 -> fc7
I1226 13:39:14.487102 94726 net.cpp:228] Setting up fc7
I1226 13:39:14.487221 94726 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 13:39:14.487257 94726 net.cpp:243] Memory required for data: 2120316928
I1226 13:39:14.487321 94726 layer_factory.hpp:114] Creating layer relu7
I1226 13:39:14.487428 94726 net.cpp:178] Creating Layer relu7
I1226 13:39:14.487483 94726 net.cpp:612] relu7 <- fc7
I1226 13:39:14.487526 94726 net.cpp:573] relu7 -> fc7 (in-place)
I1226 13:39:14.487627 94726 net.cpp:228] Setting up relu7
I1226 13:39:14.487684 94726 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 13:39:14.487710 94726 net.cpp:243] Memory required for data: 2124511232
I1226 13:39:14.487742 94726 layer_factory.hpp:114] Creating layer drop7
I1226 13:39:14.487795 94726 net.cpp:178] Creating Layer drop7
I1226 13:39:14.487871 94726 net.cpp:612] drop7 <- fc7
I1226 13:39:14.487918 94726 net.cpp:573] drop7 -> fc7 (in-place)
I1226 13:39:14.487982 94726 net.cpp:228] Setting up drop7
I1226 13:39:14.488028 94726 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 13:39:14.488059 94726 net.cpp:243] Memory required for data: 2128705536
I1226 13:39:14.488100 94726 layer_factory.hpp:114] Creating layer fc8
I1226 13:39:14.488157 94726 net.cpp:178] Creating Layer fc8
I1226 13:39:14.488193 94726 net.cpp:612] fc8 <- fc7
I1226 13:39:14.488234 94726 net.cpp:586] fc8 -> fc8
I1226 13:39:17.678855 94726 net.cpp:228] Setting up fc8
I1226 13:39:17.678975 94726 net.cpp:235] Top shape: 256 1000 (256000)
I1226 13:39:17.679009 94726 net.cpp:243] Memory required for data: 2129729536
I1226 13:39:17.679075 94726 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 13:39:17.679242 94726 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 13:39:17.679285 94726 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 13:39:17.679349 94726 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 13:39:17.679425 94726 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 13:39:17.679538 94726 net.cpp:228] Setting up fc8_fc8_0_split
I1226 13:39:17.679594 94726 net.cpp:235] Top shape: 256 1000 (256000)
I1226 13:39:17.679630 94726 net.cpp:235] Top shape: 256 1000 (256000)
I1226 13:39:17.679653 94726 net.cpp:243] Memory required for data: 2131777536
I1226 13:39:17.679692 94726 layer_factory.hpp:114] Creating layer accuracy
I1226 13:39:17.679755 94726 net.cpp:178] Creating Layer accuracy
I1226 13:39:17.679792 94726 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 13:39:17.679848 94726 net.cpp:612] accuracy <- label_data_1_split_0
I1226 13:39:17.679890 94726 net.cpp:586] accuracy -> accuracy
I1226 13:39:17.679944 94726 net.cpp:228] Setting up accuracy
I1226 13:39:17.679993 94726 net.cpp:235] Top shape: (1)
I1226 13:39:17.680019 94726 net.cpp:243] Memory required for data: 2131777540
I1226 13:39:17.680052 94726 layer_factory.hpp:114] Creating layer loss
I1226 13:39:17.680236 94726 net.cpp:178] Creating Layer loss
I1226 13:39:17.680284 94726 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 13:39:17.680320 94726 net.cpp:612] loss <- label_data_1_split_1
I1226 13:39:17.680357 94726 net.cpp:586] loss -> loss
I1226 13:39:17.680436 94726 layer_factory.hpp:114] Creating layer loss
I1226 13:39:17.713918 94726 net.cpp:228] Setting up loss
I1226 13:39:17.714032 94726 net.cpp:235] Top shape: (1)
I1226 13:39:17.714107 94726 net.cpp:238]     with loss weight 1
I1226 13:39:17.714371 94726 net.cpp:243] Memory required for data: 2131777544
I1226 13:39:17.714421 94726 net.cpp:305] loss needs backward computation.
I1226 13:39:17.714460 94726 net.cpp:307] accuracy does not need backward computation.
I1226 13:39:17.714495 94726 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 13:39:17.714537 94726 net.cpp:305] fc8 needs backward computation.
I1226 13:39:17.714570 94726 net.cpp:305] drop7 needs backward computation.
I1226 13:39:17.714609 94726 net.cpp:305] relu7 needs backward computation.
I1226 13:39:17.714638 94726 net.cpp:305] fc7 needs backward computation.
I1226 13:39:17.714668 94726 net.cpp:305] drop6 needs backward computation.
I1226 13:39:17.714700 94726 net.cpp:305] relu6 needs backward computation.
I1226 13:39:17.714730 94726 net.cpp:305] fc6 needs backward computation.
I1226 13:39:17.714761 94726 net.cpp:305] pool5 needs backward computation.
I1226 13:39:17.714803 94726 net.cpp:305] relu5 needs backward computation.
I1226 13:39:17.714869 94726 net.cpp:305] conv5 needs backward computation.
I1226 13:39:17.714901 94726 net.cpp:305] relu4 needs backward computation.
I1226 13:39:17.714931 94726 net.cpp:305] conv4 needs backward computation.
I1226 13:39:17.714962 94726 net.cpp:305] relu3 needs backward computation.
I1226 13:39:17.714993 94726 net.cpp:305] conv3 needs backward computation.
I1226 13:39:17.715024 94726 net.cpp:305] pool2 needs backward computation.
I1226 13:39:17.715065 94726 net.cpp:305] norm2 needs backward computation.
I1226 13:39:17.715096 94726 net.cpp:305] relu2 needs backward computation.
I1226 13:39:17.715126 94726 net.cpp:305] conv2 needs backward computation.
I1226 13:39:17.715157 94726 net.cpp:305] pool1 needs backward computation.
I1226 13:39:17.715194 94726 net.cpp:305] norm1 needs backward computation.
I1226 13:39:17.715225 94726 net.cpp:305] relu1 needs backward computation.
I1226 13:39:17.715265 94726 net.cpp:305] conv1 needs backward computation.
I1226 13:39:17.715299 94726 net.cpp:307] label_data_1_split does not need backward computation.
I1226 13:39:17.715339 94726 net.cpp:307] data does not need backward computation.
I1226 13:39:17.715366 94726 net.cpp:349] This network produces output accuracy
I1226 13:39:17.715404 94726 net.cpp:349] This network produces output loss
I1226 13:39:17.715499 94726 net.cpp:363] Network initialization done.
I1226 13:39:17.715987 94726 solver.cpp:107] Solver scaffolding done.
I1226 13:39:17.716260 94726 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 13:39:20.182696 96918 net.cpp:228] Setting up fc7
I1226 13:39:20.182834 96918 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 13:39:20.182868 96918 net.cpp:243] Memory required for data: 2120316928
I1226 13:39:20.182929 96918 layer_factory.hpp:114] Creating layer relu7
I1226 13:39:20.183115 96918 net.cpp:178] Creating Layer relu7
I1226 13:39:20.183168 96918 net.cpp:612] relu7 <- fc7
I1226 13:39:20.183210 96918 net.cpp:573] relu7 -> fc7 (in-place)
I1226 13:39:20.183318 96918 net.cpp:228] Setting up relu7
I1226 13:39:20.183372 96918 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 13:39:20.183403 96918 net.cpp:243] Memory required for data: 2124511232
I1226 13:39:20.183434 96918 layer_factory.hpp:114] Creating layer drop7
I1226 13:39:20.183481 96918 net.cpp:178] Creating Layer drop7
I1226 13:39:20.183516 96918 net.cpp:612] drop7 <- fc7
I1226 13:39:20.183550 96918 net.cpp:573] drop7 -> fc7 (in-place)
I1226 13:39:20.183604 96918 net.cpp:228] Setting up drop7
I1226 13:39:20.183640 96918 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 13:39:20.183665 96918 net.cpp:243] Memory required for data: 2128705536
I1226 13:39:20.183693 96918 layer_factory.hpp:114] Creating layer fc8
I1226 13:39:20.183815 96918 net.cpp:178] Creating Layer fc8
I1226 13:39:20.183858 96918 net.cpp:612] fc8 <- fc7
I1226 13:39:20.183902 96918 net.cpp:586] fc8 -> fc8
I1226 13:39:20.433564 96543 net.cpp:228] Setting up fc7
I1226 13:39:20.433679 96543 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 13:39:20.433715 96543 net.cpp:243] Memory required for data: 2120316928
I1226 13:39:20.433807 96543 layer_factory.hpp:114] Creating layer relu7
I1226 13:39:20.433921 96543 net.cpp:178] Creating Layer relu7
I1226 13:39:20.433974 96543 net.cpp:612] relu7 <- fc7
I1226 13:39:20.434026 96543 net.cpp:573] relu7 -> fc7 (in-place)
I1226 13:39:20.434129 96543 net.cpp:228] Setting up relu7
I1226 13:39:20.434198 96543 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 13:39:20.434229 96543 net.cpp:243] Memory required for data: 2124511232
I1226 13:39:20.434267 96543 layer_factory.hpp:114] Creating layer drop7
I1226 13:39:20.434314 96543 net.cpp:178] Creating Layer drop7
I1226 13:39:20.434346 96543 net.cpp:612] drop7 <- fc7
I1226 13:39:20.434387 96543 net.cpp:573] drop7 -> fc7 (in-place)
I1226 13:39:20.434442 96543 net.cpp:228] Setting up drop7
I1226 13:39:20.434481 96543 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 13:39:20.434509 96543 net.cpp:243] Memory required for data: 2128705536
I1226 13:39:20.434540 96543 layer_factory.hpp:114] Creating layer fc8
I1226 13:39:20.434630 96543 net.cpp:178] Creating Layer fc8
I1226 13:39:20.434680 96543 net.cpp:612] fc8 <- fc7
I1226 13:39:20.434729 96543 net.cpp:586] fc8 -> fc8
I1226 13:39:22.263286 94726 caffe.cpp:376] Configuring multinode setup
I1226 13:39:22.285399 94726 caffe.cpp:386] Starting parameter server in mpi environment
I1226 13:39:23.374023 96918 net.cpp:228] Setting up fc8
I1226 13:39:23.374161 96918 net.cpp:235] Top shape: 256 1000 (256000)
I1226 13:39:23.374199 96918 net.cpp:243] Memory required for data: 2129729536
I1226 13:39:23.374259 96918 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 13:39:23.374428 96918 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 13:39:23.374477 96918 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 13:39:23.374531 96918 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 13:39:23.374584 96918 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 13:39:23.374717 96918 net.cpp:228] Setting up fc8_fc8_0_split
I1226 13:39:23.374812 96918 net.cpp:235] Top shape: 256 1000 (256000)
I1226 13:39:23.374847 96918 net.cpp:235] Top shape: 256 1000 (256000)
I1226 13:39:23.374872 96918 net.cpp:243] Memory required for data: 2131777536
I1226 13:39:23.374914 96918 layer_factory.hpp:114] Creating layer accuracy
I1226 13:39:23.374977 96918 net.cpp:178] Creating Layer accuracy
I1226 13:39:23.375015 96918 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 13:39:23.375061 96918 net.cpp:612] accuracy <- label_data_1_split_0
I1226 13:39:23.375102 96918 net.cpp:586] accuracy -> accuracy
I1226 13:39:23.375151 96918 net.cpp:228] Setting up accuracy
I1226 13:39:23.375195 96918 net.cpp:235] Top shape: (1)
I1226 13:39:23.375219 96918 net.cpp:243] Memory required for data: 2131777540
I1226 13:39:23.375247 96918 layer_factory.hpp:114] Creating layer loss
I1226 13:39:23.375422 96918 net.cpp:178] Creating Layer loss
I1226 13:39:23.375470 96918 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 13:39:23.375504 96918 net.cpp:612] loss <- label_data_1_split_1
I1226 13:39:23.375550 96918 net.cpp:586] loss -> loss
I1226 13:39:23.375628 96918 layer_factory.hpp:114] Creating layer loss
I1226 13:39:23.408849 96918 net.cpp:228] Setting up loss
I1226 13:39:23.408965 96918 net.cpp:235] Top shape: (1)
I1226 13:39:23.409008 96918 net.cpp:238]     with loss weight 1
I1226 13:39:23.409168 96918 net.cpp:243] Memory required for data: 2131777544
I1226 13:39:23.409220 96918 net.cpp:305] loss needs backward computation.
I1226 13:39:23.409261 96918 net.cpp:307] accuracy does not need backward computation.
I1226 13:39:23.409296 96918 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 13:39:23.409345 96918 net.cpp:305] fc8 needs backward computation.
I1226 13:39:23.409379 96918 net.cpp:305] drop7 needs backward computation.
I1226 13:39:23.409418 96918 net.cpp:305] relu7 needs backward computation.
I1226 13:39:23.409448 96918 net.cpp:305] fc7 needs backward computation.
I1226 13:39:23.409481 96918 net.cpp:305] drop6 needs backward computation.
I1226 13:39:23.409510 96918 net.cpp:305] relu6 needs backward computation.
I1226 13:39:23.409540 96918 net.cpp:305] fc6 needs backward computation.
I1226 13:39:23.409585 96918 net.cpp:305] pool5 needs backward computation.
I1226 13:39:23.409616 96918 net.cpp:305] relu5 needs backward computation.
I1226 13:39:23.409646 96918 net.cpp:305] conv5 needs backward computation.
I1226 13:39:23.409682 96918 net.cpp:305] relu4 needs backward computation.
I1226 13:39:23.409709 96918 net.cpp:305] conv4 needs backward computation.
I1226 13:39:23.409804 96918 net.cpp:305] relu3 needs backward computation.
I1226 13:39:23.409845 96918 net.cpp:305] conv3 needs backward computation.
I1226 13:39:23.409878 96918 net.cpp:305] pool2 needs backward computation.
I1226 13:39:23.409906 96918 net.cpp:305] norm2 needs backward computation.
I1226 13:39:23.409934 96918 net.cpp:305] relu2 needs backward computation.
I1226 13:39:23.409960 96918 net.cpp:305] conv2 needs backward computation.
I1226 13:39:23.409988 96918 net.cpp:305] pool1 needs backward computation.
I1226 13:39:23.410017 96918 net.cpp:305] norm1 needs backward computation.
I1226 13:39:23.410044 96918 net.cpp:305] relu1 needs backward computation.
I1226 13:39:23.410071 96918 net.cpp:305] conv1 needs backward computation.
I1226 13:39:23.410101 96918 net.cpp:307] label_data_1_split does not need backward computation.
I1226 13:39:23.410131 96918 net.cpp:307] data does not need backward computation.
I1226 13:39:23.410154 96918 net.cpp:349] This network produces output accuracy
I1226 13:39:23.410187 96918 net.cpp:349] This network produces output loss
I1226 13:39:23.410281 96918 net.cpp:363] Network initialization done.
I1226 13:39:23.410804 96918 solver.cpp:107] Solver scaffolding done.
I1226 13:39:23.411000 96918 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 13:39:23.636129 96543 net.cpp:228] Setting up fc8
I1226 13:39:23.636273 96543 net.cpp:235] Top shape: 256 1000 (256000)
I1226 13:39:23.636319 96543 net.cpp:243] Memory required for data: 2129729536
I1226 13:39:23.636390 96543 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 13:39:23.636473 96543 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 13:39:23.636518 96543 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 13:39:23.636570 96543 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 13:39:23.636662 96543 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 13:39:23.636768 96543 net.cpp:228] Setting up fc8_fc8_0_split
I1226 13:39:23.636875 96543 net.cpp:235] Top shape: 256 1000 (256000)
I1226 13:39:23.636919 96543 net.cpp:235] Top shape: 256 1000 (256000)
I1226 13:39:23.636943 96543 net.cpp:243] Memory required for data: 2131777536
I1226 13:39:23.636979 96543 layer_factory.hpp:114] Creating layer accuracy
I1226 13:39:23.637042 96543 net.cpp:178] Creating Layer accuracy
I1226 13:39:23.637073 96543 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 13:39:23.637107 96543 net.cpp:612] accuracy <- label_data_1_split_0
I1226 13:39:23.637151 96543 net.cpp:586] accuracy -> accuracy
I1226 13:39:23.637203 96543 net.cpp:228] Setting up accuracy
I1226 13:39:23.637241 96543 net.cpp:235] Top shape: (1)
I1226 13:39:23.637267 96543 net.cpp:243] Memory required for data: 2131777540
I1226 13:39:23.637297 96543 layer_factory.hpp:114] Creating layer loss
I1226 13:39:23.637455 96543 net.cpp:178] Creating Layer loss
I1226 13:39:23.637511 96543 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 13:39:23.637547 96543 net.cpp:612] loss <- label_data_1_split_1
I1226 13:39:23.637589 96543 net.cpp:586] loss -> loss
I1226 13:39:23.637667 96543 layer_factory.hpp:114] Creating layer loss
I1226 13:39:23.668259 96543 net.cpp:228] Setting up loss
I1226 13:39:23.668371 96543 net.cpp:235] Top shape: (1)
I1226 13:39:23.668406 96543 net.cpp:238]     with loss weight 1
I1226 13:39:23.668543 96543 net.cpp:243] Memory required for data: 2131777544
I1226 13:39:23.668598 96543 net.cpp:305] loss needs backward computation.
I1226 13:39:23.668642 96543 net.cpp:307] accuracy does not need backward computation.
I1226 13:39:23.668683 96543 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 13:39:23.668718 96543 net.cpp:305] fc8 needs backward computation.
I1226 13:39:23.668751 96543 net.cpp:305] drop7 needs backward computation.
I1226 13:39:23.668782 96543 net.cpp:305] relu7 needs backward computation.
I1226 13:39:23.668846 96543 net.cpp:305] fc7 needs backward computation.
I1226 13:39:23.668880 96543 net.cpp:305] drop6 needs backward computation.
I1226 13:39:23.668912 96543 net.cpp:305] relu6 needs backward computation.
I1226 13:39:23.668944 96543 net.cpp:305] fc6 needs backward computation.
I1226 13:39:23.668977 96543 net.cpp:305] pool5 needs backward computation.
I1226 13:39:23.669013 96543 net.cpp:305] relu5 needs backward computation.
I1226 13:39:23.669044 96543 net.cpp:305] conv5 needs backward computation.
I1226 13:39:23.669077 96543 net.cpp:305] relu4 needs backward computation.
I1226 13:39:23.669109 96543 net.cpp:305] conv4 needs backward computation.
I1226 13:39:23.669143 96543 net.cpp:305] relu3 needs backward computation.
I1226 13:39:23.669175 96543 net.cpp:305] conv3 needs backward computation.
I1226 13:39:23.669209 96543 net.cpp:305] pool2 needs backward computation.
I1226 13:39:23.669242 96543 net.cpp:305] norm2 needs backward computation.
I1226 13:39:23.669275 96543 net.cpp:305] relu2 needs backward computation.
I1226 13:39:23.669304 96543 net.cpp:305] conv2 needs backward computation.
I1226 13:39:23.669337 96543 net.cpp:305] pool1 needs backward computation.
I1226 13:39:23.669370 96543 net.cpp:305] norm1 needs backward computation.
I1226 13:39:23.669401 96543 net.cpp:305] relu1 needs backward computation.
I1226 13:39:23.669432 96543 net.cpp:305] conv1 needs backward computation.
I1226 13:39:23.669466 96543 net.cpp:307] label_data_1_split does not need backward computation.
I1226 13:39:23.669500 96543 net.cpp:307] data does not need backward computation.
I1226 13:39:23.669530 96543 net.cpp:349] This network produces output accuracy
I1226 13:39:23.669567 96543 net.cpp:349] This network produces output loss
I1226 13:39:23.669682 96543 net.cpp:363] Network initialization done.
I1226 13:39:23.670178 96543 solver.cpp:107] Solver scaffolding done.
I1226 13:39:23.670410 96543 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 13:39:28.002005 96918 caffe.cpp:376] Configuring multinode setup
I1226 13:39:28.003837 96918 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 13:39:28.270160 96543 caffe.cpp:376] Configuring multinode setup
I1226 13:39:28.271975 96543 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 13:39:28.272194 96543 SynchronousNode.cpp:674] [0] [proc 0] solving
I1226 13:39:28.272294 96543 solver.cpp:354] Solving AlexNet
I1226 13:39:28.272343 96543 solver.cpp:355] Learning Rate Policy: step
I1226 13:39:28.270973 96918 SynchronousNode.cpp:674] [3] [proc 3] solving
I1226 13:39:28.271113 96918 solver.cpp:354] Solving AlexNet
I1226 13:39:28.271162 96918 solver.cpp:355] Learning Rate Policy: step
I1226 13:39:28.278606 93001 SynchronousNode.cpp:674] [1] [proc 1] solving
I1226 13:39:28.274077 92321 SynchronousNode.cpp:674] [2] [proc 2] solving
I1226 13:39:28.279824 91865 SynchronousNode.cpp:674] [4] [proc 4] solving
I1226 13:39:28.266131 122432 SynchronousNode.cpp:674] [5] [proc 5] solving
I1226 13:39:28.281708 89419 SynchronousNode.cpp:674] [6] [proc 6] solving
I1226 13:39:28.276849 91021 SynchronousNode.cpp:674] [7] [proc 7] solving
I1226 13:39:28.278882 93001 solver.cpp:354] Solving AlexNet
I1226 13:39:28.278931 93001 solver.cpp:355] Learning Rate Policy: step
I1226 13:39:28.280092 91865 solver.cpp:354] Solving AlexNet
I1226 13:39:28.280134 91865 solver.cpp:355] Learning Rate Policy: step
I1226 13:39:28.266474 122432 solver.cpp:354] Solving AlexNet
I1226 13:39:28.281985 89419 solver.cpp:354] Solving AlexNet
I1226 13:39:28.277133 91021 solver.cpp:354] Solving AlexNet
I1226 13:39:28.282035 89419 solver.cpp:355] Learning Rate Policy: step
I1226 13:39:28.274394 92321 solver.cpp:354] Solving AlexNet
I1226 13:39:28.274644 92321 solver.cpp:355] Learning Rate Policy: step
I1226 13:39:28.266538 122432 solver.cpp:355] Learning Rate Policy: step
I1226 13:39:28.277186 91021 solver.cpp:355] Learning Rate Policy: step
I1226 13:39:28.283088 96990 SynchronousNode.cpp:292] [3] Comm thread started 1 1
I1226 13:39:28.296986 96617 SynchronousNode.cpp:292] [0] Comm thread started 1 1
I1226 13:39:28.307096 92390 SynchronousNode.cpp:292] [2] Comm thread started 1 1
I1226 13:39:28.314466 91938 SynchronousNode.cpp:292] [4] Comm thread started 1 1
I1226 13:39:28.317780 89490 SynchronousNode.cpp:292] [6] Comm thread started 1 1
I1226 13:39:28.315409 91095 SynchronousNode.cpp:292] [7] Comm thread started 1 1
I1226 13:39:28.305780 122504 SynchronousNode.cpp:292] [5] Comm thread started 1 1
I1226 13:39:28.319504 93069 SynchronousNode.cpp:292] [1] Comm thread started 1 1
I1226 13:39:29.207085 92321 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 13:39:29.207201 92321 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 13:39:29.223229 89419 MultiSolver.cpp:92] [6] PROFILING END[Forward]
I1226 13:39:29.223351 89419 MultiSolver.cpp:94] [6] PROFILING BEGIN[Backward]
I1226 13:39:29.227658 93001 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 13:39:29.227752 93001 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 13:39:29.232134 91865 MultiSolver.cpp:92] [4] PROFILING END[Forward]
I1226 13:39:29.232223 91865 MultiSolver.cpp:94] [4] PROFILING BEGIN[Backward]
I1226 13:39:29.230487 91021 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 13:39:29.230564 91021 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 13:39:29.599089 122432 MultiSolver.cpp:92] [5] PROFILING END[Forward]
I1226 13:39:29.599253 122432 MultiSolver.cpp:94] [5] PROFILING BEGIN[Backward]
I1226 13:39:34.396762 96918 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 13:39:34.396860 96918 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 13:39:34.411733 96543 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 13:39:34.412140 96543 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 13:39:34.431517 89419 MultiSolver.cpp:108] [6] PROFILING END[Backward]
I1226 13:39:34.431599 89419 solver.cpp:291] [6] Iteration 1, loss = 3.28291
I1226 13:39:34.432646 89419 solver.cpp:317]     Train net output #0: accuracy = 0.3125
I1226 13:39:34.432713 89419 solver.cpp:317]     Train net output #1: loss = 3.28291 (* 1 = 3.28291 loss)
I1226 13:39:34.467413 91021 MultiSolver.cpp:108] [7] PROFILING END[Backward]
I1226 13:39:34.467480 91021 solver.cpp:291] [7] Iteration 1, loss = 3.36372
I1226 13:39:34.467535 91021 solver.cpp:317]     Train net output #0: accuracy = 0.269531
I1226 13:39:34.467584 91021 solver.cpp:317]     Train net output #1: loss = 3.36372 (* 1 = 3.36372 loss)
I1226 13:39:34.513331 91865 MultiSolver.cpp:108] [4] PROFILING END[Backward]
I1226 13:39:34.513411 91865 solver.cpp:291] [4] Iteration 1, loss = 3.44536
I1226 13:39:34.513474 91865 solver.cpp:317]     Train net output #0: accuracy = 0.265625
I1226 13:39:34.513567 91865 solver.cpp:317]     Train net output #1: loss = 3.44536 (* 1 = 3.44536 loss)
I1226 13:39:34.580175 92321 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 13:39:34.580261 92321 solver.cpp:291] [2] Iteration 1, loss = 3.28603
I1226 13:39:34.580324 92321 solver.cpp:317]     Train net output #0: accuracy = 0.292969
I1226 13:39:34.580389 92321 solver.cpp:317]     Train net output #1: loss = 3.28603 (* 1 = 3.28603 loss)
I1226 13:39:34.645529 93001 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 13:39:34.645607 93001 solver.cpp:291] [1] Iteration 1, loss = 3.29564
I1226 13:39:34.645673 93001 solver.cpp:317]     Train net output #0: accuracy = 0.28125
I1226 13:39:34.645738 93001 solver.cpp:317]     Train net output #1: loss = 3.29564 (* 1 = 3.29564 loss)
I1226 13:39:34.790611 122432 MultiSolver.cpp:108] [5] PROFILING END[Backward]
I1226 13:39:34.790748 122432 solver.cpp:291] [5] Iteration 1, loss = 3.1485
I1226 13:39:34.790837 122432 solver.cpp:317]     Train net output #0: accuracy = 0.316406
I1226 13:39:34.790947 122432 solver.cpp:317]     Train net output #1: loss = 3.1485 (* 1 = 3.1485 loss)
I1226 13:39:34.846742 89419 MultiSolver.cpp:92] [6] PROFILING END[Forward]
I1226 13:39:34.846834 89419 MultiSolver.cpp:94] [6] PROFILING BEGIN[Backward]
I1226 13:39:34.886028 91021 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 13:39:34.886102 91021 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 13:39:34.919440 91865 MultiSolver.cpp:92] [4] PROFILING END[Forward]
I1226 13:39:34.919608 91865 MultiSolver.cpp:94] [4] PROFILING BEGIN[Backward]
I1226 13:39:34.995075 92321 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 13:39:34.995162 92321 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 13:39:35.088645 93001 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 13:39:35.088742 93001 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 13:39:35.416745 122432 MultiSolver.cpp:92] [5] PROFILING END[Forward]
I1226 13:39:35.416853 122432 MultiSolver.cpp:94] [5] PROFILING BEGIN[Backward]
I1226 13:39:40.441972 91021 MultiSolver.cpp:108] [7] PROFILING END[Backward]
I1226 13:39:40.442045 91021 solver.cpp:291] [7] Iteration 2, loss = 3.05514
I1226 13:39:40.455289 91021 solver.cpp:317]     Train net output #0: accuracy = 0.351562
I1226 13:39:40.455353 91021 solver.cpp:317]     Train net output #1: loss = 3.05514 (* 1 = 3.05514 loss)
I1226 13:39:40.476555 89419 MultiSolver.cpp:108] [6] PROFILING END[Backward]
I1226 13:39:40.476665 89419 solver.cpp:291] [6] Iteration 2, loss = 3.20994
I1226 13:39:40.476727 89419 solver.cpp:317]     Train net output #0: accuracy = 0.308594
I1226 13:39:40.476794 89419 solver.cpp:317]     Train net output #1: loss = 3.20994 (* 1 = 3.20994 loss)
I1226 13:39:40.544500 92321 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 13:39:40.544582 92321 solver.cpp:291] [2] Iteration 2, loss = 3.13598
I1226 13:39:40.544647 92321 solver.cpp:317]     Train net output #0: accuracy = 0.332031
I1226 13:39:40.544713 92321 solver.cpp:317]     Train net output #1: loss = 3.13598 (* 1 = 3.13598 loss)
I1226 13:39:40.557284 91865 MultiSolver.cpp:108] [4] PROFILING END[Backward]
I1226 13:39:40.557394 91865 solver.cpp:291] [4] Iteration 2, loss = 3.19175
I1226 13:39:40.557459 91865 solver.cpp:317]     Train net output #0: accuracy = 0.3125
I1226 13:39:40.557523 91865 solver.cpp:317]     Train net output #1: loss = 3.19175 (* 1 = 3.19175 loss)
I1226 13:39:40.583605 93001 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 13:39:40.583683 93001 solver.cpp:291] [1] Iteration 2, loss = 3.20129
I1226 13:39:40.583748 93001 solver.cpp:317]     Train net output #0: accuracy = 0.296875
I1226 13:39:40.583812 93001 solver.cpp:317]     Train net output #1: loss = 3.20129 (* 1 = 3.20129 loss)
I1226 13:39:40.753509 122432 MultiSolver.cpp:108] [5] PROFILING END[Backward]
I1226 13:39:40.753612 122432 solver.cpp:291] [5] Iteration 2, loss = 3.06114
I1226 13:39:40.753722 122432 solver.cpp:317]     Train net output #0: accuracy = 0.332031
I1226 13:39:40.753809 122432 solver.cpp:317]     Train net output #1: loss = 3.06114 (* 1 = 3.06114 loss)
I1226 13:39:40.874418 91021 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 13:39:40.874876 91021 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 13:39:40.901396 89419 MultiSolver.cpp:92] [6] PROFILING END[Forward]
I1226 13:39:40.901494 89419 MultiSolver.cpp:94] [6] PROFILING BEGIN[Backward]
I1226 13:39:40.970875 92321 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 13:39:40.970993 92321 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 13:39:40.999267 93001 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 13:39:40.999362 93001 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 13:39:41.001973 91865 MultiSolver.cpp:92] [4] PROFILING END[Forward]
I1226 13:39:41.002066 91865 MultiSolver.cpp:94] [4] PROFILING BEGIN[Backward]
I1226 13:39:41.358239 122432 MultiSolver.cpp:92] [5] PROFILING END[Forward]
I1226 13:39:41.358819 122432 MultiSolver.cpp:94] [5] PROFILING BEGIN[Backward]
I1226 13:39:45.101344 91021 MultiSolver.cpp:108] [7] PROFILING END[Backward]
I1226 13:39:45.101414 91021 solver.cpp:291] [7] Iteration 3, loss = 3.14235
I1226 13:39:45.101469 91021 solver.cpp:317]     Train net output #0: accuracy = 0.351562
I1226 13:39:45.101518 91021 solver.cpp:317]     Train net output #1: loss = 3.14235 (* 1 = 3.14235 loss)
I1226 13:39:45.176241 89419 MultiSolver.cpp:108] [6] PROFILING END[Backward]
I1226 13:39:45.176322 89419 solver.cpp:291] [6] Iteration 3, loss = 3.32106
I1226 13:39:45.176385 89419 solver.cpp:317]     Train net output #0: accuracy = 0.273438
I1226 13:39:45.176451 89419 solver.cpp:317]     Train net output #1: loss = 3.32106 (* 1 = 3.32106 loss)
I1226 13:39:45.202303 92321 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 13:39:45.202385 92321 solver.cpp:291] [2] Iteration 3, loss = 3.19545
I1226 13:39:45.202450 92321 solver.cpp:317]     Train net output #0: accuracy = 0.355469
I1226 13:39:45.202535 92321 solver.cpp:317]     Train net output #1: loss = 3.19545 (* 1 = 3.19545 loss)
I1226 13:39:45.208560 93001 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 13:39:45.208640 93001 solver.cpp:291] [1] Iteration 3, loss = 3.49064
I1226 13:39:45.208704 93001 solver.cpp:317]     Train net output #0: accuracy = 0.296875
I1226 13:39:45.208768 93001 solver.cpp:317]     Train net output #1: loss = 3.49064 (* 1 = 3.49064 loss)
I1226 13:39:45.275243 91865 MultiSolver.cpp:108] [4] PROFILING END[Backward]
I1226 13:39:45.275344 91865 solver.cpp:291] [4] Iteration 3, loss = 2.87643
I1226 13:39:45.275408 91865 solver.cpp:317]     Train net output #0: accuracy = 0.378906
I1226 13:39:45.275472 91865 solver.cpp:317]     Train net output #1: loss = 2.87643 (* 1 = 2.87643 loss)
I1226 13:39:45.492203 91021 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 13:39:45.492303 91021 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 13:39:45.568377 122432 MultiSolver.cpp:108] [5] PROFILING END[Backward]
I1226 13:39:45.568480 122432 solver.cpp:291] [5] Iteration 3, loss = 3.15696
I1226 13:39:45.568570 122432 solver.cpp:317]     Train net output #0: accuracy = 0.34375
I1226 13:39:45.568688 122432 solver.cpp:317]     Train net output #1: loss = 3.15696 (* 1 = 3.15696 loss)
I1226 13:39:45.598132 92321 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 13:39:45.598258 92321 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 13:39:45.623838 89419 MultiSolver.cpp:92] [6] PROFILING END[Forward]
I1226 13:39:45.623934 89419 MultiSolver.cpp:94] [6] PROFILING BEGIN[Backward]
I1226 13:39:45.681505 93001 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 13:39:45.681592 93001 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 13:39:45.717569 91865 MultiSolver.cpp:92] [4] PROFILING END[Forward]
I1226 13:39:45.717664 91865 MultiSolver.cpp:94] [4] PROFILING BEGIN[Backward]
I1226 13:39:46.190943 122432 MultiSolver.cpp:92] [5] PROFILING END[Forward]
I1226 13:39:46.191058 122432 MultiSolver.cpp:94] [5] PROFILING BEGIN[Backward]
I1226 13:39:49.258877 96918 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 13:39:49.259759 96918 solver.cpp:291] [3] Iteration 1, loss = 3.09345
I1226 13:39:49.259835 96918 solver.cpp:317]     Train net output #0: accuracy = 0.308594
I1226 13:39:49.259896 96918 solver.cpp:317]     Train net output #1: loss = 3.09345 (* 1 = 3.09345 loss)
I1226 13:39:49.265944 96543 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 13:39:49.266885 96543 solver.cpp:291] [0] Iteration 1, loss = 3.46992
I1226 13:39:49.266980 96543 solver.cpp:317]     Train net output #0: accuracy = 0.265625
I1226 13:39:49.267067 96543 solver.cpp:317]     Train net output #1: loss = 3.46992 (* 1 = 3.46992 loss)
I1226 13:39:49.804064 91865 MultiSolver.cpp:108] [4] PROFILING END[Backward]
I1226 13:39:49.804147 91865 solver.cpp:291] [4] Iteration 4, loss = 3.28445
I1226 13:39:49.804211 91865 solver.cpp:317]     Train net output #0: accuracy = 0.300781
I1226 13:39:49.804275 91865 solver.cpp:317]     Train net output #1: loss = 3.28445 (* 1 = 3.28445 loss)
I1226 13:39:49.803517 91021 MultiSolver.cpp:108] [7] PROFILING END[Backward]
I1226 13:39:49.803589 91021 solver.cpp:291] [7] Iteration 4, loss = 3.3842
I1226 13:39:49.803643 91021 solver.cpp:317]     Train net output #0: accuracy = 0.269531
I1226 13:39:49.803694 91021 solver.cpp:317]     Train net output #1: loss = 3.3842 (* 1 = 3.3842 loss)
I1226 13:39:49.892012 93001 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 13:39:49.892096 93001 solver.cpp:291] [1] Iteration 4, loss = 3.36682
I1226 13:39:49.892158 93001 solver.cpp:317]     Train net output #0: accuracy = 0.292969
I1226 13:39:49.892223 93001 solver.cpp:317]     Train net output #1: loss = 3.36682 (* 1 = 3.36682 loss)
I1226 13:39:49.911053 92321 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 13:39:49.911135 92321 solver.cpp:291] [2] Iteration 4, loss = 3.33081
I1226 13:39:49.911201 92321 solver.cpp:317]     Train net output #0: accuracy = 0.285156
I1226 13:39:49.911264 92321 solver.cpp:317]     Train net output #1: loss = 3.33081 (* 1 = 3.33081 loss)
I1226 13:39:49.929147 89419 MultiSolver.cpp:108] [6] PROFILING END[Backward]
I1226 13:39:49.929239 89419 solver.cpp:291] [6] Iteration 4, loss = 3.31244
I1226 13:39:49.929303 89419 solver.cpp:317]     Train net output #0: accuracy = 0.316406
I1226 13:39:49.929407 89419 solver.cpp:317]     Train net output #1: loss = 3.31244 (* 1 = 3.31244 loss)
I1226 13:39:50.207234 91021 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 13:39:50.207342 91021 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 13:39:50.234972 91865 MultiSolver.cpp:92] [4] PROFILING END[Forward]
I1226 13:39:50.235061 91865 MultiSolver.cpp:94] [4] PROFILING BEGIN[Backward]
I1226 13:39:50.329157 92321 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 13:39:50.329249 92321 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 13:39:50.344704 89419 MultiSolver.cpp:92] [6] PROFILING END[Forward]
I1226 13:39:50.344789 89419 MultiSolver.cpp:94] [6] PROFILING BEGIN[Backward]
I1226 13:39:50.347795 93001 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 13:39:50.347880 93001 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 13:39:50.889202 122432 MultiSolver.cpp:108] [5] PROFILING END[Backward]
I1226 13:39:50.889307 122432 solver.cpp:291] [5] Iteration 4, loss = 3.15375
I1226 13:39:50.889395 122432 solver.cpp:317]     Train net output #0: accuracy = 0.367188
I1226 13:39:50.889534 122432 solver.cpp:317]     Train net output #1: loss = 3.15375 (* 1 = 3.15375 loss)
I1226 13:39:51.471396 122432 MultiSolver.cpp:92] [5] PROFILING END[Forward]
I1226 13:39:51.471513 122432 MultiSolver.cpp:94] [5] PROFILING BEGIN[Backward]
I1226 13:39:54.294069 96918 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 13:39:54.294167 96918 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 13:39:54.360772 96543 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 13:39:54.360893 96543 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 13:39:54.504549 91021 MultiSolver.cpp:108] [7] PROFILING END[Backward]
I1226 13:39:54.504613 91021 solver.cpp:291] [7] Iteration 5, loss = 2.95012
I1226 13:39:54.504668 91021 solver.cpp:317]     Train net output #0: accuracy = 0.347656
I1226 13:39:54.504719 91021 solver.cpp:317]     Train net output #1: loss = 2.95012 (* 1 = 2.95012 loss)
I1226 13:39:54.512425 91865 MultiSolver.cpp:108] [4] PROFILING END[Backward]
I1226 13:39:54.512501 91865 solver.cpp:291] [4] Iteration 5, loss = 3.04665
I1226 13:39:54.512567 91865 solver.cpp:317]     Train net output #0: accuracy = 0.347656
I1226 13:39:54.512634 91865 solver.cpp:317]     Train net output #1: loss = 3.04665 (* 1 = 3.04665 loss)
I1226 13:39:54.609053 89419 MultiSolver.cpp:108] [6] PROFILING END[Backward]
I1226 13:39:54.609135 89419 solver.cpp:291] [6] Iteration 5, loss = 3.29208
I1226 13:39:54.609197 89419 solver.cpp:317]     Train net output #0: accuracy = 0.332031
I1226 13:39:54.609262 89419 solver.cpp:317]     Train net output #1: loss = 3.29208 (* 1 = 3.29208 loss)
I1226 13:39:54.602685 92321 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 13:39:54.602766 92321 solver.cpp:291] [2] Iteration 5, loss = 3.18496
I1226 13:39:54.602830 92321 solver.cpp:317]     Train net output #0: accuracy = 0.351562
I1226 13:39:54.602897 92321 solver.cpp:317]     Train net output #1: loss = 3.18496 (* 1 = 3.18496 loss)
I1226 13:39:54.628679 93001 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 13:39:54.628756 93001 solver.cpp:291] [1] Iteration 5, loss = 3.18066
I1226 13:39:54.628820 93001 solver.cpp:317]     Train net output #0: accuracy = 0.324219
I1226 13:39:54.628880 93001 solver.cpp:317]     Train net output #1: loss = 3.18066 (* 1 = 3.18066 loss)
I1226 13:39:54.924186 91021 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 13:39:54.924285 91021 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 13:39:54.930865 91865 MultiSolver.cpp:92] [4] PROFILING END[Forward]
I1226 13:39:54.930980 91865 MultiSolver.cpp:94] [4] PROFILING BEGIN[Backward]
I1226 13:39:55.017408 92321 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 13:39:55.017525 92321 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 13:39:55.045913 89419 MultiSolver.cpp:92] [6] PROFILING END[Forward]
I1226 13:39:55.046013 89419 MultiSolver.cpp:94] [6] PROFILING BEGIN[Backward]
I1226 13:39:55.048825 93001 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 13:39:55.048919 93001 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 13:39:57.932267 122432 MultiSolver.cpp:108] [5] PROFILING END[Backward]
I1226 13:39:57.932397 122432 solver.cpp:291] [5] Iteration 5, loss = 3.33208
I1226 13:39:57.932724 122432 solver.cpp:317]     Train net output #0: accuracy = 0.3125
I1226 13:39:57.932819 122432 solver.cpp:317]     Train net output #1: loss = 3.33208 (* 1 = 3.33208 loss)
I1226 13:39:58.604192 122432 MultiSolver.cpp:92] [5] PROFILING END[Forward]
I1226 13:39:58.605312 122432 MultiSolver.cpp:94] [5] PROFILING BEGIN[Backward]
I1226 13:40:00.124739 91021 MultiSolver.cpp:108] [7] PROFILING END[Backward]
I1226 13:40:00.125649 91021 solver.cpp:291] [7] Iteration 6, loss = 3.12718
I1226 13:40:00.125713 91021 solver.cpp:317]     Train net output #0: accuracy = 0.324219
I1226 13:40:00.125764 91021 solver.cpp:317]     Train net output #1: loss = 3.12718 (* 1 = 3.12718 loss)
I1226 13:40:00.138705 89419 MultiSolver.cpp:108] [6] PROFILING END[Backward]
I1226 13:40:00.140301 89419 solver.cpp:291] [6] Iteration 6, loss = 3.31406
I1226 13:40:00.140394 89419 solver.cpp:317]     Train net output #0: accuracy = 0.320312
I1226 13:40:00.140465 89419 solver.cpp:317]     Train net output #1: loss = 3.31406 (* 1 = 3.31406 loss)
I1226 13:40:00.137843 91865 MultiSolver.cpp:108] [4] PROFILING END[Backward]
I1226 13:40:00.138916 91865 solver.cpp:291] [4] Iteration 6, loss = 3.32882
I1226 13:40:00.139003 91865 solver.cpp:317]     Train net output #0: accuracy = 0.328125
I1226 13:40:00.139081 91865 solver.cpp:317]     Train net output #1: loss = 3.32882 (* 1 = 3.32882 loss)
I1226 13:40:00.137020 92321 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 13:40:00.137944 92321 solver.cpp:291] [2] Iteration 6, loss = 3.36348
I1226 13:40:00.138033 92321 solver.cpp:317]     Train net output #0: accuracy = 0.28125
I1226 13:40:00.138110 92321 solver.cpp:317]     Train net output #1: loss = 3.36348 (* 1 = 3.36348 loss)
I1226 13:40:00.193109 93001 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 13:40:00.194059 93001 solver.cpp:291] [1] Iteration 6, loss = 3.15091
I1226 13:40:00.194149 93001 solver.cpp:317]     Train net output #0: accuracy = 0.320312
I1226 13:40:00.194222 93001 solver.cpp:317]     Train net output #1: loss = 3.15091 (* 1 = 3.15091 loss)
I1226 13:40:00.546794 91021 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 13:40:00.546895 91021 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 13:40:00.547091 92321 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 13:40:00.547181 92321 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 13:40:00.559533 91865 MultiSolver.cpp:92] [4] PROFILING END[Forward]
I1226 13:40:00.560380 91865 MultiSolver.cpp:94] [4] PROFILING BEGIN[Backward]
I1226 13:40:00.596917 89419 MultiSolver.cpp:92] [6] PROFILING END[Forward]
I1226 13:40:00.597000 89419 MultiSolver.cpp:94] [6] PROFILING BEGIN[Backward]
I1226 13:40:00.617609 93001 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 13:40:00.617707 93001 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 13:40:03.861749 122432 MultiSolver.cpp:108] [5] PROFILING END[Backward]
I1226 13:40:03.861862 122432 solver.cpp:291] [5] Iteration 6, loss = 3.28335
I1226 13:40:03.861958 122432 solver.cpp:317]     Train net output #0: accuracy = 0.335938
I1226 13:40:03.862061 122432 solver.cpp:317]     Train net output #1: loss = 3.28335 (* 1 = 3.28335 loss)
I1226 13:40:04.246820 89419 MultiSolver.cpp:108] [6] PROFILING END[Backward]
I1226 13:40:04.246904 89419 solver.cpp:291] [6] Iteration 7, loss = 3.13809
I1226 13:40:04.246968 89419 solver.cpp:317]     Train net output #0: accuracy = 0.332031
I1226 13:40:04.247032 89419 solver.cpp:317]     Train net output #1: loss = 3.13809 (* 1 = 3.13809 loss)
I1226 13:40:04.256065 91865 MultiSolver.cpp:108] [4] PROFILING END[Backward]
I1226 13:40:04.256150 91865 solver.cpp:291] [4] Iteration 7, loss = 3.17341
I1226 13:40:04.256212 91865 solver.cpp:317]     Train net output #0: accuracy = 0.316406
I1226 13:40:04.256278 91865 solver.cpp:317]     Train net output #1: loss = 3.17341 (* 1 = 3.17341 loss)
I1226 13:40:04.262312 91021 MultiSolver.cpp:108] [7] PROFILING END[Backward]
I1226 13:40:04.262382 91021 solver.cpp:291] [7] Iteration 7, loss = 3.47281
I1226 13:40:04.262437 91021 solver.cpp:317]     Train net output #0: accuracy = 0.296875
I1226 13:40:04.262487 91021 solver.cpp:317]     Train net output #1: loss = 3.47281 (* 1 = 3.47281 loss)
I1226 13:40:04.286078 92321 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 13:40:04.286164 92321 solver.cpp:291] [2] Iteration 7, loss = 3.12263
I1226 13:40:04.286228 92321 solver.cpp:317]     Train net output #0: accuracy = 0.328125
I1226 13:40:04.286294 92321 solver.cpp:317]     Train net output #1: loss = 3.12263 (* 1 = 3.12263 loss)
I1226 13:40:04.322584 93001 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 13:40:04.322662 93001 solver.cpp:291] [1] Iteration 7, loss = 3.14785
I1226 13:40:04.322727 93001 solver.cpp:317]     Train net output #0: accuracy = 0.347656
I1226 13:40:04.322793 93001 solver.cpp:317]     Train net output #1: loss = 3.14785 (* 1 = 3.14785 loss)
I1226 13:40:04.458890 122432 MultiSolver.cpp:92] [5] PROFILING END[Forward]
I1226 13:40:04.459072 122432 MultiSolver.cpp:94] [5] PROFILING BEGIN[Backward]
I1226 13:40:04.678145 89419 MultiSolver.cpp:92] [6] PROFILING END[Forward]
I1226 13:40:04.678266 89419 MultiSolver.cpp:94] [6] PROFILING BEGIN[Backward]
I1226 13:40:04.690825 91865 MultiSolver.cpp:92] [4] PROFILING END[Forward]
I1226 13:40:04.690909 91865 MultiSolver.cpp:94] [4] PROFILING BEGIN[Backward]
I1226 13:40:04.702033 91021 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 13:40:04.702111 91021 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 13:40:04.715744 92321 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 13:40:04.715837 92321 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 13:40:04.744850 93001 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 13:40:04.744971 93001 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 13:40:08.507200 96543 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 13:40:08.507288 96543 solver.cpp:291] [0] Iteration 2, loss = 3.1397
I1226 13:40:08.507360 96543 solver.cpp:317]     Train net output #0: accuracy = 0.304688
I1226 13:40:08.506038 96918 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 13:40:08.507459 96543 solver.cpp:317]     Train net output #1: loss = 3.1397 (* 1 = 3.1397 loss)
I1226 13:40:08.506122 96918 solver.cpp:291] [3] Iteration 2, loss = 3.1853
I1226 13:40:08.506188 96918 solver.cpp:317]     Train net output #0: accuracy = 0.324219
I1226 13:40:08.506294 96918 solver.cpp:317]     Train net output #1: loss = 3.1853 (* 1 = 3.1853 loss)
I1226 13:40:08.887814 89419 MultiSolver.cpp:108] [6] PROFILING END[Backward]
I1226 13:40:08.887897 89419 solver.cpp:291] [6] Iteration 8, loss = 3.17169
I1226 13:40:08.887969 89419 solver.cpp:317]     Train net output #0: accuracy = 0.351562
I1226 13:40:08.888036 89419 solver.cpp:317]     Train net output #1: loss = 3.17169 (* 1 = 3.17169 loss)
I1226 13:40:08.886898 91021 MultiSolver.cpp:108] [7] PROFILING END[Backward]
I1226 13:40:08.886970 91021 solver.cpp:291] [7] Iteration 8, loss = 2.99126
I1226 13:40:08.887024 91021 solver.cpp:317]     Train net output #0: accuracy = 0.363281
I1226 13:40:08.887075 91021 solver.cpp:317]     Train net output #1: loss = 2.99126 (* 1 = 2.99126 loss)
I1226 13:40:08.905239 92321 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 13:40:08.905326 92321 solver.cpp:291] [2] Iteration 8, loss = 3.20887
I1226 13:40:08.905390 92321 solver.cpp:317]     Train net output #0: accuracy = 0.3125
I1226 13:40:08.905452 92321 solver.cpp:317]     Train net output #1: loss = 3.20887 (* 1 = 3.20887 loss)
I1226 13:40:08.917726 91865 MultiSolver.cpp:108] [4] PROFILING END[Backward]
I1226 13:40:08.917819 91865 solver.cpp:291] [4] Iteration 8, loss = 3.29885
I1226 13:40:08.917891 91865 solver.cpp:317]     Train net output #0: accuracy = 0.304688
I1226 13:40:08.917994 91865 solver.cpp:317]     Train net output #1: loss = 3.29885 (* 1 = 3.29885 loss)
I1226 13:40:08.926187 93001 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 13:40:08.926265 93001 solver.cpp:291] [1] Iteration 8, loss = 3.11512
I1226 13:40:08.926331 93001 solver.cpp:317]     Train net output #0: accuracy = 0.320312
I1226 13:40:08.926411 93001 solver.cpp:317]     Train net output #1: loss = 3.11512 (* 1 = 3.11512 loss)
I1226 13:40:08.920275 122432 MultiSolver.cpp:108] [5] PROFILING END[Backward]
I1226 13:40:08.920398 122432 solver.cpp:291] [5] Iteration 7, loss = 3.25128
I1226 13:40:08.920486 122432 solver.cpp:317]     Train net output #0: accuracy = 0.296875
I1226 13:40:08.920584 122432 solver.cpp:317]     Train net output #1: loss = 3.25128 (* 1 = 3.25128 loss)
I1226 13:40:09.312554 89419 MultiSolver.cpp:92] [6] PROFILING END[Forward]
I1226 13:40:09.312702 89419 MultiSolver.cpp:94] [6] PROFILING BEGIN[Backward]
I1226 13:40:09.313225 91021 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 13:40:09.313335 91021 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 13:40:09.346045 92321 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 13:40:09.346129 92321 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 13:40:09.355665 91865 MultiSolver.cpp:92] [4] PROFILING END[Forward]
I1226 13:40:09.355777 91865 MultiSolver.cpp:94] [4] PROFILING BEGIN[Backward]
I1226 13:40:09.368847 93001 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 13:40:09.368945 93001 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 13:40:09.533310 122432 MultiSolver.cpp:92] [5] PROFILING END[Forward]
I1226 13:40:09.533428 122432 MultiSolver.cpp:94] [5] PROFILING BEGIN[Backward]
I1226 13:40:13.539664 96918 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 13:40:13.539800 96918 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 13:40:13.566889 89419 MultiSolver.cpp:108] [6] PROFILING END[Backward]
I1226 13:40:13.566969 89419 solver.cpp:291] [6] Iteration 9, loss = 3.58852
I1226 13:40:13.567034 89419 solver.cpp:317]     Train net output #0: accuracy = 0.25
I1226 13:40:13.567095 89419 solver.cpp:317]     Train net output #1: loss = 3.58852 (* 1 = 3.58852 loss)
I1226 13:40:13.578402 92321 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 13:40:13.578508 92321 solver.cpp:291] [2] Iteration 9, loss = 3.07075
I1226 13:40:13.578572 92321 solver.cpp:317]     Train net output #0: accuracy = 0.320312
I1226 13:40:13.578635 92321 solver.cpp:317]     Train net output #1: loss = 3.07075 (* 1 = 3.07075 loss)
I1226 13:40:13.590595 91021 MultiSolver.cpp:108] [7] PROFILING END[Backward]
I1226 13:40:13.590659 91021 solver.cpp:291] [7] Iteration 9, loss = 3.04969
I1226 13:40:13.590713 91021 solver.cpp:317]     Train net output #0: accuracy = 0.332031
I1226 13:40:13.590764 91021 solver.cpp:317]     Train net output #1: loss = 3.04969 (* 1 = 3.04969 loss)
I1226 13:40:13.619264 96543 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 13:40:13.619365 96543 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 13:40:13.675426 91865 MultiSolver.cpp:108] [4] PROFILING END[Backward]
I1226 13:40:13.675503 91865 solver.cpp:291] [4] Iteration 9, loss = 3.08746
I1226 13:40:13.675566 91865 solver.cpp:317]     Train net output #0: accuracy = 0.335938
I1226 13:40:13.675635 91865 solver.cpp:317]     Train net output #1: loss = 3.08746 (* 1 = 3.08746 loss)
I1226 13:40:13.683451 93001 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 13:40:13.683528 93001 solver.cpp:291] [1] Iteration 9, loss = 3.45623
I1226 13:40:13.683594 93001 solver.cpp:317]     Train net output #0: accuracy = 0.277344
I1226 13:40:13.683662 93001 solver.cpp:317]     Train net output #1: loss = 3.45623 (* 1 = 3.45623 loss)
I1226 13:40:13.866863 122432 MultiSolver.cpp:108] [5] PROFILING END[Backward]
I1226 13:40:13.866967 122432 solver.cpp:291] [5] Iteration 8, loss = 3.17087
I1226 13:40:13.867053 122432 solver.cpp:317]     Train net output #0: accuracy = 0.328125
I1226 13:40:13.867161 122432 solver.cpp:317]     Train net output #1: loss = 3.17087 (* 1 = 3.17087 loss)
I1226 13:40:13.979223 92321 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 13:40:13.979351 92321 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 13:40:13.988795 89419 MultiSolver.cpp:92] [6] PROFILING END[Forward]
I1226 13:40:13.988911 89419 MultiSolver.cpp:94] [6] PROFILING BEGIN[Backward]
I1226 13:40:14.015354 91021 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 13:40:14.015427 91021 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 13:40:14.081599 91865 MultiSolver.cpp:92] [4] PROFILING END[Forward]
I1226 13:40:14.081699 91865 MultiSolver.cpp:94] [4] PROFILING BEGIN[Backward]
I1226 13:40:14.100852 93001 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 13:40:14.101003 93001 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 13:40:14.493898 122432 MultiSolver.cpp:92] [5] PROFILING END[Forward]
I1226 13:40:14.494009 122432 MultiSolver.cpp:94] [5] PROFILING BEGIN[Backward]
I1226 13:40:19.355825 92321 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 13:40:19.355901 92321 solver.cpp:291] [2] Iteration 10, loss = 3.24214
I1226 13:40:19.355970 92321 solver.cpp:317]     Train net output #0: accuracy = 0.324219
I1226 13:40:19.356084 92321 solver.cpp:317]     Train net output #1: loss = 3.24214 (* 1 = 3.24214 loss)
I1226 13:40:19.380374 91021 MultiSolver.cpp:108] [7] PROFILING END[Backward]
I1226 13:40:19.380445 91021 solver.cpp:291] [7] Iteration 10, loss = 3.12252
I1226 13:40:19.380499 91021 solver.cpp:317]     Train net output #0: accuracy = 0.339844
I1226 13:40:19.380550 91021 solver.cpp:317]     Train net output #1: loss = 3.12252 (* 1 = 3.12252 loss)
I1226 13:40:19.388823 89419 MultiSolver.cpp:108] [6] PROFILING END[Backward]
I1226 13:40:19.388898 89419 solver.cpp:291] [6] Iteration 10, loss = 3.20733
I1226 13:40:19.388964 89419 solver.cpp:317]     Train net output #0: accuracy = 0.316406
I1226 13:40:19.389029 89419 solver.cpp:317]     Train net output #1: loss = 3.20733 (* 1 = 3.20733 loss)
I1226 13:40:19.446920 93001 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 13:40:19.447001 93001 solver.cpp:291] [1] Iteration 10, loss = 3.45922
I1226 13:40:19.447064 93001 solver.cpp:317]     Train net output #0: accuracy = 0.261719
I1226 13:40:19.447130 93001 solver.cpp:317]     Train net output #1: loss = 3.45922 (* 1 = 3.45922 loss)
I1226 13:40:19.457043 91865 MultiSolver.cpp:108] [4] PROFILING END[Backward]
I1226 13:40:19.457129 91865 solver.cpp:291] [4] Iteration 10, loss = 3.24264
I1226 13:40:19.457192 91865 solver.cpp:317]     Train net output #0: accuracy = 0.273438
I1226 13:40:19.457259 91865 solver.cpp:317]     Train net output #1: loss = 3.24264 (* 1 = 3.24264 loss)
I1226 13:40:19.668027 122432 MultiSolver.cpp:108] [5] PROFILING END[Backward]
I1226 13:40:19.668128 122432 solver.cpp:291] [5] Iteration 9, loss = 3.18745
I1226 13:40:19.668213 122432 solver.cpp:317]     Train net output #0: accuracy = 0.324219
I1226 13:40:19.668337 122432 solver.cpp:317]     Train net output #1: loss = 3.18745 (* 1 = 3.18745 loss)
I1226 13:40:19.768338 92321 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 13:40:19.768432 92321 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 13:40:19.787708 91021 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 13:40:19.787797 91021 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 13:40:19.829964 89419 MultiSolver.cpp:92] [6] PROFILING END[Forward]
I1226 13:40:19.830083 89419 MultiSolver.cpp:94] [6] PROFILING BEGIN[Backward]
I1226 13:40:19.865643 91865 MultiSolver.cpp:92] [4] PROFILING END[Forward]
I1226 13:40:19.865797 91865 MultiSolver.cpp:94] [4] PROFILING BEGIN[Backward]
I1226 13:40:19.886620 93001 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 13:40:19.886710 93001 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 13:40:20.276172 122432 MultiSolver.cpp:92] [5] PROFILING END[Forward]
I1226 13:40:20.276288 122432 MultiSolver.cpp:94] [5] PROFILING BEGIN[Backward]
I1226 13:40:23.965629 92321 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 13:40:23.965713 92321 solver.cpp:291] [2] Iteration 11, loss = 3.29568
I1226 13:40:23.965777 92321 solver.cpp:317]     Train net output #0: accuracy = 0.308594
I1226 13:40:23.965842 92321 solver.cpp:317]     Train net output #1: loss = 3.29568 (* 1 = 3.29568 loss)
I1226 13:40:24.083765 91021 MultiSolver.cpp:108] [7] PROFILING END[Backward]
I1226 13:40:24.083837 91021 solver.cpp:291] [7] Iteration 11, loss = 3.2313
I1226 13:40:24.083890 91021 solver.cpp:317]     Train net output #0: accuracy = 0.285156
I1226 13:40:24.083941 91021 solver.cpp:317]     Train net output #1: loss = 3.2313 (* 1 = 3.2313 loss)
I1226 13:40:24.097836 89419 MultiSolver.cpp:108] [6] PROFILING END[Backward]
I1226 13:40:24.097918 89419 solver.cpp:291] [6] Iteration 11, loss = 3.22259
I1226 13:40:24.097982 89419 solver.cpp:317]     Train net output #0: accuracy = 0.292969
I1226 13:40:24.098044 89419 solver.cpp:317]     Train net output #1: loss = 3.22259 (* 1 = 3.22259 loss)
I1226 13:40:24.106058 93001 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 13:40:24.106140 93001 solver.cpp:291] [1] Iteration 11, loss = 3.02858
I1226 13:40:24.106202 93001 solver.cpp:317]     Train net output #0: accuracy = 0.328125
I1226 13:40:24.106267 93001 solver.cpp:317]     Train net output #1: loss = 3.02858 (* 1 = 3.02858 loss)
I1226 13:40:24.120275 91865 MultiSolver.cpp:108] [4] PROFILING END[Backward]
I1226 13:40:24.120379 91865 solver.cpp:291] [4] Iteration 11, loss = 3.47572
I1226 13:40:24.120442 91865 solver.cpp:317]     Train net output #0: accuracy = 0.3125
I1226 13:40:24.120507 91865 solver.cpp:317]     Train net output #1: loss = 3.47572 (* 1 = 3.47572 loss)
I1226 13:40:24.398876 92321 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 13:40:24.398967 92321 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 13:40:24.505434 91021 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 13:40:24.505504 91021 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 13:40:24.533186 91865 MultiSolver.cpp:92] [4] PROFILING END[Forward]
I1226 13:40:24.533270 91865 MultiSolver.cpp:94] [4] PROFILING BEGIN[Backward]
I1226 13:40:24.540524 93001 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 13:40:24.540619 93001 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 13:40:24.562134 89419 MultiSolver.cpp:92] [6] PROFILING END[Forward]
I1226 13:40:24.562222 89419 MultiSolver.cpp:94] [6] PROFILING BEGIN[Backward]
I1226 13:40:24.805222 122432 MultiSolver.cpp:108] [5] PROFILING END[Backward]
I1226 13:40:24.805330 122432 solver.cpp:291] [5] Iteration 10, loss = 3.11621
I1226 13:40:24.805419 122432 solver.cpp:317]     Train net output #0: accuracy = 0.316406
I1226 13:40:24.805516 122432 solver.cpp:317]     Train net output #1: loss = 3.11621 (* 1 = 3.11621 loss)
I1226 13:40:25.414685 122432 MultiSolver.cpp:92] [5] PROFILING END[Forward]
I1226 13:40:25.414836 122432 MultiSolver.cpp:94] [5] PROFILING BEGIN[Backward]
User defined signal 2
