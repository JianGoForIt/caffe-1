Dec 26 14:11:03 2016 98818 3 10.1 NIOS_DEBUG: stdin_fd set to -1
Dec 26 14:11:03 2016 98818 3 10.1 NIOS_DEBUG: fds[0] has a value of -1
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 14:11:06.920450 92274 mpiutil.cpp:166] Process rank 1 from number of 9 processes running on knl-node019
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 14:11:06.920521 98828 mpiutil.cpp:166] Process rank 0 from number of 9 processes running on knl-node016
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 14:11:06.918493 121743 mpiutil.cpp:166] Process rank 5 from number of 9 processes running on knl-node035
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 14:11:06.908709 96952 mpiutil.cpp:166] Process rank 2 from number of 9 processes running on knl-node079
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 14:11:06.921016 95316 mpiutil.cpp:166] Process rank 6 from number of 9 processes running on knl-node084
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 14:11:06.917549 91405 mpiutil.cpp:166] Process rank 4 from number of 9 processes running on knl-node027
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 14:11:06.924082 92483 mpiutil.cpp:166] Process rank 8 from number of 9 processes running on knl-node060
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 14:11:06.926486 91711 mpiutil.cpp:166] Process rank 3 from number of 9 processes running on knl-node047
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 14:11:06.929322 90185 mpiutil.cpp:166] Process rank 7 from number of 9 processes running on knl-node053
I1226 14:11:06.934216 98828 caffe.cpp:316] Use CPU.
I1226 14:11:06.934440 92274 caffe.cpp:316] Use CPU.
I1226 14:11:06.933017 121743 caffe.cpp:316] Use CPU.
I1226 14:11:06.922761 96952 caffe.cpp:316] Use CPU.
I1226 14:11:06.935341 98828 solver.cpp:93] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_1.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 14:11:06.936254 98828 solver.cpp:128] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_1.prototxt
I1226 14:11:06.935559 92274 solver.cpp:93] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_2.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 14:11:06.936349 92274 solver.cpp:128] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_2.prototxt
I1226 14:11:06.942085 91711 caffe.cpp:316] Use CPU.
I1226 14:11:06.934270 121743 solver.cpp:93] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_6.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 14:11:06.924144 96952 solver.cpp:93] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_3.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 14:11:06.936269 95316 caffe.cpp:316] Use CPU.
I1226 14:11:06.935298 121743 solver.cpp:128] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_6.prototxt
I1226 14:11:06.925245 96952 solver.cpp:128] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_3.prototxt
I1226 14:11:06.943176 91711 solver.cpp:93] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_4.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 14:11:06.943989 91711 solver.cpp:128] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_4.prototxt
I1226 14:11:06.946207 90185 caffe.cpp:316] Use CPU.
I1226 14:11:06.937652 95316 solver.cpp:93] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_7.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 14:11:06.938812 95316 solver.cpp:128] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_7.prototxt
I1226 14:11:06.935154 91405 caffe.cpp:316] Use CPU.
I1226 14:11:06.947391 90185 solver.cpp:93] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_8.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 14:11:06.948267 90185 solver.cpp:128] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_8.prototxt
I1226 14:11:06.936424 91405 solver.cpp:93] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_5.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 14:11:06.937458 91405 solver.cpp:128] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_5.prototxt
I1226 14:11:06.944547 92483 caffe.cpp:316] Use CPU.
I1226 14:11:06.945885 92483 solver.cpp:93] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_dummy.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 14:11:06.946926 92483 solver.cpp:128] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_dummy.prototxt
I1226 14:11:06.966909 98828 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 14:11:06.966987 98828 cpu_info.cpp:455] Total number of sockets: 1
I1226 14:11:06.967018 98828 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 14:11:06.967044 98828 cpu_info.cpp:461] Total number of processors: 272
I1226 14:11:06.967087 98828 cpu_info.cpp:464] GPU is used: no
I1226 14:11:06.967113 98828 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 14:11:06.967139 98828 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 14:11:06.967290 92274 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 14:11:06.967409 92274 cpu_info.cpp:455] Total number of sockets: 1
I1226 14:11:06.967433 92274 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 14:11:06.967453 92274 cpu_info.cpp:461] Total number of processors: 272
I1226 14:11:06.967473 92274 cpu_info.cpp:464] GPU is used: no
I1226 14:11:06.967492 92274 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 14:11:06.967512 92274 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 14:11:06.974895 91711 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 14:11:06.974972 91711 cpu_info.cpp:455] Total number of sockets: 1
I1226 14:11:06.974994 91711 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 14:11:06.975014 91711 cpu_info.cpp:461] Total number of processors: 272
I1226 14:11:06.975033 91711 cpu_info.cpp:464] GPU is used: no
I1226 14:11:06.975054 91711 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 14:11:06.975073 91711 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 14:11:06.967586 121743 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 14:11:06.967696 121743 cpu_info.cpp:455] Total number of sockets: 1
I1226 14:11:06.967725 121743 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 14:11:06.967752 121743 cpu_info.cpp:461] Total number of processors: 272
I1226 14:11:06.967782 121743 cpu_info.cpp:464] GPU is used: no
I1226 14:11:06.967813 121743 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 14:11:06.967839 121743 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 14:11:06.959460 96952 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 14:11:06.959553 96952 cpu_info.cpp:455] Total number of sockets: 1
I1226 14:11:06.959580 96952 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 14:11:06.959602 96952 cpu_info.cpp:461] Total number of processors: 272
I1226 14:11:06.959625 96952 cpu_info.cpp:464] GPU is used: no
I1226 14:11:06.959647 96952 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 14:11:06.959691 96952 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 14:11:06.973696 95316 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 14:11:06.973788 95316 cpu_info.cpp:455] Total number of sockets: 1
I1226 14:11:06.973816 95316 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 14:11:06.973875 95316 cpu_info.cpp:461] Total number of processors: 272
I1226 14:11:06.973898 95316 cpu_info.cpp:464] GPU is used: no
I1226 14:11:06.973922 95316 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 14:11:06.973944 95316 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 14:11:06.973438 91405 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 14:11:06.973517 91405 cpu_info.cpp:455] Total number of sockets: 1
I1226 14:11:06.983230 92483 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 14:11:06.973541 91405 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 14:11:06.973563 91405 cpu_info.cpp:461] Total number of processors: 272
I1226 14:11:06.973585 91405 cpu_info.cpp:464] GPU is used: no
I1226 14:11:06.983316 92483 cpu_info.cpp:455] Total number of sockets: 1
I1226 14:11:06.983346 92483 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 14:11:06.983381 92483 cpu_info.cpp:461] Total number of processors: 272
I1226 14:11:06.992494 90185 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 14:11:06.992585 90185 cpu_info.cpp:455] Total number of sockets: 1
I1226 14:11:06.992610 90185 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 14:11:06.992632 90185 cpu_info.cpp:461] Total number of processors: 272
I1226 14:11:06.992653 90185 cpu_info.cpp:464] GPU is used: no
I1226 14:11:06.973608 91405 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 14:11:06.983412 92483 cpu_info.cpp:464] GPU is used: no
I1226 14:11:06.992674 90185 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 14:11:06.992698 90185 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 14:11:06.973628 91405 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 14:11:06.983443 92483 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 14:11:06.983470 92483 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 14:11:06.982333 96952 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 14:11:06.982694 96952 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 14:11:06.984828 96952 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_2"
    batch_size: 256
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 14:11:07.010337 91711 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 14:11:07.010782 91711 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 14:11:07.013478 91711 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_3"
    batch_size: 256
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 14:11:07.019410 92483 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 14:11:07.019719 92483 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 14:11:07.021353 92483 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "DummyData"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  dummy_data_param {
    data_filler {
      type: "gaussian"
      std: 1
    }
    data_filler {
      type: "constant"
      value: 0
    }
    shape {
      dim: 256
      dim: 3
      dim: 227
      dim: 227
    }
    shape {
      dim: 256
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 14:11:07.018589 95316 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 14:11:07.019008 95316 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 14:11:07.021097 95316 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_6"
    batch_size: 256
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 14:11:07.021613 92274 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 14:11:07.021987 92274 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 14:11:07.027786 91405 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 14:11:07.038367 90185 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 14:11:07.023747 92274 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_1"
    batch_size: 256
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 14:11:07.028086 91405 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 14:11:07.038699 90185 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 14:11:07.029652 91405 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_4"
    batch_size: 256
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 14:11:07.040280 90185 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_7"
    batch_size: 256
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 14:11:07.038570 98828 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 14:11:07.044575 91711 layer_factory.hpp:114] Creating layer data
I1226 14:11:07.038913 98828 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 14:11:07.043704 92483 layer_factory.hpp:114] Creating layer data
I1226 14:11:07.043882 92483 net.cpp:178] Creating Layer data
I1226 14:11:07.043927 92483 net.cpp:586] data -> data
I1226 14:11:07.044023 92483 net.cpp:586] data -> label
I1226 14:11:07.040719 98828 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_0"
    batch_size: 256
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 14:11:07.038404 96952 layer_factory.hpp:114] Creating layer data
I1226 14:11:07.048969 121743 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 14:11:07.049355 121743 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 14:11:07.051586 121743 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_5"
    batch_size: 256
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 14:11:07.066742 91711 net.cpp:178] Creating Layer data
I1226 14:11:07.066831 91711 net.cpp:586] data -> data
I1226 14:11:07.066910 91711 net.cpp:586] data -> label
I1226 14:11:07.080929 90185 layer_factory.hpp:114] Creating layer data
I1226 14:11:07.079037 92274 layer_factory.hpp:114] Creating layer data
I1226 14:11:07.067719 96952 net.cpp:178] Creating Layer data
I1226 14:11:07.067889 96952 net.cpp:586] data -> data
I1226 14:11:07.068130 96952 net.cpp:586] data -> label
I1226 14:11:07.087702 92483 net.cpp:228] Setting up data
I1226 14:11:07.090437 91713 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_3
I1226 14:11:07.087843 92483 net.cpp:235] Top shape: 256 3 227 227 (39574272)
I1226 14:11:07.087878 92483 net.cpp:235] Top shape: 256 1 1 1 (256)
I1226 14:11:07.087903 92483 net.cpp:243] Memory required for data: 158298112
I1226 14:11:07.087939 92483 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 14:11:07.088029 92483 net.cpp:178] Creating Layer label_data_1_split
I1226 14:11:07.088134 92483 net.cpp:612] label_data_1_split <- label
I1226 14:11:07.088170 92483 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 14:11:07.088215 92483 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 14:11:07.085908 91405 layer_factory.hpp:114] Creating layer data
I1226 14:11:07.098460 90185 net.cpp:178] Creating Layer data
I1226 14:11:07.098546 90185 net.cpp:586] data -> data
I1226 14:11:07.098668 90185 net.cpp:586] data -> label
I1226 14:11:07.090344 98828 layer_factory.hpp:114] Creating layer data
I1226 14:11:07.089233 121743 layer_factory.hpp:114] Creating layer data
I1226 14:11:07.097923 91711 data_layer.cpp:80] output data size: 256,3,227,227
I1226 14:11:07.080778 96954 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_2
I1226 14:11:07.099195 92483 net.cpp:228] Setting up label_data_1_split
I1226 14:11:07.099299 92483 net.cpp:235] Top shape: 256 1 1 1 (256)
I1226 14:11:07.099331 92483 net.cpp:235] Top shape: 256 1 1 1 (256)
I1226 14:11:07.099354 92483 net.cpp:243] Memory required for data: 158300160
I1226 14:11:07.099387 92483 layer_factory.hpp:114] Creating layer conv1
I1226 14:11:07.099483 92483 net.cpp:178] Creating Layer conv1
I1226 14:11:07.099515 92483 net.cpp:612] conv1 <- data
I1226 14:11:07.099555 92483 net.cpp:586] conv1 -> conv1
I1226 14:11:07.085500 96952 data_layer.cpp:80] output data size: 256,3,227,227
I1226 14:11:07.103179 92274 net.cpp:178] Creating Layer data
I1226 14:11:07.103267 92274 net.cpp:586] data -> data
I1226 14:11:07.103391 92274 net.cpp:586] data -> label
I1226 14:11:07.103967 121743 net.cpp:178] Creating Layer data
I1226 14:11:07.104054 121743 net.cpp:586] data -> data
I1226 14:11:07.104128 121743 net.cpp:586] data -> label
I1226 14:11:07.103865 91405 net.cpp:178] Creating Layer data
I1226 14:11:07.103953 91405 net.cpp:586] data -> data
I1226 14:11:07.104032 91405 net.cpp:586] data -> label
I1226 14:11:07.108180 98828 net.cpp:178] Creating Layer data
I1226 14:11:07.108270 98828 net.cpp:586] data -> data
I1226 14:11:07.108350 98828 net.cpp:586] data -> label
I1226 14:11:07.109522 95316 layer_factory.hpp:114] Creating layer data
I1226 14:11:07.122339 90187 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_7
I1226 14:11:07.118412 95316 net.cpp:178] Creating Layer data
I1226 14:11:07.118541 95316 net.cpp:586] data -> data
I1226 14:11:07.118662 95316 net.cpp:586] data -> label
I1226 14:11:07.121574 98834 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_0
I1226 14:11:07.130619 90185 data_layer.cpp:80] output data size: 256,3,227,227
I1226 14:11:07.125428 98828 data_layer.cpp:80] output data size: 256,3,227,227
I1226 14:11:07.125454 91407 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_4
I1226 14:11:07.131147 95318 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_6
I1226 14:11:07.129556 121745 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_5
I1226 14:11:07.135869 95316 data_layer.cpp:80] output data size: 256,3,227,227
I1226 14:11:07.133318 91405 data_layer.cpp:80] output data size: 256,3,227,227
I1226 14:11:07.137670 121743 data_layer.cpp:80] output data size: 256,3,227,227
I1226 14:11:07.158200 92276 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_1
I1226 14:11:07.167198 92274 data_layer.cpp:80] output data size: 256,3,227,227
I1226 14:11:07.237447 92483 net.cpp:228] Setting up conv1
I1226 14:11:07.237563 92483 net.cpp:235] Top shape: 256 96 55 55 (74342400)
I1226 14:11:07.237591 92483 net.cpp:243] Memory required for data: 455669760
I1226 14:11:07.237702 92483 layer_factory.hpp:114] Creating layer relu1
I1226 14:11:07.237785 92483 net.cpp:178] Creating Layer relu1
I1226 14:11:07.237855 92483 net.cpp:612] relu1 <- conv1
I1226 14:11:07.237895 92483 net.cpp:573] relu1 -> conv1 (in-place)
I1226 14:11:07.238003 92483 net.cpp:228] Setting up relu1
I1226 14:11:07.238054 92483 net.cpp:235] Top shape: 256 96 55 55 (74342400)
I1226 14:11:07.238085 92483 net.cpp:243] Memory required for data: 753039360
I1226 14:11:07.238117 92483 layer_factory.hpp:114] Creating layer norm1
I1226 14:11:07.238169 92483 net.cpp:178] Creating Layer norm1
I1226 14:11:07.238195 92483 net.cpp:612] norm1 <- conv1
I1226 14:11:07.238241 92483 net.cpp:586] norm1 -> norm1
I1226 14:11:07.238333 92483 net.cpp:228] Setting up norm1
I1226 14:11:07.238375 92483 net.cpp:235] Top shape: 256 96 55 55 (74342400)
I1226 14:11:07.238404 92483 net.cpp:243] Memory required for data: 1050408960
I1226 14:11:07.238431 92483 layer_factory.hpp:114] Creating layer pool1
I1226 14:11:07.238478 92483 net.cpp:178] Creating Layer pool1
I1226 14:11:07.238508 92483 net.cpp:612] pool1 <- norm1
I1226 14:11:07.238556 92483 net.cpp:586] pool1 -> pool1
I1226 14:11:07.238641 92483 net.cpp:228] Setting up pool1
I1226 14:11:07.238683 92483 net.cpp:235] Top shape: 256 96 27 27 (17915904)
I1226 14:11:07.238714 92483 net.cpp:243] Memory required for data: 1122072576
I1226 14:11:07.238745 92483 layer_factory.hpp:114] Creating layer conv2
I1226 14:11:07.238869 92483 net.cpp:178] Creating Layer conv2
I1226 14:11:07.238911 92483 net.cpp:612] conv2 <- pool1
I1226 14:11:07.238957 92483 net.cpp:586] conv2 -> conv2
I1226 14:11:07.338528 91711 net.cpp:228] Setting up data
I1226 14:11:07.338645 91711 net.cpp:235] Top shape: 256 3 227 227 (39574272)
I1226 14:11:07.338683 91711 net.cpp:235] Top shape: 256 (256)
I1226 14:11:07.338735 91711 net.cpp:243] Memory required for data: 158298112
I1226 14:11:07.338774 91711 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 14:11:07.338872 91711 net.cpp:178] Creating Layer label_data_1_split
I1226 14:11:07.339010 91711 net.cpp:612] label_data_1_split <- label
I1226 14:11:07.339061 91711 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 14:11:07.339112 91711 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 14:11:07.377177 91711 net.cpp:228] Setting up label_data_1_split
I1226 14:11:07.377324 91711 net.cpp:235] Top shape: 256 (256)
I1226 14:11:07.377362 91711 net.cpp:235] Top shape: 256 (256)
I1226 14:11:07.377388 91711 net.cpp:243] Memory required for data: 158300160
I1226 14:11:07.377425 91711 layer_factory.hpp:114] Creating layer conv1
I1226 14:11:07.377537 91711 net.cpp:178] Creating Layer conv1
I1226 14:11:07.377578 91711 net.cpp:612] conv1 <- data
I1226 14:11:07.377621 91711 net.cpp:586] conv1 -> conv1
I1226 14:11:07.404445 90185 net.cpp:228] Setting up data
I1226 14:11:07.404590 90185 net.cpp:235] Top shape: 256 3 227 227 (39574272)
I1226 14:11:07.404639 90185 net.cpp:235] Top shape: 256 (256)
I1226 14:11:07.404667 90185 net.cpp:243] Memory required for data: 158298112
I1226 14:11:07.404706 90185 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 14:11:07.404799 90185 net.cpp:178] Creating Layer label_data_1_split
I1226 14:11:07.404940 90185 net.cpp:612] label_data_1_split <- label
I1226 14:11:07.404996 90185 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 14:11:07.405067 90185 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 14:11:07.398614 92274 net.cpp:228] Setting up data
I1226 14:11:07.398739 92274 net.cpp:235] Top shape: 256 3 227 227 (39574272)
I1226 14:11:07.398778 92274 net.cpp:235] Top shape: 256 (256)
I1226 14:11:07.398804 92274 net.cpp:243] Memory required for data: 158298112
I1226 14:11:07.398843 92274 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 14:11:07.398941 92274 net.cpp:178] Creating Layer label_data_1_split
I1226 14:11:07.399075 92274 net.cpp:612] label_data_1_split <- label
I1226 14:11:07.399124 92274 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 14:11:07.399183 92274 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 14:11:07.404220 91405 net.cpp:228] Setting up data
I1226 14:11:07.404407 91405 net.cpp:235] Top shape: 256 3 227 227 (39574272)
I1226 14:11:07.404471 91405 net.cpp:235] Top shape: 256 (256)
I1226 14:11:07.404525 91405 net.cpp:243] Memory required for data: 158298112
I1226 14:11:07.404568 91405 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 14:11:07.404793 91405 net.cpp:178] Creating Layer label_data_1_split
I1226 14:11:07.404983 91405 net.cpp:612] label_data_1_split <- label
I1226 14:11:07.405059 91405 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 14:11:07.405128 91405 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 14:11:07.421483 90185 net.cpp:228] Setting up label_data_1_split
I1226 14:11:07.421622 90185 net.cpp:235] Top shape: 256 (256)
I1226 14:11:07.421661 90185 net.cpp:235] Top shape: 256 (256)
I1226 14:11:07.421685 90185 net.cpp:243] Memory required for data: 158300160
I1226 14:11:07.421723 90185 layer_factory.hpp:114] Creating layer conv1
I1226 14:11:07.421824 90185 net.cpp:178] Creating Layer conv1
I1226 14:11:07.421880 90185 net.cpp:612] conv1 <- data
I1226 14:11:07.421929 90185 net.cpp:586] conv1 -> conv1
I1226 14:11:07.415781 121743 net.cpp:228] Setting up data
I1226 14:11:07.415899 121743 net.cpp:235] Top shape: 256 3 227 227 (39574272)
I1226 14:11:07.415942 121743 net.cpp:235] Top shape: 256 (256)
I1226 14:11:07.415972 121743 net.cpp:243] Memory required for data: 158298112
I1226 14:11:07.416033 121743 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 14:11:07.416347 121743 net.cpp:178] Creating Layer label_data_1_split
I1226 14:11:07.416402 121743 net.cpp:612] label_data_1_split <- label
I1226 14:11:07.416465 121743 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 14:11:07.416518 121743 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 14:11:07.427019 91405 net.cpp:228] Setting up label_data_1_split
I1226 14:11:07.427168 91405 net.cpp:235] Top shape: 256 (256)
I1226 14:11:07.427239 91405 net.cpp:235] Top shape: 256 (256)
I1226 14:11:07.427273 91405 net.cpp:243] Memory required for data: 158300160
I1226 14:11:07.427422 91405 layer_factory.hpp:114] Creating layer conv1
I1226 14:11:07.430896 92274 net.cpp:228] Setting up label_data_1_split
I1226 14:11:07.427556 91405 net.cpp:178] Creating Layer conv1
I1226 14:11:07.431008 92274 net.cpp:235] Top shape: 256 (256)
I1226 14:11:07.431077 92274 net.cpp:235] Top shape: 256 (256)
I1226 14:11:07.431102 92274 net.cpp:243] Memory required for data: 158300160
I1226 14:11:07.431138 92274 layer_factory.hpp:114] Creating layer conv1
I1226 14:11:07.427630 91405 net.cpp:612] conv1 <- data
I1226 14:11:07.427696 91405 net.cpp:586] conv1 -> conv1
I1226 14:11:07.431361 92274 net.cpp:178] Creating Layer conv1
I1226 14:11:07.431417 92274 net.cpp:612] conv1 <- data
I1226 14:11:07.431473 92274 net.cpp:586] conv1 -> conv1
I1226 14:11:07.435852 98828 net.cpp:228] Setting up data
I1226 14:11:07.435972 98828 net.cpp:235] Top shape: 256 3 227 227 (39574272)
I1226 14:11:07.436010 98828 net.cpp:235] Top shape: 256 (256)
I1226 14:11:07.436035 98828 net.cpp:243] Memory required for data: 158298112
I1226 14:11:07.436095 98828 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 14:11:07.436202 98828 net.cpp:178] Creating Layer label_data_1_split
I1226 14:11:07.436353 98828 net.cpp:612] label_data_1_split <- label
I1226 14:11:07.436419 98828 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 14:11:07.436485 98828 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 14:11:07.442075 121743 net.cpp:228] Setting up label_data_1_split
I1226 14:11:07.442199 121743 net.cpp:235] Top shape: 256 (256)
I1226 14:11:07.442253 121743 net.cpp:235] Top shape: 256 (256)
I1226 14:11:07.442278 121743 net.cpp:243] Memory required for data: 158300160
I1226 14:11:07.442314 121743 layer_factory.hpp:114] Creating layer conv1
I1226 14:11:07.442422 121743 net.cpp:178] Creating Layer conv1
I1226 14:11:07.443037 121743 net.cpp:612] conv1 <- data
I1226 14:11:07.443115 121743 net.cpp:586] conv1 -> conv1
I1226 14:11:07.459899 92483 net.cpp:228] Setting up conv2
I1226 14:11:07.460016 92483 net.cpp:235] Top shape: 256 256 27 27 (47775744)
I1226 14:11:07.460047 92483 net.cpp:243] Memory required for data: 1313175552
I1226 14:11:07.460122 92483 layer_factory.hpp:114] Creating layer relu2
I1226 14:11:07.460211 92483 net.cpp:178] Creating Layer relu2
I1226 14:11:07.460254 92483 net.cpp:612] relu2 <- conv2
I1226 14:11:07.460319 92483 net.cpp:573] relu2 -> conv2 (in-place)
I1226 14:11:07.460423 92483 net.cpp:228] Setting up relu2
I1226 14:11:07.460510 92483 net.cpp:235] Top shape: 256 256 27 27 (47775744)
I1226 14:11:07.460631 92483 net.cpp:243] Memory required for data: 1504278528
I1226 14:11:07.460785 92483 layer_factory.hpp:114] Creating layer norm2
I1226 14:11:07.460882 92483 net.cpp:178] Creating Layer norm2
I1226 14:11:07.460924 92483 net.cpp:612] norm2 <- conv2
I1226 14:11:07.460963 92483 net.cpp:586] norm2 -> norm2
I1226 14:11:07.461060 92483 net.cpp:228] Setting up norm2
I1226 14:11:07.461112 92483 net.cpp:235] Top shape: 256 256 27 27 (47775744)
I1226 14:11:07.461138 92483 net.cpp:243] Memory required for data: 1695381504
I1226 14:11:07.461166 92483 layer_factory.hpp:114] Creating layer pool2
I1226 14:11:07.461247 92483 net.cpp:178] Creating Layer pool2
I1226 14:11:07.461287 92483 net.cpp:612] pool2 <- norm2
I1226 14:11:07.461320 92483 net.cpp:586] pool2 -> pool2
I1226 14:11:07.461431 92483 net.cpp:228] Setting up pool2
I1226 14:11:07.461479 92483 net.cpp:235] Top shape: 256 256 13 13 (11075584)
I1226 14:11:07.461601 92483 net.cpp:243] Memory required for data: 1739683840
I1226 14:11:07.461638 92483 layer_factory.hpp:114] Creating layer conv3
I1226 14:11:07.461747 92483 net.cpp:178] Creating Layer conv3
I1226 14:11:07.461872 92483 net.cpp:612] conv3 <- pool2
I1226 14:11:07.461977 92483 net.cpp:586] conv3 -> conv3
I1226 14:11:07.462395 98828 net.cpp:228] Setting up label_data_1_split
I1226 14:11:07.462502 98828 net.cpp:235] Top shape: 256 (256)
I1226 14:11:07.462535 98828 net.cpp:235] Top shape: 256 (256)
I1226 14:11:07.462559 98828 net.cpp:243] Memory required for data: 158300160
I1226 14:11:07.462596 98828 layer_factory.hpp:114] Creating layer conv1
I1226 14:11:07.462704 98828 net.cpp:178] Creating Layer conv1
I1226 14:11:07.462757 98828 net.cpp:612] conv1 <- data
I1226 14:11:07.462811 98828 net.cpp:586] conv1 -> conv1
I1226 14:11:07.566498 91711 net.cpp:228] Setting up conv1
I1226 14:11:07.566617 91711 net.cpp:235] Top shape: 256 96 55 55 (74342400)
I1226 14:11:07.566645 91711 net.cpp:243] Memory required for data: 455669760
I1226 14:11:07.566787 91711 layer_factory.hpp:114] Creating layer relu1
I1226 14:11:07.566882 91711 net.cpp:178] Creating Layer relu1
I1226 14:11:07.566926 91711 net.cpp:612] relu1 <- conv1
I1226 14:11:07.566968 91711 net.cpp:573] relu1 -> conv1 (in-place)
I1226 14:11:07.567088 91711 net.cpp:228] Setting up relu1
I1226 14:11:07.567137 91711 net.cpp:235] Top shape: 256 96 55 55 (74342400)
I1226 14:11:07.567162 91711 net.cpp:243] Memory required for data: 753039360
I1226 14:11:07.567193 91711 layer_factory.hpp:114] Creating layer norm1
I1226 14:11:07.567298 91711 net.cpp:178] Creating Layer norm1
I1226 14:11:07.567329 91711 net.cpp:612] norm1 <- conv1
I1226 14:11:07.567366 91711 net.cpp:586] norm1 -> norm1
I1226 14:11:07.567476 91711 net.cpp:228] Setting up norm1
I1226 14:11:07.567524 91711 net.cpp:235] Top shape: 256 96 55 55 (74342400)
I1226 14:11:07.567556 91711 net.cpp:243] Memory required for data: 1050408960
I1226 14:11:07.567586 91711 layer_factory.hpp:114] Creating layer pool1
I1226 14:11:07.567647 91711 net.cpp:178] Creating Layer pool1
I1226 14:11:07.567680 91711 net.cpp:612] pool1 <- norm1
I1226 14:11:07.567726 91711 net.cpp:586] pool1 -> pool1
I1226 14:11:07.567827 91711 net.cpp:228] Setting up pool1
I1226 14:11:07.567870 91711 net.cpp:235] Top shape: 256 96 27 27 (17915904)
I1226 14:11:07.567898 91711 net.cpp:243] Memory required for data: 1122072576
I1226 14:11:07.567930 91711 layer_factory.hpp:114] Creating layer conv2
I1226 14:11:07.568003 91711 net.cpp:178] Creating Layer conv2
I1226 14:11:07.568042 91711 net.cpp:612] conv2 <- pool1
I1226 14:11:07.568092 91711 net.cpp:586] conv2 -> conv2
I1226 14:11:07.636430 90185 net.cpp:228] Setting up conv1
I1226 14:11:07.636543 90185 net.cpp:235] Top shape: 256 96 55 55 (74342400)
I1226 14:11:07.636616 90185 net.cpp:243] Memory required for data: 455669760
I1226 14:11:07.636831 90185 layer_factory.hpp:114] Creating layer relu1
I1226 14:11:07.636945 90185 net.cpp:178] Creating Layer relu1
I1226 14:11:07.636992 90185 net.cpp:612] relu1 <- conv1
I1226 14:11:07.637033 90185 net.cpp:573] relu1 -> conv1 (in-place)
I1226 14:11:07.637141 90185 net.cpp:228] Setting up relu1
I1226 14:11:07.637217 90185 net.cpp:235] Top shape: 256 96 55 55 (74342400)
I1226 14:11:07.637382 90185 net.cpp:243] Memory required for data: 753039360
I1226 14:11:07.637415 90185 layer_factory.hpp:114] Creating layer norm1
I1226 14:11:07.637780 90185 net.cpp:178] Creating Layer norm1
I1226 14:11:07.637827 90185 net.cpp:612] norm1 <- conv1
I1226 14:11:07.637873 90185 net.cpp:586] norm1 -> norm1
I1226 14:11:07.638314 90185 net.cpp:228] Setting up norm1
I1226 14:11:07.638444 90185 net.cpp:235] Top shape: 256 96 55 55 (74342400)
I1226 14:11:07.638478 90185 net.cpp:243] Memory required for data: 1050408960
I1226 14:11:07.638520 90185 layer_factory.hpp:114] Creating layer pool1
I1226 14:11:07.638646 90185 net.cpp:178] Creating Layer pool1
I1226 14:11:07.638677 90185 net.cpp:612] pool1 <- norm1
I1226 14:11:07.638747 90185 net.cpp:586] pool1 -> pool1
I1226 14:11:07.638864 90185 net.cpp:228] Setting up pool1
I1226 14:11:07.638913 90185 net.cpp:235] Top shape: 256 96 27 27 (17915904)
I1226 14:11:07.638936 90185 net.cpp:243] Memory required for data: 1122072576
I1226 14:11:07.638965 90185 layer_factory.hpp:114] Creating layer conv2
I1226 14:11:07.639075 90185 net.cpp:178] Creating Layer conv2
I1226 14:11:07.639112 90185 net.cpp:612] conv2 <- pool1
I1226 14:11:07.639163 90185 net.cpp:586] conv2 -> conv2
I1226 14:11:07.660027 92274 net.cpp:228] Setting up conv1
I1226 14:11:07.660142 92274 net.cpp:235] Top shape: 256 96 55 55 (74342400)
I1226 14:11:07.660171 92274 net.cpp:243] Memory required for data: 455669760
I1226 14:11:07.660339 92274 layer_factory.hpp:114] Creating layer relu1
I1226 14:11:07.660440 92274 net.cpp:178] Creating Layer relu1
I1226 14:11:07.660490 92274 net.cpp:612] relu1 <- conv1
I1226 14:11:07.660532 92274 net.cpp:573] relu1 -> conv1 (in-place)
I1226 14:11:07.660642 92274 net.cpp:228] Setting up relu1
I1226 14:11:07.660692 92274 net.cpp:235] Top shape: 256 96 55 55 (74342400)
I1226 14:11:07.660717 92274 net.cpp:243] Memory required for data: 753039360
I1226 14:11:07.660748 92274 layer_factory.hpp:114] Creating layer norm1
I1226 14:11:07.660820 92274 net.cpp:178] Creating Layer norm1
I1226 14:11:07.660857 92274 net.cpp:612] norm1 <- conv1
I1226 14:11:07.660893 92274 net.cpp:586] norm1 -> norm1
I1226 14:11:07.660996 92274 net.cpp:228] Setting up norm1
I1226 14:11:07.661042 92274 net.cpp:235] Top shape: 256 96 55 55 (74342400)
I1226 14:11:07.661067 92274 net.cpp:243] Memory required for data: 1050408960
I1226 14:11:07.661095 92274 layer_factory.hpp:114] Creating layer pool1
I1226 14:11:07.661165 92274 net.cpp:178] Creating Layer pool1
I1226 14:11:07.661202 92274 net.cpp:612] pool1 <- norm1
I1226 14:11:07.661245 92274 net.cpp:586] pool1 -> pool1
I1226 14:11:07.661373 92274 net.cpp:228] Setting up pool1
I1226 14:11:07.661429 92274 net.cpp:235] Top shape: 256 96 27 27 (17915904)
I1226 14:11:07.661453 92274 net.cpp:243] Memory required for data: 1122072576
I1226 14:11:07.661489 92274 layer_factory.hpp:114] Creating layer conv2
I1226 14:11:07.661572 92274 net.cpp:178] Creating Layer conv2
I1226 14:11:07.661604 92274 net.cpp:612] conv2 <- pool1
I1226 14:11:07.661674 92274 net.cpp:586] conv2 -> conv2
I1226 14:11:07.680681 91405 net.cpp:228] Setting up conv1
I1226 14:11:07.680819 91405 net.cpp:235] Top shape: 256 96 55 55 (74342400)
I1226 14:11:07.680850 91405 net.cpp:243] Memory required for data: 455669760
I1226 14:11:07.681447 91405 layer_factory.hpp:114] Creating layer relu1
I1226 14:11:07.681622 91405 net.cpp:178] Creating Layer relu1
I1226 14:11:07.681792 91405 net.cpp:612] relu1 <- conv1
I1226 14:11:07.681881 91405 net.cpp:573] relu1 -> conv1 (in-place)
I1226 14:11:07.682030 91405 net.cpp:228] Setting up relu1
I1226 14:11:07.682095 91405 net.cpp:235] Top shape: 256 96 55 55 (74342400)
I1226 14:11:07.682121 91405 net.cpp:243] Memory required for data: 753039360
I1226 14:11:07.682155 91405 layer_factory.hpp:114] Creating layer norm1
I1226 14:11:07.682240 91405 net.cpp:178] Creating Layer norm1
I1226 14:11:07.682282 91405 net.cpp:612] norm1 <- conv1
I1226 14:11:07.682322 91405 net.cpp:586] norm1 -> norm1
I1226 14:11:07.682420 91405 net.cpp:228] Setting up norm1
I1226 14:11:07.682474 91405 net.cpp:235] Top shape: 256 96 55 55 (74342400)
I1226 14:11:07.682499 91405 net.cpp:243] Memory required for data: 1050408960
I1226 14:11:07.682529 91405 layer_factory.hpp:114] Creating layer pool1
I1226 14:11:07.682593 91405 net.cpp:178] Creating Layer pool1
I1226 14:11:07.682622 91405 net.cpp:612] pool1 <- norm1
I1226 14:11:07.682665 91405 net.cpp:586] pool1 -> pool1
I1226 14:11:07.682806 91405 net.cpp:228] Setting up pool1
I1226 14:11:07.682862 91405 net.cpp:235] Top shape: 256 96 27 27 (17915904)
I1226 14:11:07.682894 91405 net.cpp:243] Memory required for data: 1122072576
I1226 14:11:07.682931 91405 layer_factory.hpp:114] Creating layer conv2
I1226 14:11:07.683012 91405 net.cpp:178] Creating Layer conv2
I1226 14:11:07.683046 91405 net.cpp:612] conv2 <- pool1
I1226 14:11:07.683084 91405 net.cpp:586] conv2 -> conv2
I1226 14:11:07.691576 92483 net.cpp:228] Setting up conv3
I1226 14:11:07.691709 92483 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 14:11:07.691742 92483 net.cpp:243] Memory required for data: 1806137344
I1226 14:11:07.691882 92483 layer_factory.hpp:114] Creating layer relu3
I1226 14:11:07.691978 92483 net.cpp:178] Creating Layer relu3
I1226 14:11:07.692028 92483 net.cpp:612] relu3 <- conv3
I1226 14:11:07.692075 92483 net.cpp:573] relu3 -> conv3 (in-place)
I1226 14:11:07.692173 92483 net.cpp:228] Setting up relu3
I1226 14:11:07.692229 92483 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 14:11:07.692255 92483 net.cpp:243] Memory required for data: 1872590848
I1226 14:11:07.692284 92483 layer_factory.hpp:114] Creating layer conv4
I1226 14:11:07.692356 92483 net.cpp:178] Creating Layer conv4
I1226 14:11:07.692396 92483 net.cpp:612] conv4 <- conv3
I1226 14:11:07.692440 92483 net.cpp:586] conv4 -> conv4
I1226 14:11:07.705718 98828 net.cpp:228] Setting up conv1
I1226 14:11:07.705845 98828 net.cpp:235] Top shape: 256 96 55 55 (74342400)
I1226 14:11:07.705878 98828 net.cpp:243] Memory required for data: 455669760
I1226 14:11:07.705992 98828 layer_factory.hpp:114] Creating layer relu1
I1226 14:11:07.706104 98828 net.cpp:178] Creating Layer relu1
I1226 14:11:07.706146 98828 net.cpp:612] relu1 <- conv1
I1226 14:11:07.706193 98828 net.cpp:573] relu1 -> conv1 (in-place)
I1226 14:11:07.706313 98828 net.cpp:228] Setting up relu1
I1226 14:11:07.706501 98828 net.cpp:235] Top shape: 256 96 55 55 (74342400)
I1226 14:11:07.706528 98828 net.cpp:243] Memory required for data: 753039360
I1226 14:11:07.706562 98828 layer_factory.hpp:114] Creating layer norm1
I1226 14:11:07.706647 98828 net.cpp:178] Creating Layer norm1
I1226 14:11:07.706801 98828 net.cpp:612] norm1 <- conv1
I1226 14:11:07.706847 98828 net.cpp:586] norm1 -> norm1
I1226 14:11:07.706956 98828 net.cpp:228] Setting up norm1
I1226 14:11:07.707162 98828 net.cpp:235] Top shape: 256 96 55 55 (74342400)
I1226 14:11:07.707190 98828 net.cpp:243] Memory required for data: 1050408960
I1226 14:11:07.707224 98828 layer_factory.hpp:114] Creating layer pool1
I1226 14:11:07.707298 98828 net.cpp:178] Creating Layer pool1
I1226 14:11:07.707329 98828 net.cpp:612] pool1 <- norm1
I1226 14:11:07.707370 98828 net.cpp:586] pool1 -> pool1
I1226 14:11:07.707497 98828 net.cpp:228] Setting up pool1
I1226 14:11:07.707715 98828 net.cpp:235] Top shape: 256 96 27 27 (17915904)
I1226 14:11:07.707741 98828 net.cpp:243] Memory required for data: 1122072576
I1226 14:11:07.707772 98828 layer_factory.hpp:114] Creating layer conv2
I1226 14:11:07.707854 98828 net.cpp:178] Creating Layer conv2
I1226 14:11:07.707883 98828 net.cpp:612] conv2 <- pool1
I1226 14:11:07.707931 98828 net.cpp:586] conv2 -> conv2
I1226 14:11:07.710532 121743 net.cpp:228] Setting up conv1
I1226 14:11:07.711133 121743 net.cpp:235] Top shape: 256 96 55 55 (74342400)
I1226 14:11:07.711225 121743 net.cpp:243] Memory required for data: 455669760
I1226 14:11:07.711377 121743 layer_factory.hpp:114] Creating layer relu1
I1226 14:11:07.711489 121743 net.cpp:178] Creating Layer relu1
I1226 14:11:07.711649 121743 net.cpp:612] relu1 <- conv1
I1226 14:11:07.711707 121743 net.cpp:573] relu1 -> conv1 (in-place)
I1226 14:11:07.712144 121743 net.cpp:228] Setting up relu1
I1226 14:11:07.712237 121743 net.cpp:235] Top shape: 256 96 55 55 (74342400)
I1226 14:11:07.712380 121743 net.cpp:243] Memory required for data: 753039360
I1226 14:11:07.712430 121743 layer_factory.hpp:114] Creating layer norm1
I1226 14:11:07.712525 121743 net.cpp:178] Creating Layer norm1
I1226 14:11:07.712577 121743 net.cpp:612] norm1 <- conv1
I1226 14:11:07.712651 121743 net.cpp:586] norm1 -> norm1
I1226 14:11:07.725854 121743 net.cpp:228] Setting up norm1
I1226 14:11:07.725924 121743 net.cpp:235] Top shape: 256 96 55 55 (74342400)
I1226 14:11:07.725950 121743 net.cpp:243] Memory required for data: 1050408960
I1226 14:11:07.725982 121743 layer_factory.hpp:114] Creating layer pool1
I1226 14:11:07.726058 121743 net.cpp:178] Creating Layer pool1
I1226 14:11:07.726089 121743 net.cpp:612] pool1 <- norm1
I1226 14:11:07.726135 121743 net.cpp:586] pool1 -> pool1
I1226 14:11:07.726235 121743 net.cpp:228] Setting up pool1
I1226 14:11:07.726279 121743 net.cpp:235] Top shape: 256 96 27 27 (17915904)
I1226 14:11:07.726307 121743 net.cpp:243] Memory required for data: 1122072576
I1226 14:11:07.726335 121743 layer_factory.hpp:114] Creating layer conv2
I1226 14:11:07.726408 121743 net.cpp:178] Creating Layer conv2
I1226 14:11:07.726442 121743 net.cpp:612] conv2 <- pool1
I1226 14:11:07.726481 121743 net.cpp:586] conv2 -> conv2
I1226 14:11:07.893707 92483 net.cpp:228] Setting up conv4
I1226 14:11:07.893849 92483 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 14:11:07.893884 92483 net.cpp:243] Memory required for data: 1939044352
I1226 14:11:07.893945 92483 layer_factory.hpp:114] Creating layer relu4
I1226 14:11:07.894040 92483 net.cpp:178] Creating Layer relu4
I1226 14:11:07.894081 92483 net.cpp:612] relu4 <- conv4
I1226 14:11:07.894143 92483 net.cpp:573] relu4 -> conv4 (in-place)
I1226 14:11:07.894239 92483 net.cpp:228] Setting up relu4
I1226 14:11:07.894289 92483 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 14:11:07.894314 92483 net.cpp:243] Memory required for data: 2005497856
I1226 14:11:07.894342 92483 layer_factory.hpp:114] Creating layer conv5
I1226 14:11:07.894428 92483 net.cpp:178] Creating Layer conv5
I1226 14:11:07.894465 92483 net.cpp:612] conv5 <- conv4
I1226 14:11:07.894520 92483 net.cpp:586] conv5 -> conv5
I1226 14:11:07.920660 91711 net.cpp:228] Setting up conv2
I1226 14:11:07.920812 91711 net.cpp:235] Top shape: 256 256 27 27 (47775744)
I1226 14:11:07.920851 91711 net.cpp:243] Memory required for data: 1313175552
I1226 14:11:07.921048 91711 layer_factory.hpp:114] Creating layer relu2
I1226 14:11:07.921141 91711 net.cpp:178] Creating Layer relu2
I1226 14:11:07.921200 91711 net.cpp:612] relu2 <- conv2
I1226 14:11:07.921278 91711 net.cpp:573] relu2 -> conv2 (in-place)
I1226 14:11:07.921432 91711 net.cpp:228] Setting up relu2
I1226 14:11:07.921557 91711 net.cpp:235] Top shape: 256 256 27 27 (47775744)
I1226 14:11:07.921588 91711 net.cpp:243] Memory required for data: 1504278528
I1226 14:11:07.921622 91711 layer_factory.hpp:114] Creating layer norm2
I1226 14:11:07.921702 91711 net.cpp:178] Creating Layer norm2
I1226 14:11:07.921737 91711 net.cpp:612] norm2 <- conv2
I1226 14:11:07.921777 91711 net.cpp:586] norm2 -> norm2
I1226 14:11:07.921869 91711 net.cpp:228] Setting up norm2
I1226 14:11:07.921918 91711 net.cpp:235] Top shape: 256 256 27 27 (47775744)
I1226 14:11:07.921950 91711 net.cpp:243] Memory required for data: 1695381504
I1226 14:11:07.921983 91711 layer_factory.hpp:114] Creating layer pool2
I1226 14:11:07.922035 91711 net.cpp:178] Creating Layer pool2
I1226 14:11:07.922063 91711 net.cpp:612] pool2 <- norm2
I1226 14:11:07.922106 91711 net.cpp:586] pool2 -> pool2
I1226 14:11:07.922188 91711 net.cpp:228] Setting up pool2
I1226 14:11:07.922363 91711 net.cpp:235] Top shape: 256 256 13 13 (11075584)
I1226 14:11:07.922392 91711 net.cpp:243] Memory required for data: 1739683840
I1226 14:11:07.922425 91711 layer_factory.hpp:114] Creating layer conv3
I1226 14:11:07.922516 91711 net.cpp:178] Creating Layer conv3
I1226 14:11:07.922551 91711 net.cpp:612] conv3 <- pool2
I1226 14:11:07.922600 91711 net.cpp:586] conv3 -> conv3
I1226 14:11:08.001996 90185 net.cpp:228] Setting up conv2
I1226 14:11:08.002115 90185 net.cpp:235] Top shape: 256 256 27 27 (47775744)
I1226 14:11:08.002148 90185 net.cpp:243] Memory required for data: 1313175552
I1226 14:11:08.002228 90185 layer_factory.hpp:114] Creating layer relu2
I1226 14:11:08.002329 90185 net.cpp:178] Creating Layer relu2
I1226 14:11:08.002552 90185 net.cpp:612] relu2 <- conv2
I1226 14:11:08.002688 90185 net.cpp:573] relu2 -> conv2 (in-place)
I1226 14:11:08.003108 90185 net.cpp:228] Setting up relu2
I1226 14:11:08.003229 90185 net.cpp:235] Top shape: 256 256 27 27 (47775744)
I1226 14:11:08.003258 90185 net.cpp:243] Memory required for data: 1504278528
I1226 14:11:08.003294 90185 layer_factory.hpp:114] Creating layer norm2
I1226 14:11:08.003404 90185 net.cpp:178] Creating Layer norm2
I1226 14:11:08.003471 90185 net.cpp:612] norm2 <- conv2
I1226 14:11:08.003515 90185 net.cpp:586] norm2 -> norm2
I1226 14:11:08.003659 90185 net.cpp:228] Setting up norm2
I1226 14:11:08.003720 90185 net.cpp:235] Top shape: 256 256 27 27 (47775744)
I1226 14:11:08.003744 90185 net.cpp:243] Memory required for data: 1695381504
I1226 14:11:08.003774 90185 layer_factory.hpp:114] Creating layer pool2
I1226 14:11:08.003844 90185 net.cpp:178] Creating Layer pool2
I1226 14:11:08.003873 90185 net.cpp:612] pool2 <- norm2
I1226 14:11:08.003911 90185 net.cpp:586] pool2 -> pool2
I1226 14:11:08.004053 90185 net.cpp:228] Setting up pool2
I1226 14:11:08.004431 90185 net.cpp:235] Top shape: 256 256 13 13 (11075584)
I1226 14:11:08.004464 90185 net.cpp:243] Memory required for data: 1739683840
I1226 14:11:08.004498 90185 layer_factory.hpp:114] Creating layer conv3
I1226 14:11:08.004931 90185 net.cpp:178] Creating Layer conv3
I1226 14:11:08.005024 90185 net.cpp:612] conv3 <- pool2
I1226 14:11:08.005138 90185 net.cpp:586] conv3 -> conv3
I1226 14:11:08.019237 92274 net.cpp:228] Setting up conv2
I1226 14:11:08.019383 92274 net.cpp:235] Top shape: 256 256 27 27 (47775744)
I1226 14:11:08.019444 92274 net.cpp:243] Memory required for data: 1313175552
I1226 14:11:08.019528 92274 layer_factory.hpp:114] Creating layer relu2
I1226 14:11:08.019697 92274 net.cpp:178] Creating Layer relu2
I1226 14:11:08.019739 92274 net.cpp:612] relu2 <- conv2
I1226 14:11:08.019781 92274 net.cpp:573] relu2 -> conv2 (in-place)
I1226 14:11:08.019884 92274 net.cpp:228] Setting up relu2
I1226 14:11:08.019965 92274 net.cpp:235] Top shape: 256 256 27 27 (47775744)
I1226 14:11:08.019992 92274 net.cpp:243] Memory required for data: 1504278528
I1226 14:11:08.020022 92274 layer_factory.hpp:114] Creating layer norm2
I1226 14:11:08.020087 92274 net.cpp:178] Creating Layer norm2
I1226 14:11:08.020182 92274 net.cpp:612] norm2 <- conv2
I1226 14:11:08.020220 92274 net.cpp:586] norm2 -> norm2
I1226 14:11:08.020336 92274 net.cpp:228] Setting up norm2
I1226 14:11:08.020826 92274 net.cpp:235] Top shape: 256 256 27 27 (47775744)
I1226 14:11:08.020951 92274 net.cpp:243] Memory required for data: 1695381504
I1226 14:11:08.020992 92274 layer_factory.hpp:114] Creating layer pool2
I1226 14:11:08.021093 92274 net.cpp:178] Creating Layer pool2
I1226 14:11:08.021163 92274 net.cpp:612] pool2 <- norm2
I1226 14:11:08.021206 92274 net.cpp:586] pool2 -> pool2
I1226 14:11:08.021344 92274 net.cpp:228] Setting up pool2
I1226 14:11:08.021546 92274 net.cpp:235] Top shape: 256 256 13 13 (11075584)
I1226 14:11:08.021591 92274 net.cpp:243] Memory required for data: 1739683840
I1226 14:11:08.021646 92274 layer_factory.hpp:114] Creating layer conv3
I1226 14:11:08.021741 92274 net.cpp:178] Creating Layer conv3
I1226 14:11:08.021782 92274 net.cpp:612] conv3 <- pool2
I1226 14:11:08.021841 92274 net.cpp:586] conv3 -> conv3
I1226 14:11:08.043020 92483 net.cpp:228] Setting up conv5
I1226 14:11:08.043133 92483 net.cpp:235] Top shape: 256 256 13 13 (11075584)
I1226 14:11:08.043160 92483 net.cpp:243] Memory required for data: 2049800192
I1226 14:11:08.043236 92483 layer_factory.hpp:114] Creating layer relu5
I1226 14:11:08.043323 92483 net.cpp:178] Creating Layer relu5
I1226 14:11:08.043365 92483 net.cpp:612] relu5 <- conv5
I1226 14:11:08.043404 92483 net.cpp:573] relu5 -> conv5 (in-place)
I1226 14:11:08.043498 92483 net.cpp:228] Setting up relu5
I1226 14:11:08.043546 92483 net.cpp:235] Top shape: 256 256 13 13 (11075584)
I1226 14:11:08.043570 92483 net.cpp:243] Memory required for data: 2094102528
I1226 14:11:08.043598 92483 layer_factory.hpp:114] Creating layer pool5
I1226 14:11:08.043658 92483 net.cpp:178] Creating Layer pool5
I1226 14:11:08.043694 92483 net.cpp:612] pool5 <- conv5
I1226 14:11:08.043740 92483 net.cpp:586] pool5 -> pool5
I1226 14:11:08.043854 92483 net.cpp:228] Setting up pool5
I1226 14:11:08.043907 92483 net.cpp:235] Top shape: 256 256 6 6 (2359296)
I1226 14:11:08.043931 92483 net.cpp:243] Memory required for data: 2103539712
I1226 14:11:08.043961 92483 layer_factory.hpp:114] Creating layer fc6
I1226 14:11:08.044042 92483 net.cpp:178] Creating Layer fc6
I1226 14:11:08.044075 92483 net.cpp:612] fc6 <- pool5
I1226 14:11:08.044113 92483 net.cpp:586] fc6 -> fc6
I1226 14:11:08.063729 91405 net.cpp:228] Setting up conv2
I1226 14:11:08.063843 91405 net.cpp:235] Top shape: 256 256 27 27 (47775744)
I1226 14:11:08.063890 91405 net.cpp:243] Memory required for data: 1313175552
I1226 14:11:08.063971 91405 layer_factory.hpp:114] Creating layer relu2
I1226 14:11:08.064061 91405 net.cpp:178] Creating Layer relu2
I1226 14:11:08.064097 91405 net.cpp:612] relu2 <- conv2
I1226 14:11:08.064296 91405 net.cpp:573] relu2 -> conv2 (in-place)
I1226 14:11:08.064437 91405 net.cpp:228] Setting up relu2
I1226 14:11:08.064587 91405 net.cpp:235] Top shape: 256 256 27 27 (47775744)
I1226 14:11:08.064616 91405 net.cpp:243] Memory required for data: 1504278528
I1226 14:11:08.064651 91405 layer_factory.hpp:114] Creating layer norm2
I1226 14:11:08.064744 91405 net.cpp:178] Creating Layer norm2
I1226 14:11:08.065320 91405 net.cpp:612] norm2 <- conv2
I1226 14:11:08.065428 91405 net.cpp:586] norm2 -> norm2
I1226 14:11:08.065634 91405 net.cpp:228] Setting up norm2
I1226 14:11:08.065753 91405 net.cpp:235] Top shape: 256 256 27 27 (47775744)
I1226 14:11:08.065785 91405 net.cpp:243] Memory required for data: 1695381504
I1226 14:11:08.065819 91405 layer_factory.hpp:114] Creating layer pool2
I1226 14:11:08.065912 91405 net.cpp:178] Creating Layer pool2
I1226 14:11:08.065946 91405 net.cpp:612] pool2 <- norm2
I1226 14:11:08.065986 91405 net.cpp:586] pool2 -> pool2
I1226 14:11:08.066124 91405 net.cpp:228] Setting up pool2
I1226 14:11:08.066576 91405 net.cpp:235] Top shape: 256 256 13 13 (11075584)
I1226 14:11:08.066673 91405 net.cpp:243] Memory required for data: 1739683840
I1226 14:11:08.066751 91405 layer_factory.hpp:114] Creating layer conv3
I1226 14:11:08.066900 91405 net.cpp:178] Creating Layer conv3
I1226 14:11:08.066967 91405 net.cpp:612] conv3 <- pool2
I1226 14:11:08.067019 91405 net.cpp:586] conv3 -> conv3
I1226 14:11:08.060570 96952 net.cpp:228] Setting up data
I1226 14:11:08.060694 96952 net.cpp:235] Top shape: 256 3 227 227 (39574272)
I1226 14:11:08.060741 96952 net.cpp:235] Top shape: 256 (256)
I1226 14:11:08.060770 96952 net.cpp:243] Memory required for data: 158298112
I1226 14:11:08.060839 96952 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 14:11:08.060936 96952 net.cpp:178] Creating Layer label_data_1_split
I1226 14:11:08.061091 96952 net.cpp:612] label_data_1_split <- label
I1226 14:11:08.061146 96952 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 14:11:08.061311 96952 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 14:11:08.074682 96952 net.cpp:228] Setting up label_data_1_split
I1226 14:11:08.087893 96952 net.cpp:235] Top shape: 256 (256)
I1226 14:11:08.087999 96952 net.cpp:235] Top shape: 256 (256)
I1226 14:11:08.088030 96952 net.cpp:243] Memory required for data: 158300160
I1226 14:11:08.088071 96952 layer_factory.hpp:114] Creating layer conv1
I1226 14:11:08.088181 96952 net.cpp:178] Creating Layer conv1
I1226 14:11:08.088227 96952 net.cpp:612] conv1 <- data
I1226 14:11:08.088274 96952 net.cpp:586] conv1 -> conv1
I1226 14:11:08.112480 95316 net.cpp:228] Setting up data
I1226 14:11:08.112602 95316 net.cpp:235] Top shape: 256 3 227 227 (39574272)
I1226 14:11:08.112645 95316 net.cpp:235] Top shape: 256 (256)
I1226 14:11:08.112673 95316 net.cpp:243] Memory required for data: 158298112
I1226 14:11:08.112711 95316 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 14:11:08.112812 95316 net.cpp:178] Creating Layer label_data_1_split
I1226 14:11:08.112977 95316 net.cpp:612] label_data_1_split <- label
I1226 14:11:08.113032 95316 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 14:11:08.113087 95316 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 14:11:08.112715 121743 net.cpp:228] Setting up conv2
I1226 14:11:08.112828 121743 net.cpp:235] Top shape: 256 256 27 27 (47775744)
I1226 14:11:08.112856 121743 net.cpp:243] Memory required for data: 1313175552
I1226 14:11:08.112929 121743 layer_factory.hpp:114] Creating layer relu2
I1226 14:11:08.113006 121743 net.cpp:178] Creating Layer relu2
I1226 14:11:08.113044 121743 net.cpp:612] relu2 <- conv2
I1226 14:11:08.113083 121743 net.cpp:573] relu2 -> conv2 (in-place)
I1226 14:11:08.113173 121743 net.cpp:228] Setting up relu2
I1226 14:11:08.113219 121743 net.cpp:235] Top shape: 256 256 27 27 (47775744)
I1226 14:11:08.113242 121743 net.cpp:243] Memory required for data: 1504278528
I1226 14:11:08.113270 121743 layer_factory.hpp:114] Creating layer norm2
I1226 14:11:08.113320 121743 net.cpp:178] Creating Layer norm2
I1226 14:11:08.113354 121743 net.cpp:612] norm2 <- conv2
I1226 14:11:08.113390 121743 net.cpp:586] norm2 -> norm2
I1226 14:11:08.113487 121743 net.cpp:228] Setting up norm2
I1226 14:11:08.113533 121743 net.cpp:235] Top shape: 256 256 27 27 (47775744)
I1226 14:11:08.113556 121743 net.cpp:243] Memory required for data: 1695381504
I1226 14:11:08.113584 121743 layer_factory.hpp:114] Creating layer pool2
I1226 14:11:08.113668 121743 net.cpp:178] Creating Layer pool2
I1226 14:11:08.113704 121743 net.cpp:612] pool2 <- norm2
I1226 14:11:08.113746 121743 net.cpp:586] pool2 -> pool2
I1226 14:11:08.113940 121743 net.cpp:228] Setting up pool2
I1226 14:11:08.113996 121743 net.cpp:235] Top shape: 256 256 13 13 (11075584)
I1226 14:11:08.114018 121743 net.cpp:243] Memory required for data: 1739683840
I1226 14:11:08.114048 121743 layer_factory.hpp:114] Creating layer conv3
I1226 14:11:08.114123 121743 net.cpp:178] Creating Layer conv3
I1226 14:11:08.114157 121743 net.cpp:612] conv3 <- pool2
I1226 14:11:08.114199 121743 net.cpp:586] conv3 -> conv3
I1226 14:11:08.139348 95316 net.cpp:228] Setting up label_data_1_split
I1226 14:11:08.139508 95316 net.cpp:235] Top shape: 256 (256)
I1226 14:11:08.139549 95316 net.cpp:235] Top shape: 256 (256)
I1226 14:11:08.139575 95316 net.cpp:243] Memory required for data: 158300160
I1226 14:11:08.139796 95316 layer_factory.hpp:114] Creating layer conv1
I1226 14:11:08.139948 95316 net.cpp:178] Creating Layer conv1
I1226 14:11:08.139997 95316 net.cpp:612] conv1 <- data
I1226 14:11:08.140043 95316 net.cpp:586] conv1 -> conv1
I1226 14:11:08.147547 98828 net.cpp:228] Setting up conv2
I1226 14:11:08.147663 98828 net.cpp:235] Top shape: 256 256 27 27 (47775744)
I1226 14:11:08.147691 98828 net.cpp:243] Memory required for data: 1313175552
I1226 14:11:08.147768 98828 layer_factory.hpp:114] Creating layer relu2
I1226 14:11:08.147833 98828 net.cpp:178] Creating Layer relu2
I1226 14:11:08.147868 98828 net.cpp:612] relu2 <- conv2
I1226 14:11:08.147912 98828 net.cpp:573] relu2 -> conv2 (in-place)
I1226 14:11:08.148016 98828 net.cpp:228] Setting up relu2
I1226 14:11:08.148103 98828 net.cpp:235] Top shape: 256 256 27 27 (47775744)
I1226 14:11:08.148134 98828 net.cpp:243] Memory required for data: 1504278528
I1226 14:11:08.148169 98828 layer_factory.hpp:114] Creating layer norm2
I1226 14:11:08.148243 98828 net.cpp:178] Creating Layer norm2
I1226 14:11:08.148298 98828 net.cpp:612] norm2 <- conv2
I1226 14:11:08.148342 98828 net.cpp:586] norm2 -> norm2
I1226 14:11:08.148449 98828 net.cpp:228] Setting up norm2
I1226 14:11:08.148517 98828 net.cpp:235] Top shape: 256 256 27 27 (47775744)
I1226 14:11:08.148542 98828 net.cpp:243] Memory required for data: 1695381504
I1226 14:11:08.148573 98828 layer_factory.hpp:114] Creating layer pool2
I1226 14:11:08.148622 98828 net.cpp:178] Creating Layer pool2
I1226 14:11:08.148651 98828 net.cpp:612] pool2 <- norm2
I1226 14:11:08.148689 98828 net.cpp:586] pool2 -> pool2
I1226 14:11:08.148775 98828 net.cpp:228] Setting up pool2
I1226 14:11:08.148923 98828 net.cpp:235] Top shape: 256 256 13 13 (11075584)
I1226 14:11:08.148953 98828 net.cpp:243] Memory required for data: 1739683840
I1226 14:11:08.148985 98828 layer_factory.hpp:114] Creating layer conv3
I1226 14:11:08.149096 98828 net.cpp:178] Creating Layer conv3
I1226 14:11:08.149133 98828 net.cpp:612] conv3 <- pool2
I1226 14:11:08.149180 98828 net.cpp:586] conv3 -> conv3
I1226 14:11:08.310051 91711 net.cpp:228] Setting up conv3
I1226 14:11:08.310190 91711 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 14:11:08.310238 91711 net.cpp:243] Memory required for data: 1806137344
I1226 14:11:08.310344 91711 layer_factory.hpp:114] Creating layer relu3
I1226 14:11:08.310415 91711 net.cpp:178] Creating Layer relu3
I1226 14:11:08.310447 91711 net.cpp:612] relu3 <- conv3
I1226 14:11:08.310488 91711 net.cpp:573] relu3 -> conv3 (in-place)
I1226 14:11:08.310595 91711 net.cpp:228] Setting up relu3
I1226 14:11:08.310645 91711 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 14:11:08.310669 91711 net.cpp:243] Memory required for data: 1872590848
I1226 14:11:08.310705 91711 layer_factory.hpp:114] Creating layer conv4
I1226 14:11:08.310796 91711 net.cpp:178] Creating Layer conv4
I1226 14:11:08.310830 91711 net.cpp:612] conv4 <- conv3
I1226 14:11:08.310883 91711 net.cpp:586] conv4 -> conv4
I1226 14:11:08.385023 92274 net.cpp:228] Setting up conv3
I1226 14:11:08.385134 92274 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 14:11:08.385166 92274 net.cpp:243] Memory required for data: 1806137344
I1226 14:11:08.385242 92274 layer_factory.hpp:114] Creating layer relu3
I1226 14:11:08.385299 92274 net.cpp:178] Creating Layer relu3
I1226 14:11:08.385357 92274 net.cpp:612] relu3 <- conv3
I1226 14:11:08.385401 92274 net.cpp:573] relu3 -> conv3 (in-place)
I1226 14:11:08.385510 92274 net.cpp:228] Setting up relu3
I1226 14:11:08.385557 92274 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 14:11:08.385583 92274 net.cpp:243] Memory required for data: 1872590848
I1226 14:11:08.385613 92274 layer_factory.hpp:114] Creating layer conv4
I1226 14:11:08.385710 92274 net.cpp:178] Creating Layer conv4
I1226 14:11:08.385748 92274 net.cpp:612] conv4 <- conv3
I1226 14:11:08.385798 92274 net.cpp:586] conv4 -> conv4
I1226 14:11:08.404537 90185 net.cpp:228] Setting up conv3
I1226 14:11:08.404676 90185 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 14:11:08.404731 90185 net.cpp:243] Memory required for data: 1806137344
I1226 14:11:08.404834 90185 layer_factory.hpp:114] Creating layer relu3
I1226 14:11:08.404973 90185 net.cpp:178] Creating Layer relu3
I1226 14:11:08.405045 90185 net.cpp:612] relu3 <- conv3
I1226 14:11:08.405144 90185 net.cpp:573] relu3 -> conv3 (in-place)
I1226 14:11:08.405279 90185 net.cpp:228] Setting up relu3
I1226 14:11:08.405823 90185 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 14:11:08.405952 90185 net.cpp:243] Memory required for data: 1872590848
I1226 14:11:08.406014 90185 layer_factory.hpp:114] Creating layer conv4
I1226 14:11:08.406167 90185 net.cpp:178] Creating Layer conv4
I1226 14:11:08.406210 90185 net.cpp:612] conv4 <- conv3
I1226 14:11:08.406273 90185 net.cpp:586] conv4 -> conv4
I1226 14:11:08.478215 91405 net.cpp:228] Setting up conv3
I1226 14:11:08.478333 91405 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 14:11:08.478363 91405 net.cpp:243] Memory required for data: 1806137344
I1226 14:11:08.478440 91405 layer_factory.hpp:114] Creating layer relu3
I1226 14:11:08.478507 91405 net.cpp:178] Creating Layer relu3
I1226 14:11:08.478538 91405 net.cpp:612] relu3 <- conv3
I1226 14:11:08.478591 91405 net.cpp:573] relu3 -> conv3 (in-place)
I1226 14:11:08.478700 91405 net.cpp:228] Setting up relu3
I1226 14:11:08.478898 91405 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 14:11:08.478924 91405 net.cpp:243] Memory required for data: 1872590848
I1226 14:11:08.478956 91405 layer_factory.hpp:114] Creating layer conv4
I1226 14:11:08.479389 91405 net.cpp:178] Creating Layer conv4
I1226 14:11:08.479487 91405 net.cpp:612] conv4 <- conv3
I1226 14:11:08.479549 91405 net.cpp:586] conv4 -> conv4
I1226 14:11:08.493224 121743 net.cpp:228] Setting up conv3
I1226 14:11:08.493674 121743 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 14:11:08.493721 121743 net.cpp:243] Memory required for data: 1806137344
I1226 14:11:08.493818 121743 layer_factory.hpp:114] Creating layer relu3
I1226 14:11:08.493979 121743 net.cpp:178] Creating Layer relu3
I1226 14:11:08.494027 121743 net.cpp:612] relu3 <- conv3
I1226 14:11:08.494067 121743 net.cpp:573] relu3 -> conv3 (in-place)
I1226 14:11:08.494189 121743 net.cpp:228] Setting up relu3
I1226 14:11:08.494298 121743 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 14:11:08.494324 121743 net.cpp:243] Memory required for data: 1872590848
I1226 14:11:08.494411 121743 layer_factory.hpp:114] Creating layer conv4
I1226 14:11:08.494520 121743 net.cpp:178] Creating Layer conv4
I1226 14:11:08.494575 121743 net.cpp:612] conv4 <- conv3
I1226 14:11:08.495048 121743 net.cpp:586] conv4 -> conv4
I1226 14:11:08.616508 98828 net.cpp:228] Setting up conv3
I1226 14:11:08.616621 98828 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 14:11:08.616644 98828 net.cpp:243] Memory required for data: 1806137344
I1226 14:11:08.616714 98828 layer_factory.hpp:114] Creating layer relu3
I1226 14:11:08.616794 98828 net.cpp:178] Creating Layer relu3
I1226 14:11:08.616830 98828 net.cpp:612] relu3 <- conv3
I1226 14:11:08.616870 98828 net.cpp:573] relu3 -> conv3 (in-place)
I1226 14:11:08.616945 98828 net.cpp:228] Setting up relu3
I1226 14:11:08.616994 98828 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 14:11:08.617018 98828 net.cpp:243] Memory required for data: 1872590848
I1226 14:11:08.617045 98828 layer_factory.hpp:114] Creating layer conv4
I1226 14:11:08.617143 98828 net.cpp:178] Creating Layer conv4
I1226 14:11:08.617175 98828 net.cpp:612] conv4 <- conv3
I1226 14:11:08.617233 98828 net.cpp:586] conv4 -> conv4
I1226 14:11:08.630059 91711 net.cpp:228] Setting up conv4
I1226 14:11:08.630172 91711 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 14:11:08.630200 91711 net.cpp:243] Memory required for data: 1939044352
I1226 14:11:08.630314 91711 layer_factory.hpp:114] Creating layer relu4
I1226 14:11:08.630391 91711 net.cpp:178] Creating Layer relu4
I1226 14:11:08.630430 91711 net.cpp:612] relu4 <- conv4
I1226 14:11:08.630478 91711 net.cpp:573] relu4 -> conv4 (in-place)
I1226 14:11:08.630555 91711 net.cpp:228] Setting up relu4
I1226 14:11:08.630599 91711 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 14:11:08.630625 91711 net.cpp:243] Memory required for data: 2005497856
I1226 14:11:08.630653 91711 layer_factory.hpp:114] Creating layer conv5
I1226 14:11:08.630719 91711 net.cpp:178] Creating Layer conv5
I1226 14:11:08.630753 91711 net.cpp:612] conv5 <- conv4
I1226 14:11:08.630794 91711 net.cpp:586] conv5 -> conv5
I1226 14:11:08.685717 92274 net.cpp:228] Setting up conv4
I1226 14:11:08.685865 92274 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 14:11:08.685897 92274 net.cpp:243] Memory required for data: 1939044352
I1226 14:11:08.685958 92274 layer_factory.hpp:114] Creating layer relu4
I1226 14:11:08.686105 92274 net.cpp:178] Creating Layer relu4
I1226 14:11:08.686223 92274 net.cpp:612] relu4 <- conv4
I1226 14:11:08.686642 92274 net.cpp:573] relu4 -> conv4 (in-place)
I1226 14:11:08.686882 92274 net.cpp:228] Setting up relu4
I1226 14:11:08.686944 92274 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 14:11:08.686969 92274 net.cpp:243] Memory required for data: 2005497856
I1226 14:11:08.687000 92274 layer_factory.hpp:114] Creating layer conv5
I1226 14:11:08.687126 92274 net.cpp:178] Creating Layer conv5
I1226 14:11:08.687270 92274 net.cpp:612] conv5 <- conv4
I1226 14:11:08.687500 92274 net.cpp:586] conv5 -> conv5
I1226 14:11:08.741083 90185 net.cpp:228] Setting up conv4
I1226 14:11:08.741406 90185 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 14:11:08.741487 90185 net.cpp:243] Memory required for data: 1939044352
I1226 14:11:08.741703 90185 layer_factory.hpp:114] Creating layer relu4
I1226 14:11:08.741974 90185 net.cpp:178] Creating Layer relu4
I1226 14:11:08.742241 90185 net.cpp:612] relu4 <- conv4
I1226 14:11:08.742323 90185 net.cpp:573] relu4 -> conv4 (in-place)
I1226 14:11:08.742548 90185 net.cpp:228] Setting up relu4
I1226 14:11:08.742779 90185 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 14:11:08.743108 90185 net.cpp:243] Memory required for data: 2005497856
I1226 14:11:08.743212 90185 layer_factory.hpp:114] Creating layer conv5
I1226 14:11:08.743455 90185 net.cpp:178] Creating Layer conv5
I1226 14:11:08.743544 90185 net.cpp:612] conv5 <- conv4
I1226 14:11:08.743808 90185 net.cpp:586] conv5 -> conv5
I1226 14:11:08.820268 91711 net.cpp:228] Setting up conv5
I1226 14:11:08.820377 91711 net.cpp:235] Top shape: 256 256 13 13 (11075584)
I1226 14:11:08.820405 91711 net.cpp:243] Memory required for data: 2049800192
I1226 14:11:08.820497 91711 layer_factory.hpp:114] Creating layer relu5
I1226 14:11:08.820569 91711 net.cpp:178] Creating Layer relu5
I1226 14:11:08.820606 91711 net.cpp:612] relu5 <- conv5
I1226 14:11:08.820665 91711 net.cpp:573] relu5 -> conv5 (in-place)
I1226 14:11:08.820752 91711 net.cpp:228] Setting up relu5
I1226 14:11:08.820796 91711 net.cpp:235] Top shape: 256 256 13 13 (11075584)
I1226 14:11:08.820819 91711 net.cpp:243] Memory required for data: 2094102528
I1226 14:11:08.820854 91711 layer_factory.hpp:114] Creating layer pool5
I1226 14:11:08.820925 91711 net.cpp:178] Creating Layer pool5
I1226 14:11:08.820953 91711 net.cpp:612] pool5 <- conv5
I1226 14:11:08.820996 91711 net.cpp:586] pool5 -> pool5
I1226 14:11:08.821087 91711 net.cpp:228] Setting up pool5
I1226 14:11:08.821130 91711 net.cpp:235] Top shape: 256 256 6 6 (2359296)
I1226 14:11:08.821159 91711 net.cpp:243] Memory required for data: 2103539712
I1226 14:11:08.821194 91711 layer_factory.hpp:114] Creating layer fc6
I1226 14:11:08.806841 96952 net.cpp:228] Setting up conv1
I1226 14:11:08.806964 96952 net.cpp:235] Top shape: 256 96 55 55 (74342400)
I1226 14:11:08.806998 96952 net.cpp:243] Memory required for data: 455669760
I1226 14:11:08.807116 96952 layer_factory.hpp:114] Creating layer relu1
I1226 14:11:08.807202 96952 net.cpp:178] Creating Layer relu1
I1226 14:11:08.807246 96952 net.cpp:612] relu1 <- conv1
I1226 14:11:08.807302 96952 net.cpp:573] relu1 -> conv1 (in-place)
I1226 14:11:08.807418 96952 net.cpp:228] Setting up relu1
I1226 14:11:08.807473 96952 net.cpp:235] Top shape: 256 96 55 55 (74342400)
I1226 14:11:08.807499 96952 net.cpp:243] Memory required for data: 753039360
I1226 14:11:08.807529 96952 layer_factory.hpp:114] Creating layer norm1
I1226 14:11:08.807590 96952 net.cpp:178] Creating Layer norm1
I1226 14:11:08.807626 96952 net.cpp:612] norm1 <- conv1
I1226 14:11:08.807689 96952 net.cpp:586] norm1 -> norm1
I1226 14:11:08.839861 91711 net.cpp:178] Creating Layer fc6
I1226 14:11:08.839984 91711 net.cpp:612] fc6 <- pool5
I1226 14:11:08.840062 91711 net.cpp:586] fc6 -> fc6
I1226 14:11:08.822850 96952 net.cpp:228] Setting up norm1
I1226 14:11:08.822968 96952 net.cpp:235] Top shape: 256 96 55 55 (74342400)
I1226 14:11:08.822998 96952 net.cpp:243] Memory required for data: 1050408960
I1226 14:11:08.823035 96952 layer_factory.hpp:114] Creating layer pool1
I1226 14:11:08.823112 96952 net.cpp:178] Creating Layer pool1
I1226 14:11:08.823153 96952 net.cpp:612] pool1 <- norm1
I1226 14:11:08.823212 96952 net.cpp:586] pool1 -> pool1
I1226 14:11:08.823341 96952 net.cpp:228] Setting up pool1
I1226 14:11:08.823393 96952 net.cpp:235] Top shape: 256 96 27 27 (17915904)
I1226 14:11:08.823418 96952 net.cpp:243] Memory required for data: 1122072576
I1226 14:11:08.823447 96952 layer_factory.hpp:114] Creating layer conv2
I1226 14:11:08.823539 96952 net.cpp:178] Creating Layer conv2
I1226 14:11:08.823576 96952 net.cpp:612] conv2 <- pool1
I1226 14:11:08.823618 96952 net.cpp:586] conv2 -> conv2
I1226 14:11:08.851933 91405 net.cpp:228] Setting up conv4
I1226 14:11:08.852052 91405 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 14:11:08.852084 91405 net.cpp:243] Memory required for data: 1939044352
I1226 14:11:08.852147 91405 layer_factory.hpp:114] Creating layer relu4
I1226 14:11:08.852246 91405 net.cpp:178] Creating Layer relu4
I1226 14:11:08.852293 91405 net.cpp:612] relu4 <- conv4
I1226 14:11:08.852345 91405 net.cpp:573] relu4 -> conv4 (in-place)
I1226 14:11:08.852448 91405 net.cpp:228] Setting up relu4
I1226 14:11:08.852494 91405 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 14:11:08.852521 91405 net.cpp:243] Memory required for data: 2005497856
I1226 14:11:08.852552 91405 layer_factory.hpp:114] Creating layer conv5
I1226 14:11:08.852640 91405 net.cpp:178] Creating Layer conv5
I1226 14:11:08.852675 91405 net.cpp:612] conv5 <- conv4
I1226 14:11:08.852751 91405 net.cpp:586] conv5 -> conv5
I1226 14:11:08.880558 95316 net.cpp:228] Setting up conv1
I1226 14:11:08.880677 95316 net.cpp:235] Top shape: 256 96 55 55 (74342400)
I1226 14:11:08.880707 95316 net.cpp:243] Memory required for data: 455669760
I1226 14:11:08.880847 95316 layer_factory.hpp:114] Creating layer relu1
I1226 14:11:08.880934 95316 net.cpp:178] Creating Layer relu1
I1226 14:11:08.880977 95316 net.cpp:612] relu1 <- conv1
I1226 14:11:08.881037 95316 net.cpp:573] relu1 -> conv1 (in-place)
I1226 14:11:08.881150 95316 net.cpp:228] Setting up relu1
I1226 14:11:08.881201 95316 net.cpp:235] Top shape: 256 96 55 55 (74342400)
I1226 14:11:08.881227 95316 net.cpp:243] Memory required for data: 753039360
I1226 14:11:08.881258 95316 layer_factory.hpp:114] Creating layer norm1
I1226 14:11:08.881330 95316 net.cpp:178] Creating Layer norm1
I1226 14:11:08.881357 95316 net.cpp:612] norm1 <- conv1
I1226 14:11:08.881415 95316 net.cpp:586] norm1 -> norm1
I1226 14:11:08.881629 95316 net.cpp:228] Setting up norm1
I1226 14:11:08.881713 95316 net.cpp:235] Top shape: 256 96 55 55 (74342400)
I1226 14:11:08.881738 95316 net.cpp:243] Memory required for data: 1050408960
I1226 14:11:08.881768 95316 layer_factory.hpp:114] Creating layer pool1
I1226 14:11:08.881850 95316 net.cpp:178] Creating Layer pool1
I1226 14:11:08.881889 95316 net.cpp:612] pool1 <- norm1
I1226 14:11:08.881940 95316 net.cpp:586] pool1 -> pool1
I1226 14:11:08.882048 95316 net.cpp:228] Setting up pool1
I1226 14:11:08.882097 95316 net.cpp:235] Top shape: 256 96 27 27 (17915904)
I1226 14:11:08.882120 95316 net.cpp:243] Memory required for data: 1122072576
I1226 14:11:08.882150 95316 layer_factory.hpp:114] Creating layer conv2
I1226 14:11:08.882225 95316 net.cpp:178] Creating Layer conv2
I1226 14:11:08.882259 95316 net.cpp:612] conv2 <- pool1
I1226 14:11:08.882300 95316 net.cpp:586] conv2 -> conv2
I1226 14:11:08.897274 121743 net.cpp:228] Setting up conv4
I1226 14:11:08.897394 121743 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 14:11:08.897439 121743 net.cpp:243] Memory required for data: 1939044352
I1226 14:11:08.897501 121743 layer_factory.hpp:114] Creating layer relu4
I1226 14:11:08.897696 121743 net.cpp:178] Creating Layer relu4
I1226 14:11:08.897814 121743 net.cpp:612] relu4 <- conv4
I1226 14:11:08.897856 121743 net.cpp:573] relu4 -> conv4 (in-place)
I1226 14:11:08.898046 121743 net.cpp:228] Setting up relu4
I1226 14:11:08.899499 121743 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 14:11:08.899588 121743 net.cpp:243] Memory required for data: 2005497856
I1226 14:11:08.899657 121743 layer_factory.hpp:114] Creating layer conv5
I1226 14:11:08.899770 121743 net.cpp:178] Creating Layer conv5
I1226 14:11:08.899873 121743 net.cpp:612] conv5 <- conv4
I1226 14:11:08.899965 121743 net.cpp:586] conv5 -> conv5
I1226 14:11:08.937794 92274 net.cpp:228] Setting up conv5
I1226 14:11:08.937928 92274 net.cpp:235] Top shape: 256 256 13 13 (11075584)
I1226 14:11:08.937961 92274 net.cpp:243] Memory required for data: 2049800192
I1226 14:11:08.938040 92274 layer_factory.hpp:114] Creating layer relu5
I1226 14:11:08.938156 92274 net.cpp:178] Creating Layer relu5
I1226 14:11:08.938205 92274 net.cpp:612] relu5 <- conv5
I1226 14:11:08.938268 92274 net.cpp:573] relu5 -> conv5 (in-place)
I1226 14:11:08.938397 92274 net.cpp:228] Setting up relu5
I1226 14:11:08.938765 92274 net.cpp:235] Top shape: 256 256 13 13 (11075584)
I1226 14:11:08.938853 92274 net.cpp:243] Memory required for data: 2094102528
I1226 14:11:08.938901 92274 layer_factory.hpp:114] Creating layer pool5
I1226 14:11:08.939018 92274 net.cpp:178] Creating Layer pool5
I1226 14:11:08.939118 92274 net.cpp:612] pool5 <- conv5
I1226 14:11:08.939162 92274 net.cpp:586] pool5 -> pool5
I1226 14:11:08.939262 92274 net.cpp:228] Setting up pool5
I1226 14:11:08.939335 92274 net.cpp:235] Top shape: 256 256 6 6 (2359296)
I1226 14:11:08.939384 92274 net.cpp:243] Memory required for data: 2103539712
I1226 14:11:08.939414 92274 layer_factory.hpp:114] Creating layer fc6
I1226 14:11:08.939707 92274 net.cpp:178] Creating Layer fc6
I1226 14:11:08.939743 92274 net.cpp:612] fc6 <- pool5
I1226 14:11:08.939780 92274 net.cpp:586] fc6 -> fc6
I1226 14:11:08.988157 90185 net.cpp:228] Setting up conv5
I1226 14:11:08.988288 90185 net.cpp:235] Top shape: 256 256 13 13 (11075584)
I1226 14:11:08.988314 90185 net.cpp:243] Memory required for data: 2049800192
I1226 14:11:08.988402 90185 layer_factory.hpp:114] Creating layer relu5
I1226 14:11:08.988461 90185 net.cpp:178] Creating Layer relu5
I1226 14:11:08.988497 90185 net.cpp:612] relu5 <- conv5
I1226 14:11:08.988533 90185 net.cpp:573] relu5 -> conv5 (in-place)
I1226 14:11:08.988653 90185 net.cpp:228] Setting up relu5
I1226 14:11:08.988703 90185 net.cpp:235] Top shape: 256 256 13 13 (11075584)
I1226 14:11:08.988731 90185 net.cpp:243] Memory required for data: 2094102528
I1226 14:11:08.988759 90185 layer_factory.hpp:114] Creating layer pool5
I1226 14:11:08.988804 90185 net.cpp:178] Creating Layer pool5
I1226 14:11:08.988832 90185 net.cpp:612] pool5 <- conv5
I1226 14:11:08.988865 90185 net.cpp:586] pool5 -> pool5
I1226 14:11:08.988941 90185 net.cpp:228] Setting up pool5
I1226 14:11:08.988978 90185 net.cpp:235] Top shape: 256 256 6 6 (2359296)
I1226 14:11:08.989003 90185 net.cpp:243] Memory required for data: 2103539712
I1226 14:11:08.989028 90185 layer_factory.hpp:114] Creating layer fc6
I1226 14:11:08.989080 90185 net.cpp:178] Creating Layer fc6
I1226 14:11:08.989110 90185 net.cpp:612] fc6 <- pool5
I1226 14:11:08.989158 90185 net.cpp:586] fc6 -> fc6
I1226 14:11:09.034369 98828 net.cpp:228] Setting up conv4
I1226 14:11:09.034476 98828 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 14:11:09.034510 98828 net.cpp:243] Memory required for data: 1939044352
I1226 14:11:09.034579 98828 layer_factory.hpp:114] Creating layer relu4
I1226 14:11:09.034704 98828 net.cpp:178] Creating Layer relu4
I1226 14:11:09.034765 98828 net.cpp:612] relu4 <- conv4
I1226 14:11:09.034843 98828 net.cpp:573] relu4 -> conv4 (in-place)
I1226 14:11:09.034986 98828 net.cpp:228] Setting up relu4
I1226 14:11:09.035040 98828 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 14:11:09.035092 98828 net.cpp:243] Memory required for data: 2005497856
I1226 14:11:09.035130 98828 layer_factory.hpp:114] Creating layer conv5
I1226 14:11:09.035221 98828 net.cpp:178] Creating Layer conv5
I1226 14:11:09.035267 98828 net.cpp:612] conv5 <- conv4
I1226 14:11:09.035315 98828 net.cpp:586] conv5 -> conv5
I1226 14:11:09.096554 91405 net.cpp:228] Setting up conv5
I1226 14:11:09.096670 91405 net.cpp:235] Top shape: 256 256 13 13 (11075584)
I1226 14:11:09.096698 91405 net.cpp:243] Memory required for data: 2049800192
I1226 14:11:09.096803 91405 layer_factory.hpp:114] Creating layer relu5
I1226 14:11:09.096892 91405 net.cpp:178] Creating Layer relu5
I1226 14:11:09.096942 91405 net.cpp:612] relu5 <- conv5
I1226 14:11:09.096983 91405 net.cpp:573] relu5 -> conv5 (in-place)
I1226 14:11:09.097085 91405 net.cpp:228] Setting up relu5
I1226 14:11:09.097134 91405 net.cpp:235] Top shape: 256 256 13 13 (11075584)
I1226 14:11:09.097164 91405 net.cpp:243] Memory required for data: 2094102528
I1226 14:11:09.097194 91405 layer_factory.hpp:114] Creating layer pool5
I1226 14:11:09.097245 91405 net.cpp:178] Creating Layer pool5
I1226 14:11:09.097273 91405 net.cpp:612] pool5 <- conv5
I1226 14:11:09.097316 91405 net.cpp:586] pool5 -> pool5
I1226 14:11:09.097419 91405 net.cpp:228] Setting up pool5
I1226 14:11:09.097468 91405 net.cpp:235] Top shape: 256 256 6 6 (2359296)
I1226 14:11:09.097491 91405 net.cpp:243] Memory required for data: 2103539712
I1226 14:11:09.097518 91405 layer_factory.hpp:114] Creating layer fc6
I1226 14:11:09.097597 91405 net.cpp:178] Creating Layer fc6
I1226 14:11:09.097635 91405 net.cpp:612] fc6 <- pool5
I1226 14:11:09.097681 91405 net.cpp:586] fc6 -> fc6
I1226 14:11:09.175945 121743 net.cpp:228] Setting up conv5
I1226 14:11:09.176069 121743 net.cpp:235] Top shape: 256 256 13 13 (11075584)
I1226 14:11:09.176097 121743 net.cpp:243] Memory required for data: 2049800192
I1226 14:11:09.176173 121743 layer_factory.hpp:114] Creating layer relu5
I1226 14:11:09.176240 121743 net.cpp:178] Creating Layer relu5
I1226 14:11:09.176277 121743 net.cpp:612] relu5 <- conv5
I1226 14:11:09.176323 121743 net.cpp:573] relu5 -> conv5 (in-place)
I1226 14:11:09.176420 121743 net.cpp:228] Setting up relu5
I1226 14:11:09.176465 121743 net.cpp:235] Top shape: 256 256 13 13 (11075584)
I1226 14:11:09.176487 121743 net.cpp:243] Memory required for data: 2094102528
I1226 14:11:09.176515 121743 layer_factory.hpp:114] Creating layer pool5
I1226 14:11:09.176578 121743 net.cpp:178] Creating Layer pool5
I1226 14:11:09.176627 121743 net.cpp:612] pool5 <- conv5
I1226 14:11:09.176671 121743 net.cpp:586] pool5 -> pool5
I1226 14:11:09.176770 121743 net.cpp:228] Setting up pool5
I1226 14:11:09.176818 121743 net.cpp:235] Top shape: 256 256 6 6 (2359296)
I1226 14:11:09.176846 121743 net.cpp:243] Memory required for data: 2103539712
I1226 14:11:09.176872 121743 layer_factory.hpp:114] Creating layer fc6
I1226 14:11:09.176928 121743 net.cpp:178] Creating Layer fc6
I1226 14:11:09.176961 121743 net.cpp:612] fc6 <- pool5
I1226 14:11:09.177013 121743 net.cpp:586] fc6 -> fc6
I1226 14:11:09.324849 98828 net.cpp:228] Setting up conv5
I1226 14:11:09.324960 98828 net.cpp:235] Top shape: 256 256 13 13 (11075584)
I1226 14:11:09.324987 98828 net.cpp:243] Memory required for data: 2049800192
I1226 14:11:09.325081 98828 layer_factory.hpp:114] Creating layer relu5
I1226 14:11:09.325170 98828 net.cpp:178] Creating Layer relu5
I1226 14:11:09.325211 98828 net.cpp:612] relu5 <- conv5
I1226 14:11:09.325251 98828 net.cpp:573] relu5 -> conv5 (in-place)
I1226 14:11:09.325348 98828 net.cpp:228] Setting up relu5
I1226 14:11:09.325392 98828 net.cpp:235] Top shape: 256 256 13 13 (11075584)
I1226 14:11:09.325417 98828 net.cpp:243] Memory required for data: 2094102528
I1226 14:11:09.325448 98828 layer_factory.hpp:114] Creating layer pool5
I1226 14:11:09.325496 98828 net.cpp:178] Creating Layer pool5
I1226 14:11:09.325523 98828 net.cpp:612] pool5 <- conv5
I1226 14:11:09.325561 98828 net.cpp:586] pool5 -> pool5
I1226 14:11:09.325651 98828 net.cpp:228] Setting up pool5
I1226 14:11:09.325703 98828 net.cpp:235] Top shape: 256 256 6 6 (2359296)
I1226 14:11:09.325727 98828 net.cpp:243] Memory required for data: 2103539712
I1226 14:11:09.325757 98828 layer_factory.hpp:114] Creating layer fc6
I1226 14:11:09.325826 98828 net.cpp:178] Creating Layer fc6
I1226 14:11:09.325858 98828 net.cpp:612] fc6 <- pool5
I1226 14:11:09.325906 98828 net.cpp:586] fc6 -> fc6
I1226 14:11:10.371073 96952 net.cpp:228] Setting up conv2
I1226 14:11:10.371183 96952 net.cpp:235] Top shape: 256 256 27 27 (47775744)
I1226 14:11:10.371213 96952 net.cpp:243] Memory required for data: 1313175552
I1226 14:11:10.371290 96952 layer_factory.hpp:114] Creating layer relu2
I1226 14:11:10.371376 96952 net.cpp:178] Creating Layer relu2
I1226 14:11:10.371417 96952 net.cpp:612] relu2 <- conv2
I1226 14:11:10.371476 96952 net.cpp:573] relu2 -> conv2 (in-place)
I1226 14:11:10.385166 95316 net.cpp:228] Setting up conv2
I1226 14:11:10.385282 95316 net.cpp:235] Top shape: 256 256 27 27 (47775744)
I1226 14:11:10.385315 95316 net.cpp:243] Memory required for data: 1313175552
I1226 14:11:10.385392 95316 layer_factory.hpp:114] Creating layer relu2
I1226 14:11:10.385612 95316 net.cpp:178] Creating Layer relu2
I1226 14:11:10.385660 95316 net.cpp:612] relu2 <- conv2
I1226 14:11:10.385715 95316 net.cpp:573] relu2 -> conv2 (in-place)
I1226 14:11:10.385848 95316 net.cpp:228] Setting up relu2
I1226 14:11:10.385908 95316 net.cpp:235] Top shape: 256 256 27 27 (47775744)
I1226 14:11:10.385936 95316 net.cpp:243] Memory required for data: 1504278528
I1226 14:11:10.385968 95316 layer_factory.hpp:114] Creating layer norm2
I1226 14:11:10.386030 95316 net.cpp:178] Creating Layer norm2
I1226 14:11:10.386065 95316 net.cpp:612] norm2 <- conv2
I1226 14:11:10.386116 95316 net.cpp:586] norm2 -> norm2
I1226 14:11:10.386219 95316 net.cpp:228] Setting up norm2
I1226 14:11:10.386272 95316 net.cpp:235] Top shape: 256 256 27 27 (47775744)
I1226 14:11:10.386298 95316 net.cpp:243] Memory required for data: 1695381504
I1226 14:11:10.386328 95316 layer_factory.hpp:114] Creating layer pool2
I1226 14:11:10.386385 95316 net.cpp:178] Creating Layer pool2
I1226 14:11:10.386421 95316 net.cpp:612] pool2 <- norm2
I1226 14:11:10.386459 95316 net.cpp:586] pool2 -> pool2
I1226 14:11:10.386564 95316 net.cpp:228] Setting up pool2
I1226 14:11:10.386718 95316 net.cpp:235] Top shape: 256 256 13 13 (11075584)
I1226 14:11:10.386749 95316 net.cpp:243] Memory required for data: 1739683840
I1226 14:11:10.386781 95316 layer_factory.hpp:114] Creating layer conv3
I1226 14:11:10.386898 95316 net.cpp:178] Creating Layer conv3
I1226 14:11:10.386943 95316 net.cpp:612] conv3 <- pool2
I1226 14:11:10.386999 95316 net.cpp:586] conv3 -> conv3
I1226 14:11:10.383924 96952 net.cpp:228] Setting up relu2
I1226 14:11:10.384029 96952 net.cpp:235] Top shape: 256 256 27 27 (47775744)
I1226 14:11:10.384057 96952 net.cpp:243] Memory required for data: 1504278528
I1226 14:11:10.384093 96952 layer_factory.hpp:114] Creating layer norm2
I1226 14:11:10.384156 96952 net.cpp:178] Creating Layer norm2
I1226 14:11:10.384197 96952 net.cpp:612] norm2 <- conv2
I1226 14:11:10.384243 96952 net.cpp:586] norm2 -> norm2
I1226 14:11:10.384352 96952 net.cpp:228] Setting up norm2
I1226 14:11:10.384407 96952 net.cpp:235] Top shape: 256 256 27 27 (47775744)
I1226 14:11:10.384430 96952 net.cpp:243] Memory required for data: 1695381504
I1226 14:11:10.384459 96952 layer_factory.hpp:114] Creating layer pool2
I1226 14:11:10.384516 96952 net.cpp:178] Creating Layer pool2
I1226 14:11:10.384652 96952 net.cpp:612] pool2 <- norm2
I1226 14:11:10.384697 96952 net.cpp:586] pool2 -> pool2
I1226 14:11:10.384816 96952 net.cpp:228] Setting up pool2
I1226 14:11:10.384969 96952 net.cpp:235] Top shape: 256 256 13 13 (11075584)
I1226 14:11:10.384999 96952 net.cpp:243] Memory required for data: 1739683840
I1226 14:11:10.385031 96952 layer_factory.hpp:114] Creating layer conv3
I1226 14:11:10.385233 96952 net.cpp:178] Creating Layer conv3
I1226 14:11:10.385319 96952 net.cpp:612] conv3 <- pool2
I1226 14:11:10.385375 96952 net.cpp:586] conv3 -> conv3
I1226 14:11:12.287999 95316 net.cpp:228] Setting up conv3
I1226 14:11:12.288111 95316 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 14:11:12.288142 95316 net.cpp:243] Memory required for data: 1806137344
I1226 14:11:12.288221 95316 layer_factory.hpp:114] Creating layer relu3
I1226 14:11:12.288419 95316 net.cpp:178] Creating Layer relu3
I1226 14:11:12.288457 95316 net.cpp:612] relu3 <- conv3
I1226 14:11:12.288503 95316 net.cpp:573] relu3 -> conv3 (in-place)
I1226 14:11:12.288625 95316 net.cpp:228] Setting up relu3
I1226 14:11:12.288682 95316 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 14:11:12.288708 95316 net.cpp:243] Memory required for data: 1872590848
I1226 14:11:12.288740 95316 layer_factory.hpp:114] Creating layer conv4
I1226 14:11:12.288863 95316 net.cpp:178] Creating Layer conv4
I1226 14:11:12.288909 95316 net.cpp:612] conv4 <- conv3
I1226 14:11:12.288969 95316 net.cpp:586] conv4 -> conv4
I1226 14:11:12.325630 96952 net.cpp:228] Setting up conv3
I1226 14:11:12.326092 96952 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 14:11:12.326273 96952 net.cpp:243] Memory required for data: 1806137344
I1226 14:11:12.326407 96952 layer_factory.hpp:114] Creating layer relu3
I1226 14:11:12.326596 96952 net.cpp:178] Creating Layer relu3
I1226 14:11:12.326769 96952 net.cpp:612] relu3 <- conv3
I1226 14:11:12.326839 96952 net.cpp:573] relu3 -> conv3 (in-place)
I1226 14:11:12.327368 96952 net.cpp:228] Setting up relu3
I1226 14:11:12.327512 96952 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 14:11:12.327687 96952 net.cpp:243] Memory required for data: 1872590848
I1226 14:11:12.327746 96952 layer_factory.hpp:114] Creating layer conv4
I1226 14:11:12.327936 96952 net.cpp:178] Creating Layer conv4
I1226 14:11:12.328089 96952 net.cpp:612] conv4 <- conv3
I1226 14:11:12.328161 96952 net.cpp:586] conv4 -> conv4
I1226 14:11:13.141988 92483 net.cpp:228] Setting up fc6
I1226 14:11:13.142101 92483 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 14:11:13.142130 92483 net.cpp:243] Memory required for data: 2107734016
I1226 14:11:13.142189 92483 layer_factory.hpp:114] Creating layer relu6
I1226 14:11:13.142266 92483 net.cpp:178] Creating Layer relu6
I1226 14:11:13.142386 92483 net.cpp:612] relu6 <- fc6
I1226 14:11:13.142446 92483 net.cpp:573] relu6 -> fc6 (in-place)
I1226 14:11:13.142544 92483 net.cpp:228] Setting up relu6
I1226 14:11:13.142696 92483 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 14:11:13.142727 92483 net.cpp:243] Memory required for data: 2111928320
I1226 14:11:13.142760 92483 layer_factory.hpp:114] Creating layer drop6
I1226 14:11:13.142853 92483 net.cpp:178] Creating Layer drop6
I1226 14:11:13.142885 92483 net.cpp:612] drop6 <- fc6
I1226 14:11:13.142923 92483 net.cpp:573] drop6 -> fc6 (in-place)
I1226 14:11:13.142982 92483 net.cpp:228] Setting up drop6
I1226 14:11:13.143025 92483 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 14:11:13.143049 92483 net.cpp:243] Memory required for data: 2116122624
I1226 14:11:13.143075 92483 layer_factory.hpp:114] Creating layer fc7
I1226 14:11:13.143148 92483 net.cpp:178] Creating Layer fc7
I1226 14:11:13.143185 92483 net.cpp:612] fc7 <- fc6
I1226 14:11:13.143234 92483 net.cpp:586] fc7 -> fc7
I1226 14:11:13.967447 95316 net.cpp:228] Setting up conv4
I1226 14:11:13.967561 95316 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 14:11:13.967591 95316 net.cpp:243] Memory required for data: 1939044352
I1226 14:11:13.967653 95316 layer_factory.hpp:114] Creating layer relu4
I1226 14:11:13.967725 95316 net.cpp:178] Creating Layer relu4
I1226 14:11:13.967766 95316 net.cpp:612] relu4 <- conv4
I1226 14:11:13.967844 95316 net.cpp:573] relu4 -> conv4 (in-place)
I1226 14:11:13.967970 95316 net.cpp:228] Setting up relu4
I1226 14:11:13.968031 95316 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 14:11:13.968060 95316 net.cpp:243] Memory required for data: 2005497856
I1226 14:11:13.968092 95316 layer_factory.hpp:114] Creating layer conv5
I1226 14:11:13.968191 95316 net.cpp:178] Creating Layer conv5
I1226 14:11:13.968230 95316 net.cpp:612] conv5 <- conv4
I1226 14:11:13.968276 95316 net.cpp:586] conv5 -> conv5
I1226 14:11:14.036826 96952 net.cpp:228] Setting up conv4
I1226 14:11:14.036980 96952 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 14:11:14.037017 96952 net.cpp:243] Memory required for data: 1939044352
I1226 14:11:14.037464 96952 layer_factory.hpp:114] Creating layer relu4
I1226 14:11:14.037675 96952 net.cpp:178] Creating Layer relu4
I1226 14:11:14.037888 96952 net.cpp:612] relu4 <- conv4
I1226 14:11:14.037968 96952 net.cpp:573] relu4 -> conv4 (in-place)
I1226 14:11:14.038404 96952 net.cpp:228] Setting up relu4
I1226 14:11:14.038504 96952 net.cpp:235] Top shape: 256 384 13 13 (16613376)
I1226 14:11:14.038589 96952 net.cpp:243] Memory required for data: 2005497856
I1226 14:11:14.038663 96952 layer_factory.hpp:114] Creating layer conv5
I1226 14:11:14.038960 96952 net.cpp:178] Creating Layer conv5
I1226 14:11:14.039471 96952 net.cpp:612] conv5 <- conv4
I1226 14:11:14.039643 96952 net.cpp:586] conv5 -> conv5
I1226 14:11:15.097525 95316 net.cpp:228] Setting up conv5
I1226 14:11:15.097704 95316 net.cpp:235] Top shape: 256 256 13 13 (11075584)
I1226 14:11:15.097748 95316 net.cpp:243] Memory required for data: 2049800192
I1226 14:11:15.097867 95316 layer_factory.hpp:114] Creating layer relu5
I1226 14:11:15.097995 95316 net.cpp:178] Creating Layer relu5
I1226 14:11:15.098062 95316 net.cpp:612] relu5 <- conv5
I1226 14:11:15.098158 95316 net.cpp:573] relu5 -> conv5 (in-place)
I1226 14:11:15.098417 95316 net.cpp:228] Setting up relu5
I1226 14:11:15.098883 95316 net.cpp:235] Top shape: 256 256 13 13 (11075584)
I1226 14:11:15.099035 95316 net.cpp:243] Memory required for data: 2094102528
I1226 14:11:15.099098 95316 layer_factory.hpp:114] Creating layer pool5
I1226 14:11:15.099207 95316 net.cpp:178] Creating Layer pool5
I1226 14:11:15.099339 95316 net.cpp:612] pool5 <- conv5
I1226 14:11:15.099388 95316 net.cpp:586] pool5 -> pool5
I1226 14:11:15.100111 95316 net.cpp:228] Setting up pool5
I1226 14:11:15.100244 95316 net.cpp:235] Top shape: 256 256 6 6 (2359296)
I1226 14:11:15.100275 95316 net.cpp:243] Memory required for data: 2103539712
I1226 14:11:15.100313 95316 layer_factory.hpp:114] Creating layer fc6
I1226 14:11:15.100411 95316 net.cpp:178] Creating Layer fc6
I1226 14:11:15.100535 95316 net.cpp:612] fc6 <- pool5
I1226 14:11:15.100637 95316 net.cpp:586] fc6 -> fc6
I1226 14:11:15.270503 96952 net.cpp:228] Setting up conv5
I1226 14:11:15.270643 96952 net.cpp:235] Top shape: 256 256 13 13 (11075584)
I1226 14:11:15.270673 96952 net.cpp:243] Memory required for data: 2049800192
I1226 14:11:15.270828 96952 layer_factory.hpp:114] Creating layer relu5
I1226 14:11:15.270905 96952 net.cpp:178] Creating Layer relu5
I1226 14:11:15.270946 96952 net.cpp:612] relu5 <- conv5
I1226 14:11:15.270988 96952 net.cpp:573] relu5 -> conv5 (in-place)
I1226 14:11:15.271085 96952 net.cpp:228] Setting up relu5
I1226 14:11:15.271137 96952 net.cpp:235] Top shape: 256 256 13 13 (11075584)
I1226 14:11:15.271162 96952 net.cpp:243] Memory required for data: 2094102528
I1226 14:11:15.271190 96952 layer_factory.hpp:114] Creating layer pool5
I1226 14:11:15.271251 96952 net.cpp:178] Creating Layer pool5
I1226 14:11:15.271286 96952 net.cpp:612] pool5 <- conv5
I1226 14:11:15.271325 96952 net.cpp:586] pool5 -> pool5
I1226 14:11:15.271416 96952 net.cpp:228] Setting up pool5
I1226 14:11:15.271466 96952 net.cpp:235] Top shape: 256 256 6 6 (2359296)
I1226 14:11:15.271491 96952 net.cpp:243] Memory required for data: 2103539712
I1226 14:11:15.271524 96952 layer_factory.hpp:114] Creating layer fc6
I1226 14:11:15.271585 96952 net.cpp:178] Creating Layer fc6
I1226 14:11:15.271620 96952 net.cpp:612] fc6 <- pool5
I1226 14:11:15.271659 96952 net.cpp:586] fc6 -> fc6
I1226 14:11:15.420254 92483 net.cpp:228] Setting up fc7
I1226 14:11:15.420367 92483 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 14:11:15.420397 92483 net.cpp:243] Memory required for data: 2120316928
I1226 14:11:15.420480 92483 layer_factory.hpp:114] Creating layer relu7
I1226 14:11:15.420550 92483 net.cpp:178] Creating Layer relu7
I1226 14:11:15.420591 92483 net.cpp:612] relu7 <- fc7
I1226 14:11:15.420629 92483 net.cpp:573] relu7 -> fc7 (in-place)
I1226 14:11:15.420717 92483 net.cpp:228] Setting up relu7
I1226 14:11:15.420786 92483 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 14:11:15.420843 92483 net.cpp:243] Memory required for data: 2124511232
I1226 14:11:15.420876 92483 layer_factory.hpp:114] Creating layer drop7
I1226 14:11:15.420914 92483 net.cpp:178] Creating Layer drop7
I1226 14:11:15.420943 92483 net.cpp:612] drop7 <- fc7
I1226 14:11:15.420977 92483 net.cpp:573] drop7 -> fc7 (in-place)
I1226 14:11:15.421021 92483 net.cpp:228] Setting up drop7
I1226 14:11:15.421063 92483 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 14:11:15.421087 92483 net.cpp:243] Memory required for data: 2128705536
I1226 14:11:15.421113 92483 layer_factory.hpp:114] Creating layer fc8
I1226 14:11:15.421171 92483 net.cpp:178] Creating Layer fc8
I1226 14:11:15.421201 92483 net.cpp:612] fc8 <- fc7
I1226 14:11:15.421253 92483 net.cpp:586] fc8 -> fc8
I1226 14:11:15.974243 92483 net.cpp:228] Setting up fc8
I1226 14:11:15.974359 92483 net.cpp:235] Top shape: 256 1000 (256000)
I1226 14:11:15.974387 92483 net.cpp:243] Memory required for data: 2129729536
I1226 14:11:15.974467 92483 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 14:11:15.974561 92483 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 14:11:15.974612 92483 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 14:11:15.974717 92483 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 14:11:15.974781 92483 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 14:11:15.974895 92483 net.cpp:228] Setting up fc8_fc8_0_split
I1226 14:11:15.974941 92483 net.cpp:235] Top shape: 256 1000 (256000)
I1226 14:11:15.974980 92483 net.cpp:235] Top shape: 256 1000 (256000)
I1226 14:11:15.975004 92483 net.cpp:243] Memory required for data: 2131777536
I1226 14:11:15.975033 92483 layer_factory.hpp:114] Creating layer accuracy
I1226 14:11:15.975141 92483 net.cpp:178] Creating Layer accuracy
I1226 14:11:15.975173 92483 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 14:11:15.975205 92483 net.cpp:612] accuracy <- label_data_1_split_0
I1226 14:11:15.975261 92483 net.cpp:586] accuracy -> accuracy
I1226 14:11:15.975308 92483 net.cpp:228] Setting up accuracy
I1226 14:11:15.975348 92483 net.cpp:235] Top shape: (1)
I1226 14:11:15.975371 92483 net.cpp:243] Memory required for data: 2131777540
I1226 14:11:15.975396 92483 layer_factory.hpp:114] Creating layer loss
I1226 14:11:15.975450 92483 net.cpp:178] Creating Layer loss
I1226 14:11:15.975476 92483 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 14:11:15.975505 92483 net.cpp:612] loss <- label_data_1_split_1
I1226 14:11:15.975538 92483 net.cpp:586] loss -> loss
I1226 14:11:15.975600 92483 layer_factory.hpp:114] Creating layer loss
I1226 14:11:16.009358 92483 net.cpp:228] Setting up loss
I1226 14:11:16.009471 92483 net.cpp:235] Top shape: (1)
I1226 14:11:16.009641 92483 net.cpp:238]     with loss weight 1
I1226 14:11:16.009786 92483 net.cpp:243] Memory required for data: 2131777544
I1226 14:11:16.009865 92483 net.cpp:305] loss needs backward computation.
I1226 14:11:16.009909 92483 net.cpp:307] accuracy does not need backward computation.
I1226 14:11:16.009961 92483 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 14:11:16.010005 92483 net.cpp:305] fc8 needs backward computation.
I1226 14:11:16.010058 92483 net.cpp:305] drop7 needs backward computation.
I1226 14:11:16.010102 92483 net.cpp:305] relu7 needs backward computation.
I1226 14:11:16.010139 92483 net.cpp:305] fc7 needs backward computation.
I1226 14:11:16.010170 92483 net.cpp:305] drop6 needs backward computation.
I1226 14:11:16.010202 92483 net.cpp:305] relu6 needs backward computation.
I1226 14:11:16.010241 92483 net.cpp:305] fc6 needs backward computation.
I1226 14:11:16.010272 92483 net.cpp:305] pool5 needs backward computation.
I1226 14:11:16.010304 92483 net.cpp:305] relu5 needs backward computation.
I1226 14:11:16.010336 92483 net.cpp:305] conv5 needs backward computation.
I1226 14:11:16.010375 92483 net.cpp:305] relu4 needs backward computation.
I1226 14:11:16.010406 92483 net.cpp:305] conv4 needs backward computation.
I1226 14:11:16.010445 92483 net.cpp:305] relu3 needs backward computation.
I1226 14:11:16.010480 92483 net.cpp:305] conv3 needs backward computation.
I1226 14:11:16.010514 92483 net.cpp:305] pool2 needs backward computation.
I1226 14:11:16.010545 92483 net.cpp:305] norm2 needs backward computation.
I1226 14:11:16.010581 92483 net.cpp:305] relu2 needs backward computation.
I1226 14:11:16.010619 92483 net.cpp:305] conv2 needs backward computation.
I1226 14:11:16.010655 92483 net.cpp:305] pool1 needs backward computation.
I1226 14:11:16.010694 92483 net.cpp:305] norm1 needs backward computation.
I1226 14:11:16.010732 92483 net.cpp:305] relu1 needs backward computation.
I1226 14:11:16.010766 92483 net.cpp:305] conv1 needs backward computation.
I1226 14:11:16.010820 92483 net.cpp:307] label_data_1_split does not need backward computation.
I1226 14:11:16.010862 92483 net.cpp:307] data does not need backward computation.
I1226 14:11:16.010898 92483 net.cpp:349] This network produces output accuracy
I1226 14:11:16.010933 92483 net.cpp:349] This network produces output loss
I1226 14:11:16.011028 92483 net.cpp:363] Network initialization done.
I1226 14:11:16.011477 92483 solver.cpp:107] Solver scaffolding done.
I1226 14:11:16.011694 92483 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 14:11:17.540357 98828 net.cpp:228] Setting up fc6
I1226 14:11:17.540467 98828 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 14:11:17.540498 98828 net.cpp:243] Memory required for data: 2107734016
I1226 14:11:17.540560 98828 layer_factory.hpp:114] Creating layer relu6
I1226 14:11:17.540639 98828 net.cpp:178] Creating Layer relu6
I1226 14:11:17.540680 98828 net.cpp:612] relu6 <- fc6
I1226 14:11:17.540752 98828 net.cpp:573] relu6 -> fc6 (in-place)
I1226 14:11:17.540977 98828 net.cpp:228] Setting up relu6
I1226 14:11:17.541049 98828 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 14:11:17.541110 98828 net.cpp:243] Memory required for data: 2111928320
I1226 14:11:17.541148 98828 layer_factory.hpp:114] Creating layer drop6
I1226 14:11:17.541213 98828 net.cpp:178] Creating Layer drop6
I1226 14:11:17.541249 98828 net.cpp:612] drop6 <- fc6
I1226 14:11:17.541291 98828 net.cpp:573] drop6 -> fc6 (in-place)
I1226 14:11:17.541355 98828 net.cpp:228] Setting up drop6
I1226 14:11:17.541394 98828 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 14:11:17.541419 98828 net.cpp:243] Memory required for data: 2116122624
I1226 14:11:17.541448 98828 layer_factory.hpp:114] Creating layer fc7
I1226 14:11:17.541507 98828 net.cpp:178] Creating Layer fc7
I1226 14:11:17.541535 98828 net.cpp:612] fc7 <- fc6
I1226 14:11:17.541574 98828 net.cpp:586] fc7 -> fc7
I1226 14:11:17.649605 91711 net.cpp:228] Setting up fc6
I1226 14:11:17.649746 91711 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 14:11:17.649778 91711 net.cpp:243] Memory required for data: 2107734016
I1226 14:11:17.649864 91711 layer_factory.hpp:114] Creating layer relu6
I1226 14:11:17.650009 91711 net.cpp:178] Creating Layer relu6
I1226 14:11:17.650049 91711 net.cpp:612] relu6 <- fc6
I1226 14:11:17.650094 91711 net.cpp:573] relu6 -> fc6 (in-place)
I1226 14:11:17.650336 91711 net.cpp:228] Setting up relu6
I1226 14:11:17.650465 91711 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 14:11:17.650493 91711 net.cpp:243] Memory required for data: 2111928320
I1226 14:11:17.650635 91711 layer_factory.hpp:114] Creating layer drop6
I1226 14:11:17.650715 91711 net.cpp:178] Creating Layer drop6
I1226 14:11:17.650754 91711 net.cpp:612] drop6 <- fc6
I1226 14:11:17.650791 91711 net.cpp:573] drop6 -> fc6 (in-place)
I1226 14:11:17.651154 91711 net.cpp:228] Setting up drop6
I1226 14:11:17.651324 91711 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 14:11:17.651357 91711 net.cpp:243] Memory required for data: 2116122624
I1226 14:11:17.651433 91711 layer_factory.hpp:114] Creating layer fc7
I1226 14:11:17.651516 91711 net.cpp:178] Creating Layer fc7
I1226 14:11:17.651577 91711 net.cpp:612] fc7 <- fc6
I1226 14:11:17.651623 91711 net.cpp:586] fc7 -> fc7
I1226 14:11:17.678467 92274 net.cpp:228] Setting up fc6
I1226 14:11:17.678581 92274 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 14:11:17.678611 92274 net.cpp:243] Memory required for data: 2107734016
I1226 14:11:17.678666 92274 layer_factory.hpp:114] Creating layer relu6
I1226 14:11:17.678740 92274 net.cpp:178] Creating Layer relu6
I1226 14:11:17.678772 92274 net.cpp:612] relu6 <- fc6
I1226 14:11:17.678810 92274 net.cpp:573] relu6 -> fc6 (in-place)
I1226 14:11:17.679010 92274 net.cpp:228] Setting up relu6
I1226 14:11:17.679065 92274 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 14:11:17.679090 92274 net.cpp:243] Memory required for data: 2111928320
I1226 14:11:17.679119 92274 layer_factory.hpp:114] Creating layer drop6
I1226 14:11:17.679188 92274 net.cpp:178] Creating Layer drop6
I1226 14:11:17.679222 92274 net.cpp:612] drop6 <- fc6
I1226 14:11:17.679260 92274 net.cpp:573] drop6 -> fc6 (in-place)
I1226 14:11:17.679335 92274 net.cpp:228] Setting up drop6
I1226 14:11:17.679374 92274 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 14:11:17.679397 92274 net.cpp:243] Memory required for data: 2116122624
I1226 14:11:17.679425 92274 layer_factory.hpp:114] Creating layer fc7
I1226 14:11:17.679484 92274 net.cpp:178] Creating Layer fc7
I1226 14:11:17.679517 92274 net.cpp:612] fc7 <- fc6
I1226 14:11:17.679558 92274 net.cpp:586] fc7 -> fc7
I1226 14:11:18.015473 90185 net.cpp:228] Setting up fc6
I1226 14:11:18.015628 90185 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 14:11:18.015656 90185 net.cpp:243] Memory required for data: 2107734016
I1226 14:11:18.015710 90185 layer_factory.hpp:114] Creating layer relu6
I1226 14:11:18.015770 90185 net.cpp:178] Creating Layer relu6
I1226 14:11:18.015807 90185 net.cpp:612] relu6 <- fc6
I1226 14:11:18.015843 90185 net.cpp:573] relu6 -> fc6 (in-place)
I1226 14:11:18.016014 90185 net.cpp:228] Setting up relu6
I1226 14:11:18.016059 90185 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 14:11:18.016083 90185 net.cpp:243] Memory required for data: 2111928320
I1226 14:11:18.016111 90185 layer_factory.hpp:114] Creating layer drop6
I1226 14:11:18.016160 90185 net.cpp:178] Creating Layer drop6
I1226 14:11:18.016185 90185 net.cpp:612] drop6 <- fc6
I1226 14:11:18.016218 90185 net.cpp:573] drop6 -> fc6 (in-place)
I1226 14:11:18.016273 90185 net.cpp:228] Setting up drop6
I1226 14:11:18.016304 90185 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 14:11:18.016327 90185 net.cpp:243] Memory required for data: 2116122624
I1226 14:11:18.016352 90185 layer_factory.hpp:114] Creating layer fc7
I1226 14:11:18.016409 90185 net.cpp:178] Creating Layer fc7
I1226 14:11:18.016434 90185 net.cpp:612] fc7 <- fc6
I1226 14:11:18.016469 90185 net.cpp:586] fc7 -> fc7
I1226 14:11:18.089889 121743 net.cpp:228] Setting up fc6
I1226 14:11:18.089998 121743 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 14:11:18.090021 121743 net.cpp:243] Memory required for data: 2107734016
I1226 14:11:18.090101 121743 layer_factory.hpp:114] Creating layer relu6
I1226 14:11:18.090163 121743 net.cpp:178] Creating Layer relu6
I1226 14:11:18.090301 121743 net.cpp:612] relu6 <- fc6
I1226 14:11:18.090347 121743 net.cpp:573] relu6 -> fc6 (in-place)
I1226 14:11:18.090445 121743 net.cpp:228] Setting up relu6
I1226 14:11:18.090492 121743 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 14:11:18.090515 121743 net.cpp:243] Memory required for data: 2111928320
I1226 14:11:18.090544 121743 layer_factory.hpp:114] Creating layer drop6
I1226 14:11:18.090592 121743 net.cpp:178] Creating Layer drop6
I1226 14:11:18.090646 121743 net.cpp:612] drop6 <- fc6
I1226 14:11:18.090682 121743 net.cpp:573] drop6 -> fc6 (in-place)
I1226 14:11:18.090837 121743 net.cpp:228] Setting up drop6
I1226 14:11:18.090873 121743 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 14:11:18.090903 121743 net.cpp:243] Memory required for data: 2116122624
I1226 14:11:18.090929 121743 layer_factory.hpp:114] Creating layer fc7
I1226 14:11:18.090983 121743 net.cpp:178] Creating Layer fc7
I1226 14:11:18.091013 121743 net.cpp:612] fc7 <- fc6
I1226 14:11:18.091050 121743 net.cpp:586] fc7 -> fc7
I1226 14:11:18.110352 91405 net.cpp:228] Setting up fc6
I1226 14:11:18.110465 91405 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 14:11:18.110493 91405 net.cpp:243] Memory required for data: 2107734016
I1226 14:11:18.110551 91405 layer_factory.hpp:114] Creating layer relu6
I1226 14:11:18.110647 91405 net.cpp:178] Creating Layer relu6
I1226 14:11:18.110790 91405 net.cpp:612] relu6 <- fc6
I1226 14:11:18.110838 91405 net.cpp:573] relu6 -> fc6 (in-place)
I1226 14:11:18.111029 91405 net.cpp:228] Setting up relu6
I1226 14:11:18.111089 91405 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 14:11:18.111115 91405 net.cpp:243] Memory required for data: 2111928320
I1226 14:11:18.111145 91405 layer_factory.hpp:114] Creating layer drop6
I1226 14:11:18.111204 91405 net.cpp:178] Creating Layer drop6
I1226 14:11:18.111233 91405 net.cpp:612] drop6 <- fc6
I1226 14:11:18.111279 91405 net.cpp:573] drop6 -> fc6 (in-place)
I1226 14:11:18.111340 91405 net.cpp:228] Setting up drop6
I1226 14:11:18.111373 91405 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 14:11:18.111397 91405 net.cpp:243] Memory required for data: 2116122624
I1226 14:11:18.111423 91405 layer_factory.hpp:114] Creating layer fc7
I1226 14:11:18.111480 91405 net.cpp:178] Creating Layer fc7
I1226 14:11:18.111505 91405 net.cpp:612] fc7 <- fc6
I1226 14:11:18.111558 91405 net.cpp:586] fc7 -> fc7
I1226 14:11:19.860056 98828 net.cpp:228] Setting up fc7
I1226 14:11:19.860229 98828 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 14:11:19.860260 98828 net.cpp:243] Memory required for data: 2120316928
I1226 14:11:19.860345 98828 layer_factory.hpp:114] Creating layer relu7
I1226 14:11:19.860417 98828 net.cpp:178] Creating Layer relu7
I1226 14:11:19.860457 98828 net.cpp:612] relu7 <- fc7
I1226 14:11:19.860502 98828 net.cpp:573] relu7 -> fc7 (in-place)
I1226 14:11:19.860599 98828 net.cpp:228] Setting up relu7
I1226 14:11:19.860659 98828 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 14:11:19.860690 98828 net.cpp:243] Memory required for data: 2124511232
I1226 14:11:19.860723 98828 layer_factory.hpp:114] Creating layer drop7
I1226 14:11:19.860764 98828 net.cpp:178] Creating Layer drop7
I1226 14:11:19.860791 98828 net.cpp:612] drop7 <- fc7
I1226 14:11:19.860844 98828 net.cpp:573] drop7 -> fc7 (in-place)
I1226 14:11:19.860899 98828 net.cpp:228] Setting up drop7
I1226 14:11:19.860934 98828 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 14:11:19.860955 98828 net.cpp:243] Memory required for data: 2128705536
I1226 14:11:19.860980 98828 layer_factory.hpp:114] Creating layer fc8
I1226 14:11:19.861035 98828 net.cpp:178] Creating Layer fc8
I1226 14:11:19.861111 98828 net.cpp:612] fc8 <- fc7
I1226 14:11:19.861160 98828 net.cpp:586] fc8 -> fc8
I1226 14:11:20.089467 92483 caffe.cpp:376] Configuring multinode setup
I1226 14:11:20.106945 92483 caffe.cpp:386] Starting parameter server in mpi environment
I1226 14:11:20.288198 91711 net.cpp:228] Setting up fc7
I1226 14:11:20.288338 91711 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 14:11:20.288368 91711 net.cpp:243] Memory required for data: 2120316928
I1226 14:11:20.288451 91711 layer_factory.hpp:114] Creating layer relu7
I1226 14:11:20.288530 91711 net.cpp:178] Creating Layer relu7
I1226 14:11:20.288658 91711 net.cpp:612] relu7 <- fc7
I1226 14:11:20.288712 91711 net.cpp:573] relu7 -> fc7 (in-place)
I1226 14:11:20.288802 91711 net.cpp:228] Setting up relu7
I1226 14:11:20.288856 91711 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 14:11:20.288880 91711 net.cpp:243] Memory required for data: 2124511232
I1226 14:11:20.288908 91711 layer_factory.hpp:114] Creating layer drop7
I1226 14:11:20.288946 91711 net.cpp:178] Creating Layer drop7
I1226 14:11:20.288972 91711 net.cpp:612] drop7 <- fc7
I1226 14:11:20.289005 91711 net.cpp:573] drop7 -> fc7 (in-place)
I1226 14:11:20.289052 91711 net.cpp:228] Setting up drop7
I1226 14:11:20.289085 91711 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 14:11:20.289108 91711 net.cpp:243] Memory required for data: 2128705536
I1226 14:11:20.289134 91711 layer_factory.hpp:114] Creating layer fc8
I1226 14:11:20.289196 91711 net.cpp:178] Creating Layer fc8
I1226 14:11:20.289253 91711 net.cpp:612] fc8 <- fc7
I1226 14:11:20.289293 91711 net.cpp:586] fc8 -> fc8
I1226 14:11:20.315302 92274 net.cpp:228] Setting up fc7
I1226 14:11:20.315444 92274 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 14:11:20.315474 92274 net.cpp:243] Memory required for data: 2120316928
I1226 14:11:20.315532 92274 layer_factory.hpp:114] Creating layer relu7
I1226 14:11:20.315600 92274 net.cpp:178] Creating Layer relu7
I1226 14:11:20.315644 92274 net.cpp:612] relu7 <- fc7
I1226 14:11:20.315685 92274 net.cpp:573] relu7 -> fc7 (in-place)
I1226 14:11:20.315786 92274 net.cpp:228] Setting up relu7
I1226 14:11:20.315832 92274 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 14:11:20.315855 92274 net.cpp:243] Memory required for data: 2124511232
I1226 14:11:20.315886 92274 layer_factory.hpp:114] Creating layer drop7
I1226 14:11:20.315928 92274 net.cpp:178] Creating Layer drop7
I1226 14:11:20.315958 92274 net.cpp:612] drop7 <- fc7
I1226 14:11:20.315992 92274 net.cpp:573] drop7 -> fc7 (in-place)
I1226 14:11:20.316037 92274 net.cpp:228] Setting up drop7
I1226 14:11:20.316069 92274 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 14:11:20.316092 92274 net.cpp:243] Memory required for data: 2128705536
I1226 14:11:20.316129 92274 layer_factory.hpp:114] Creating layer fc8
I1226 14:11:20.316195 92274 net.cpp:178] Creating Layer fc8
I1226 14:11:20.316228 92274 net.cpp:612] fc8 <- fc7
I1226 14:11:20.316265 92274 net.cpp:586] fc8 -> fc8
I1226 14:11:20.366427 121743 net.cpp:228] Setting up fc7
I1226 14:11:20.366538 121743 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 14:11:20.366564 121743 net.cpp:243] Memory required for data: 2120316928
I1226 14:11:20.366649 121743 layer_factory.hpp:114] Creating layer relu7
I1226 14:11:20.366734 121743 net.cpp:178] Creating Layer relu7
I1226 14:11:20.366829 121743 net.cpp:612] relu7 <- fc7
I1226 14:11:20.366869 121743 net.cpp:573] relu7 -> fc7 (in-place)
I1226 14:11:20.366953 121743 net.cpp:228] Setting up relu7
I1226 14:11:20.366999 121743 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 14:11:20.367022 121743 net.cpp:243] Memory required for data: 2124511232
I1226 14:11:20.367051 121743 layer_factory.hpp:114] Creating layer drop7
I1226 14:11:20.367089 121743 net.cpp:178] Creating Layer drop7
I1226 14:11:20.367112 121743 net.cpp:612] drop7 <- fc7
I1226 14:11:20.367144 121743 net.cpp:573] drop7 -> fc7 (in-place)
I1226 14:11:20.367190 121743 net.cpp:228] Setting up drop7
I1226 14:11:20.367233 121743 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 14:11:20.367254 121743 net.cpp:243] Memory required for data: 2128705536
I1226 14:11:20.367280 121743 layer_factory.hpp:114] Creating layer fc8
I1226 14:11:20.367337 121743 net.cpp:178] Creating Layer fc8
I1226 14:11:20.367368 121743 net.cpp:612] fc8 <- fc7
I1226 14:11:20.367418 121743 net.cpp:586] fc8 -> fc8
I1226 14:11:20.390130 90185 net.cpp:228] Setting up fc7
I1226 14:11:20.390244 90185 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 14:11:20.390272 90185 net.cpp:243] Memory required for data: 2120316928
I1226 14:11:20.390328 90185 layer_factory.hpp:114] Creating layer relu7
I1226 14:11:20.390419 90185 net.cpp:178] Creating Layer relu7
I1226 14:11:20.390524 90185 net.cpp:612] relu7 <- fc7
I1226 14:11:20.390594 90185 net.cpp:573] relu7 -> fc7 (in-place)
I1226 14:11:20.390694 90185 net.cpp:228] Setting up relu7
I1226 14:11:20.390748 90185 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 14:11:20.390771 90185 net.cpp:243] Memory required for data: 2124511232
I1226 14:11:20.390801 90185 layer_factory.hpp:114] Creating layer drop7
I1226 14:11:20.390844 90185 net.cpp:178] Creating Layer drop7
I1226 14:11:20.390877 90185 net.cpp:612] drop7 <- fc7
I1226 14:11:20.390919 90185 net.cpp:573] drop7 -> fc7 (in-place)
I1226 14:11:20.390970 90185 net.cpp:228] Setting up drop7
I1226 14:11:20.391008 90185 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 14:11:20.391031 90185 net.cpp:243] Memory required for data: 2128705536
I1226 14:11:20.391057 90185 layer_factory.hpp:114] Creating layer fc8
I1226 14:11:20.391110 90185 net.cpp:178] Creating Layer fc8
I1226 14:11:20.391144 90185 net.cpp:612] fc8 <- fc7
I1226 14:11:20.391196 90185 net.cpp:586] fc8 -> fc8
I1226 14:11:20.401304 91405 net.cpp:228] Setting up fc7
I1226 14:11:20.401418 91405 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 14:11:20.401448 91405 net.cpp:243] Memory required for data: 2120316928
I1226 14:11:20.401535 91405 layer_factory.hpp:114] Creating layer relu7
I1226 14:11:20.401619 91405 net.cpp:178] Creating Layer relu7
I1226 14:11:20.401664 91405 net.cpp:612] relu7 <- fc7
I1226 14:11:20.401729 91405 net.cpp:573] relu7 -> fc7 (in-place)
I1226 14:11:20.401831 91405 net.cpp:228] Setting up relu7
I1226 14:11:20.401890 91405 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 14:11:20.401913 91405 net.cpp:243] Memory required for data: 2124511232
I1226 14:11:20.401944 91405 layer_factory.hpp:114] Creating layer drop7
I1226 14:11:20.401999 91405 net.cpp:178] Creating Layer drop7
I1226 14:11:20.402039 91405 net.cpp:612] drop7 <- fc7
I1226 14:11:20.402076 91405 net.cpp:573] drop7 -> fc7 (in-place)
I1226 14:11:20.402125 91405 net.cpp:228] Setting up drop7
I1226 14:11:20.402168 91405 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 14:11:20.402191 91405 net.cpp:243] Memory required for data: 2128705536
I1226 14:11:20.402217 91405 layer_factory.hpp:114] Creating layer fc8
I1226 14:11:20.402272 91405 net.cpp:178] Creating Layer fc8
I1226 14:11:20.402304 91405 net.cpp:612] fc8 <- fc7
I1226 14:11:20.402344 91405 net.cpp:586] fc8 -> fc8
I1226 14:11:20.427166 98828 net.cpp:228] Setting up fc8
I1226 14:11:20.427283 98828 net.cpp:235] Top shape: 256 1000 (256000)
I1226 14:11:20.427310 98828 net.cpp:243] Memory required for data: 2129729536
I1226 14:11:20.427371 98828 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 14:11:20.427444 98828 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 14:11:20.427484 98828 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 14:11:20.427551 98828 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 14:11:20.427613 98828 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 14:11:20.427705 98828 net.cpp:228] Setting up fc8_fc8_0_split
I1226 14:11:20.427753 98828 net.cpp:235] Top shape: 256 1000 (256000)
I1226 14:11:20.427785 98828 net.cpp:235] Top shape: 256 1000 (256000)
I1226 14:11:20.427809 98828 net.cpp:243] Memory required for data: 2131777536
I1226 14:11:20.427839 98828 layer_factory.hpp:114] Creating layer accuracy
I1226 14:11:20.427894 98828 net.cpp:178] Creating Layer accuracy
I1226 14:11:20.427923 98828 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 14:11:20.427955 98828 net.cpp:612] accuracy <- label_data_1_split_0
I1226 14:11:20.427992 98828 net.cpp:586] accuracy -> accuracy
I1226 14:11:20.428040 98828 net.cpp:228] Setting up accuracy
I1226 14:11:20.428099 98828 net.cpp:235] Top shape: (1)
I1226 14:11:20.428128 98828 net.cpp:243] Memory required for data: 2131777540
I1226 14:11:20.428158 98828 layer_factory.hpp:114] Creating layer loss
I1226 14:11:20.428225 98828 net.cpp:178] Creating Layer loss
I1226 14:11:20.428256 98828 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 14:11:20.428288 98828 net.cpp:612] loss <- label_data_1_split_1
I1226 14:11:20.428324 98828 net.cpp:586] loss -> loss
I1226 14:11:20.428390 98828 layer_factory.hpp:114] Creating layer loss
I1226 14:11:20.473541 98828 net.cpp:228] Setting up loss
I1226 14:11:20.473743 98828 net.cpp:235] Top shape: (1)
I1226 14:11:20.473783 98828 net.cpp:238]     with loss weight 1
I1226 14:11:20.473927 98828 net.cpp:243] Memory required for data: 2131777544
I1226 14:11:20.473978 98828 net.cpp:305] loss needs backward computation.
I1226 14:11:20.474021 98828 net.cpp:307] accuracy does not need backward computation.
I1226 14:11:20.474056 98828 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 14:11:20.474125 98828 net.cpp:305] fc8 needs backward computation.
I1226 14:11:20.474158 98828 net.cpp:305] drop7 needs backward computation.
I1226 14:11:20.474185 98828 net.cpp:305] relu7 needs backward computation.
I1226 14:11:20.474212 98828 net.cpp:305] fc7 needs backward computation.
I1226 14:11:20.474241 98828 net.cpp:305] drop6 needs backward computation.
I1226 14:11:20.474267 98828 net.cpp:305] relu6 needs backward computation.
I1226 14:11:20.474293 98828 net.cpp:305] fc6 needs backward computation.
I1226 14:11:20.474321 98828 net.cpp:305] pool5 needs backward computation.
I1226 14:11:20.474351 98828 net.cpp:305] relu5 needs backward computation.
I1226 14:11:20.474381 98828 net.cpp:305] conv5 needs backward computation.
I1226 14:11:20.474414 98828 net.cpp:305] relu4 needs backward computation.
I1226 14:11:20.474447 98828 net.cpp:305] conv4 needs backward computation.
I1226 14:11:20.474481 98828 net.cpp:305] relu3 needs backward computation.
I1226 14:11:20.474514 98828 net.cpp:305] conv3 needs backward computation.
I1226 14:11:20.474548 98828 net.cpp:305] pool2 needs backward computation.
I1226 14:11:20.474581 98828 net.cpp:305] norm2 needs backward computation.
I1226 14:11:20.474613 98828 net.cpp:305] relu2 needs backward computation.
I1226 14:11:20.474644 98828 net.cpp:305] conv2 needs backward computation.
I1226 14:11:20.474678 98828 net.cpp:305] pool1 needs backward computation.
I1226 14:11:20.474707 98828 net.cpp:305] norm1 needs backward computation.
I1226 14:11:20.474740 98828 net.cpp:305] relu1 needs backward computation.
I1226 14:11:20.474771 98828 net.cpp:305] conv1 needs backward computation.
I1226 14:11:20.474805 98828 net.cpp:307] label_data_1_split does not need backward computation.
I1226 14:11:20.474839 98828 net.cpp:307] data does not need backward computation.
I1226 14:11:20.474867 98828 net.cpp:349] This network produces output accuracy
I1226 14:11:20.474903 98828 net.cpp:349] This network produces output loss
I1226 14:11:20.475010 98828 net.cpp:363] Network initialization done.
I1226 14:11:20.475504 98828 solver.cpp:107] Solver scaffolding done.
I1226 14:11:20.475730 98828 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 14:11:20.852887 91711 net.cpp:228] Setting up fc8
I1226 14:11:20.853003 91711 net.cpp:235] Top shape: 256 1000 (256000)
I1226 14:11:20.853036 91711 net.cpp:243] Memory required for data: 2129729536
I1226 14:11:20.853096 91711 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 14:11:20.853168 91711 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 14:11:20.853201 91711 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 14:11:20.853289 91711 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 14:11:20.853353 91711 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 14:11:20.853440 91711 net.cpp:228] Setting up fc8_fc8_0_split
I1226 14:11:20.853493 91711 net.cpp:235] Top shape: 256 1000 (256000)
I1226 14:11:20.853523 91711 net.cpp:235] Top shape: 256 1000 (256000)
I1226 14:11:20.853546 91711 net.cpp:243] Memory required for data: 2131777536
I1226 14:11:20.853576 91711 layer_factory.hpp:114] Creating layer accuracy
I1226 14:11:20.853637 91711 net.cpp:178] Creating Layer accuracy
I1226 14:11:20.853670 91711 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 14:11:20.853700 91711 net.cpp:612] accuracy <- label_data_1_split_0
I1226 14:11:20.853734 91711 net.cpp:586] accuracy -> accuracy
I1226 14:11:20.853785 91711 net.cpp:228] Setting up accuracy
I1226 14:11:20.853823 91711 net.cpp:235] Top shape: (1)
I1226 14:11:20.853845 91711 net.cpp:243] Memory required for data: 2131777540
I1226 14:11:20.853873 91711 layer_factory.hpp:114] Creating layer loss
I1226 14:11:20.853935 91711 net.cpp:178] Creating Layer loss
I1226 14:11:20.853961 91711 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 14:11:20.853989 91711 net.cpp:612] loss <- label_data_1_split_1
I1226 14:11:20.854027 91711 net.cpp:586] loss -> loss
I1226 14:11:20.854089 91711 layer_factory.hpp:114] Creating layer loss
I1226 14:11:20.880775 91711 net.cpp:228] Setting up loss
I1226 14:11:20.880983 91711 net.cpp:235] Top shape: (1)
I1226 14:11:20.881039 91711 net.cpp:238]     with loss weight 1
I1226 14:11:20.881187 91711 net.cpp:243] Memory required for data: 2131777544
I1226 14:11:20.881284 91711 net.cpp:305] loss needs backward computation.
I1226 14:11:20.881326 91711 net.cpp:307] accuracy does not need backward computation.
I1226 14:11:20.881361 91711 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 14:11:20.881393 91711 net.cpp:305] fc8 needs backward computation.
I1226 14:11:20.881424 91711 net.cpp:305] drop7 needs backward computation.
I1226 14:11:20.881454 91711 net.cpp:305] relu7 needs backward computation.
I1226 14:11:20.881484 91711 net.cpp:305] fc7 needs backward computation.
I1226 14:11:20.881532 91711 net.cpp:305] drop6 needs backward computation.
I1226 14:11:20.881563 91711 net.cpp:305] relu6 needs backward computation.
I1226 14:11:20.881592 91711 net.cpp:305] fc6 needs backward computation.
I1226 14:11:20.881625 91711 net.cpp:305] pool5 needs backward computation.
I1226 14:11:20.881656 91711 net.cpp:305] relu5 needs backward computation.
I1226 14:11:20.881696 91711 net.cpp:305] conv5 needs backward computation.
I1226 14:11:20.881728 91711 net.cpp:305] relu4 needs backward computation.
I1226 14:11:20.881767 91711 net.cpp:305] conv4 needs backward computation.
I1226 14:11:20.881799 91711 net.cpp:305] relu3 needs backward computation.
I1226 14:11:20.881829 91711 net.cpp:305] conv3 needs backward computation.
I1226 14:11:20.881860 91711 net.cpp:305] pool2 needs backward computation.
I1226 14:11:20.881892 91711 net.cpp:305] norm2 needs backward computation.
I1226 14:11:20.881933 91711 net.cpp:305] relu2 needs backward computation.
I1226 14:11:20.881963 91711 net.cpp:305] conv2 needs backward computation.
I1226 14:11:20.882004 91711 net.cpp:305] pool1 needs backward computation.
I1226 14:11:20.882042 91711 net.cpp:305] norm1 needs backward computation.
I1226 14:11:20.882073 91711 net.cpp:305] relu1 needs backward computation.
I1226 14:11:20.882109 91711 net.cpp:305] conv1 needs backward computation.
I1226 14:11:20.882148 91711 net.cpp:307] label_data_1_split does not need backward computation.
I1226 14:11:20.882189 91711 net.cpp:307] data does not need backward computation.
I1226 14:11:20.882247 91711 net.cpp:349] This network produces output accuracy
I1226 14:11:20.882293 91711 net.cpp:349] This network produces output loss
I1226 14:11:20.882400 91711 net.cpp:363] Network initialization done.
I1226 14:11:20.882855 91711 solver.cpp:107] Solver scaffolding done.
I1226 14:11:20.883071 91711 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 14:11:20.881296 92274 net.cpp:228] Setting up fc8
I1226 14:11:20.881434 92274 net.cpp:235] Top shape: 256 1000 (256000)
I1226 14:11:20.881463 92274 net.cpp:243] Memory required for data: 2129729536
I1226 14:11:20.881520 92274 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 14:11:20.881589 92274 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 14:11:20.881697 92274 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 14:11:20.881750 92274 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 14:11:20.881805 92274 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 14:11:20.881883 92274 net.cpp:228] Setting up fc8_fc8_0_split
I1226 14:11:20.881924 92274 net.cpp:235] Top shape: 256 1000 (256000)
I1226 14:11:20.881963 92274 net.cpp:235] Top shape: 256 1000 (256000)
I1226 14:11:20.881990 92274 net.cpp:243] Memory required for data: 2131777536
I1226 14:11:20.882019 92274 layer_factory.hpp:114] Creating layer accuracy
I1226 14:11:20.882067 92274 net.cpp:178] Creating Layer accuracy
I1226 14:11:20.882093 92274 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 14:11:20.882123 92274 net.cpp:612] accuracy <- label_data_1_split_0
I1226 14:11:20.882155 92274 net.cpp:586] accuracy -> accuracy
I1226 14:11:20.882282 92274 net.cpp:228] Setting up accuracy
I1226 14:11:20.882336 92274 net.cpp:235] Top shape: (1)
I1226 14:11:20.882362 92274 net.cpp:243] Memory required for data: 2131777540
I1226 14:11:20.882390 92274 layer_factory.hpp:114] Creating layer loss
I1226 14:11:20.882441 92274 net.cpp:178] Creating Layer loss
I1226 14:11:20.882477 92274 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 14:11:20.882506 92274 net.cpp:612] loss <- label_data_1_split_1
I1226 14:11:20.882539 92274 net.cpp:586] loss -> loss
I1226 14:11:20.882596 92274 layer_factory.hpp:114] Creating layer loss
I1226 14:11:20.914922 92274 net.cpp:228] Setting up loss
I1226 14:11:20.915119 92274 net.cpp:235] Top shape: (1)
I1226 14:11:20.915205 92274 net.cpp:238]     with loss weight 1
I1226 14:11:20.915454 92274 net.cpp:243] Memory required for data: 2131777544
I1226 14:11:20.915503 92274 net.cpp:305] loss needs backward computation.
I1226 14:11:20.915565 92274 net.cpp:307] accuracy does not need backward computation.
I1226 14:11:20.915609 92274 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 14:11:20.915644 92274 net.cpp:305] fc8 needs backward computation.
I1226 14:11:20.915678 92274 net.cpp:305] drop7 needs backward computation.
I1226 14:11:20.915707 92274 net.cpp:305] relu7 needs backward computation.
I1226 14:11:20.915758 92274 net.cpp:305] fc7 needs backward computation.
I1226 14:11:20.915797 92274 net.cpp:305] drop6 needs backward computation.
I1226 14:11:20.915859 92274 net.cpp:305] relu6 needs backward computation.
I1226 14:11:20.916040 92274 net.cpp:305] fc6 needs backward computation.
I1226 14:11:20.916075 92274 net.cpp:305] pool5 needs backward computation.
I1226 14:11:20.916107 92274 net.cpp:305] relu5 needs backward computation.
I1226 14:11:20.916168 92274 net.cpp:305] conv5 needs backward computation.
I1226 14:11:20.916203 92274 net.cpp:305] relu4 needs backward computation.
I1226 14:11:20.916440 92274 net.cpp:305] conv4 needs backward computation.
I1226 14:11:20.916476 92274 net.cpp:305] relu3 needs backward computation.
I1226 14:11:20.916504 92274 net.cpp:305] conv3 needs backward computation.
I1226 14:11:20.916538 92274 net.cpp:305] pool2 needs backward computation.
I1226 14:11:20.916569 92274 net.cpp:305] norm2 needs backward computation.
I1226 14:11:20.916601 92274 net.cpp:305] relu2 needs backward computation.
I1226 14:11:20.916811 92274 net.cpp:305] conv2 needs backward computation.
I1226 14:11:20.916843 92274 net.cpp:305] pool1 needs backward computation.
I1226 14:11:20.916874 92274 net.cpp:305] norm1 needs backward computation.
I1226 14:11:20.916905 92274 net.cpp:305] relu1 needs backward computation.
I1226 14:11:20.916934 92274 net.cpp:305] conv1 needs backward computation.
I1226 14:11:20.916966 92274 net.cpp:307] label_data_1_split does not need backward computation.
I1226 14:11:20.916999 92274 net.cpp:307] data does not need backward computation.
I1226 14:11:20.917035 92274 net.cpp:349] This network produces output accuracy
I1226 14:11:20.917254 92274 net.cpp:349] This network produces output loss
I1226 14:11:20.917374 92274 net.cpp:363] Network initialization done.
I1226 14:11:20.917881 92274 solver.cpp:107] Solver scaffolding done.
I1226 14:11:20.918144 92274 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 14:11:20.929810 121743 net.cpp:228] Setting up fc8
I1226 14:11:20.929920 121743 net.cpp:235] Top shape: 256 1000 (256000)
I1226 14:11:20.929947 121743 net.cpp:243] Memory required for data: 2129729536
I1226 14:11:20.930001 121743 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 14:11:20.930073 121743 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 14:11:20.930102 121743 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 14:11:20.930140 121743 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 14:11:20.930193 121743 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 14:11:20.930290 121743 net.cpp:228] Setting up fc8_fc8_0_split
I1226 14:11:20.930346 121743 net.cpp:235] Top shape: 256 1000 (256000)
I1226 14:11:20.930375 121743 net.cpp:235] Top shape: 256 1000 (256000)
I1226 14:11:20.930397 121743 net.cpp:243] Memory required for data: 2131777536
I1226 14:11:20.930424 121743 layer_factory.hpp:114] Creating layer accuracy
I1226 14:11:20.930483 121743 net.cpp:178] Creating Layer accuracy
I1226 14:11:20.930515 121743 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 14:11:20.930546 121743 net.cpp:612] accuracy <- label_data_1_split_0
I1226 14:11:20.930578 121743 net.cpp:586] accuracy -> accuracy
I1226 14:11:20.930647 121743 net.cpp:228] Setting up accuracy
I1226 14:11:20.930702 121743 net.cpp:235] Top shape: (1)
I1226 14:11:20.930727 121743 net.cpp:243] Memory required for data: 2131777540
I1226 14:11:20.930766 121743 layer_factory.hpp:114] Creating layer loss
I1226 14:11:20.930811 121743 net.cpp:178] Creating Layer loss
I1226 14:11:20.930843 121743 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 14:11:20.930871 121743 net.cpp:612] loss <- label_data_1_split_1
I1226 14:11:20.931010 121743 net.cpp:586] loss -> loss
I1226 14:11:20.931090 121743 layer_factory.hpp:114] Creating layer loss
I1226 14:11:20.950422 90185 net.cpp:228] Setting up fc8
I1226 14:11:20.950544 90185 net.cpp:235] Top shape: 256 1000 (256000)
I1226 14:11:20.950608 90185 net.cpp:243] Memory required for data: 2129729536
I1226 14:11:20.950669 90185 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 14:11:20.950729 90185 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 14:11:20.950776 90185 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 14:11:20.950819 90185 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 14:11:20.950871 90185 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 14:11:20.950964 90185 net.cpp:228] Setting up fc8_fc8_0_split
I1226 14:11:20.951010 90185 net.cpp:235] Top shape: 256 1000 (256000)
I1226 14:11:20.951040 90185 net.cpp:235] Top shape: 256 1000 (256000)
I1226 14:11:20.951062 90185 net.cpp:243] Memory required for data: 2131777536
I1226 14:11:20.951098 90185 layer_factory.hpp:114] Creating layer accuracy
I1226 14:11:20.951169 90185 net.cpp:178] Creating Layer accuracy
I1226 14:11:20.951203 90185 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 14:11:20.951238 90185 net.cpp:612] accuracy <- label_data_1_split_0
I1226 14:11:20.951277 90185 net.cpp:586] accuracy -> accuracy
I1226 14:11:20.951323 90185 net.cpp:228] Setting up accuracy
I1226 14:11:20.951359 90185 net.cpp:235] Top shape: (1)
I1226 14:11:20.951386 90185 net.cpp:243] Memory required for data: 2131777540
I1226 14:11:20.951419 90185 layer_factory.hpp:114] Creating layer loss
I1226 14:11:20.951472 90185 net.cpp:178] Creating Layer loss
I1226 14:11:20.951496 90185 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 14:11:20.951524 90185 net.cpp:612] loss <- label_data_1_split_1
I1226 14:11:20.951589 90185 net.cpp:586] loss -> loss
I1226 14:11:20.951660 90185 layer_factory.hpp:114] Creating layer loss
I1226 14:11:20.966327 121743 net.cpp:228] Setting up loss
I1226 14:11:20.966439 121743 net.cpp:235] Top shape: (1)
I1226 14:11:20.966477 121743 net.cpp:238]     with loss weight 1
I1226 14:11:20.966648 121743 net.cpp:243] Memory required for data: 2131777544
I1226 14:11:20.966699 121743 net.cpp:305] loss needs backward computation.
I1226 14:11:20.966742 121743 net.cpp:307] accuracy does not need backward computation.
I1226 14:11:20.966943 121743 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 14:11:20.966976 121743 net.cpp:305] fc8 needs backward computation.
I1226 14:11:20.967010 121743 net.cpp:305] drop7 needs backward computation.
I1226 14:11:20.967048 121743 net.cpp:305] relu7 needs backward computation.
I1226 14:11:20.967078 121743 net.cpp:305] fc7 needs backward computation.
I1226 14:11:20.967108 121743 net.cpp:305] drop6 needs backward computation.
I1226 14:11:20.967141 121743 net.cpp:305] relu6 needs backward computation.
I1226 14:11:20.967171 121743 net.cpp:305] fc6 needs backward computation.
I1226 14:11:20.967201 121743 net.cpp:305] pool5 needs backward computation.
I1226 14:11:20.967232 121743 net.cpp:305] relu5 needs backward computation.
I1226 14:11:20.967270 121743 net.cpp:305] conv5 needs backward computation.
I1226 14:11:20.967303 121743 net.cpp:305] relu4 needs backward computation.
I1226 14:11:20.967331 121743 net.cpp:305] conv4 needs backward computation.
I1226 14:11:20.967362 121743 net.cpp:305] relu3 needs backward computation.
I1226 14:11:20.967392 121743 net.cpp:305] conv3 needs backward computation.
I1226 14:11:20.967423 121743 net.cpp:305] pool2 needs backward computation.
I1226 14:11:20.967453 121743 net.cpp:305] norm2 needs backward computation.
I1226 14:11:20.967492 121743 net.cpp:305] relu2 needs backward computation.
I1226 14:11:20.967521 121743 net.cpp:305] conv2 needs backward computation.
I1226 14:11:20.967561 121743 net.cpp:305] pool1 needs backward computation.
I1226 14:11:20.967592 121743 net.cpp:305] norm1 needs backward computation.
I1226 14:11:20.966480 91405 net.cpp:228] Setting up fc8
I1226 14:11:20.967650 121743 net.cpp:305] relu1 needs backward computation.
I1226 14:11:20.966596 91405 net.cpp:235] Top shape: 256 1000 (256000)
I1226 14:11:20.966627 91405 net.cpp:243] Memory required for data: 2129729536
I1226 14:11:20.967679 121743 net.cpp:305] conv1 needs backward computation.
I1226 14:11:20.966686 91405 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 14:11:20.967712 121743 net.cpp:307] label_data_1_split does not need backward computation.
I1226 14:11:20.966791 91405 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 14:11:20.967743 121743 net.cpp:307] data does not need backward computation.
I1226 14:11:20.966836 91405 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 14:11:20.966897 91405 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 14:11:20.967777 121743 net.cpp:349] This network produces output accuracy
I1226 14:11:20.967813 121743 net.cpp:349] This network produces output loss
I1226 14:11:20.967919 121743 net.cpp:363] Network initialization done.
I1226 14:11:20.968374 121743 solver.cpp:107] Solver scaffolding done.
I1226 14:11:20.966951 91405 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 14:11:20.967036 91405 net.cpp:228] Setting up fc8_fc8_0_split
I1226 14:11:20.967087 91405 net.cpp:235] Top shape: 256 1000 (256000)
I1226 14:11:20.968591 121743 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 14:11:20.967124 91405 net.cpp:235] Top shape: 256 1000 (256000)
I1226 14:11:20.967147 91405 net.cpp:243] Memory required for data: 2131777536
I1226 14:11:20.967176 91405 layer_factory.hpp:114] Creating layer accuracy
I1226 14:11:20.967229 91405 net.cpp:178] Creating Layer accuracy
I1226 14:11:20.967345 91405 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 14:11:20.967377 91405 net.cpp:612] accuracy <- label_data_1_split_0
I1226 14:11:20.967416 91405 net.cpp:586] accuracy -> accuracy
I1226 14:11:20.967461 91405 net.cpp:228] Setting up accuracy
I1226 14:11:20.967504 91405 net.cpp:235] Top shape: (1)
I1226 14:11:20.967528 91405 net.cpp:243] Memory required for data: 2131777540
I1226 14:11:20.967557 91405 layer_factory.hpp:114] Creating layer loss
I1226 14:11:20.967607 91405 net.cpp:178] Creating Layer loss
I1226 14:11:20.967638 91405 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 14:11:20.967667 91405 net.cpp:612] loss <- label_data_1_split_1
I1226 14:11:20.967700 91405 net.cpp:586] loss -> loss
I1226 14:11:20.967797 91405 layer_factory.hpp:114] Creating layer loss
I1226 14:11:20.986871 90185 net.cpp:228] Setting up loss
I1226 14:11:20.987078 90185 net.cpp:235] Top shape: (1)
I1226 14:11:20.987130 90185 net.cpp:238]     with loss weight 1
I1226 14:11:20.987284 90185 net.cpp:243] Memory required for data: 2131777544
I1226 14:11:20.987336 90185 net.cpp:305] loss needs backward computation.
I1226 14:11:20.987388 90185 net.cpp:307] accuracy does not need backward computation.
I1226 14:11:20.987423 90185 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 14:11:20.987457 90185 net.cpp:305] fc8 needs backward computation.
I1226 14:11:20.987489 90185 net.cpp:305] drop7 needs backward computation.
I1226 14:11:20.987519 90185 net.cpp:305] relu7 needs backward computation.
I1226 14:11:20.987548 90185 net.cpp:305] fc7 needs backward computation.
I1226 14:11:20.987609 90185 net.cpp:305] drop6 needs backward computation.
I1226 14:11:20.987638 90185 net.cpp:305] relu6 needs backward computation.
I1226 14:11:20.987666 90185 net.cpp:305] fc6 needs backward computation.
I1226 14:11:20.987694 90185 net.cpp:305] pool5 needs backward computation.
I1226 14:11:20.987722 90185 net.cpp:305] relu5 needs backward computation.
I1226 14:11:20.987751 90185 net.cpp:305] conv5 needs backward computation.
I1226 14:11:20.987779 90185 net.cpp:305] relu4 needs backward computation.
I1226 14:11:20.987807 90185 net.cpp:305] conv4 needs backward computation.
I1226 14:11:20.987838 90185 net.cpp:305] relu3 needs backward computation.
I1226 14:11:20.987870 90185 net.cpp:305] conv3 needs backward computation.
I1226 14:11:20.987907 90185 net.cpp:305] pool2 needs backward computation.
I1226 14:11:20.987938 90185 net.cpp:305] norm2 needs backward computation.
I1226 14:11:20.987970 90185 net.cpp:305] relu2 needs backward computation.
I1226 14:11:20.988000 90185 net.cpp:305] conv2 needs backward computation.
I1226 14:11:20.988044 90185 net.cpp:305] pool1 needs backward computation.
I1226 14:11:20.988077 90185 net.cpp:305] norm1 needs backward computation.
I1226 14:11:20.988108 90185 net.cpp:305] relu1 needs backward computation.
I1226 14:11:20.988144 90185 net.cpp:305] conv1 needs backward computation.
I1226 14:11:20.988178 90185 net.cpp:307] label_data_1_split does not need backward computation.
I1226 14:11:20.988211 90185 net.cpp:307] data does not need backward computation.
I1226 14:11:20.988250 90185 net.cpp:349] This network produces output accuracy
I1226 14:11:20.988284 90185 net.cpp:349] This network produces output loss
I1226 14:11:20.988386 90185 net.cpp:363] Network initialization done.
I1226 14:11:20.988873 90185 solver.cpp:107] Solver scaffolding done.
I1226 14:11:20.989096 90185 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 14:11:21.009663 91405 net.cpp:228] Setting up loss
I1226 14:11:21.009904 91405 net.cpp:235] Top shape: (1)
I1226 14:11:21.009965 91405 net.cpp:238]     with loss weight 1
I1226 14:11:21.010131 91405 net.cpp:243] Memory required for data: 2131777544
I1226 14:11:21.010205 91405 net.cpp:305] loss needs backward computation.
I1226 14:11:21.010263 91405 net.cpp:307] accuracy does not need backward computation.
I1226 14:11:21.010313 91405 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 14:11:21.010355 91405 net.cpp:305] fc8 needs backward computation.
I1226 14:11:21.010398 91405 net.cpp:305] drop7 needs backward computation.
I1226 14:11:21.010431 91405 net.cpp:305] relu7 needs backward computation.
I1226 14:11:21.010468 91405 net.cpp:305] fc7 needs backward computation.
I1226 14:11:21.010500 91405 net.cpp:305] drop6 needs backward computation.
I1226 14:11:21.010532 91405 net.cpp:305] relu6 needs backward computation.
I1226 14:11:21.010567 91405 net.cpp:305] fc6 needs backward computation.
I1226 14:11:21.010601 91405 net.cpp:305] pool5 needs backward computation.
I1226 14:11:21.010633 91405 net.cpp:305] relu5 needs backward computation.
I1226 14:11:21.010674 91405 net.cpp:305] conv5 needs backward computation.
I1226 14:11:21.010731 91405 net.cpp:305] relu4 needs backward computation.
I1226 14:11:21.010767 91405 net.cpp:305] conv4 needs backward computation.
I1226 14:11:21.010807 91405 net.cpp:305] relu3 needs backward computation.
I1226 14:11:21.010838 91405 net.cpp:305] conv3 needs backward computation.
I1226 14:11:21.010870 91405 net.cpp:305] pool2 needs backward computation.
I1226 14:11:21.010911 91405 net.cpp:305] norm2 needs backward computation.
I1226 14:11:21.010942 91405 net.cpp:305] relu2 needs backward computation.
I1226 14:11:21.010977 91405 net.cpp:305] conv2 needs backward computation.
I1226 14:11:21.011010 91405 net.cpp:305] pool1 needs backward computation.
I1226 14:11:21.011042 91405 net.cpp:305] norm1 needs backward computation.
I1226 14:11:21.011081 91405 net.cpp:305] relu1 needs backward computation.
I1226 14:11:21.011111 91405 net.cpp:305] conv1 needs backward computation.
I1226 14:11:21.011154 91405 net.cpp:307] label_data_1_split does not need backward computation.
I1226 14:11:21.011193 91405 net.cpp:307] data does not need backward computation.
I1226 14:11:21.011226 91405 net.cpp:349] This network produces output accuracy
I1226 14:11:21.011261 91405 net.cpp:349] This network produces output loss
I1226 14:11:21.011369 91405 net.cpp:363] Network initialization done.
I1226 14:11:21.011855 91405 solver.cpp:107] Solver scaffolding done.
I1226 14:11:21.012080 91405 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 14:11:23.770823 98828 caffe.cpp:376] Configuring multinode setup
I1226 14:11:23.772305 98828 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 14:11:24.118898 91711 caffe.cpp:376] Configuring multinode setup
I1226 14:11:24.120424 91711 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 14:11:24.164935 92274 caffe.cpp:376] Configuring multinode setup
I1226 14:11:24.166479 92274 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 14:11:24.266645 91405 caffe.cpp:376] Configuring multinode setup
I1226 14:11:24.268118 91405 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 14:11:24.999325 121743 caffe.cpp:376] Configuring multinode setup
I1226 14:11:25.000890 121743 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 14:11:25.031244 90185 caffe.cpp:376] Configuring multinode setup
I1226 14:11:25.032680 90185 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 14:11:45.155295 96952 net.cpp:228] Setting up fc6
I1226 14:11:45.155560 96952 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 14:11:45.155606 96952 net.cpp:243] Memory required for data: 2107734016
I1226 14:11:45.155670 96952 layer_factory.hpp:114] Creating layer relu6
I1226 14:11:45.155872 96952 net.cpp:178] Creating Layer relu6
I1226 14:11:45.155916 96952 net.cpp:612] relu6 <- fc6
I1226 14:11:45.155959 96952 net.cpp:573] relu6 -> fc6 (in-place)
I1226 14:11:45.156059 96952 net.cpp:228] Setting up relu6
I1226 14:11:45.156123 96952 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 14:11:45.156152 96952 net.cpp:243] Memory required for data: 2111928320
I1226 14:11:45.156186 96952 layer_factory.hpp:114] Creating layer drop6
I1226 14:11:45.156251 96952 net.cpp:178] Creating Layer drop6
I1226 14:11:45.156289 96952 net.cpp:612] drop6 <- fc6
I1226 14:11:45.156328 96952 net.cpp:573] drop6 -> fc6 (in-place)
I1226 14:11:45.156390 96952 net.cpp:228] Setting up drop6
I1226 14:11:45.156437 96952 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 14:11:45.156468 96952 net.cpp:243] Memory required for data: 2116122624
I1226 14:11:45.156499 96952 layer_factory.hpp:114] Creating layer fc7
I1226 14:11:45.156558 96952 net.cpp:178] Creating Layer fc7
I1226 14:11:45.156592 96952 net.cpp:612] fc7 <- fc6
I1226 14:11:45.156633 96952 net.cpp:586] fc7 -> fc7
I1226 14:11:45.232882 95316 net.cpp:228] Setting up fc6
I1226 14:11:45.233147 95316 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 14:11:45.233192 95316 net.cpp:243] Memory required for data: 2107734016
I1226 14:11:45.233260 95316 layer_factory.hpp:114] Creating layer relu6
I1226 14:11:45.233345 95316 net.cpp:178] Creating Layer relu6
I1226 14:11:45.233394 95316 net.cpp:612] relu6 <- fc6
I1226 14:11:45.233439 95316 net.cpp:573] relu6 -> fc6 (in-place)
I1226 14:11:45.233546 95316 net.cpp:228] Setting up relu6
I1226 14:11:45.233608 95316 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 14:11:45.233636 95316 net.cpp:243] Memory required for data: 2111928320
I1226 14:11:45.233669 95316 layer_factory.hpp:114] Creating layer drop6
I1226 14:11:45.233731 95316 net.cpp:178] Creating Layer drop6
I1226 14:11:45.233762 95316 net.cpp:612] drop6 <- fc6
I1226 14:11:45.233799 95316 net.cpp:573] drop6 -> fc6 (in-place)
I1226 14:11:45.233989 95316 net.cpp:228] Setting up drop6
I1226 14:11:45.234030 95316 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 14:11:45.234055 95316 net.cpp:243] Memory required for data: 2116122624
I1226 14:11:45.234084 95316 layer_factory.hpp:114] Creating layer fc7
I1226 14:11:45.234141 95316 net.cpp:178] Creating Layer fc7
I1226 14:11:45.234171 95316 net.cpp:612] fc7 <- fc6
I1226 14:11:45.234213 95316 net.cpp:586] fc7 -> fc7
I1226 14:11:58.224218 96952 net.cpp:228] Setting up fc7
I1226 14:11:58.224335 96952 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 14:11:58.224370 96952 net.cpp:243] Memory required for data: 2120316928
I1226 14:11:58.224434 96952 layer_factory.hpp:114] Creating layer relu7
I1226 14:11:58.224611 96952 net.cpp:178] Creating Layer relu7
I1226 14:11:58.224666 96952 net.cpp:612] relu7 <- fc7
I1226 14:11:58.224711 96952 net.cpp:573] relu7 -> fc7 (in-place)
I1226 14:11:58.224846 96952 net.cpp:228] Setting up relu7
I1226 14:11:58.224915 96952 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 14:11:58.224943 96952 net.cpp:243] Memory required for data: 2124511232
I1226 14:11:58.224975 96952 layer_factory.hpp:114] Creating layer drop7
I1226 14:11:58.225018 96952 net.cpp:178] Creating Layer drop7
I1226 14:11:58.225044 96952 net.cpp:612] drop7 <- fc7
I1226 14:11:58.225082 96952 net.cpp:573] drop7 -> fc7 (in-place)
I1226 14:11:58.225129 96952 net.cpp:228] Setting up drop7
I1226 14:11:58.225165 96952 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 14:11:58.225189 96952 net.cpp:243] Memory required for data: 2128705536
I1226 14:11:58.225216 96952 layer_factory.hpp:114] Creating layer fc8
I1226 14:11:58.225273 96952 net.cpp:178] Creating Layer fc8
I1226 14:11:58.225301 96952 net.cpp:612] fc8 <- fc7
I1226 14:11:58.225337 96952 net.cpp:586] fc8 -> fc8
I1226 14:11:58.296857 95316 net.cpp:228] Setting up fc7
I1226 14:11:58.296973 95316 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 14:11:58.297009 95316 net.cpp:243] Memory required for data: 2120316928
I1226 14:11:58.297072 95316 layer_factory.hpp:114] Creating layer relu7
I1226 14:11:58.297257 95316 net.cpp:178] Creating Layer relu7
I1226 14:11:58.297303 95316 net.cpp:612] relu7 <- fc7
I1226 14:11:58.297350 95316 net.cpp:573] relu7 -> fc7 (in-place)
I1226 14:11:58.297456 95316 net.cpp:228] Setting up relu7
I1226 14:11:58.297515 95316 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 14:11:58.297544 95316 net.cpp:243] Memory required for data: 2124511232
I1226 14:11:58.297580 95316 layer_factory.hpp:114] Creating layer drop7
I1226 14:11:58.297626 95316 net.cpp:178] Creating Layer drop7
I1226 14:11:58.297664 95316 net.cpp:612] drop7 <- fc7
I1226 14:11:58.297700 95316 net.cpp:573] drop7 -> fc7 (in-place)
I1226 14:11:58.297749 95316 net.cpp:228] Setting up drop7
I1226 14:11:58.297796 95316 net.cpp:235] Top shape: 256 4096 (1048576)
I1226 14:11:58.297843 95316 net.cpp:243] Memory required for data: 2128705536
I1226 14:11:58.297879 95316 layer_factory.hpp:114] Creating layer fc8
I1226 14:11:58.297940 95316 net.cpp:178] Creating Layer fc8
I1226 14:11:58.297968 95316 net.cpp:612] fc8 <- fc7
I1226 14:11:58.298012 95316 net.cpp:586] fc8 -> fc8
I1226 14:12:01.422323 96952 net.cpp:228] Setting up fc8
I1226 14:12:01.422443 96952 net.cpp:235] Top shape: 256 1000 (256000)
I1226 14:12:01.422483 96952 net.cpp:243] Memory required for data: 2129729536
I1226 14:12:01.422549 96952 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 14:12:01.422628 96952 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 14:12:01.422677 96952 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 14:12:01.422729 96952 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 14:12:01.422811 96952 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 14:12:01.422942 96952 net.cpp:228] Setting up fc8_fc8_0_split
I1226 14:12:01.422993 96952 net.cpp:235] Top shape: 256 1000 (256000)
I1226 14:12:01.423027 96952 net.cpp:235] Top shape: 256 1000 (256000)
I1226 14:12:01.423050 96952 net.cpp:243] Memory required for data: 2131777536
I1226 14:12:01.423082 96952 layer_factory.hpp:114] Creating layer accuracy
I1226 14:12:01.423141 96952 net.cpp:178] Creating Layer accuracy
I1226 14:12:01.423176 96952 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 14:12:01.423210 96952 net.cpp:612] accuracy <- label_data_1_split_0
I1226 14:12:01.423264 96952 net.cpp:586] accuracy -> accuracy
I1226 14:12:01.423326 96952 net.cpp:228] Setting up accuracy
I1226 14:12:01.423391 96952 net.cpp:235] Top shape: (1)
I1226 14:12:01.423424 96952 net.cpp:243] Memory required for data: 2131777540
I1226 14:12:01.423456 96952 layer_factory.hpp:114] Creating layer loss
I1226 14:12:01.423744 96952 net.cpp:178] Creating Layer loss
I1226 14:12:01.423830 96952 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 14:12:01.423868 96952 net.cpp:612] loss <- label_data_1_split_1
I1226 14:12:01.423918 96952 net.cpp:586] loss -> loss
I1226 14:12:01.423998 96952 layer_factory.hpp:114] Creating layer loss
I1226 14:12:01.456833 96952 net.cpp:228] Setting up loss
I1226 14:12:01.456954 96952 net.cpp:235] Top shape: (1)
I1226 14:12:01.456997 96952 net.cpp:238]     with loss weight 1
I1226 14:12:01.457151 96952 net.cpp:243] Memory required for data: 2131777544
I1226 14:12:01.457202 96952 net.cpp:305] loss needs backward computation.
I1226 14:12:01.457244 96952 net.cpp:307] accuracy does not need backward computation.
I1226 14:12:01.457293 96952 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 14:12:01.457329 96952 net.cpp:305] fc8 needs backward computation.
I1226 14:12:01.457362 96952 net.cpp:305] drop7 needs backward computation.
I1226 14:12:01.457392 96952 net.cpp:305] relu7 needs backward computation.
I1226 14:12:01.457437 96952 net.cpp:305] fc7 needs backward computation.
I1226 14:12:01.457471 96952 net.cpp:305] drop6 needs backward computation.
I1226 14:12:01.457502 96952 net.cpp:305] relu6 needs backward computation.
I1226 14:12:01.457532 96952 net.cpp:305] fc6 needs backward computation.
I1226 14:12:01.457563 96952 net.cpp:305] pool5 needs backward computation.
I1226 14:12:01.457594 96952 net.cpp:305] relu5 needs backward computation.
I1226 14:12:01.457625 96952 net.cpp:305] conv5 needs backward computation.
I1226 14:12:01.457657 96952 net.cpp:305] relu4 needs backward computation.
I1226 14:12:01.457696 96952 net.cpp:305] conv4 needs backward computation.
I1226 14:12:01.457728 96952 net.cpp:305] relu3 needs backward computation.
I1226 14:12:01.457770 96952 net.cpp:305] conv3 needs backward computation.
I1226 14:12:01.457831 96952 net.cpp:305] pool2 needs backward computation.
I1226 14:12:01.457864 96952 net.cpp:305] norm2 needs backward computation.
I1226 14:12:01.457893 96952 net.cpp:305] relu2 needs backward computation.
I1226 14:12:01.457922 96952 net.cpp:305] conv2 needs backward computation.
I1226 14:12:01.457952 96952 net.cpp:305] pool1 needs backward computation.
I1226 14:12:01.457980 96952 net.cpp:305] norm1 needs backward computation.
I1226 14:12:01.458009 96952 net.cpp:305] relu1 needs backward computation.
I1226 14:12:01.458037 96952 net.cpp:305] conv1 needs backward computation.
I1226 14:12:01.458068 96952 net.cpp:307] label_data_1_split does not need backward computation.
I1226 14:12:01.458099 96952 net.cpp:307] data does not need backward computation.
I1226 14:12:01.458125 96952 net.cpp:349] This network produces output accuracy
I1226 14:12:01.458158 96952 net.cpp:349] This network produces output loss
I1226 14:12:01.458261 96952 net.cpp:363] Network initialization done.
I1226 14:12:01.458830 96952 solver.cpp:107] Solver scaffolding done.
I1226 14:12:01.459051 96952 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 14:12:01.486876 95316 net.cpp:228] Setting up fc8
I1226 14:12:01.487016 95316 net.cpp:235] Top shape: 256 1000 (256000)
I1226 14:12:01.487059 95316 net.cpp:243] Memory required for data: 2129729536
I1226 14:12:01.487125 95316 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 14:12:01.487280 95316 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 14:12:01.487334 95316 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 14:12:01.487380 95316 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 14:12:01.487435 95316 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 14:12:01.487540 95316 net.cpp:228] Setting up fc8_fc8_0_split
I1226 14:12:01.487602 95316 net.cpp:235] Top shape: 256 1000 (256000)
I1226 14:12:01.487637 95316 net.cpp:235] Top shape: 256 1000 (256000)
I1226 14:12:01.487663 95316 net.cpp:243] Memory required for data: 2131777536
I1226 14:12:01.487694 95316 layer_factory.hpp:114] Creating layer accuracy
I1226 14:12:01.487763 95316 net.cpp:178] Creating Layer accuracy
I1226 14:12:01.487807 95316 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 14:12:01.487869 95316 net.cpp:612] accuracy <- label_data_1_split_0
I1226 14:12:01.487910 95316 net.cpp:586] accuracy -> accuracy
I1226 14:12:01.487964 95316 net.cpp:228] Setting up accuracy
I1226 14:12:01.488034 95316 net.cpp:235] Top shape: (1)
I1226 14:12:01.488070 95316 net.cpp:243] Memory required for data: 2131777540
I1226 14:12:01.488103 95316 layer_factory.hpp:114] Creating layer loss
I1226 14:12:01.488265 95316 net.cpp:178] Creating Layer loss
I1226 14:12:01.488313 95316 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 14:12:01.488348 95316 net.cpp:612] loss <- label_data_1_split_1
I1226 14:12:01.488387 95316 net.cpp:586] loss -> loss
I1226 14:12:01.488461 95316 layer_factory.hpp:114] Creating layer loss
I1226 14:12:01.537112 95316 net.cpp:228] Setting up loss
I1226 14:12:01.537223 95316 net.cpp:235] Top shape: (1)
I1226 14:12:01.537257 95316 net.cpp:238]     with loss weight 1
I1226 14:12:01.537396 95316 net.cpp:243] Memory required for data: 2131777544
I1226 14:12:01.537443 95316 net.cpp:305] loss needs backward computation.
I1226 14:12:01.537482 95316 net.cpp:307] accuracy does not need backward computation.
I1226 14:12:01.537529 95316 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 14:12:01.537562 95316 net.cpp:305] fc8 needs backward computation.
I1226 14:12:01.537595 95316 net.cpp:305] drop7 needs backward computation.
I1226 14:12:01.537626 95316 net.cpp:305] relu7 needs backward computation.
I1226 14:12:01.537665 95316 net.cpp:305] fc7 needs backward computation.
I1226 14:12:01.537698 95316 net.cpp:305] drop6 needs backward computation.
I1226 14:12:01.537729 95316 net.cpp:305] relu6 needs backward computation.
I1226 14:12:01.537757 95316 net.cpp:305] fc6 needs backward computation.
I1226 14:12:01.537794 95316 net.cpp:305] pool5 needs backward computation.
I1226 14:12:01.537848 95316 net.cpp:305] relu5 needs backward computation.
I1226 14:12:01.537884 95316 net.cpp:305] conv5 needs backward computation.
I1226 14:12:01.537919 95316 net.cpp:305] relu4 needs backward computation.
I1226 14:12:01.537950 95316 net.cpp:305] conv4 needs backward computation.
I1226 14:12:01.537982 95316 net.cpp:305] relu3 needs backward computation.
I1226 14:12:01.538012 95316 net.cpp:305] conv3 needs backward computation.
I1226 14:12:01.538044 95316 net.cpp:305] pool2 needs backward computation.
I1226 14:12:01.538077 95316 net.cpp:305] norm2 needs backward computation.
I1226 14:12:01.538120 95316 net.cpp:305] relu2 needs backward computation.
I1226 14:12:01.538151 95316 net.cpp:305] conv2 needs backward computation.
I1226 14:12:01.538182 95316 net.cpp:305] pool1 needs backward computation.
I1226 14:12:01.538213 95316 net.cpp:305] norm1 needs backward computation.
I1226 14:12:01.538242 95316 net.cpp:305] relu1 needs backward computation.
I1226 14:12:01.538275 95316 net.cpp:305] conv1 needs backward computation.
I1226 14:12:01.538316 95316 net.cpp:307] label_data_1_split does not need backward computation.
I1226 14:12:01.538352 95316 net.cpp:307] data does not need backward computation.
I1226 14:12:01.538388 95316 net.cpp:349] This network produces output accuracy
I1226 14:12:01.538422 95316 net.cpp:349] This network produces output loss
I1226 14:12:01.538527 95316 net.cpp:363] Network initialization done.
I1226 14:12:01.539005 95316 solver.cpp:107] Solver scaffolding done.
I1226 14:12:01.539217 95316 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 14:12:06.005854 96952 caffe.cpp:376] Configuring multinode setup
I1226 14:12:06.007622 96952 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 14:12:06.086397 95316 caffe.cpp:376] Configuring multinode setup
I1226 14:12:06.088178 95316 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 14:12:06.076131 96952 SynchronousNode.cpp:675] [2] [proc 2] solving
I1226 14:12:06.088394 95316 SynchronousNode.cpp:675] [6] [proc 6] solving
I1226 14:12:06.088556 92274 SynchronousNode.cpp:675] [1] [proc 1] solving
I1226 14:12:06.088521 98828 SynchronousNode.cpp:675] [0] [proc 0] solving
I1226 14:12:06.076266 96952 solver.cpp:354] Solving AlexNet
I1226 14:12:06.088503 95316 solver.cpp:354] Solving AlexNet
I1226 14:12:06.088832 92274 solver.cpp:354] Solving AlexNet
I1226 14:12:06.088860 98828 solver.cpp:354] Solving AlexNet
I1226 14:12:06.088917 98828 solver.cpp:355] Learning Rate Policy: step
I1226 14:12:06.085013 91405 SynchronousNode.cpp:675] [4] [proc 4] solving
I1226 14:12:06.094287 91711 SynchronousNode.cpp:675] [3] [proc 3] solving
I1226 14:12:06.097443 90185 SynchronousNode.cpp:675] [7] [proc 7] solving
I1226 14:12:06.086515 121743 SynchronousNode.cpp:675] [5] [proc 5] solving
I1226 14:12:06.076310 96952 solver.cpp:355] Learning Rate Policy: step
I1226 14:12:06.088549 95316 solver.cpp:355] Learning Rate Policy: step
I1226 14:12:06.089094 92274 solver.cpp:355] Learning Rate Policy: step
I1226 14:12:06.085290 91405 solver.cpp:354] Solving AlexNet
I1226 14:12:06.094569 91711 solver.cpp:354] Solving AlexNet
I1226 14:12:06.094616 91711 solver.cpp:355] Learning Rate Policy: step
I1226 14:12:06.097759 90185 solver.cpp:354] Solving AlexNet
I1226 14:12:06.086812 121743 solver.cpp:354] Solving AlexNet
I1226 14:12:06.085343 91405 solver.cpp:355] Learning Rate Policy: step
I1226 14:12:06.097811 90185 solver.cpp:355] Learning Rate Policy: step
I1226 14:12:06.086905 121743 solver.cpp:355] Learning Rate Policy: step
I1226 14:12:06.088229 97021 SynchronousNode.cpp:293] [2] Comm thread started 1 1
I1226 14:12:06.101809 95388 SynchronousNode.cpp:293] [6] Comm thread started 1 1
I1226 14:12:06.114508 91477 SynchronousNode.cpp:293] [4] Comm thread started 1 1
I1226 14:12:06.128693 90257 SynchronousNode.cpp:293] [7] Comm thread started 1 1
I1226 14:12:06.123824 92343 SynchronousNode.cpp:293] [1] Comm thread started 1 1
I1226 14:12:06.126310 98904 SynchronousNode.cpp:293] [0] Comm thread started 1 1
I1226 14:12:06.136746 91785 SynchronousNode.cpp:293] [3] Comm thread started 1 1
I1226 14:12:06.131724 121816 SynchronousNode.cpp:293] [5] Comm thread started 1 1
I1226 14:12:07.019392 121743 MultiSolver.cpp:92] [5] PROFILING END[Forward]
I1226 14:12:07.019553 121743 MultiSolver.cpp:94] [5] PROFILING BEGIN[Backward]
I1226 14:12:07.018816 91405 MultiSolver.cpp:92] [4] PROFILING END[Forward]
I1226 14:12:07.018913 91405 MultiSolver.cpp:94] [4] PROFILING BEGIN[Backward]
I1226 14:12:07.052343 91711 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 14:12:07.052430 91711 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 14:12:07.061544 90185 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 14:12:07.061658 90185 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 14:12:07.072226 98828 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 14:12:07.072320 98828 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 14:12:07.084549 92274 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 14:12:07.084642 92274 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 14:12:09.401010 92274 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 14:12:09.401082 92274 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 14:12:09.401126 92274 solver.cpp:291] [1] Iteration 1, loss = 3.38926
I1226 14:12:09.401190 92274 solver.cpp:317]     Train net output #0: accuracy = 0.3125
I1226 14:12:09.401264 92274 solver.cpp:317]     Train net output #1: loss = 3.38926 (* 1 = 3.38926 loss)
I1226 14:12:09.443207 91405 MultiSolver.cpp:108] [4] PROFILING END[Backward]
I1226 14:12:09.443289 91405 MultiSolver.cpp:109] [4] PROFILING BEGIN[WaitingToSync]
I1226 14:12:09.443333 91405 solver.cpp:291] [4] Iteration 1, loss = 3.3675
I1226 14:12:09.443398 91405 solver.cpp:317]     Train net output #0: accuracy = 0.289062
I1226 14:12:09.443471 91405 solver.cpp:317]     Train net output #1: loss = 3.3675 (* 1 = 3.3675 loss)
I1226 14:12:09.483163 121743 MultiSolver.cpp:108] [5] PROFILING END[Backward]
I1226 14:12:09.483240 121743 MultiSolver.cpp:109] [5] PROFILING BEGIN[WaitingToSync]
I1226 14:12:09.483285 121743 solver.cpp:291] [5] Iteration 1, loss = 3.24276
I1226 14:12:09.483348 121743 solver.cpp:317]     Train net output #0: accuracy = 0.296875
I1226 14:12:09.483422 121743 solver.cpp:317]     Train net output #1: loss = 3.24276 (* 1 = 3.24276 loss)
I1226 14:12:09.495705 90185 MultiSolver.cpp:108] [7] PROFILING END[Backward]
I1226 14:12:09.495779 90185 MultiSolver.cpp:109] [7] PROFILING BEGIN[WaitingToSync]
I1226 14:12:09.495842 90185 solver.cpp:291] [7] Iteration 1, loss = 3.35514
I1226 14:12:09.495908 90185 solver.cpp:317]     Train net output #0: accuracy = 0.304688
I1226 14:12:09.495986 90185 solver.cpp:317]     Train net output #1: loss = 3.35514 (* 1 = 3.35514 loss)
I1226 14:12:09.515190 91711 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 14:12:09.515296 91711 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 14:12:09.515339 91711 solver.cpp:291] [3] Iteration 1, loss = 2.95161
I1226 14:12:09.515403 91711 solver.cpp:317]     Train net output #0: accuracy = 0.351562
I1226 14:12:09.515475 91711 solver.cpp:317]     Train net output #1: loss = 2.95161 (* 1 = 2.95161 loss)
I1226 14:12:09.607702 98828 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 14:12:09.607774 98828 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 14:12:09.607817 98828 solver.cpp:291] [0] Iteration 1, loss = 3.38913
I1226 14:12:09.608695 98828 solver.cpp:317]     Train net output #0: accuracy = 0.289062
I1226 14:12:09.608798 98828 solver.cpp:317]     Train net output #1: loss = 3.38913 (* 1 = 3.38913 loss)
I1226 14:12:09.829704 92274 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 14:12:09.829799 92274 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 14:12:09.861227 91405 MultiSolver.cpp:92] [4] PROFILING END[Forward]
I1226 14:12:09.861320 91405 MultiSolver.cpp:94] [4] PROFILING BEGIN[Backward]
I1226 14:12:09.916368 90185 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 14:12:09.916460 90185 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 14:12:09.924763 121743 MultiSolver.cpp:92] [5] PROFILING END[Forward]
I1226 14:12:09.924850 121743 MultiSolver.cpp:94] [5] PROFILING BEGIN[Backward]
I1226 14:12:09.956614 91711 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 14:12:09.956712 91711 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 14:12:10.041182 98828 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 14:12:10.041275 98828 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 14:12:11.253924 92274 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 14:12:11.253999 92274 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 14:12:11.254040 92274 solver.cpp:291] [1] Iteration 2, loss = 3.30529
I1226 14:12:11.254104 92274 solver.cpp:317]     Train net output #0: accuracy = 0.300781
I1226 14:12:11.254385 92274 solver.cpp:317]     Train net output #1: loss = 3.30529 (* 1 = 3.30529 loss)
I1226 14:12:11.350438 90185 MultiSolver.cpp:108] [7] PROFILING END[Backward]
I1226 14:12:11.350520 90185 MultiSolver.cpp:109] [7] PROFILING BEGIN[WaitingToSync]
I1226 14:12:11.350582 90185 solver.cpp:291] [7] Iteration 2, loss = 2.98094
I1226 14:12:11.350647 90185 solver.cpp:317]     Train net output #0: accuracy = 0.34375
I1226 14:12:11.350714 90185 solver.cpp:317]     Train net output #1: loss = 2.98094 (* 1 = 2.98094 loss)
I1226 14:12:11.354799 91405 MultiSolver.cpp:108] [4] PROFILING END[Backward]
I1226 14:12:11.355072 91405 MultiSolver.cpp:109] [4] PROFILING BEGIN[WaitingToSync]
I1226 14:12:11.355113 91405 solver.cpp:291] [4] Iteration 2, loss = 3.22364
I1226 14:12:11.355181 91405 solver.cpp:317]     Train net output #0: accuracy = 0.332031
I1226 14:12:11.355248 91405 solver.cpp:317]     Train net output #1: loss = 3.22364 (* 1 = 3.22364 loss)
I1226 14:12:11.425792 121743 MultiSolver.cpp:108] [5] PROFILING END[Backward]
I1226 14:12:11.425871 121743 MultiSolver.cpp:109] [5] PROFILING BEGIN[WaitingToSync]
I1226 14:12:11.425914 121743 solver.cpp:291] [5] Iteration 2, loss = 3.15321
I1226 14:12:11.425976 121743 solver.cpp:317]     Train net output #0: accuracy = 0.300781
I1226 14:12:11.426041 121743 solver.cpp:317]     Train net output #1: loss = 3.15321 (* 1 = 3.15321 loss)
I1226 14:12:11.464391 91711 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 14:12:11.464480 91711 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 14:12:11.464540 91711 solver.cpp:291] [3] Iteration 2, loss = 3.16258
I1226 14:12:11.464612 91711 solver.cpp:317]     Train net output #0: accuracy = 0.324219
I1226 14:12:11.464699 91711 solver.cpp:317]     Train net output #1: loss = 3.16258 (* 1 = 3.16258 loss)
I1226 14:12:11.532413 98828 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 14:12:11.532491 98828 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 14:12:11.532533 98828 solver.cpp:291] [0] Iteration 2, loss = 3.18764
I1226 14:12:11.532598 98828 solver.cpp:317]     Train net output #0: accuracy = 0.304688
I1226 14:12:11.532683 98828 solver.cpp:317]     Train net output #1: loss = 3.18764 (* 1 = 3.18764 loss)
I1226 14:12:11.685533 92274 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 14:12:11.685727 92274 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 14:12:11.766847 91405 MultiSolver.cpp:92] [4] PROFILING END[Forward]
I1226 14:12:11.767174 91405 MultiSolver.cpp:94] [4] PROFILING BEGIN[Backward]
I1226 14:12:11.787751 90185 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 14:12:11.787840 90185 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 14:12:11.871911 91711 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 14:12:11.871996 91711 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 14:12:11.878160 121743 MultiSolver.cpp:92] [5] PROFILING END[Forward]
I1226 14:12:11.878250 121743 MultiSolver.cpp:94] [5] PROFILING BEGIN[Backward]
I1226 14:12:11.965605 98828 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 14:12:11.965719 98828 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 14:12:12.092608 96952 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 14:12:12.092712 96952 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 14:12:12.149214 95316 MultiSolver.cpp:92] [6] PROFILING END[Forward]
I1226 14:12:12.149312 95316 MultiSolver.cpp:94] [6] PROFILING BEGIN[Backward]
I1226 14:12:13.341152 91405 MultiSolver.cpp:108] [4] PROFILING END[Backward]
I1226 14:12:13.341390 91405 MultiSolver.cpp:109] [4] PROFILING BEGIN[WaitingToSync]
I1226 14:12:13.341435 91405 solver.cpp:291] [4] Iteration 3, loss = 2.87175
I1226 14:12:13.341500 91405 solver.cpp:317]     Train net output #0: accuracy = 0.375
I1226 14:12:13.341590 91405 solver.cpp:317]     Train net output #1: loss = 2.87175 (* 1 = 2.87175 loss)
I1226 14:12:13.362594 92274 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 14:12:13.362682 92274 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 14:12:13.362722 92274 solver.cpp:291] [1] Iteration 3, loss = 3.47629
I1226 14:12:13.371470 92274 solver.cpp:317]     Train net output #0: accuracy = 0.273438
I1226 14:12:13.371547 92274 solver.cpp:317]     Train net output #1: loss = 3.47629 (* 1 = 3.47629 loss)
I1226 14:12:13.443763 90185 MultiSolver.cpp:108] [7] PROFILING END[Backward]
I1226 14:12:13.443867 90185 MultiSolver.cpp:109] [7] PROFILING BEGIN[WaitingToSync]
I1226 14:12:13.443930 90185 solver.cpp:291] [7] Iteration 3, loss = 3.25006
I1226 14:12:13.443995 90185 solver.cpp:317]     Train net output #0: accuracy = 0.316406
I1226 14:12:13.444252 90185 solver.cpp:317]     Train net output #1: loss = 3.25006 (* 1 = 3.25006 loss)
I1226 14:12:13.484952 91711 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 14:12:13.485038 91711 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 14:12:13.485100 91711 solver.cpp:291] [3] Iteration 3, loss = 3.34138
I1226 14:12:13.485167 91711 solver.cpp:317]     Train net output #0: accuracy = 0.355469
I1226 14:12:13.479225 121743 MultiSolver.cpp:108] [5] PROFILING END[Backward]
I1226 14:12:13.479297 121743 MultiSolver.cpp:109] [5] PROFILING BEGIN[WaitingToSync]
I1226 14:12:13.479336 121743 solver.cpp:291] [5] Iteration 3, loss = 3.17761
I1226 14:12:13.479398 121743 solver.cpp:317]     Train net output #0: accuracy = 0.335938
I1226 14:12:13.479465 121743 solver.cpp:317]     Train net output #1: loss = 3.17761 (* 1 = 3.17761 loss)
I1226 14:12:13.502873 91711 solver.cpp:317]     Train net output #1: loss = 3.34138 (* 1 = 3.34138 loss)
I1226 14:12:13.600395 98828 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 14:12:13.600476 98828 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 14:12:13.600517 98828 solver.cpp:291] [0] Iteration 3, loss = 3.54883
I1226 14:12:13.600584 98828 solver.cpp:317]     Train net output #0: accuracy = 0.269531
I1226 14:12:13.600667 98828 solver.cpp:317]     Train net output #1: loss = 3.54883 (* 1 = 3.54883 loss)
I1226 14:12:13.778838 92274 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 14:12:13.778934 92274 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 14:12:13.786938 91405 MultiSolver.cpp:92] [4] PROFILING END[Forward]
I1226 14:12:13.787029 91405 MultiSolver.cpp:94] [4] PROFILING BEGIN[Backward]
I1226 14:12:13.874303 90185 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 14:12:13.874395 90185 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 14:12:13.910393 121743 MultiSolver.cpp:92] [5] PROFILING END[Forward]
I1226 14:12:13.910482 121743 MultiSolver.cpp:94] [5] PROFILING BEGIN[Backward]
I1226 14:12:13.936468 91711 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 14:12:13.936558 91711 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 14:12:14.059059 98828 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 14:12:14.059168 98828 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 14:12:15.628695 91711 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 14:12:15.628782 91711 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 14:12:15.628836 91711 solver.cpp:291] [3] Iteration 4, loss = 3.3263
I1226 14:12:15.628904 91711 solver.cpp:317]     Train net output #0: accuracy = 0.300781
I1226 14:12:15.629272 91711 solver.cpp:317]     Train net output #1: loss = 3.3263 (* 1 = 3.3263 loss)
I1226 14:12:15.644529 121743 MultiSolver.cpp:108] [5] PROFILING END[Backward]
I1226 14:12:15.644634 121743 MultiSolver.cpp:109] [5] PROFILING BEGIN[WaitingToSync]
I1226 14:12:15.644678 121743 solver.cpp:291] [5] Iteration 4, loss = 3.32055
I1226 14:12:15.644742 121743 solver.cpp:317]     Train net output #0: accuracy = 0.335938
I1226 14:12:15.644829 121743 solver.cpp:317]     Train net output #1: loss = 3.32055 (* 1 = 3.32055 loss)
I1226 14:12:15.647693 91405 MultiSolver.cpp:108] [4] PROFILING END[Backward]
I1226 14:12:15.647814 91405 MultiSolver.cpp:109] [4] PROFILING BEGIN[WaitingToSync]
I1226 14:12:15.647855 91405 solver.cpp:291] [4] Iteration 4, loss = 3.19808
I1226 14:12:15.647920 91405 solver.cpp:317]     Train net output #0: accuracy = 0.3125
I1226 14:12:15.647995 91405 solver.cpp:317]     Train net output #1: loss = 3.19808 (* 1 = 3.19808 loss)
I1226 14:12:15.671885 92274 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 14:12:15.671962 92274 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 14:12:15.672003 92274 solver.cpp:291] [1] Iteration 4, loss = 3.33792
I1226 14:12:15.672065 92274 solver.cpp:317]     Train net output #0: accuracy = 0.296875
I1226 14:12:15.672140 92274 solver.cpp:317]     Train net output #1: loss = 3.33792 (* 1 = 3.33792 loss)
I1226 14:12:15.791950 90185 MultiSolver.cpp:108] [7] PROFILING END[Backward]
I1226 14:12:15.792048 90185 MultiSolver.cpp:109] [7] PROFILING BEGIN[WaitingToSync]
I1226 14:12:15.792090 90185 solver.cpp:291] [7] Iteration 4, loss = 3.2027
I1226 14:12:15.792156 90185 solver.cpp:317]     Train net output #0: accuracy = 0.320312
I1226 14:12:15.792352 90185 solver.cpp:317]     Train net output #1: loss = 3.2027 (* 1 = 3.2027 loss)
I1226 14:12:15.801303 98828 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 14:12:15.801383 98828 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 14:12:15.801422 98828 solver.cpp:291] [0] Iteration 4, loss = 3.33104
I1226 14:12:15.802259 98828 solver.cpp:317]     Train net output #0: accuracy = 0.300781
I1226 14:12:15.802557 98828 solver.cpp:317]     Train net output #1: loss = 3.33104 (* 1 = 3.33104 loss)
I1226 14:12:16.060590 91711 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 14:12:16.060732 91711 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 14:12:16.054911 91405 MultiSolver.cpp:92] [4] PROFILING END[Forward]
I1226 14:12:16.056087 91405 MultiSolver.cpp:94] [4] PROFILING BEGIN[Backward]
I1226 14:12:16.067679 121743 MultiSolver.cpp:92] [5] PROFILING END[Forward]
I1226 14:12:16.067787 121743 MultiSolver.cpp:94] [5] PROFILING BEGIN[Backward]
I1226 14:12:16.078107 92274 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 14:12:16.078199 92274 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 14:12:16.215824 90185 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 14:12:16.215932 90185 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 14:12:16.216284 98828 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 14:12:16.216382 98828 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 14:12:17.480535 91405 MultiSolver.cpp:108] [4] PROFILING END[Backward]
I1226 14:12:17.480623 91405 MultiSolver.cpp:109] [4] PROFILING BEGIN[WaitingToSync]
I1226 14:12:17.480940 91405 solver.cpp:291] [4] Iteration 5, loss = 3.15151
I1226 14:12:17.481045 91405 solver.cpp:317]     Train net output #0: accuracy = 0.332031
I1226 14:12:17.481309 91405 solver.cpp:317]     Train net output #1: loss = 3.15151 (* 1 = 3.15151 loss)
I1226 14:12:17.504366 91711 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 14:12:17.504446 91711 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 14:12:17.504484 91711 solver.cpp:291] [3] Iteration 5, loss = 3.34171
I1226 14:12:17.504550 91711 solver.cpp:317]     Train net output #0: accuracy = 0.292969
I1226 14:12:17.504626 91711 solver.cpp:317]     Train net output #1: loss = 3.34171 (* 1 = 3.34171 loss)
I1226 14:12:17.504817 121743 MultiSolver.cpp:108] [5] PROFILING END[Backward]
I1226 14:12:17.504902 121743 MultiSolver.cpp:109] [5] PROFILING BEGIN[WaitingToSync]
I1226 14:12:17.504945 121743 solver.cpp:291] [5] Iteration 5, loss = 3.27726
I1226 14:12:17.505007 121743 solver.cpp:317]     Train net output #0: accuracy = 0.320312
I1226 14:12:17.505082 121743 solver.cpp:317]     Train net output #1: loss = 3.27726 (* 1 = 3.27726 loss)
I1226 14:12:17.577145 92274 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 14:12:17.577225 92274 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 14:12:17.577267 92274 solver.cpp:291] [1] Iteration 5, loss = 3.28033
I1226 14:12:17.577352 92274 solver.cpp:317]     Train net output #0: accuracy = 0.308594
I1226 14:12:17.577409 92274 solver.cpp:317]     Train net output #1: loss = 3.28033 (* 1 = 3.28033 loss)
I1226 14:12:17.603390 90185 MultiSolver.cpp:108] [7] PROFILING END[Backward]
I1226 14:12:17.603471 90185 MultiSolver.cpp:109] [7] PROFILING BEGIN[WaitingToSync]
I1226 14:12:17.603518 90185 solver.cpp:291] [7] Iteration 5, loss = 3.06729
I1226 14:12:17.603629 90185 solver.cpp:317]     Train net output #0: accuracy = 0.328125
I1226 14:12:17.610793 90185 solver.cpp:317]     Train net output #1: loss = 3.06729 (* 1 = 3.06729 loss)
I1226 14:12:17.783574 98828 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 14:12:17.783649 98828 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 14:12:17.783689 98828 solver.cpp:291] [0] Iteration 5, loss = 3.45858
I1226 14:12:17.783754 98828 solver.cpp:317]     Train net output #0: accuracy = 0.273438
I1226 14:12:17.783867 98828 solver.cpp:317]     Train net output #1: loss = 3.45858 (* 1 = 3.45858 loss)
I1226 14:12:17.903501 91405 MultiSolver.cpp:92] [4] PROFILING END[Forward]
I1226 14:12:17.903589 91405 MultiSolver.cpp:94] [4] PROFILING BEGIN[Backward]
I1226 14:12:17.931452 91711 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 14:12:17.931541 91711 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 14:12:17.925228 121743 MultiSolver.cpp:92] [5] PROFILING END[Forward]
I1226 14:12:17.925317 121743 MultiSolver.cpp:94] [5] PROFILING BEGIN[Backward]
I1226 14:12:17.993080 92274 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 14:12:17.993165 92274 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 14:12:18.042135 90185 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 14:12:18.042224 90185 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 14:12:18.228901 98828 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 14:12:18.229308 98828 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 14:12:19.440085 91405 MultiSolver.cpp:108] [4] PROFILING END[Backward]
I1226 14:12:19.440165 91405 MultiSolver.cpp:109] [4] PROFILING BEGIN[WaitingToSync]
I1226 14:12:19.440207 91405 solver.cpp:291] [4] Iteration 6, loss = 3.33099
I1226 14:12:19.440270 91405 solver.cpp:317]     Train net output #0: accuracy = 0.304688
I1226 14:12:19.440338 91405 solver.cpp:317]     Train net output #1: loss = 3.33099 (* 1 = 3.33099 loss)
I1226 14:12:19.482542 121743 MultiSolver.cpp:108] [5] PROFILING END[Backward]
I1226 14:12:19.482666 121743 MultiSolver.cpp:109] [5] PROFILING BEGIN[WaitingToSync]
I1226 14:12:19.482736 121743 solver.cpp:291] [5] Iteration 6, loss = 3.30566
I1226 14:12:19.482935 121743 solver.cpp:317]     Train net output #0: accuracy = 0.308594
I1226 14:12:19.483012 121743 solver.cpp:317]     Train net output #1: loss = 3.30566 (* 1 = 3.30566 loss)
I1226 14:12:19.497300 91711 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 14:12:19.497388 91711 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 14:12:19.497447 91711 solver.cpp:291] [3] Iteration 6, loss = 3.49507
I1226 14:12:19.497690 91711 solver.cpp:317]     Train net output #0: accuracy = 0.28125
I1226 14:12:19.498107 91711 solver.cpp:317]     Train net output #1: loss = 3.49507 (* 1 = 3.49507 loss)
I1226 14:12:19.562667 92274 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 14:12:19.562747 92274 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 14:12:19.562786 92274 solver.cpp:291] [1] Iteration 6, loss = 3.23955
I1226 14:12:19.562849 92274 solver.cpp:317]     Train net output #0: accuracy = 0.3125
I1226 14:12:19.562914 92274 solver.cpp:317]     Train net output #1: loss = 3.23955 (* 1 = 3.23955 loss)
I1226 14:12:19.605854 90185 MultiSolver.cpp:108] [7] PROFILING END[Backward]
I1226 14:12:19.605936 90185 MultiSolver.cpp:109] [7] PROFILING BEGIN[WaitingToSync]
I1226 14:12:19.605994 90185 solver.cpp:291] [7] Iteration 6, loss = 3.10579
I1226 14:12:19.606329 90185 solver.cpp:317]     Train net output #0: accuracy = 0.335938
I1226 14:12:19.606521 90185 solver.cpp:317]     Train net output #1: loss = 3.10579 (* 1 = 3.10579 loss)
I1226 14:12:19.750407 98828 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 14:12:19.750486 98828 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 14:12:19.750527 98828 solver.cpp:291] [0] Iteration 6, loss = 3.28231
I1226 14:12:19.750591 98828 solver.cpp:317]     Train net output #0: accuracy = 0.292969
I1226 14:12:19.750679 98828 solver.cpp:317]     Train net output #1: loss = 3.28231 (* 1 = 3.28231 loss)
I1226 14:12:19.870628 91405 MultiSolver.cpp:92] [4] PROFILING END[Forward]
I1226 14:12:19.870744 91405 MultiSolver.cpp:94] [4] PROFILING BEGIN[Backward]
I1226 14:12:19.881330 121743 MultiSolver.cpp:92] [5] PROFILING END[Forward]
I1226 14:12:19.881412 121743 MultiSolver.cpp:94] [5] PROFILING BEGIN[Backward]
I1226 14:12:19.947438 91711 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 14:12:19.947522 91711 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 14:12:19.990826 92274 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 14:12:19.990914 92274 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 14:12:20.038998 90185 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 14:12:20.039089 90185 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 14:12:20.201300 98828 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 14:12:20.201388 98828 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 14:12:21.145886 96952 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 14:12:21.158941 96952 MultiSolver.cpp:109] [2] PROFILING BEGIN[WaitingToSync]
I1226 14:12:21.159004 96952 solver.cpp:291] [2] Iteration 1, loss = 3.2451
I1226 14:12:21.159099 96952 solver.cpp:317]     Train net output #0: accuracy = 0.289062
I1226 14:12:21.159173 96952 solver.cpp:317]     Train net output #1: loss = 3.2451 (* 1 = 3.2451 loss)
I1226 14:12:21.328601 95316 MultiSolver.cpp:108] [6] PROFILING END[Backward]
I1226 14:12:21.329476 95316 MultiSolver.cpp:109] [6] PROFILING BEGIN[WaitingToSync]
I1226 14:12:21.329538 95316 solver.cpp:291] [6] Iteration 1, loss = 3.15186
I1226 14:12:21.329629 95316 solver.cpp:317]     Train net output #0: accuracy = 0.324219
I1226 14:12:21.329706 95316 solver.cpp:317]     Train net output #1: loss = 3.15186 (* 1 = 3.15186 loss)
I1226 14:12:21.361280 91405 MultiSolver.cpp:108] [4] PROFILING END[Backward]
I1226 14:12:21.361359 91405 MultiSolver.cpp:109] [4] PROFILING BEGIN[WaitingToSync]
I1226 14:12:21.361402 91405 solver.cpp:291] [4] Iteration 7, loss = 3.19288
I1226 14:12:21.361553 91405 solver.cpp:317]     Train net output #0: accuracy = 0.339844
I1226 14:12:21.361631 91405 solver.cpp:317]     Train net output #1: loss = 3.19288 (* 1 = 3.19288 loss)
I1226 14:12:21.384466 121743 MultiSolver.cpp:108] [5] PROFILING END[Backward]
I1226 14:12:21.384541 121743 MultiSolver.cpp:109] [5] PROFILING BEGIN[WaitingToSync]
I1226 14:12:21.384579 121743 solver.cpp:291] [5] Iteration 7, loss = 3.20493
I1226 14:12:21.384806 121743 solver.cpp:317]     Train net output #0: accuracy = 0.34375
I1226 14:12:21.385083 121743 solver.cpp:317]     Train net output #1: loss = 3.20493 (* 1 = 3.20493 loss)
I1226 14:12:21.388161 92274 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 14:12:21.388240 92274 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 14:12:21.388281 92274 solver.cpp:291] [1] Iteration 7, loss = 3.25208
I1226 14:12:21.388624 92274 solver.cpp:317]     Train net output #0: accuracy = 0.300781
I1226 14:12:21.388697 92274 solver.cpp:317]     Train net output #1: loss = 3.25208 (* 1 = 3.25208 loss)
I1226 14:12:21.446676 91711 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 14:12:21.446768 91711 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 14:12:21.446827 91711 solver.cpp:291] [3] Iteration 7, loss = 3.3791
I1226 14:12:21.447412 91711 solver.cpp:317]     Train net output #0: accuracy = 0.273438
I1226 14:12:21.447528 91711 solver.cpp:317]     Train net output #1: loss = 3.3791 (* 1 = 3.3791 loss)
I1226 14:12:21.536172 90185 MultiSolver.cpp:108] [7] PROFILING END[Backward]
I1226 14:12:21.536253 90185 MultiSolver.cpp:109] [7] PROFILING BEGIN[WaitingToSync]
I1226 14:12:21.536293 90185 solver.cpp:291] [7] Iteration 7, loss = 3.42385
I1226 14:12:21.536448 90185 solver.cpp:317]     Train net output #0: accuracy = 0.265625
I1226 14:12:21.536525 90185 solver.cpp:317]     Train net output #1: loss = 3.42385 (* 1 = 3.42385 loss)
I1226 14:12:21.742050 98828 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 14:12:21.742154 98828 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 14:12:21.742211 98828 solver.cpp:291] [0] Iteration 7, loss = 3.30392
I1226 14:12:21.742424 98828 solver.cpp:317]     Train net output #0: accuracy = 0.324219
I1226 14:12:21.742707 98828 solver.cpp:317]     Train net output #1: loss = 3.30392 (* 1 = 3.30392 loss)
I1226 14:12:21.800616 92274 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 14:12:21.800748 92274 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 14:12:21.799291 91405 MultiSolver.cpp:92] [4] PROFILING END[Forward]
I1226 14:12:21.799430 91405 MultiSolver.cpp:94] [4] PROFILING BEGIN[Backward]
I1226 14:12:21.826184 121743 MultiSolver.cpp:92] [5] PROFILING END[Forward]
I1226 14:12:21.826290 121743 MultiSolver.cpp:94] [5] PROFILING BEGIN[Backward]
I1226 14:12:21.880483 91711 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 14:12:21.880569 91711 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 14:12:21.976642 90185 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 14:12:21.976735 90185 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 14:12:22.157135 98828 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 14:12:22.157249 98828 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 14:12:23.222393 92274 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 14:12:23.222467 92274 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 14:12:23.222506 92274 solver.cpp:291] [1] Iteration 8, loss = 3.24692
I1226 14:12:23.222573 92274 solver.cpp:317]     Train net output #0: accuracy = 0.285156
I1226 14:12:23.223436 92274 solver.cpp:317]     Train net output #1: loss = 3.24692 (* 1 = 3.24692 loss)
I1226 14:12:23.245882 91405 MultiSolver.cpp:108] [4] PROFILING END[Backward]
I1226 14:12:23.245959 91405 MultiSolver.cpp:109] [4] PROFILING BEGIN[WaitingToSync]
I1226 14:12:23.246001 91405 solver.cpp:291] [4] Iteration 8, loss = 3.27318
I1226 14:12:23.246065 91405 solver.cpp:317]     Train net output #0: accuracy = 0.3125
I1226 14:12:23.246140 91405 solver.cpp:317]     Train net output #1: loss = 3.27318 (* 1 = 3.27318 loss)
I1226 14:12:23.373049 121743 MultiSolver.cpp:108] [5] PROFILING END[Backward]
I1226 14:12:23.373131 121743 MultiSolver.cpp:109] [5] PROFILING BEGIN[WaitingToSync]
I1226 14:12:23.373172 121743 solver.cpp:291] [5] Iteration 8, loss = 3.07505
I1226 14:12:23.373234 121743 solver.cpp:317]     Train net output #0: accuracy = 0.382812
I1226 14:12:23.373306 121743 solver.cpp:317]     Train net output #1: loss = 3.07505 (* 1 = 3.07505 loss)
I1226 14:12:23.389878 91711 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 14:12:23.389960 91711 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 14:12:23.390000 91711 solver.cpp:291] [3] Iteration 8, loss = 3.27151
I1226 14:12:23.390064 91711 solver.cpp:317]     Train net output #0: accuracy = 0.332031
I1226 14:12:23.390136 91711 solver.cpp:317]     Train net output #1: loss = 3.27151 (* 1 = 3.27151 loss)
I1226 14:12:23.448396 90185 MultiSolver.cpp:108] [7] PROFILING END[Backward]
I1226 14:12:23.448479 90185 MultiSolver.cpp:109] [7] PROFILING BEGIN[WaitingToSync]
I1226 14:12:23.448520 90185 solver.cpp:291] [7] Iteration 8, loss = 3.0777
I1226 14:12:23.448603 90185 solver.cpp:317]     Train net output #0: accuracy = 0.324219
I1226 14:12:23.448673 90185 solver.cpp:317]     Train net output #1: loss = 3.0777 (* 1 = 3.0777 loss)
I1226 14:12:23.620748 98828 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 14:12:23.620828 98828 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 14:12:23.620870 98828 solver.cpp:291] [0] Iteration 8, loss = 3.24278
I1226 14:12:23.620937 98828 solver.cpp:317]     Train net output #0: accuracy = 0.300781
I1226 14:12:23.621022 98828 solver.cpp:317]     Train net output #1: loss = 3.24278 (* 1 = 3.24278 loss)
I1226 14:12:23.652695 92274 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 14:12:23.652858 92274 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 14:12:23.692968 91405 MultiSolver.cpp:92] [4] PROFILING END[Forward]
I1226 14:12:23.693061 91405 MultiSolver.cpp:94] [4] PROFILING BEGIN[Backward]
I1226 14:12:23.791820 121743 MultiSolver.cpp:92] [5] PROFILING END[Forward]
I1226 14:12:23.791926 121743 MultiSolver.cpp:94] [5] PROFILING BEGIN[Backward]
I1226 14:12:23.813444 91711 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 14:12:23.813622 91711 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 14:12:23.866806 90185 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 14:12:23.866897 90185 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 14:12:24.057570 98828 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 14:12:24.057692 98828 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 14:12:25.137189 92274 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 14:12:25.137547 92274 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 14:12:25.137856 92274 solver.cpp:291] [1] Iteration 9, loss = 3.57689
I1226 14:12:25.137925 92274 solver.cpp:317]     Train net output #0: accuracy = 0.292969
I1226 14:12:25.139255 92274 solver.cpp:317]     Train net output #1: loss = 3.57689 (* 1 = 3.57689 loss)
I1226 14:12:25.151029 91405 MultiSolver.cpp:108] [4] PROFILING END[Backward]
I1226 14:12:25.151111 91405 MultiSolver.cpp:109] [4] PROFILING BEGIN[WaitingToSync]
I1226 14:12:25.151150 91405 solver.cpp:291] [4] Iteration 9, loss = 2.98092
I1226 14:12:25.151213 91405 solver.cpp:317]     Train net output #0: accuracy = 0.367188
I1226 14:12:25.151286 91405 solver.cpp:317]     Train net output #1: loss = 2.98092 (* 1 = 2.98092 loss)
I1226 14:12:25.330683 91711 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 14:12:25.330761 91711 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 14:12:25.330802 91711 solver.cpp:291] [3] Iteration 9, loss = 3.25029
I1226 14:12:25.330865 91711 solver.cpp:317]     Train net output #0: accuracy = 0.296875
I1226 14:12:25.330938 91711 solver.cpp:317]     Train net output #1: loss = 3.25029 (* 1 = 3.25029 loss)
I1226 14:12:25.338968 121743 MultiSolver.cpp:108] [5] PROFILING END[Backward]
I1226 14:12:25.339046 121743 MultiSolver.cpp:109] [5] PROFILING BEGIN[WaitingToSync]
I1226 14:12:25.339084 121743 solver.cpp:291] [5] Iteration 9, loss = 3.18022
I1226 14:12:25.339148 121743 solver.cpp:317]     Train net output #0: accuracy = 0.300781
I1226 14:12:25.339221 121743 solver.cpp:317]     Train net output #1: loss = 3.18022 (* 1 = 3.18022 loss)
I1226 14:12:25.388787 90185 MultiSolver.cpp:108] [7] PROFILING END[Backward]
I1226 14:12:25.388877 90185 MultiSolver.cpp:109] [7] PROFILING BEGIN[WaitingToSync]
I1226 14:12:25.388917 90185 solver.cpp:291] [7] Iteration 9, loss = 3.01736
I1226 14:12:25.388979 90185 solver.cpp:317]     Train net output #0: accuracy = 0.328125
I1226 14:12:25.389046 90185 solver.cpp:317]     Train net output #1: loss = 3.01736 (* 1 = 3.01736 loss)
I1226 14:12:25.531646 98828 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 14:12:25.531724 98828 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 14:12:25.531766 98828 solver.cpp:291] [0] Iteration 9, loss = 3.38121
I1226 14:12:25.531831 98828 solver.cpp:317]     Train net output #0: accuracy = 0.285156
I1226 14:12:25.531915 98828 solver.cpp:317]     Train net output #1: loss = 3.38121 (* 1 = 3.38121 loss)
I1226 14:12:25.559589 92274 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 14:12:25.559676 92274 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 14:12:25.566522 91405 MultiSolver.cpp:92] [4] PROFILING END[Forward]
I1226 14:12:25.566630 91405 MultiSolver.cpp:94] [4] PROFILING BEGIN[Backward]
I1226 14:12:25.731930 91711 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 14:12:25.732030 91711 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 14:12:25.751813 121743 MultiSolver.cpp:92] [5] PROFILING END[Forward]
I1226 14:12:25.751902 121743 MultiSolver.cpp:94] [5] PROFILING BEGIN[Backward]
I1226 14:12:25.804714 90185 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 14:12:25.804860 90185 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 14:12:25.973501 98828 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 14:12:25.973618 98828 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 14:12:26.197250 96952 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 14:12:26.197345 96952 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 14:12:26.409112 95316 MultiSolver.cpp:92] [6] PROFILING END[Forward]
I1226 14:12:26.409214 95316 MultiSolver.cpp:94] [6] PROFILING BEGIN[Backward]
I1226 14:12:27.118209 92274 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 14:12:27.118280 92274 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 14:12:27.118335 92274 solver.cpp:291] [1] Iteration 10, loss = 3.33613
I1226 14:12:27.118401 92274 solver.cpp:317]     Train net output #0: accuracy = 0.277344
I1226 14:12:27.118468 92274 solver.cpp:317]     Train net output #1: loss = 3.33613 (* 1 = 3.33613 loss)
I1226 14:12:27.122534 91405 MultiSolver.cpp:108] [4] PROFILING END[Backward]
I1226 14:12:27.122617 91405 MultiSolver.cpp:109] [4] PROFILING BEGIN[WaitingToSync]
I1226 14:12:27.122659 91405 solver.cpp:291] [4] Iteration 10, loss = 3.29161
I1226 14:12:27.122751 91405 solver.cpp:317]     Train net output #0: accuracy = 0.285156
I1226 14:12:27.122829 91405 solver.cpp:317]     Train net output #1: loss = 3.29161 (* 1 = 3.29161 loss)
I1226 14:12:27.124522 91471 blocking_queue.cpp:87] Waiting for data
I1226 14:12:27.307814 91711 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 14:12:27.307896 91711 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 14:12:27.307938 91711 solver.cpp:291] [3] Iteration 10, loss = 3.15334
I1226 14:12:27.308001 91711 solver.cpp:317]     Train net output #0: accuracy = 0.382812
I1226 14:12:27.308078 91711 solver.cpp:317]     Train net output #1: loss = 3.15334 (* 1 = 3.15334 loss)
I1226 14:12:27.309721 91778 blocking_queue.cpp:87] Waiting for data
I1226 14:12:27.519191 92274 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 14:12:27.519279 92274 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 14:12:27.588747 121743 MultiSolver.cpp:108] [5] PROFILING END[Backward]
I1226 14:12:27.588820 121743 MultiSolver.cpp:109] [5] PROFILING BEGIN[WaitingToSync]
I1226 14:12:27.588863 121743 solver.cpp:291] [5] Iteration 10, loss = 3.24167
I1226 14:12:27.588927 121743 solver.cpp:317]     Train net output #0: accuracy = 0.3125
I1226 14:12:27.589004 121743 solver.cpp:317]     Train net output #1: loss = 3.24167 (* 1 = 3.24167 loss)
I1226 14:12:27.591408 121811 blocking_queue.cpp:87] Waiting for data
I1226 14:12:27.607872 90185 MultiSolver.cpp:108] [7] PROFILING END[Backward]
I1226 14:12:27.607959 90185 MultiSolver.cpp:109] [7] PROFILING BEGIN[WaitingToSync]
I1226 14:12:27.608016 90185 solver.cpp:291] [7] Iteration 10, loss = 3.01734
I1226 14:12:27.608129 90185 solver.cpp:317]     Train net output #0: accuracy = 0.324219
I1226 14:12:27.608340 90185 solver.cpp:317]     Train net output #1: loss = 3.01734 (* 1 = 3.01734 loss)
I1226 14:12:27.610224 90252 blocking_queue.cpp:87] Waiting for data
I1226 14:12:27.661377 98828 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 14:12:27.661491 98828 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 14:12:27.661543 98828 solver.cpp:291] [0] Iteration 10, loss = 3.29153
I1226 14:12:27.662004 98828 solver.cpp:317]     Train net output #0: accuracy = 0.269531
I1226 14:12:27.662223 98828 solver.cpp:317]     Train net output #1: loss = 3.29153 (* 1 = 3.29153 loss)
I1226 14:12:27.664597 98899 blocking_queue.cpp:87] Waiting for data
I1226 14:12:28.187867 121743 MultiSolver.cpp:92] [5] PROFILING END[Forward]
I1226 14:12:28.187937 121743 MultiSolver.cpp:94] [5] PROFILING BEGIN[Backward]
I1226 14:12:28.321748 91405 MultiSolver.cpp:92] [4] PROFILING END[Forward]
I1226 14:12:28.321828 91405 MultiSolver.cpp:94] [4] PROFILING BEGIN[Backward]
I1226 14:12:28.368461 91711 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 14:12:28.368537 91711 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 14:12:28.483852 98828 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 14:12:28.483928 98828 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 14:12:28.785845 90185 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 14:12:28.785922 90185 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 14:12:28.829381 92274 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 14:12:28.829452 92274 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 14:12:28.829493 92274 solver.cpp:291] [1] Iteration 11, loss = 3.01826
I1226 14:12:28.829558 92274 solver.cpp:317]     Train net output #0: accuracy = 0.351562
I1226 14:12:28.829624 92274 solver.cpp:317]     Train net output #1: loss = 3.01826 (* 1 = 3.01826 loss)
I1226 14:12:28.831331 92340 blocking_queue.cpp:87] Waiting for data
I1226 14:12:29.473747 121743 MultiSolver.cpp:108] [5] PROFILING END[Backward]
I1226 14:12:29.473820 121743 MultiSolver.cpp:109] [5] PROFILING BEGIN[WaitingToSync]
I1226 14:12:29.473855 121743 solver.cpp:291] [5] Iteration 11, loss = 3.47507
I1226 14:12:29.474220 121743 solver.cpp:317]     Train net output #0: accuracy = 0.289062
I1226 14:12:29.474351 121743 solver.cpp:317]     Train net output #1: loss = 3.47507 (* 1 = 3.47507 loss)
I1226 14:12:29.661449 91405 MultiSolver.cpp:108] [4] PROFILING END[Backward]
I1226 14:12:29.661522 91405 MultiSolver.cpp:109] [4] PROFILING BEGIN[WaitingToSync]
I1226 14:12:29.661556 91405 solver.cpp:291] [4] Iteration 11, loss = 3.40673
I1226 14:12:29.661610 91405 solver.cpp:317]     Train net output #0: accuracy = 0.277344
I1226 14:12:29.661674 91405 solver.cpp:317]     Train net output #1: loss = 3.40673 (* 1 = 3.40673 loss)
I1226 14:12:29.840649 98828 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 14:12:29.840716 98828 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 14:12:29.840749 98828 solver.cpp:291] [0] Iteration 11, loss = 3.17532
I1226 14:12:29.840803 98828 solver.cpp:317]     Train net output #0: accuracy = 0.339844
I1226 14:12:29.840852 98828 solver.cpp:317]     Train net output #1: loss = 3.17532 (* 1 = 3.17532 loss)
I1226 14:12:29.879803 91711 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 14:12:29.879873 91711 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 14:12:29.879906 91711 solver.cpp:291] [3] Iteration 11, loss = 3.12412
I1226 14:12:29.879989 91711 solver.cpp:317]     Train net output #0: accuracy = 0.328125
I1226 14:12:29.880053 91711 solver.cpp:317]     Train net output #1: loss = 3.12412 (* 1 = 3.12412 loss)
I1226 14:12:29.948745 92274 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 14:12:29.949801 92274 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 14:12:30.074681 90185 MultiSolver.cpp:108] [7] PROFILING END[Backward]
I1226 14:12:30.074756 90185 MultiSolver.cpp:109] [7] PROFILING BEGIN[WaitingToSync]
I1226 14:12:30.074803 90185 solver.cpp:291] [7] Iteration 11, loss = 3.13231
I1226 14:12:30.074898 90185 solver.cpp:317]     Train net output #0: accuracy = 0.332031
I1226 14:12:30.075352 90185 solver.cpp:317]     Train net output #1: loss = 3.13231 (* 1 = 3.13231 loss)
I1226 14:12:30.938940 92274 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 14:12:30.939106 92274 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 14:12:30.939144 92274 solver.cpp:291] [1] Iteration 12, loss = 3.2761
I1226 14:12:30.947335 92274 solver.cpp:317]     Train net output #0: accuracy = 0.3125
I1226 14:12:30.947392 92274 solver.cpp:317]     Train net output #1: loss = 3.2761 (* 1 = 3.2761 loss)
I1226 14:12:31.122855 121743 MultiSolver.cpp:92] [5] PROFILING END[Forward]
I1226 14:12:31.122934 121743 MultiSolver.cpp:94] [5] PROFILING BEGIN[Backward]
I1226 14:12:31.246196 91405 MultiSolver.cpp:92] [4] PROFILING END[Forward]
I1226 14:12:31.246274 91405 MultiSolver.cpp:94] [4] PROFILING BEGIN[Backward]
I1226 14:12:31.333796 91711 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 14:12:31.333901 91711 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 14:12:31.333309 98828 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 14:12:31.333380 98828 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 14:12:31.782816 90185 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 14:12:31.782934 90185 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 14:12:32.591277 121743 MultiSolver.cpp:108] [5] PROFILING END[Backward]
I1226 14:12:32.591347 121743 MultiSolver.cpp:109] [5] PROFILING BEGIN[WaitingToSync]
I1226 14:12:32.591380 121743 solver.cpp:291] [5] Iteration 12, loss = 3.09838
I1226 14:12:32.591434 121743 solver.cpp:317]     Train net output #0: accuracy = 0.335938
I1226 14:12:32.591508 121743 solver.cpp:317]     Train net output #1: loss = 3.09838 (* 1 = 3.09838 loss)
I1226 14:12:32.640884 98828 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 14:12:32.640952 98828 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 14:12:32.640986 98828 solver.cpp:291] [0] Iteration 12, loss = 3.24942
I1226 14:12:32.641039 98828 solver.cpp:317]     Train net output #0: accuracy = 0.304688
I1226 14:12:32.641115 98828 solver.cpp:317]     Train net output #1: loss = 3.24942 (* 1 = 3.24942 loss)
I1226 14:12:32.700762 91405 MultiSolver.cpp:108] [4] PROFILING END[Backward]
I1226 14:12:32.700829 91405 MultiSolver.cpp:109] [4] PROFILING BEGIN[WaitingToSync]
I1226 14:12:32.700862 91405 solver.cpp:291] [4] Iteration 12, loss = 3.33177
I1226 14:12:32.700917 91405 solver.cpp:317]     Train net output #0: accuracy = 0.289062
I1226 14:12:32.700966 91405 solver.cpp:317]     Train net output #1: loss = 3.33177 (* 1 = 3.33177 loss)
I1226 14:12:32.753435 91711 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 14:12:32.753504 91711 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 14:12:32.753537 91711 solver.cpp:291] [3] Iteration 12, loss = 3.14837
I1226 14:12:32.753590 91711 solver.cpp:317]     Train net output #0: accuracy = 0.324219
I1226 14:12:32.753640 91711 solver.cpp:317]     Train net output #1: loss = 3.14837 (* 1 = 3.14837 loss)
I1226 14:12:32.925133 90185 MultiSolver.cpp:108] [7] PROFILING END[Backward]
I1226 14:12:32.925202 90185 MultiSolver.cpp:109] [7] PROFILING BEGIN[WaitingToSync]
I1226 14:12:32.925236 90185 solver.cpp:291] [7] Iteration 12, loss = 3.23675
I1226 14:12:32.925290 90185 solver.cpp:317]     Train net output #0: accuracy = 0.3125
I1226 14:12:32.925341 90185 solver.cpp:317]     Train net output #1: loss = 3.23675 (* 1 = 3.23675 loss)
I1226 14:12:32.947628 92274 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 14:12:32.948063 92274 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 14:12:33.718585 121743 MultiSolver.cpp:92] [5] PROFILING END[Forward]
I1226 14:12:33.718703 121743 MultiSolver.cpp:94] [5] PROFILING BEGIN[Backward]
I1226 14:12:33.850921 96952 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 14:12:33.851001 96952 MultiSolver.cpp:109] [2] PROFILING BEGIN[WaitingToSync]
I1226 14:12:33.851039 96952 solver.cpp:291] [2] Iteration 2, loss = 3.18203
I1226 14:12:33.851104 96952 solver.cpp:317]     Train net output #0: accuracy = 0.328125
I1226 14:12:33.851179 96952 solver.cpp:317]     Train net output #1: loss = 3.18203 (* 1 = 3.18203 loss)
I1226 14:12:33.988272 92274 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 14:12:33.988358 92274 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 14:12:33.988390 92274 solver.cpp:291] [1] Iteration 13, loss = 3.38927
I1226 14:12:33.988647 92274 solver.cpp:317]     Train net output #0: accuracy = 0.339844
I1226 14:12:33.988703 92274 solver.cpp:317]     Train net output #1: loss = 3.38927 (* 1 = 3.38927 loss)
I1226 14:12:34.005802 91405 MultiSolver.cpp:92] [4] PROFILING END[Forward]
I1226 14:12:34.006873 91405 MultiSolver.cpp:94] [4] PROFILING BEGIN[Backward]
I1226 14:12:34.139961 95316 MultiSolver.cpp:108] [6] PROFILING END[Backward]
I1226 14:12:34.140034 95316 MultiSolver.cpp:109] [6] PROFILING BEGIN[WaitingToSync]
I1226 14:12:34.140074 95316 solver.cpp:291] [6] Iteration 2, loss = 3.27313
I1226 14:12:34.140139 95316 solver.cpp:317]     Train net output #0: accuracy = 0.3125
I1226 14:12:34.140215 95316 solver.cpp:317]     Train net output #1: loss = 3.27313 (* 1 = 3.27313 loss)
I1226 14:12:34.330653 91711 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 14:12:34.330729 91711 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 14:12:34.391767 98828 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 14:12:34.391860 98828 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 14:12:34.547132 90185 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 14:12:34.547210 90185 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 14:12:34.949748 121743 MultiSolver.cpp:108] [5] PROFILING END[Backward]
I1226 14:12:34.949812 121743 MultiSolver.cpp:109] [5] PROFILING BEGIN[WaitingToSync]
I1226 14:12:34.949846 121743 solver.cpp:291] [5] Iteration 13, loss = 3.20466
I1226 14:12:34.949899 121743 solver.cpp:317]     Train net output #0: accuracy = 0.316406
I1226 14:12:34.949947 121743 solver.cpp:317]     Train net output #1: loss = 3.20466 (* 1 = 3.20466 loss)
I1226 14:12:35.406519 91405 MultiSolver.cpp:108] [4] PROFILING END[Backward]
I1226 14:12:35.406584 91405 MultiSolver.cpp:109] [4] PROFILING BEGIN[WaitingToSync]
I1226 14:12:35.406618 91405 solver.cpp:291] [4] Iteration 13, loss = 3.46869
I1226 14:12:35.406672 91405 solver.cpp:317]     Train net output #0: accuracy = 0.273438
I1226 14:12:35.406761 91405 solver.cpp:317]     Train net output #1: loss = 3.46869 (* 1 = 3.46869 loss)
I1226 14:12:35.511739 91711 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 14:12:35.511807 91711 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 14:12:35.511840 91711 solver.cpp:291] [3] Iteration 13, loss = 3.33992
I1226 14:12:35.511893 91711 solver.cpp:317]     Train net output #0: accuracy = 0.253906
I1226 14:12:35.511942 91711 solver.cpp:317]     Train net output #1: loss = 3.33992 (* 1 = 3.33992 loss)
I1226 14:12:35.725607 90185 MultiSolver.cpp:108] [7] PROFILING END[Backward]
I1226 14:12:35.725678 90185 MultiSolver.cpp:109] [7] PROFILING BEGIN[WaitingToSync]
I1226 14:12:35.716893 98828 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 14:12:35.716962 98828 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 14:12:35.725711 90185 solver.cpp:291] [7] Iteration 13, loss = 3.14847
I1226 14:12:35.716996 98828 solver.cpp:291] [0] Iteration 13, loss = 2.95563
I1226 14:12:35.717051 98828 solver.cpp:317]     Train net output #0: accuracy = 0.355469
I1226 14:12:35.717120 98828 solver.cpp:317]     Train net output #1: loss = 2.95563 (* 1 = 2.95563 loss)
I1226 14:12:35.725764 90185 solver.cpp:317]     Train net output #0: accuracy = 0.308594
I1226 14:12:35.725814 90185 solver.cpp:317]     Train net output #1: loss = 3.14847 (* 1 = 3.14847 loss)
I1226 14:12:36.146508 92274 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 14:12:36.147529 92274 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 14:12:36.774523 121743 MultiSolver.cpp:92] [5] PROFILING END[Forward]
I1226 14:12:36.781103 121743 MultiSolver.cpp:94] [5] PROFILING BEGIN[Backward]
I1226 14:12:36.994226 91405 MultiSolver.cpp:92] [4] PROFILING END[Forward]
I1226 14:12:36.997526 91405 MultiSolver.cpp:94] [4] PROFILING BEGIN[Backward]
I1226 14:12:37.259080 91711 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 14:12:37.260457 91711 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 14:12:37.316639 92274 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 14:12:37.316701 92274 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 14:12:37.316735 92274 solver.cpp:291] [1] Iteration 14, loss = 3.24595
I1226 14:12:37.316787 92274 solver.cpp:317]     Train net output #0: accuracy = 0.339844
I1226 14:12:37.316836 92274 solver.cpp:317]     Train net output #1: loss = 3.24595 (* 1 = 3.24595 loss)
I1226 14:12:37.344280 98828 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 14:12:37.346171 98828 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 14:12:37.543112 90185 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 14:12:37.543987 90185 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 14:12:38.129680 121743 MultiSolver.cpp:108] [5] PROFILING END[Backward]
I1226 14:12:38.129742 121743 MultiSolver.cpp:109] [5] PROFILING BEGIN[WaitingToSync]
I1226 14:12:38.129779 121743 solver.cpp:291] [5] Iteration 14, loss = 3.33205
I1226 14:12:38.129833 121743 solver.cpp:317]     Train net output #0: accuracy = 0.324219
I1226 14:12:38.129884 121743 solver.cpp:317]     Train net output #1: loss = 3.33205 (* 1 = 3.33205 loss)
I1226 14:12:38.417007 91405 MultiSolver.cpp:108] [4] PROFILING END[Backward]
I1226 14:12:38.417075 91405 MultiSolver.cpp:109] [4] PROFILING BEGIN[WaitingToSync]
I1226 14:12:38.417109 91405 solver.cpp:291] [4] Iteration 14, loss = 3.2426
I1226 14:12:38.417162 91405 solver.cpp:317]     Train net output #0: accuracy = 0.296875
I1226 14:12:38.417214 91405 solver.cpp:317]     Train net output #1: loss = 3.2426 (* 1 = 3.2426 loss)
I1226 14:12:38.551884 91711 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 14:12:38.551975 91711 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 14:12:38.552008 91711 solver.cpp:291] [3] Iteration 14, loss = 3.03507
I1226 14:12:38.552062 91711 solver.cpp:317]     Train net output #0: accuracy = 0.328125
I1226 14:12:38.552112 91711 solver.cpp:317]     Train net output #1: loss = 3.03507 (* 1 = 3.03507 loss)
I1226 14:12:38.555624 98828 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 14:12:38.555693 98828 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 14:12:38.555727 98828 solver.cpp:291] [0] Iteration 14, loss = 3.35047
I1226 14:12:38.555780 98828 solver.cpp:317]     Train net output #0: accuracy = 0.285156
I1226 14:12:38.555830 98828 solver.cpp:317]     Train net output #1: loss = 3.35047 (* 1 = 3.35047 loss)
I1226 14:12:38.731737 90185 MultiSolver.cpp:108] [7] PROFILING END[Backward]
I1226 14:12:38.731807 90185 MultiSolver.cpp:109] [7] PROFILING BEGIN[WaitingToSync]
I1226 14:12:38.731842 90185 solver.cpp:291] [7] Iteration 14, loss = 3.43058
I1226 14:12:38.731894 90185 solver.cpp:317]     Train net output #0: accuracy = 0.28125
I1226 14:12:38.731945 90185 solver.cpp:317]     Train net output #1: loss = 3.43058 (* 1 = 3.43058 loss)
I1226 14:12:38.946070 96952 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 14:12:38.946169 96952 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 14:12:39.074385 92274 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 14:12:39.074457 92274 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 14:12:39.203341 95316 MultiSolver.cpp:92] [6] PROFILING END[Forward]
I1226 14:12:39.203435 95316 MultiSolver.cpp:94] [6] PROFILING BEGIN[Backward]
I1226 14:12:39.555968 121743 MultiSolver.cpp:92] [5] PROFILING END[Forward]
I1226 14:12:39.556681 121743 MultiSolver.cpp:94] [5] PROFILING BEGIN[Backward]
I1226 14:12:40.110337 91405 MultiSolver.cpp:92] [4] PROFILING END[Forward]
I1226 14:12:40.110425 91405 MultiSolver.cpp:94] [4] PROFILING BEGIN[Backward]
I1226 14:12:40.331894 91711 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 14:12:40.331987 91711 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 14:12:40.380388 92274 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 14:12:40.380455 92274 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 14:12:40.380519 92274 solver.cpp:291] [1] Iteration 15, loss = 3.43607
I1226 14:12:40.380573 92274 solver.cpp:317]     Train net output #0: accuracy = 0.328125
I1226 14:12:40.380909 92274 solver.cpp:317]     Train net output #1: loss = 3.43607 (* 1 = 3.43607 loss)
I1226 14:12:40.400141 98828 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 14:12:40.400223 98828 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 14:12:40.752676 90185 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 14:12:40.752784 90185 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 14:12:41.153177 121743 MultiSolver.cpp:108] [5] PROFILING END[Backward]
I1226 14:12:41.153242 121743 MultiSolver.cpp:109] [5] PROFILING BEGIN[WaitingToSync]
I1226 14:12:41.153659 121743 solver.cpp:291] [5] Iteration 15, loss = 3.41946
I1226 14:12:41.153730 121743 solver.cpp:317]     Train net output #0: accuracy = 0.285156
I1226 14:12:41.154359 121743 solver.cpp:317]     Train net output #1: loss = 3.41946 (* 1 = 3.41946 loss)
I1226 14:12:41.414811 91405 MultiSolver.cpp:108] [4] PROFILING END[Backward]
I1226 14:12:41.414880 91405 MultiSolver.cpp:109] [4] PROFILING BEGIN[WaitingToSync]
I1226 14:12:41.414913 91405 solver.cpp:291] [4] Iteration 15, loss = 3.11343
I1226 14:12:41.414968 91405 solver.cpp:317]     Train net output #0: accuracy = 0.320312
I1226 14:12:41.415017 91405 solver.cpp:317]     Train net output #1: loss = 3.11343 (* 1 = 3.11343 loss)
I1226 14:12:41.542063 91711 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 14:12:41.542135 91711 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 14:12:41.542167 91711 solver.cpp:291] [3] Iteration 15, loss = 3.09975
I1226 14:12:41.542249 91711 solver.cpp:317]     Train net output #0: accuracy = 0.339844
I1226 14:12:41.542301 91711 solver.cpp:317]     Train net output #1: loss = 3.09975 (* 1 = 3.09975 loss)
I1226 14:12:41.626214 98828 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 14:12:41.626278 98828 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 14:12:41.626312 98828 solver.cpp:291] [0] Iteration 15, loss = 3.12844
I1226 14:12:41.626365 98828 solver.cpp:317]     Train net output #0: accuracy = 0.324219
I1226 14:12:41.626415 98828 solver.cpp:317]     Train net output #1: loss = 3.12844 (* 1 = 3.12844 loss)
I1226 14:12:41.769366 90185 MultiSolver.cpp:108] [7] PROFILING END[Backward]
I1226 14:12:41.769436 90185 MultiSolver.cpp:109] [7] PROFILING BEGIN[WaitingToSync]
I1226 14:12:41.769470 90185 solver.cpp:291] [7] Iteration 15, loss = 3.01185
I1226 14:12:41.769523 90185 solver.cpp:317]     Train net output #0: accuracy = 0.355469
I1226 14:12:41.769598 90185 solver.cpp:317]     Train net output #1: loss = 3.01185 (* 1 = 3.01185 loss)
I1226 14:12:42.022419 92274 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 14:12:42.022529 92274 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 14:12:42.439831 121743 MultiSolver.cpp:92] [5] PROFILING END[Forward]
I1226 14:12:42.439904 121743 MultiSolver.cpp:94] [5] PROFILING BEGIN[Backward]
I1226 14:12:43.085480 92274 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 14:12:43.085551 92274 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 14:12:43.085583 92274 solver.cpp:291] [1] Iteration 16, loss = 3.41884
I1226 14:12:43.085636 92274 solver.cpp:317]     Train net output #0: accuracy = 0.25
I1226 14:12:43.085686 92274 solver.cpp:317]     Train net output #1: loss = 3.41884 (* 1 = 3.41884 loss)
I1226 14:12:43.170920 91405 MultiSolver.cpp:92] [4] PROFILING END[Forward]
I1226 14:12:43.171023 91405 MultiSolver.cpp:94] [4] PROFILING BEGIN[Backward]
I1226 14:12:43.240792 98828 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 14:12:43.240864 98828 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 14:12:43.330059 91711 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 14:12:43.330135 91711 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 14:12:43.529638 90185 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 14:12:43.529745 90185 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 14:12:43.586684 121743 MultiSolver.cpp:108] [5] PROFILING END[Backward]
I1226 14:12:43.586747 121743 MultiSolver.cpp:109] [5] PROFILING BEGIN[WaitingToSync]
I1226 14:12:43.586781 121743 solver.cpp:291] [5] Iteration 16, loss = 3.1455
I1226 14:12:43.586834 121743 solver.cpp:317]     Train net output #0: accuracy = 0.335938
I1226 14:12:43.586891 121743 solver.cpp:317]     Train net output #1: loss = 3.1455 (* 1 = 3.1455 loss)
I1226 14:12:44.404644 91405 MultiSolver.cpp:108] [4] PROFILING END[Backward]
I1226 14:12:44.404731 91405 MultiSolver.cpp:109] [4] PROFILING BEGIN[WaitingToSync]
I1226 14:12:44.404765 91405 solver.cpp:291] [4] Iteration 16, loss = 3.1956
I1226 14:12:44.404853 91405 solver.cpp:317]     Train net output #0: accuracy = 0.324219
I1226 14:12:44.404906 91405 solver.cpp:317]     Train net output #1: loss = 3.1956 (* 1 = 3.1956 loss)
I1226 14:12:44.496201 98828 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 14:12:44.496263 98828 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 14:12:44.496297 98828 solver.cpp:291] [0] Iteration 16, loss = 3.26785
I1226 14:12:44.496350 98828 solver.cpp:317]     Train net output #0: accuracy = 0.269531
I1226 14:12:44.496399 98828 solver.cpp:317]     Train net output #1: loss = 3.26785 (* 1 = 3.26785 loss)
I1226 14:12:44.525967 91711 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 14:12:44.526034 91711 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 14:12:44.526068 91711 solver.cpp:291] [3] Iteration 16, loss = 3.35159
I1226 14:12:44.526121 91711 solver.cpp:317]     Train net output #0: accuracy = 0.289062
I1226 14:12:44.526171 91711 solver.cpp:317]     Train net output #1: loss = 3.35159 (* 1 = 3.35159 loss)
I1226 14:12:44.756937 90185 MultiSolver.cpp:108] [7] PROFILING END[Backward]
I1226 14:12:44.757014 90185 MultiSolver.cpp:109] [7] PROFILING BEGIN[WaitingToSync]
I1226 14:12:44.757050 90185 solver.cpp:291] [7] Iteration 16, loss = 3.22284
I1226 14:12:44.757139 90185 solver.cpp:317]     Train net output #0: accuracy = 0.34375
I1226 14:12:44.757220 90185 solver.cpp:317]     Train net output #1: loss = 3.22284 (* 1 = 3.22284 loss)
I1226 14:12:44.906769 92274 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 14:12:44.906843 92274 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 14:12:45.305024 121743 MultiSolver.cpp:92] [5] PROFILING END[Forward]
I1226 14:12:45.305132 121743 MultiSolver.cpp:94] [5] PROFILING BEGIN[Backward]
I1226 14:12:45.893843 98828 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 14:12:45.893918 98828 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 14:12:45.948807 92274 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 14:12:45.948895 92274 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 14:12:45.948932 92274 solver.cpp:291] [1] Iteration 17, loss = 3.17763
I1226 14:12:45.949023 92274 solver.cpp:317]     Train net output #0: accuracy = 0.34375
I1226 14:12:45.949813 92274 solver.cpp:317]     Train net output #1: loss = 3.17763 (* 1 = 3.17763 loss)
I1226 14:12:46.031939 91405 MultiSolver.cpp:92] [4] PROFILING END[Forward]
I1226 14:12:46.032013 91405 MultiSolver.cpp:94] [4] PROFILING BEGIN[Backward]
I1226 14:12:46.261515 91711 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 14:12:46.261602 91711 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 14:12:46.532799 90185 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 14:12:46.532871 90185 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 14:12:46.525080 121743 MultiSolver.cpp:108] [5] PROFILING END[Backward]
I1226 14:12:46.525146 121743 MultiSolver.cpp:109] [5] PROFILING BEGIN[WaitingToSync]
I1226 14:12:46.525179 121743 solver.cpp:291] [5] Iteration 17, loss = 3.12994
I1226 14:12:46.525234 121743 solver.cpp:317]     Train net output #0: accuracy = 0.289062
I1226 14:12:46.525286 121743 solver.cpp:317]     Train net output #1: loss = 3.12994 (* 1 = 3.12994 loss)
I1226 14:12:46.520921 96952 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 14:12:46.520993 96952 MultiSolver.cpp:109] [2] PROFILING BEGIN[WaitingToSync]
I1226 14:12:46.521034 96952 solver.cpp:291] [2] Iteration 3, loss = 3.27216
I1226 14:12:46.521100 96952 solver.cpp:317]     Train net output #0: accuracy = 0.335938
I1226 14:12:46.521176 96952 solver.cpp:317]     Train net output #1: loss = 3.27216 (* 1 = 3.27216 loss)
I1226 14:12:46.791965 95316 MultiSolver.cpp:108] [6] PROFILING END[Backward]
I1226 14:12:46.792039 95316 MultiSolver.cpp:109] [6] PROFILING BEGIN[WaitingToSync]
I1226 14:12:46.792078 95316 solver.cpp:291] [6] Iteration 3, loss = 3.22917
I1226 14:12:46.792142 95316 solver.cpp:317]     Train net output #0: accuracy = 0.316406
I1226 14:12:46.792209 95316 solver.cpp:317]     Train net output #1: loss = 3.22917 (* 1 = 3.22917 loss)
I1226 14:12:47.281795 91405 MultiSolver.cpp:108] [4] PROFILING END[Backward]
I1226 14:12:47.281862 91405 MultiSolver.cpp:109] [4] PROFILING BEGIN[WaitingToSync]
I1226 14:12:47.281898 91405 solver.cpp:291] [4] Iteration 17, loss = 3.17504
I1226 14:12:47.281951 91405 solver.cpp:317]     Train net output #0: accuracy = 0.304688
I1226 14:12:47.282001 91405 solver.cpp:317]     Train net output #1: loss = 3.17504 (* 1 = 3.17504 loss)
I1226 14:12:47.313594 98828 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 14:12:47.313663 98828 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 14:12:47.313697 98828 solver.cpp:291] [0] Iteration 17, loss = 3.18685
I1226 14:12:47.313751 98828 solver.cpp:317]     Train net output #0: accuracy = 0.320312
I1226 14:12:47.313802 98828 solver.cpp:317]     Train net output #1: loss = 3.18685 (* 1 = 3.18685 loss)
I1226 14:12:47.433661 91711 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 14:12:47.433737 91711 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 14:12:47.433771 91711 solver.cpp:291] [3] Iteration 17, loss = 3.21766
I1226 14:12:47.433825 91711 solver.cpp:317]     Train net output #0: accuracy = 0.316406
I1226 14:12:47.433876 91711 solver.cpp:317]     Train net output #1: loss = 3.21766 (* 1 = 3.21766 loss)
I1226 14:12:47.681541 90185 MultiSolver.cpp:108] [7] PROFILING END[Backward]
I1226 14:12:47.681632 90185 MultiSolver.cpp:109] [7] PROFILING BEGIN[WaitingToSync]
I1226 14:12:47.681666 90185 solver.cpp:291] [7] Iteration 17, loss = 3.19778
I1226 14:12:47.681720 90185 solver.cpp:317]     Train net output #0: accuracy = 0.34375
I1226 14:12:47.681768 90185 solver.cpp:317]     Train net output #1: loss = 3.19778 (* 1 = 3.19778 loss)
I1226 14:12:47.683864 92274 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 14:12:47.683936 92274 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 14:12:48.315016 121743 MultiSolver.cpp:92] [5] PROFILING END[Forward]
I1226 14:12:48.315106 121743 MultiSolver.cpp:94] [5] PROFILING BEGIN[Backward]
I1226 14:12:48.701675 92274 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 14:12:48.701771 92274 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 14:12:48.701833 92274 solver.cpp:291] [1] Iteration 18, loss = 3.02604
I1226 14:12:48.701886 92274 solver.cpp:317]     Train net output #0: accuracy = 0.371094
I1226 14:12:48.702222 92274 solver.cpp:317]     Train net output #1: loss = 3.02604 (* 1 = 3.02604 loss)
I1226 14:12:48.914469 98828 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 14:12:48.914538 98828 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 14:12:49.041748 91405 MultiSolver.cpp:92] [4] PROFILING END[Forward]
I1226 14:12:49.042418 91405 MultiSolver.cpp:94] [4] PROFILING BEGIN[Backward]
I1226 14:12:49.255079 91711 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 14:12:49.255151 91711 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 14:12:49.428246 90185 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 14:12:49.428323 90185 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 14:12:49.532008 121743 MultiSolver.cpp:108] [5] PROFILING END[Backward]
I1226 14:12:49.532094 121743 MultiSolver.cpp:109] [5] PROFILING BEGIN[WaitingToSync]
I1226 14:12:49.532130 121743 solver.cpp:291] [5] Iteration 18, loss = 3.21284
I1226 14:12:49.532182 121743 solver.cpp:317]     Train net output #0: accuracy = 0.339844
I1226 14:12:49.532232 121743 solver.cpp:317]     Train net output #1: loss = 3.21284 (* 1 = 3.21284 loss)
I1226 14:12:50.226027 98828 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 14:12:50.226125 98828 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 14:12:50.226157 98828 solver.cpp:291] [0] Iteration 18, loss = 2.99461
I1226 14:12:50.226210 98828 solver.cpp:317]     Train net output #0: accuracy = 0.328125
I1226 14:12:50.226260 98828 solver.cpp:317]     Train net output #1: loss = 2.99461 (* 1 = 2.99461 loss)
I1226 14:12:50.227448 91405 MultiSolver.cpp:108] [4] PROFILING END[Backward]
I1226 14:12:50.227526 91405 MultiSolver.cpp:109] [4] PROFILING BEGIN[WaitingToSync]
I1226 14:12:50.227573 91405 solver.cpp:291] [4] Iteration 18, loss = 3.09008
I1226 14:12:50.227629 91405 solver.cpp:317]     Train net output #0: accuracy = 0.335938
I1226 14:12:50.227958 91405 solver.cpp:317]     Train net output #1: loss = 3.09008 (* 1 = 3.09008 loss)
I1226 14:12:50.412559 92274 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 14:12:50.412637 92274 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 14:12:50.443387 91711 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 14:12:50.443459 91711 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 14:12:50.443492 91711 solver.cpp:291] [3] Iteration 18, loss = 3.192
I1226 14:12:50.443547 91711 solver.cpp:317]     Train net output #0: accuracy = 0.296875
I1226 14:12:50.443598 91711 solver.cpp:317]     Train net output #1: loss = 3.192 (* 1 = 3.192 loss)
I1226 14:12:50.552649 90185 MultiSolver.cpp:108] [7] PROFILING END[Backward]
I1226 14:12:50.552717 90185 MultiSolver.cpp:109] [7] PROFILING BEGIN[WaitingToSync]
I1226 14:12:50.552752 90185 solver.cpp:291] [7] Iteration 18, loss = 2.9994
I1226 14:12:50.552805 90185 solver.cpp:317]     Train net output #0: accuracy = 0.328125
I1226 14:12:50.552855 90185 solver.cpp:317]     Train net output #1: loss = 2.9994 (* 1 = 2.9994 loss)
I1226 14:12:51.341091 121743 MultiSolver.cpp:92] [5] PROFILING END[Forward]
I1226 14:12:51.341162 121743 MultiSolver.cpp:94] [5] PROFILING BEGIN[Backward]
I1226 14:12:51.399803 92274 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 14:12:51.399873 92274 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 14:12:51.399906 92274 solver.cpp:291] [1] Iteration 19, loss = 3.24503
I1226 14:12:51.399960 92274 solver.cpp:317]     Train net output #0: accuracy = 0.304688
I1226 14:12:51.400010 92274 solver.cpp:317]     Train net output #1: loss = 3.24503 (* 1 = 3.24503 loss)
I1226 14:12:51.566068 96952 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 14:12:51.566962 96952 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 14:12:51.643061 91762 blocking_queue.cpp:87] Waiting for data
I1226 14:12:51.780542 92341 blocking_queue.cpp:87] Waiting for data
I1226 14:12:51.791246 90252 blocking_queue.cpp:87] Waiting for data
I1226 14:12:51.791780 95316 MultiSolver.cpp:92] [6] PROFILING END[Forward]
I1226 14:12:51.793001 95316 MultiSolver.cpp:94] [6] PROFILING BEGIN[Backward]
I1226 14:12:51.903857 91405 MultiSolver.cpp:92] [4] PROFILING END[Forward]
I1226 14:12:51.903972 91405 MultiSolver.cpp:94] [4] PROFILING BEGIN[Backward]
I1226 14:12:52.005017 98828 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 14:12:52.005129 98828 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 14:12:52.259349 90185 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 14:12:52.259428 90185 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 14:12:52.334662 91711 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 14:12:52.334738 91711 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 14:12:52.712735 121743 MultiSolver.cpp:108] [5] PROFILING END[Backward]
I1226 14:12:52.712800 121743 MultiSolver.cpp:109] [5] PROFILING BEGIN[WaitingToSync]
I1226 14:12:52.712833 121743 solver.cpp:291] [5] Iteration 19, loss = 3.03409
I1226 14:12:52.712884 121743 solver.cpp:317]     Train net output #0: accuracy = 0.347656
I1226 14:12:52.712934 121743 solver.cpp:317]     Train net output #1: loss = 3.03409 (* 1 = 3.03409 loss)
I1226 14:12:53.291999 92274 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 14:12:53.292068 92274 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 14:12:53.351748 91405 MultiSolver.cpp:108] [4] PROFILING END[Backward]
I1226 14:12:53.351817 91405 MultiSolver.cpp:109] [4] PROFILING BEGIN[WaitingToSync]
I1226 14:12:53.351851 91405 solver.cpp:291] [4] Iteration 19, loss = 3.32492
I1226 14:12:53.351905 91405 solver.cpp:317]     Train net output #0: accuracy = 0.328125
I1226 14:12:53.351954 91405 solver.cpp:317]     Train net output #1: loss = 3.32492 (* 1 = 3.32492 loss)
I1226 14:12:53.363211 98828 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 14:12:53.363278 98828 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 14:12:53.363312 98828 solver.cpp:291] [0] Iteration 19, loss = 3.26829
I1226 14:12:53.363364 98828 solver.cpp:317]     Train net output #0: accuracy = 0.304688
I1226 14:12:53.363415 98828 solver.cpp:317]     Train net output #1: loss = 3.26829 (* 1 = 3.26829 loss)
I1226 14:12:53.557816 90185 MultiSolver.cpp:108] [7] PROFILING END[Backward]
I1226 14:12:53.557883 90185 MultiSolver.cpp:109] [7] PROFILING BEGIN[WaitingToSync]
I1226 14:12:53.557952 90185 solver.cpp:291] [7] Iteration 19, loss = 3.2592
I1226 14:12:53.559587 90185 solver.cpp:317]     Train net output #0: accuracy = 0.300781
I1226 14:12:53.559643 90185 solver.cpp:317]     Train net output #1: loss = 3.2592 (* 1 = 3.2592 loss)
I1226 14:12:53.710429 91711 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 14:12:53.710502 91711 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 14:12:53.710536 91711 solver.cpp:291] [3] Iteration 19, loss = 2.97733
I1226 14:12:53.710590 91711 solver.cpp:317]     Train net output #0: accuracy = 0.367188
I1226 14:12:53.710639 91711 solver.cpp:317]     Train net output #1: loss = 2.97733 (* 1 = 2.97733 loss)
I1226 14:12:54.095747 121743 MultiSolver.cpp:92] [5] PROFILING END[Forward]
I1226 14:12:54.095819 121743 MultiSolver.cpp:94] [5] PROFILING BEGIN[Backward]
I1226 14:12:54.330418 92274 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 14:12:54.330672 92274 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 14:12:54.330705 92274 solver.cpp:291] [1] Iteration 20, loss = 3.12261
I1226 14:12:54.331244 92274 solver.cpp:317]     Train net output #0: accuracy = 0.328125
I1226 14:12:54.331588 92274 solver.cpp:317]     Train net output #1: loss = 3.12261 (* 1 = 3.12261 loss)
I1226 14:12:54.855931 91405 MultiSolver.cpp:92] [4] PROFILING END[Forward]
I1226 14:12:54.856021 91405 MultiSolver.cpp:94] [4] PROFILING BEGIN[Backward]
I1226 14:12:54.932332 98828 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 14:12:54.932405 98828 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 14:12:55.203732 91711 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 14:12:55.203816 91711 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 14:12:55.229717 90185 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 14:12:55.229789 90185 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 14:12:55.377926 121743 MultiSolver.cpp:108] [5] PROFILING END[Backward]
I1226 14:12:55.378000 121743 MultiSolver.cpp:109] [5] PROFILING BEGIN[WaitingToSync]
I1226 14:12:55.378033 121743 solver.cpp:291] [5] Iteration 20, loss = 3.32846
I1226 14:12:55.378087 121743 solver.cpp:317]     Train net output #0: accuracy = 0.328125
I1226 14:12:55.378136 121743 solver.cpp:317]     Train net output #1: loss = 3.32846 (* 1 = 3.32846 loss)
I1226 14:12:56.167752 91405 MultiSolver.cpp:108] [4] PROFILING END[Backward]
I1226 14:12:56.167816 91405 MultiSolver.cpp:109] [4] PROFILING BEGIN[WaitingToSync]
I1226 14:12:56.167855 91405 solver.cpp:291] [4] Iteration 20, loss = 3.34643
I1226 14:12:56.167906 91405 solver.cpp:317]     Train net output #0: accuracy = 0.277344
I1226 14:12:56.167956 91405 solver.cpp:317]     Train net output #1: loss = 3.34643 (* 1 = 3.34643 loss)
I1226 14:12:56.204524 92274 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 14:12:56.204726 92274 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 14:12:56.267555 98828 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 14:12:56.267626 98828 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 14:12:56.267659 98828 solver.cpp:291] [0] Iteration 20, loss = 3.33491
I1226 14:12:56.267714 98828 solver.cpp:317]     Train net output #0: accuracy = 0.300781
I1226 14:12:56.267762 98828 solver.cpp:317]     Train net output #1: loss = 3.33491 (* 1 = 3.33491 loss)
I1226 14:12:56.291479 91472 blocking_queue.cpp:87] Waiting for data
I1226 14:12:56.483512 98898 blocking_queue.cpp:87] Waiting for data
I1226 14:12:56.563716 90185 MultiSolver.cpp:108] [7] PROFILING END[Backward]
I1226 14:12:56.563803 90185 MultiSolver.cpp:109] [7] PROFILING BEGIN[WaitingToSync]
I1226 14:12:56.563849 90185 solver.cpp:291] [7] Iteration 20, loss = 3.15991
I1226 14:12:56.563902 90185 solver.cpp:317]     Train net output #0: accuracy = 0.332031
I1226 14:12:56.560665 91711 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 14:12:56.560755 91711 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 14:12:56.563951 90185 solver.cpp:317]     Train net output #1: loss = 3.15991 (* 1 = 3.15991 loss)
I1226 14:12:56.560789 91711 solver.cpp:291] [3] Iteration 20, loss = 3.28093
I1226 14:12:56.560842 91711 solver.cpp:317]     Train net output #0: accuracy = 0.300781
I1226 14:12:56.560891 91711 solver.cpp:317]     Train net output #1: loss = 3.28093 (* 1 = 3.28093 loss)
I1226 14:12:57.017024 121743 MultiSolver.cpp:92] [5] PROFILING END[Forward]
I1226 14:12:57.017271 121743 MultiSolver.cpp:94] [5] PROFILING BEGIN[Backward]
I1226 14:12:57.378623 92274 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 14:12:57.378693 92274 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 14:12:57.378728 92274 solver.cpp:291] [1] Iteration 21, loss = 3.21922
I1226 14:12:57.378782 92274 solver.cpp:317]     Train net output #0: accuracy = 0.300781
I1226 14:12:57.379513 92274 solver.cpp:317]     Train net output #1: loss = 3.21922 (* 1 = 3.21922 loss)
I1226 14:12:57.632287 91405 MultiSolver.cpp:92] [4] PROFILING END[Forward]
I1226 14:12:57.632467 91405 MultiSolver.cpp:94] [4] PROFILING BEGIN[Backward]
I1226 14:12:57.737256 98828 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 14:12:57.737432 98828 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 14:12:58.147006 121743 MultiSolver.cpp:108] [5] PROFILING END[Backward]
I1226 14:12:58.147073 121743 MultiSolver.cpp:109] [5] PROFILING BEGIN[WaitingToSync]
I1226 14:12:58.147145 121743 solver.cpp:291] [5] Iteration 21, loss = 2.97131
I1226 14:12:58.147519 121743 solver.cpp:317]     Train net output #0: accuracy = 0.367188
I1226 14:12:58.147594 121743 solver.cpp:317]     Train net output #1: loss = 2.97131 (* 1 = 2.97131 loss)
I1226 14:12:58.171778 91711 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 14:12:58.172667 91711 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 14:12:58.225826 90185 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 14:12:58.225991 90185 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 14:12:58.386724 121812 blocking_queue.cpp:87] Waiting for data
I1226 14:12:58.769425 91405 MultiSolver.cpp:108] [4] PROFILING END[Backward]
I1226 14:12:58.769516 91405 MultiSolver.cpp:109] [4] PROFILING BEGIN[WaitingToSync]
I1226 14:12:58.769596 91405 solver.cpp:291] [4] Iteration 21, loss = 3.2667
I1226 14:12:58.769986 91405 solver.cpp:317]     Train net output #0: accuracy = 0.289062
I1226 14:12:58.770081 91405 solver.cpp:317]     Train net output #1: loss = 3.2667 (* 1 = 3.2667 loss)
I1226 14:12:58.980545 98828 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 14:12:58.980609 98828 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 14:12:58.980643 98828 solver.cpp:291] [0] Iteration 21, loss = 3.32589
I1226 14:12:58.980696 98828 solver.cpp:317]     Train net output #0: accuracy = 0.300781
I1226 14:12:58.980746 98828 solver.cpp:317]     Train net output #1: loss = 3.32589 (* 1 = 3.32589 loss)
I1226 14:12:59.003583 92274 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 14:12:59.003655 92274 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 14:12:59.047277 96952 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 14:12:59.047365 96952 MultiSolver.cpp:109] [2] PROFILING BEGIN[WaitingToSync]
I1226 14:12:59.047408 96952 solver.cpp:291] [2] Iteration 4, loss = 3.41456
I1226 14:12:59.047475 96952 solver.cpp:317]     Train net output #0: accuracy = 0.320312
I1226 14:12:59.047547 96952 solver.cpp:317]     Train net output #1: loss = 3.41456 (* 1 = 3.41456 loss)
I1226 14:12:59.233114 91711 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 14:12:59.233197 91711 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 14:12:59.233256 91711 solver.cpp:291] [3] Iteration 21, loss = 3.10271
I1226 14:12:59.233311 91711 solver.cpp:317]     Train net output #0: accuracy = 0.363281
I1226 14:12:59.233361 91711 solver.cpp:317]     Train net output #1: loss = 3.10271 (* 1 = 3.10271 loss)
I1226 14:12:59.394745 95316 MultiSolver.cpp:108] [6] PROFILING END[Backward]
I1226 14:12:59.394842 95316 MultiSolver.cpp:109] [6] PROFILING BEGIN[WaitingToSync]
I1226 14:12:59.394886 95316 solver.cpp:291] [6] Iteration 4, loss = 3.33578
I1226 14:12:59.394949 95316 solver.cpp:317]     Train net output #0: accuracy = 0.28125
I1226 14:12:59.395025 95316 solver.cpp:317]     Train net output #1: loss = 3.33578 (* 1 = 3.33578 loss)
I1226 14:12:59.414705 90185 MultiSolver.cpp:108] [7] PROFILING END[Backward]
I1226 14:12:59.414770 90185 MultiSolver.cpp:109] [7] PROFILING BEGIN[WaitingToSync]
I1226 14:12:59.414805 90185 solver.cpp:291] [7] Iteration 21, loss = 3.0375
I1226 14:12:59.414860 90185 solver.cpp:317]     Train net output #0: accuracy = 0.320312
I1226 14:12:59.414908 90185 solver.cpp:317]     Train net output #1: loss = 3.0375 (* 1 = 3.0375 loss)
I1226 14:12:59.927001 121743 MultiSolver.cpp:92] [5] PROFILING END[Forward]
I1226 14:12:59.927528 121743 MultiSolver.cpp:94] [5] PROFILING BEGIN[Backward]
I1226 14:13:00.033395 92274 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 14:13:00.033481 92274 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 14:13:00.033534 92274 solver.cpp:291] [1] Iteration 22, loss = 3.10029
I1226 14:13:00.033586 92274 solver.cpp:317]     Train net output #0: accuracy = 0.375
I1226 14:13:00.033658 92274 solver.cpp:317]     Train net output #1: loss = 3.10029 (* 1 = 3.10029 loss)
I1226 14:13:00.551573 91405 MultiSolver.cpp:92] [4] PROFILING END[Forward]
I1226 14:13:00.552081 91405 MultiSolver.cpp:94] [4] PROFILING BEGIN[Backward]
I1226 14:13:00.748364 98828 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 14:13:00.748440 98828 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 14:13:01.002019 121743 MultiSolver.cpp:108] [5] PROFILING END[Backward]
I1226 14:13:01.002082 121743 MultiSolver.cpp:109] [5] PROFILING BEGIN[WaitingToSync]
I1226 14:13:01.002115 121743 solver.cpp:291] [5] Iteration 22, loss = 3.19266
I1226 14:13:01.002197 121743 solver.cpp:317]     Train net output #0: accuracy = 0.292969
I1226 14:13:01.002537 121743 solver.cpp:317]     Train net output #1: loss = 3.19266 (* 1 = 3.19266 loss)
I1226 14:13:01.090546 90185 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 14:13:01.090644 90185 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 14:13:01.318545 91711 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 14:13:01.318619 91711 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 14:13:01.641435 92274 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 14:13:01.641510 92274 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 14:13:01.877956 91405 MultiSolver.cpp:108] [4] PROFILING END[Backward]
I1226 14:13:01.878027 91405 MultiSolver.cpp:109] [4] PROFILING BEGIN[WaitingToSync]
I1226 14:13:01.878063 91405 solver.cpp:291] [4] Iteration 22, loss = 3.26439
I1226 14:13:01.878116 91405 solver.cpp:317]     Train net output #0: accuracy = 0.335938
I1226 14:13:01.878167 91405 solver.cpp:317]     Train net output #1: loss = 3.26439 (* 1 = 3.26439 loss)
I1226 14:13:02.221443 98828 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 14:13:02.221514 98828 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 14:13:02.221549 98828 solver.cpp:291] [0] Iteration 22, loss = 3.07409
I1226 14:13:02.221603 98828 solver.cpp:317]     Train net output #0: accuracy = 0.378906
I1226 14:13:02.221652 98828 solver.cpp:317]     Train net output #1: loss = 3.07409 (* 1 = 3.07409 loss)
I1226 14:13:02.286051 90185 MultiSolver.cpp:108] [7] PROFILING END[Backward]
I1226 14:13:02.286120 90185 MultiSolver.cpp:109] [7] PROFILING BEGIN[WaitingToSync]
I1226 14:13:02.286155 90185 solver.cpp:291] [7] Iteration 22, loss = 3.26249
I1226 14:13:02.286208 90185 solver.cpp:317]     Train net output #0: accuracy = 0.277344
I1226 14:13:02.286257 90185 solver.cpp:317]     Train net output #1: loss = 3.26249 (* 1 = 3.26249 loss)
I1226 14:13:02.439522 91711 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 14:13:02.439615 91711 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 14:13:02.439651 91711 solver.cpp:291] [3] Iteration 22, loss = 2.98897
I1226 14:13:02.439728 91711 solver.cpp:317]     Train net output #0: accuracy = 0.347656
I1226 14:13:02.440026 91711 solver.cpp:317]     Train net output #1: loss = 2.98897 (* 1 = 2.98897 loss)
I1226 14:13:02.719463 92274 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 14:13:02.719529 92274 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 14:13:02.719563 92274 solver.cpp:291] [1] Iteration 23, loss = 3.42058
I1226 14:13:02.719616 92274 solver.cpp:317]     Train net output #0: accuracy = 0.289062
I1226 14:13:02.719666 92274 solver.cpp:317]     Train net output #1: loss = 3.42058 (* 1 = 3.42058 loss)
I1226 14:13:02.797124 121743 MultiSolver.cpp:92] [5] PROFILING END[Forward]
I1226 14:13:02.797194 121743 MultiSolver.cpp:94] [5] PROFILING BEGIN[Backward]
I1226 14:13:03.493880 91405 MultiSolver.cpp:92] [4] PROFILING END[Forward]
I1226 14:13:03.493964 91405 MultiSolver.cpp:94] [4] PROFILING BEGIN[Backward]
I1226 14:13:03.610417 98828 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 14:13:03.610491 98828 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 14:13:03.896947 121743 MultiSolver.cpp:108] [5] PROFILING END[Backward]
I1226 14:13:03.897017 121743 MultiSolver.cpp:109] [5] PROFILING BEGIN[WaitingToSync]
I1226 14:13:03.897050 121743 solver.cpp:291] [5] Iteration 23, loss = 3.37247
I1226 14:13:03.897104 121743 solver.cpp:317]     Train net output #0: accuracy = 0.277344
I1226 14:13:03.897155 121743 solver.cpp:317]     Train net output #1: loss = 3.37247 (* 1 = 3.37247 loss)
I1226 14:13:03.957926 90185 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 14:13:03.958001 90185 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 14:13:04.051462 96952 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 14:13:04.051555 96952 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 14:13:04.211876 91711 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 14:13:04.211957 91711 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 14:13:04.406330 95316 MultiSolver.cpp:92] [6] PROFILING END[Forward]
I1226 14:13:04.406425 95316 MultiSolver.cpp:94] [6] PROFILING BEGIN[Backward]
I1226 14:13:04.461006 92274 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 14:13:04.461860 92274 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 14:13:04.973912 91405 MultiSolver.cpp:108] [4] PROFILING END[Backward]
I1226 14:13:04.974014 91405 MultiSolver.cpp:109] [4] PROFILING BEGIN[WaitingToSync]
I1226 14:13:04.974069 91405 solver.cpp:291] [4] Iteration 23, loss = 3.37338
I1226 14:13:04.974128 91405 solver.cpp:317]     Train net output #0: accuracy = 0.324219
I1226 14:13:04.974478 91405 solver.cpp:317]     Train net output #1: loss = 3.37338 (* 1 = 3.37338 loss)
I1226 14:13:05.173084 98828 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 14:13:05.173153 98828 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 14:13:05.173223 98828 solver.cpp:291] [0] Iteration 23, loss = 3.14264
I1226 14:13:05.173281 98828 solver.cpp:317]     Train net output #0: accuracy = 0.347656
I1226 14:13:05.173363 98828 solver.cpp:317]     Train net output #1: loss = 3.14264 (* 1 = 3.14264 loss)
I1226 14:13:05.320426 90185 MultiSolver.cpp:108] [7] PROFILING END[Backward]
I1226 14:13:05.320492 90185 MultiSolver.cpp:109] [7] PROFILING BEGIN[WaitingToSync]
I1226 14:13:05.320528 90185 solver.cpp:291] [7] Iteration 23, loss = 3.29813
I1226 14:13:05.320602 90185 solver.cpp:317]     Train net output #0: accuracy = 0.3125
I1226 14:13:05.320652 90185 solver.cpp:317]     Train net output #1: loss = 3.29813 (* 1 = 3.29813 loss)
I1226 14:13:05.595705 91711 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 14:13:05.595775 91711 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 14:13:05.595827 91711 solver.cpp:291] [3] Iteration 23, loss = 3.26178
I1226 14:13:05.595888 91711 solver.cpp:317]     Train net output #0: accuracy = 0.339844
I1226 14:13:05.595958 91711 solver.cpp:317]     Train net output #1: loss = 3.26178 (* 1 = 3.26178 loss)
I1226 14:13:05.708959 121743 MultiSolver.cpp:92] [5] PROFILING END[Forward]
I1226 14:13:05.709031 121743 MultiSolver.cpp:94] [5] PROFILING BEGIN[Backward]
I1226 14:13:05.772450 92274 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 14:13:05.772512 92274 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 14:13:05.772545 92274 solver.cpp:291] [1] Iteration 24, loss = 3.0581
I1226 14:13:05.772599 92274 solver.cpp:317]     Train net output #0: accuracy = 0.363281
I1226 14:13:05.772671 92274 solver.cpp:317]     Train net output #1: loss = 3.0581 (* 1 = 3.0581 loss)
I1226 14:13:06.550220 91405 MultiSolver.cpp:92] [4] PROFILING END[Forward]
I1226 14:13:06.550298 91405 MultiSolver.cpp:94] [4] PROFILING BEGIN[Backward]
User defined signal 2
