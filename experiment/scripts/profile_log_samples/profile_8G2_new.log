Dec 26 13:43:54 2016 94216 3 10.1 NIOS_DEBUG: stdin_fd set to -1
Dec 26 13:43:54 2016 94216 3 10.1 NIOS_DEBUG: fds[0] has a value of -1
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 13:43:56.822737 85984 mpiutil.cpp:166] Process rank 5 from number of 9 processes running on knl-node022
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 13:43:56.827522 98538 mpiutil.cpp:166] Process rank 3 from number of 9 processes running on knl-node016
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 13:43:56.837998 91279 mpiutil.cpp:166] Process rank 2 from number of 9 processes running on knl-node047
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 13:43:56.809137 91110 mpiutil.cpp:166] Process rank 6 from number of 9 processes running on knl-node032
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 13:43:56.821396 85406 mpiutil.cpp:166] Process rank 8 from number of 9 processes running on knl-node039
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 13:43:56.817914 97018 mpiutil.cpp:166] Process rank 7 from number of 9 processes running on knl-node078
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 13:43:56.811712 94226 mpiutil.cpp:166] Process rank 0 from number of 9 processes running on knl-node015
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 13:43:56.829113 94885 mpiutil.cpp:166] Process rank 4 from number of 9 processes running on knl-node084
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 13:43:56.822108 92417 mpiutil.cpp:166] Process rank 1 from number of 9 processes running on knl-node025
I1226 13:43:56.842392 94885 caffe.cpp:316] Use CPU.
I1226 13:43:56.841503 98538 caffe.cpp:316] Use CPU.
I1226 13:43:56.831715 97018 caffe.cpp:316] Use CPU.
I1226 13:43:56.837721 85984 caffe.cpp:316] Use CPU.
I1226 13:43:56.842612 98538 solver.cpp:93] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_4.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 13:43:56.826629 94226 caffe.cpp:316] Use CPU.
I1226 13:43:56.843479 98538 solver.cpp:128] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_4.prototxt
I1226 13:43:56.843776 94885 solver.cpp:93] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_5.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 13:43:56.837116 85406 caffe.cpp:316] Use CPU.
I1226 13:43:56.845038 94885 solver.cpp:128] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_5.prototxt
I1226 13:43:56.833076 97018 solver.cpp:93] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_8.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 13:43:56.834177 97018 solver.cpp:128] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_8.prototxt
I1226 13:43:56.838816 85984 solver.cpp:93] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_6.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 13:43:56.839792 85984 solver.cpp:128] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_6.prototxt
I1226 13:43:56.827790 94226 solver.cpp:93] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_1.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 13:43:56.828791 94226 solver.cpp:128] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_1.prototxt
I1226 13:43:56.838240 85406 solver.cpp:93] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_dummy.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 13:43:56.839077 85406 solver.cpp:128] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_dummy.prototxt
I1226 13:43:56.826180 91110 caffe.cpp:316] Use CPU.
I1226 13:43:56.839114 92417 caffe.cpp:316] Use CPU.
I1226 13:43:56.840291 92417 solver.cpp:93] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_2.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 13:43:56.827564 91110 solver.cpp:93] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_7.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 13:43:56.841305 92417 solver.cpp:128] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_2.prototxt
I1226 13:43:56.828656 91110 solver.cpp:128] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_7.prototxt
I1226 13:43:56.862406 91279 caffe.cpp:316] Use CPU.
I1226 13:43:56.863544 91279 solver.cpp:93] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_3.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 13:43:56.864473 91279 solver.cpp:128] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_3.prototxt
I1226 13:43:56.875545 98538 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 13:43:56.875615 98538 cpu_info.cpp:455] Total number of sockets: 1
I1226 13:43:56.869359 85406 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 13:43:56.869434 85406 cpu_info.cpp:455] Total number of sockets: 1
I1226 13:43:56.875636 98538 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 13:43:56.875655 98538 cpu_info.cpp:461] Total number of processors: 272
I1226 13:43:56.875674 98538 cpu_info.cpp:464] GPU is used: no
I1226 13:43:56.875692 98538 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 13:43:56.869457 85406 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 13:43:56.869477 85406 cpu_info.cpp:461] Total number of processors: 272
I1226 13:43:56.869498 85406 cpu_info.cpp:464] GPU is used: no
I1226 13:43:56.869518 85406 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 13:43:56.875740 98538 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 13:43:56.869537 85406 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 13:43:56.871256 85984 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 13:43:56.871330 85984 cpu_info.cpp:455] Total number of sockets: 1
I1226 13:43:56.871353 85984 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 13:43:56.871372 85984 cpu_info.cpp:461] Total number of processors: 272
I1226 13:43:56.871392 85984 cpu_info.cpp:464] GPU is used: no
I1226 13:43:56.871412 85984 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 13:43:56.871430 85984 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 13:43:56.860929 94226 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 13:43:56.861001 94226 cpu_info.cpp:455] Total number of sockets: 1
I1226 13:43:56.861029 94226 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 13:43:56.861054 94226 cpu_info.cpp:461] Total number of processors: 272
I1226 13:43:56.861081 94226 cpu_info.cpp:464] GPU is used: no
I1226 13:43:56.861109 94226 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 13:43:56.861135 94226 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 13:43:56.867805 97018 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 13:43:56.867895 97018 cpu_info.cpp:455] Total number of sockets: 1
I1226 13:43:56.867924 97018 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 13:43:56.867946 97018 cpu_info.cpp:461] Total number of processors: 272
I1226 13:43:56.867969 97018 cpu_info.cpp:464] GPU is used: no
I1226 13:43:56.867992 97018 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 13:43:56.868013 97018 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 13:43:56.879179 94885 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 13:43:56.879266 94885 cpu_info.cpp:455] Total number of sockets: 1
I1226 13:43:56.879292 94885 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 13:43:56.879315 94885 cpu_info.cpp:461] Total number of processors: 272
I1226 13:43:56.879338 94885 cpu_info.cpp:464] GPU is used: no
I1226 13:43:56.879360 94885 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 13:43:56.879405 94885 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 13:43:56.860967 91110 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 13:43:56.861037 91110 cpu_info.cpp:455] Total number of sockets: 1
I1226 13:43:56.861078 91110 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 13:43:56.861101 91110 cpu_info.cpp:461] Total number of processors: 272
I1226 13:43:56.861121 91110 cpu_info.cpp:464] GPU is used: no
I1226 13:43:56.861141 91110 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 13:43:56.861160 91110 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 13:43:56.873936 92417 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 13:43:56.874011 92417 cpu_info.cpp:455] Total number of sockets: 1
I1226 13:43:56.874034 92417 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 13:43:56.874053 92417 cpu_info.cpp:461] Total number of processors: 272
I1226 13:43:56.874073 92417 cpu_info.cpp:464] GPU is used: no
I1226 13:43:56.874094 92417 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 13:43:56.874112 92417 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 13:43:56.895015 91279 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 13:43:56.895089 91279 cpu_info.cpp:455] Total number of sockets: 1
I1226 13:43:56.895112 91279 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 13:43:56.895131 91279 cpu_info.cpp:461] Total number of processors: 272
I1226 13:43:56.895151 91279 cpu_info.cpp:464] GPU is used: no
I1226 13:43:56.895171 91279 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 13:43:56.895197 91279 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 13:43:56.904412 94885 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 13:43:56.904772 94885 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 13:43:56.906915 94885 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_4"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 13:43:56.902854 94226 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 13:43:56.903249 94226 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 13:43:56.914564 85406 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 13:43:56.915027 85406 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 13:43:56.905349 94226 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_0"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 13:43:56.913930 97018 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 13:43:56.917363 85406 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "DummyData"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  dummy_data_param {
    data_filler {
      type: "gaussian"
      std: 1
    }
    data_filler {
      type: "constant"
      value: 0
    }
    shape {
      dim: 64
      dim: 3
      dim: 227
      dim: 227
    }
    shape {
      dim: 64
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 13:43:56.914273 97018 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 13:43:56.924185 98538 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 13:43:56.924566 98538 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 13:43:56.916343 97018 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_7"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 13:43:56.921813 85984 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 13:43:56.926498 98538 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_3"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 13:43:56.922194 85984 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 13:43:56.921281 92417 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 13:43:56.921723 92417 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 13:43:56.924257 85984 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_5"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 13:43:56.923789 92417 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_1"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 13:43:56.915638 91110 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 13:43:56.916028 91110 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 13:43:56.918308 91110 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_6"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 13:43:56.941812 94885 layer_factory.hpp:114] Creating layer data
I1226 13:43:56.947702 98538 layer_factory.hpp:114] Creating layer data
I1226 13:43:56.962442 91279 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 13:43:56.962824 91279 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 13:43:56.965188 91279 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_2"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 13:43:56.960206 98538 net.cpp:178] Creating Layer data
I1226 13:43:56.960289 98538 net.cpp:586] data -> data
I1226 13:43:56.960364 98538 net.cpp:586] data -> label
I1226 13:43:56.947895 91110 layer_factory.hpp:114] Creating layer data
I1226 13:43:56.968507 94885 net.cpp:178] Creating Layer data
I1226 13:43:56.968634 94885 net.cpp:586] data -> data
I1226 13:43:56.968758 94885 net.cpp:586] data -> label
I1226 13:43:56.961685 85406 layer_factory.hpp:114] Creating layer data
I1226 13:43:56.961835 85406 net.cpp:178] Creating Layer data
I1226 13:43:56.961882 85406 net.cpp:586] data -> data
I1226 13:43:56.961977 85406 net.cpp:586] data -> label
I1226 13:43:56.960616 91110 net.cpp:178] Creating Layer data
I1226 13:43:56.960706 91110 net.cpp:586] data -> data
I1226 13:43:56.960790 91110 net.cpp:586] data -> label
I1226 13:43:56.963933 94226 layer_factory.hpp:114] Creating layer data
I1226 13:43:56.981459 98540 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_3
I1226 13:43:56.994040 91279 layer_factory.hpp:114] Creating layer data
I1226 13:43:56.990145 94888 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_4
I1226 13:43:57.004319 91279 net.cpp:178] Creating Layer data
I1226 13:43:57.004413 91279 net.cpp:586] data -> data
I1226 13:43:57.004488 91279 net.cpp:586] data -> label
I1226 13:43:56.978446 94226 net.cpp:178] Creating Layer data
I1226 13:43:56.978533 94226 net.cpp:586] data -> data
I1226 13:43:56.978612 94226 net.cpp:586] data -> label
I1226 13:43:56.990571 92417 layer_factory.hpp:114] Creating layer data
I1226 13:43:57.003119 98538 data_layer.cpp:80] output data size: 64,3,227,227
I1226 13:43:56.998949 92417 net.cpp:178] Creating Layer data
I1226 13:43:56.999030 92417 net.cpp:586] data -> data
I1226 13:43:56.999107 92417 net.cpp:586] data -> label
I1226 13:43:56.986593 91112 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_6
I1226 13:43:57.000454 85406 net.cpp:228] Setting up data
I1226 13:43:57.000573 85406 net.cpp:235] Top shape: 64 3 227 227 (9893568)
I1226 13:43:57.000630 85406 net.cpp:235] Top shape: 64 1 1 1 (64)
I1226 13:43:57.000655 85406 net.cpp:243] Memory required for data: 39574528
I1226 13:43:57.000692 85406 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 13:43:57.000782 85406 net.cpp:178] Creating Layer label_data_1_split
I1226 13:43:57.000813 85406 net.cpp:612] label_data_1_split <- label
I1226 13:43:57.000928 85406 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 13:43:57.000978 85406 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 13:43:56.992063 94229 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_0
I1226 13:43:57.012254 94885 data_layer.cpp:80] output data size: 64,3,227,227
I1226 13:43:56.996109 94226 data_layer.cpp:80] output data size: 64,3,227,227
I1226 13:43:56.995004 91110 data_layer.cpp:80] output data size: 64,3,227,227
I1226 13:43:57.012137 92420 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_1
I1226 13:43:57.012773 85406 net.cpp:228] Setting up label_data_1_split
I1226 13:43:57.012876 85406 net.cpp:235] Top shape: 64 1 1 1 (64)
I1226 13:43:57.012908 85406 net.cpp:235] Top shape: 64 1 1 1 (64)
I1226 13:43:57.012930 85406 net.cpp:243] Memory required for data: 39575040
I1226 13:43:57.012964 85406 layer_factory.hpp:114] Creating layer conv1
I1226 13:43:57.013058 85406 net.cpp:178] Creating Layer conv1
I1226 13:43:57.013090 85406 net.cpp:612] conv1 <- data
I1226 13:43:57.013130 85406 net.cpp:586] conv1 -> conv1
I1226 13:43:57.029939 91281 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_2
I1226 13:43:57.016103 92417 data_layer.cpp:80] output data size: 64,3,227,227
I1226 13:43:57.038460 91279 data_layer.cpp:80] output data size: 64,3,227,227
I1226 13:43:57.087352 97018 layer_factory.hpp:114] Creating layer data
I1226 13:43:57.101269 98538 net.cpp:228] Setting up data
I1226 13:43:57.101387 98538 net.cpp:235] Top shape: 64 3 227 227 (9893568)
I1226 13:43:57.101424 98538 net.cpp:235] Top shape: 64 (64)
I1226 13:43:57.101449 98538 net.cpp:243] Memory required for data: 39574528
I1226 13:43:57.101487 98538 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 13:43:57.101583 98538 net.cpp:178] Creating Layer label_data_1_split
I1226 13:43:57.101712 98538 net.cpp:612] label_data_1_split <- label
I1226 13:43:57.101760 98538 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 13:43:57.101812 98538 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 13:43:57.095777 97018 net.cpp:178] Creating Layer data
I1226 13:43:57.095916 97018 net.cpp:586] data -> data
I1226 13:43:57.096048 97018 net.cpp:586] data -> label
I1226 13:43:57.102082 85984 layer_factory.hpp:114] Creating layer data
I1226 13:43:57.092820 91110 net.cpp:228] Setting up data
I1226 13:43:57.092984 91110 net.cpp:235] Top shape: 64 3 227 227 (9893568)
I1226 13:43:57.093022 91110 net.cpp:235] Top shape: 64 (64)
I1226 13:43:57.093047 91110 net.cpp:243] Memory required for data: 39574528
I1226 13:43:57.093116 91110 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 13:43:57.093206 91110 net.cpp:178] Creating Layer label_data_1_split
I1226 13:43:57.093369 91110 net.cpp:612] label_data_1_split <- label
I1226 13:43:57.093418 91110 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 13:43:57.093487 91110 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 13:43:57.108970 85984 net.cpp:178] Creating Layer data
I1226 13:43:57.109053 85984 net.cpp:586] data -> data
I1226 13:43:57.109153 85984 net.cpp:586] data -> label
I1226 13:43:57.099930 94226 net.cpp:228] Setting up data
I1226 13:43:57.100066 94226 net.cpp:235] Top shape: 64 3 227 227 (9893568)
I1226 13:43:57.100106 94226 net.cpp:235] Top shape: 64 (64)
I1226 13:43:57.100132 94226 net.cpp:243] Memory required for data: 39574528
I1226 13:43:57.100174 94226 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 13:43:57.100267 94226 net.cpp:178] Creating Layer label_data_1_split
I1226 13:43:57.100437 94226 net.cpp:612] label_data_1_split <- label
I1226 13:43:57.100497 94226 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 13:43:57.100555 94226 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 13:43:57.106859 97021 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_7
I1226 13:43:57.111469 97018 data_layer.cpp:80] output data size: 64,3,227,227
I1226 13:43:57.122140 98538 net.cpp:228] Setting up label_data_1_split
I1226 13:43:57.122258 98538 net.cpp:235] Top shape: 64 (64)
I1226 13:43:57.122293 98538 net.cpp:235] Top shape: 64 (64)
I1226 13:43:57.122318 98538 net.cpp:243] Memory required for data: 39575040
I1226 13:43:57.122372 98538 layer_factory.hpp:114] Creating layer conv1
I1226 13:43:57.122596 98538 net.cpp:178] Creating Layer conv1
I1226 13:43:57.122640 98538 net.cpp:612] conv1 <- data
I1226 13:43:57.123008 98538 net.cpp:586] conv1 -> conv1
I1226 13:43:57.137650 91279 net.cpp:228] Setting up data
I1226 13:43:57.137776 91279 net.cpp:235] Top shape: 64 3 227 227 (9893568)
I1226 13:43:57.137815 91279 net.cpp:235] Top shape: 64 (64)
I1226 13:43:57.138120 91279 net.cpp:243] Memory required for data: 39574528
I1226 13:43:57.138190 91279 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 13:43:57.138351 91279 net.cpp:178] Creating Layer label_data_1_split
I1226 13:43:57.138497 91279 net.cpp:612] label_data_1_split <- label
I1226 13:43:57.138546 91279 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 13:43:57.138695 91279 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 13:43:57.111088 91110 net.cpp:228] Setting up label_data_1_split
I1226 13:43:57.111196 91110 net.cpp:235] Top shape: 64 (64)
I1226 13:43:57.111263 91110 net.cpp:235] Top shape: 64 (64)
I1226 13:43:57.111404 91110 net.cpp:243] Memory required for data: 39575040
I1226 13:43:57.111441 91110 layer_factory.hpp:114] Creating layer conv1
I1226 13:43:57.111559 91110 net.cpp:178] Creating Layer conv1
I1226 13:43:57.111596 91110 net.cpp:612] conv1 <- data
I1226 13:43:57.111639 91110 net.cpp:586] conv1 -> conv1
I1226 13:43:57.124833 92417 net.cpp:228] Setting up data
I1226 13:43:57.124954 92417 net.cpp:235] Top shape: 64 3 227 227 (9893568)
I1226 13:43:57.124995 92417 net.cpp:235] Top shape: 64 (64)
I1226 13:43:57.125020 92417 net.cpp:243] Memory required for data: 39574528
I1226 13:43:57.125072 92417 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 13:43:57.125164 92417 net.cpp:178] Creating Layer label_data_1_split
I1226 13:43:57.125313 92417 net.cpp:612] label_data_1_split <- label
I1226 13:43:57.125365 92417 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 13:43:57.125417 92417 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 13:43:57.157348 91279 net.cpp:228] Setting up label_data_1_split
I1226 13:43:57.157462 91279 net.cpp:235] Top shape: 64 (64)
I1226 13:43:57.157512 91279 net.cpp:235] Top shape: 64 (64)
I1226 13:43:57.157539 91279 net.cpp:243] Memory required for data: 39575040
I1226 13:43:57.157676 91279 layer_factory.hpp:114] Creating layer conv1
I1226 13:43:57.157778 91279 net.cpp:178] Creating Layer conv1
I1226 13:43:57.157817 91279 net.cpp:612] conv1 <- data
I1226 13:43:57.157860 91279 net.cpp:586] conv1 -> conv1
I1226 13:43:57.143105 85986 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_5
I1226 13:43:57.137712 94226 net.cpp:228] Setting up label_data_1_split
I1226 13:43:57.137817 94226 net.cpp:235] Top shape: 64 (64)
I1226 13:43:57.137850 94226 net.cpp:235] Top shape: 64 (64)
I1226 13:43:57.137873 94226 net.cpp:243] Memory required for data: 39575040
I1226 13:43:57.137910 94226 layer_factory.hpp:114] Creating layer conv1
I1226 13:43:57.138015 94226 net.cpp:178] Creating Layer conv1
I1226 13:43:57.138063 94226 net.cpp:612] conv1 <- data
I1226 13:43:57.138110 94226 net.cpp:586] conv1 -> conv1
I1226 13:43:57.151688 85984 data_layer.cpp:80] output data size: 64,3,227,227
I1226 13:43:57.151168 92417 net.cpp:228] Setting up label_data_1_split
I1226 13:43:57.151270 92417 net.cpp:235] Top shape: 64 (64)
I1226 13:43:57.151304 92417 net.cpp:235] Top shape: 64 (64)
I1226 13:43:57.151327 92417 net.cpp:243] Memory required for data: 39575040
I1226 13:43:57.151363 92417 layer_factory.hpp:114] Creating layer conv1
I1226 13:43:57.151615 92417 net.cpp:178] Creating Layer conv1
I1226 13:43:57.151718 92417 net.cpp:612] conv1 <- data
I1226 13:43:57.151764 92417 net.cpp:586] conv1 -> conv1
I1226 13:43:57.155400 85406 net.cpp:228] Setting up conv1
I1226 13:43:57.155514 85406 net.cpp:235] Top shape: 64 96 55 55 (18585600)
I1226 13:43:57.155539 85406 net.cpp:243] Memory required for data: 113917440
I1226 13:43:57.155715 85406 layer_factory.hpp:114] Creating layer relu1
I1226 13:43:57.155793 85406 net.cpp:178] Creating Layer relu1
I1226 13:43:57.155831 85406 net.cpp:612] relu1 <- conv1
I1226 13:43:57.155869 85406 net.cpp:573] relu1 -> conv1 (in-place)
I1226 13:43:57.155972 85406 net.cpp:228] Setting up relu1
I1226 13:43:57.156016 85406 net.cpp:235] Top shape: 64 96 55 55 (18585600)
I1226 13:43:57.156039 85406 net.cpp:243] Memory required for data: 188259840
I1226 13:43:57.156066 85406 layer_factory.hpp:114] Creating layer norm1
I1226 13:43:57.156128 85406 net.cpp:178] Creating Layer norm1
I1226 13:43:57.156159 85406 net.cpp:612] norm1 <- conv1
I1226 13:43:57.156191 85406 net.cpp:586] norm1 -> norm1
I1226 13:43:57.156272 85406 net.cpp:228] Setting up norm1
I1226 13:43:57.156316 85406 net.cpp:235] Top shape: 64 96 55 55 (18585600)
I1226 13:43:57.156337 85406 net.cpp:243] Memory required for data: 262602240
I1226 13:43:57.156363 85406 layer_factory.hpp:114] Creating layer pool1
I1226 13:43:57.156416 85406 net.cpp:178] Creating Layer pool1
I1226 13:43:57.156447 85406 net.cpp:612] pool1 <- norm1
I1226 13:43:57.156478 85406 net.cpp:586] pool1 -> pool1
I1226 13:43:57.156555 85406 net.cpp:228] Setting up pool1
I1226 13:43:57.156615 85406 net.cpp:235] Top shape: 64 96 27 27 (4478976)
I1226 13:43:57.156646 85406 net.cpp:243] Memory required for data: 280518144
I1226 13:43:57.156672 85406 layer_factory.hpp:114] Creating layer conv2
I1226 13:43:57.156743 85406 net.cpp:178] Creating Layer conv2
I1226 13:43:57.156774 85406 net.cpp:612] conv2 <- pool1
I1226 13:43:57.156816 85406 net.cpp:586] conv2 -> conv2
I1226 13:43:57.253216 94885 net.cpp:228] Setting up data
I1226 13:43:57.253342 94885 net.cpp:235] Top shape: 64 3 227 227 (9893568)
I1226 13:43:57.253392 94885 net.cpp:235] Top shape: 64 (64)
I1226 13:43:57.253422 94885 net.cpp:243] Memory required for data: 39574528
I1226 13:43:57.253484 94885 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 13:43:57.253597 94885 net.cpp:178] Creating Layer label_data_1_split
I1226 13:43:57.253747 94885 net.cpp:612] label_data_1_split <- label
I1226 13:43:57.253806 94885 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 13:43:57.253912 94885 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 13:43:57.251577 85984 net.cpp:228] Setting up data
I1226 13:43:57.251740 85984 net.cpp:235] Top shape: 64 3 227 227 (9893568)
I1226 13:43:57.251785 85984 net.cpp:235] Top shape: 64 (64)
I1226 13:43:57.251811 85984 net.cpp:243] Memory required for data: 39574528
I1226 13:43:57.251850 85984 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 13:43:57.251948 85984 net.cpp:178] Creating Layer label_data_1_split
I1226 13:43:57.252080 85984 net.cpp:612] label_data_1_split <- label
I1226 13:43:57.252125 85984 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 13:43:57.252182 85984 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 13:43:57.270416 85984 net.cpp:228] Setting up label_data_1_split
I1226 13:43:57.270540 85984 net.cpp:235] Top shape: 64 (64)
I1226 13:43:57.270577 85984 net.cpp:235] Top shape: 64 (64)
I1226 13:43:57.270601 85984 net.cpp:243] Memory required for data: 39575040
I1226 13:43:57.270638 85984 layer_factory.hpp:114] Creating layer conv1
I1226 13:43:57.271196 85984 net.cpp:178] Creating Layer conv1
I1226 13:43:57.271311 85984 net.cpp:612] conv1 <- data
I1226 13:43:57.271378 85984 net.cpp:586] conv1 -> conv1
I1226 13:43:57.279928 94885 net.cpp:228] Setting up label_data_1_split
I1226 13:43:57.280045 94885 net.cpp:235] Top shape: 64 (64)
I1226 13:43:57.280086 94885 net.cpp:235] Top shape: 64 (64)
I1226 13:43:57.280113 94885 net.cpp:243] Memory required for data: 39575040
I1226 13:43:57.280150 94885 layer_factory.hpp:114] Creating layer conv1
I1226 13:43:57.280380 94885 net.cpp:178] Creating Layer conv1
I1226 13:43:57.280472 94885 net.cpp:612] conv1 <- data
I1226 13:43:57.280524 94885 net.cpp:586] conv1 -> conv1
I1226 13:43:57.327819 98538 net.cpp:228] Setting up conv1
I1226 13:43:57.327955 98538 net.cpp:235] Top shape: 64 96 55 55 (18585600)
I1226 13:43:57.328272 98538 net.cpp:243] Memory required for data: 113917440
I1226 13:43:57.328461 98538 layer_factory.hpp:114] Creating layer relu1
I1226 13:43:57.329195 98538 net.cpp:178] Creating Layer relu1
I1226 13:43:57.329324 98538 net.cpp:612] relu1 <- conv1
I1226 13:43:57.329421 98538 net.cpp:573] relu1 -> conv1 (in-place)
I1226 13:43:57.329696 98538 net.cpp:228] Setting up relu1
I1226 13:43:57.329855 98538 net.cpp:235] Top shape: 64 96 55 55 (18585600)
I1226 13:43:57.329886 98538 net.cpp:243] Memory required for data: 188259840
I1226 13:43:57.329921 98538 layer_factory.hpp:114] Creating layer norm1
I1226 13:43:57.330142 98538 net.cpp:178] Creating Layer norm1
I1226 13:43:57.330193 98538 net.cpp:612] norm1 <- conv1
I1226 13:43:57.330236 98538 net.cpp:586] norm1 -> norm1
I1226 13:43:57.330703 98538 net.cpp:228] Setting up norm1
I1226 13:43:57.330783 98538 net.cpp:235] Top shape: 64 96 55 55 (18585600)
I1226 13:43:57.330829 98538 net.cpp:243] Memory required for data: 262602240
I1226 13:43:57.330869 98538 layer_factory.hpp:114] Creating layer pool1
I1226 13:43:57.330950 98538 net.cpp:178] Creating Layer pool1
I1226 13:43:57.330988 98538 net.cpp:612] pool1 <- norm1
I1226 13:43:57.331037 98538 net.cpp:586] pool1 -> pool1
I1226 13:43:57.331488 98538 net.cpp:228] Setting up pool1
I1226 13:43:57.331564 98538 net.cpp:235] Top shape: 64 96 27 27 (4478976)
I1226 13:43:57.331590 98538 net.cpp:243] Memory required for data: 280518144
I1226 13:43:57.331621 98538 layer_factory.hpp:114] Creating layer conv2
I1226 13:43:57.331704 98538 net.cpp:178] Creating Layer conv2
I1226 13:43:57.331743 98538 net.cpp:612] conv2 <- pool1
I1226 13:43:57.331789 98538 net.cpp:586] conv2 -> conv2
I1226 13:43:57.325325 91110 net.cpp:228] Setting up conv1
I1226 13:43:57.325448 91110 net.cpp:235] Top shape: 64 96 55 55 (18585600)
I1226 13:43:57.325505 91110 net.cpp:243] Memory required for data: 113917440
I1226 13:43:57.325659 91110 layer_factory.hpp:114] Creating layer relu1
I1226 13:43:57.325785 91110 net.cpp:178] Creating Layer relu1
I1226 13:43:57.325947 91110 net.cpp:612] relu1 <- conv1
I1226 13:43:57.325996 91110 net.cpp:573] relu1 -> conv1 (in-place)
I1226 13:43:57.326128 91110 net.cpp:228] Setting up relu1
I1226 13:43:57.326527 91110 net.cpp:235] Top shape: 64 96 55 55 (18585600)
I1226 13:43:57.326591 91110 net.cpp:243] Memory required for data: 188259840
I1226 13:43:57.326632 91110 layer_factory.hpp:114] Creating layer norm1
I1226 13:43:57.326753 91110 net.cpp:178] Creating Layer norm1
I1226 13:43:57.326843 91110 net.cpp:612] norm1 <- conv1
I1226 13:43:57.326899 91110 net.cpp:586] norm1 -> norm1
I1226 13:43:57.327026 91110 net.cpp:228] Setting up norm1
I1226 13:43:57.327157 91110 net.cpp:235] Top shape: 64 96 55 55 (18585600)
I1226 13:43:57.327196 91110 net.cpp:243] Memory required for data: 262602240
I1226 13:43:57.327245 91110 layer_factory.hpp:114] Creating layer pool1
I1226 13:43:57.327543 91110 net.cpp:178] Creating Layer pool1
I1226 13:43:57.327597 91110 net.cpp:612] pool1 <- norm1
I1226 13:43:57.327652 91110 net.cpp:586] pool1 -> pool1
I1226 13:43:57.327790 91110 net.cpp:228] Setting up pool1
I1226 13:43:57.327849 91110 net.cpp:235] Top shape: 64 96 27 27 (4478976)
I1226 13:43:57.328212 91110 net.cpp:243] Memory required for data: 280518144
I1226 13:43:57.328303 91110 layer_factory.hpp:114] Creating layer conv2
I1226 13:43:57.328408 91110 net.cpp:178] Creating Layer conv2
I1226 13:43:57.328490 91110 net.cpp:612] conv2 <- pool1
I1226 13:43:57.328550 91110 net.cpp:586] conv2 -> conv2
I1226 13:43:57.372267 91279 net.cpp:228] Setting up conv1
I1226 13:43:57.372381 91279 net.cpp:235] Top shape: 64 96 55 55 (18585600)
I1226 13:43:57.372411 91279 net.cpp:243] Memory required for data: 113917440
I1226 13:43:57.372531 91279 layer_factory.hpp:114] Creating layer relu1
I1226 13:43:57.372628 91279 net.cpp:178] Creating Layer relu1
I1226 13:43:57.372670 91279 net.cpp:612] relu1 <- conv1
I1226 13:43:57.372711 91279 net.cpp:573] relu1 -> conv1 (in-place)
I1226 13:43:57.372838 91279 net.cpp:228] Setting up relu1
I1226 13:43:57.372886 91279 net.cpp:235] Top shape: 64 96 55 55 (18585600)
I1226 13:43:57.372911 91279 net.cpp:243] Memory required for data: 188259840
I1226 13:43:57.372942 91279 layer_factory.hpp:114] Creating layer norm1
I1226 13:43:57.373019 91279 net.cpp:178] Creating Layer norm1
I1226 13:43:57.373056 91279 net.cpp:612] norm1 <- conv1
I1226 13:43:57.373095 91279 net.cpp:586] norm1 -> norm1
I1226 13:43:57.373190 91279 net.cpp:228] Setting up norm1
I1226 13:43:57.373270 91279 net.cpp:235] Top shape: 64 96 55 55 (18585600)
I1226 13:43:57.373294 91279 net.cpp:243] Memory required for data: 262602240
I1226 13:43:57.373325 91279 layer_factory.hpp:114] Creating layer pool1
I1226 13:43:57.373404 91279 net.cpp:178] Creating Layer pool1
I1226 13:43:57.373435 91279 net.cpp:612] pool1 <- norm1
I1226 13:43:57.373482 91279 net.cpp:586] pool1 -> pool1
I1226 13:43:57.373589 91279 net.cpp:228] Setting up pool1
I1226 13:43:57.373638 91279 net.cpp:235] Top shape: 64 96 27 27 (4478976)
I1226 13:43:57.373662 91279 net.cpp:243] Memory required for data: 280518144
I1226 13:43:57.373690 91279 layer_factory.hpp:114] Creating layer conv2
I1226 13:43:57.373769 91279 net.cpp:178] Creating Layer conv2
I1226 13:43:57.373805 91279 net.cpp:612] conv2 <- pool1
I1226 13:43:57.373845 91279 net.cpp:586] conv2 -> conv2
I1226 13:43:57.367310 85406 net.cpp:228] Setting up conv2
I1226 13:43:57.367422 85406 net.cpp:235] Top shape: 64 256 27 27 (11943936)
I1226 13:43:57.367449 85406 net.cpp:243] Memory required for data: 328293888
I1226 13:43:57.367522 85406 layer_factory.hpp:114] Creating layer relu2
I1226 13:43:57.367620 85406 net.cpp:178] Creating Layer relu2
I1226 13:43:57.367667 85406 net.cpp:612] relu2 <- conv2
I1226 13:43:57.367710 85406 net.cpp:573] relu2 -> conv2 (in-place)
I1226 13:43:57.367815 85406 net.cpp:228] Setting up relu2
I1226 13:43:57.367867 85406 net.cpp:235] Top shape: 64 256 27 27 (11943936)
I1226 13:43:57.367902 85406 net.cpp:243] Memory required for data: 376069632
I1226 13:43:57.367938 85406 layer_factory.hpp:114] Creating layer norm2
I1226 13:43:57.367988 85406 net.cpp:178] Creating Layer norm2
I1226 13:43:57.368021 85406 net.cpp:612] norm2 <- conv2
I1226 13:43:57.368072 85406 net.cpp:586] norm2 -> norm2
I1226 13:43:57.368156 85406 net.cpp:228] Setting up norm2
I1226 13:43:57.368206 85406 net.cpp:235] Top shape: 64 256 27 27 (11943936)
I1226 13:43:57.368238 85406 net.cpp:243] Memory required for data: 423845376
I1226 13:43:57.368271 85406 layer_factory.hpp:114] Creating layer pool2
I1226 13:43:57.368327 85406 net.cpp:178] Creating Layer pool2
I1226 13:43:57.368353 85406 net.cpp:612] pool2 <- norm2
I1226 13:43:57.368387 85406 net.cpp:586] pool2 -> pool2
I1226 13:43:57.368461 85406 net.cpp:228] Setting up pool2
I1226 13:43:57.368506 85406 net.cpp:235] Top shape: 64 256 13 13 (2768896)
I1226 13:43:57.368660 85406 net.cpp:243] Memory required for data: 434920960
I1226 13:43:57.368695 85406 layer_factory.hpp:114] Creating layer conv3
I1226 13:43:57.368785 85406 net.cpp:178] Creating Layer conv3
I1226 13:43:57.368818 85406 net.cpp:612] conv3 <- pool2
I1226 13:43:57.368860 85406 net.cpp:586] conv3 -> conv3
I1226 13:43:57.389461 92417 net.cpp:228] Setting up conv1
I1226 13:43:57.390058 92417 net.cpp:235] Top shape: 64 96 55 55 (18585600)
I1226 13:43:57.390120 92417 net.cpp:243] Memory required for data: 113917440
I1226 13:43:57.390236 92417 layer_factory.hpp:114] Creating layer relu1
I1226 13:43:57.390319 92417 net.cpp:178] Creating Layer relu1
I1226 13:43:57.390358 92417 net.cpp:612] relu1 <- conv1
I1226 13:43:57.391077 92417 net.cpp:573] relu1 -> conv1 (in-place)
I1226 13:43:57.391249 92417 net.cpp:228] Setting up relu1
I1226 13:43:57.391307 92417 net.cpp:235] Top shape: 64 96 55 55 (18585600)
I1226 13:43:57.391331 92417 net.cpp:243] Memory required for data: 188259840
I1226 13:43:57.391362 92417 layer_factory.hpp:114] Creating layer norm1
I1226 13:43:57.391434 92417 net.cpp:178] Creating Layer norm1
I1226 13:43:57.391500 92417 net.cpp:612] norm1 <- conv1
I1226 13:43:57.391546 92417 net.cpp:586] norm1 -> norm1
I1226 13:43:57.391651 92417 net.cpp:228] Setting up norm1
I1226 13:43:57.391705 92417 net.cpp:235] Top shape: 64 96 55 55 (18585600)
I1226 13:43:57.392313 92417 net.cpp:243] Memory required for data: 262602240
I1226 13:43:57.392396 92417 layer_factory.hpp:114] Creating layer pool1
I1226 13:43:57.392462 92417 net.cpp:178] Creating Layer pool1
I1226 13:43:57.392549 92417 net.cpp:612] pool1 <- norm1
I1226 13:43:57.392612 92417 net.cpp:586] pool1 -> pool1
I1226 13:43:57.392742 92417 net.cpp:228] Setting up pool1
I1226 13:43:57.392796 92417 net.cpp:235] Top shape: 64 96 27 27 (4478976)
I1226 13:43:57.392824 92417 net.cpp:243] Memory required for data: 280518144
I1226 13:43:57.392858 92417 layer_factory.hpp:114] Creating layer conv2
I1226 13:43:57.385284 94226 net.cpp:228] Setting up conv1
I1226 13:43:57.385421 94226 net.cpp:235] Top shape: 64 96 55 55 (18585600)
I1226 13:43:57.385452 94226 net.cpp:243] Memory required for data: 113917440
I1226 13:43:57.385581 94226 layer_factory.hpp:114] Creating layer relu1
I1226 13:43:57.385670 94226 net.cpp:178] Creating Layer relu1
I1226 13:43:57.385711 94226 net.cpp:612] relu1 <- conv1
I1226 13:43:57.385754 94226 net.cpp:573] relu1 -> conv1 (in-place)
I1226 13:43:57.385881 94226 net.cpp:228] Setting up relu1
I1226 13:43:57.385947 94226 net.cpp:235] Top shape: 64 96 55 55 (18585600)
I1226 13:43:57.385978 94226 net.cpp:243] Memory required for data: 188259840
I1226 13:43:57.386010 94226 layer_factory.hpp:114] Creating layer norm1
I1226 13:43:57.386090 94226 net.cpp:178] Creating Layer norm1
I1226 13:43:57.386122 94226 net.cpp:612] norm1 <- conv1
I1226 13:43:57.386163 94226 net.cpp:586] norm1 -> norm1
I1226 13:43:57.386266 94226 net.cpp:228] Setting up norm1
I1226 13:43:57.386335 94226 net.cpp:235] Top shape: 64 96 55 55 (18585600)
I1226 13:43:57.386363 94226 net.cpp:243] Memory required for data: 262602240
I1226 13:43:57.386420 94226 layer_factory.hpp:114] Creating layer pool1
I1226 13:43:57.386476 94226 net.cpp:178] Creating Layer pool1
I1226 13:43:57.386504 94226 net.cpp:612] pool1 <- norm1
I1226 13:43:57.386553 94226 net.cpp:586] pool1 -> pool1
I1226 13:43:57.386656 94226 net.cpp:228] Setting up pool1
I1226 13:43:57.386711 94226 net.cpp:235] Top shape: 64 96 27 27 (4478976)
I1226 13:43:57.386735 94226 net.cpp:243] Memory required for data: 280518144
I1226 13:43:57.386765 94226 layer_factory.hpp:114] Creating layer conv2
I1226 13:43:57.386844 94226 net.cpp:178] Creating Layer conv2
I1226 13:43:57.386875 94226 net.cpp:612] conv2 <- pool1
I1226 13:43:57.386924 94226 net.cpp:586] conv2 -> conv2
I1226 13:43:57.398428 97018 net.cpp:228] Setting up data
I1226 13:43:57.398552 97018 net.cpp:235] Top shape: 64 3 227 227 (9893568)
I1226 13:43:57.398600 97018 net.cpp:235] Top shape: 64 (64)
I1226 13:43:57.398630 97018 net.cpp:243] Memory required for data: 39574528
I1226 13:43:57.398690 97018 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 13:43:57.398823 97018 net.cpp:178] Creating Layer label_data_1_split
I1226 13:43:57.398977 97018 net.cpp:612] label_data_1_split <- label
I1226 13:43:57.399034 97018 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 13:43:57.399106 97018 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 13:43:57.405588 92417 net.cpp:178] Creating Layer conv2
I1226 13:43:57.405681 92417 net.cpp:612] conv2 <- pool1
I1226 13:43:57.405761 92417 net.cpp:586] conv2 -> conv2
I1226 13:43:57.427700 97018 net.cpp:228] Setting up label_data_1_split
I1226 13:43:57.427860 97018 net.cpp:235] Top shape: 64 (64)
I1226 13:43:57.427901 97018 net.cpp:235] Top shape: 64 (64)
I1226 13:43:57.427927 97018 net.cpp:243] Memory required for data: 39575040
I1226 13:43:57.427964 97018 layer_factory.hpp:114] Creating layer conv1
I1226 13:43:57.428079 97018 net.cpp:178] Creating Layer conv1
I1226 13:43:57.428123 97018 net.cpp:612] conv1 <- data
I1226 13:43:57.428172 97018 net.cpp:586] conv1 -> conv1
I1226 13:43:57.481510 85984 net.cpp:228] Setting up conv1
I1226 13:43:57.481647 85984 net.cpp:235] Top shape: 64 96 55 55 (18585600)
I1226 13:43:57.481706 85984 net.cpp:243] Memory required for data: 113917440
I1226 13:43:57.481827 85984 layer_factory.hpp:114] Creating layer relu1
I1226 13:43:57.481911 85984 net.cpp:178] Creating Layer relu1
I1226 13:43:57.481950 85984 net.cpp:612] relu1 <- conv1
I1226 13:43:57.482007 85984 net.cpp:573] relu1 -> conv1 (in-place)
I1226 13:43:57.482120 85984 net.cpp:228] Setting up relu1
I1226 13:43:57.482192 85984 net.cpp:235] Top shape: 64 96 55 55 (18585600)
I1226 13:43:57.482234 85984 net.cpp:243] Memory required for data: 188259840
I1226 13:43:57.482432 85984 layer_factory.hpp:114] Creating layer norm1
I1226 13:43:57.482626 85984 net.cpp:178] Creating Layer norm1
I1226 13:43:57.483103 85984 net.cpp:612] norm1 <- conv1
I1226 13:43:57.483203 85984 net.cpp:586] norm1 -> norm1
I1226 13:43:57.483414 85984 net.cpp:228] Setting up norm1
I1226 13:43:57.483484 85984 net.cpp:235] Top shape: 64 96 55 55 (18585600)
I1226 13:43:57.483508 85984 net.cpp:243] Memory required for data: 262602240
I1226 13:43:57.483541 85984 layer_factory.hpp:114] Creating layer pool1
I1226 13:43:57.483611 85984 net.cpp:178] Creating Layer pool1
I1226 13:43:57.483647 85984 net.cpp:612] pool1 <- norm1
I1226 13:43:57.483736 85984 net.cpp:586] pool1 -> pool1
I1226 13:43:57.483852 85984 net.cpp:228] Setting up pool1
I1226 13:43:57.483906 85984 net.cpp:235] Top shape: 64 96 27 27 (4478976)
I1226 13:43:57.483928 85984 net.cpp:243] Memory required for data: 280518144
I1226 13:43:57.483979 85984 layer_factory.hpp:114] Creating layer conv2
I1226 13:43:57.484087 85984 net.cpp:178] Creating Layer conv2
I1226 13:43:57.484127 85984 net.cpp:612] conv2 <- pool1
I1226 13:43:57.484186 85984 net.cpp:586] conv2 -> conv2
I1226 13:43:57.597334 85406 net.cpp:228] Setting up conv3
I1226 13:43:57.597448 85406 net.cpp:235] Top shape: 64 384 13 13 (4153344)
I1226 13:43:57.597475 85406 net.cpp:243] Memory required for data: 451534336
I1226 13:43:57.597544 85406 layer_factory.hpp:114] Creating layer relu3
I1226 13:43:57.597651 85406 net.cpp:178] Creating Layer relu3
I1226 13:43:57.597692 85406 net.cpp:612] relu3 <- conv3
I1226 13:43:57.597750 85406 net.cpp:573] relu3 -> conv3 (in-place)
I1226 13:43:57.597856 85406 net.cpp:228] Setting up relu3
I1226 13:43:57.597903 85406 net.cpp:235] Top shape: 64 384 13 13 (4153344)
I1226 13:43:57.597925 85406 net.cpp:243] Memory required for data: 468147712
I1226 13:43:57.597955 85406 layer_factory.hpp:114] Creating layer conv4
I1226 13:43:57.598024 85406 net.cpp:178] Creating Layer conv4
I1226 13:43:57.598062 85406 net.cpp:612] conv4 <- conv3
I1226 13:43:57.598103 85406 net.cpp:586] conv4 -> conv4
I1226 13:43:57.694396 91110 net.cpp:228] Setting up conv2
I1226 13:43:57.694499 91110 net.cpp:235] Top shape: 64 256 27 27 (11943936)
I1226 13:43:57.694526 91110 net.cpp:243] Memory required for data: 328293888
I1226 13:43:57.694602 91110 layer_factory.hpp:114] Creating layer relu2
I1226 13:43:57.694658 91110 net.cpp:178] Creating Layer relu2
I1226 13:43:57.694703 91110 net.cpp:612] relu2 <- conv2
I1226 13:43:57.694762 91110 net.cpp:573] relu2 -> conv2 (in-place)
I1226 13:43:57.694861 91110 net.cpp:228] Setting up relu2
I1226 13:43:57.694903 91110 net.cpp:235] Top shape: 64 256 27 27 (11943936)
I1226 13:43:57.694936 91110 net.cpp:243] Memory required for data: 376069632
I1226 13:43:57.694965 91110 layer_factory.hpp:114] Creating layer norm2
I1226 13:43:57.695016 91110 net.cpp:178] Creating Layer norm2
I1226 13:43:57.695042 91110 net.cpp:612] norm2 <- conv2
I1226 13:43:57.695112 91110 net.cpp:586] norm2 -> norm2
I1226 13:43:57.695207 91110 net.cpp:228] Setting up norm2
I1226 13:43:57.695255 91110 net.cpp:235] Top shape: 64 256 27 27 (11943936)
I1226 13:43:57.695277 91110 net.cpp:243] Memory required for data: 423845376
I1226 13:43:57.695307 91110 layer_factory.hpp:114] Creating layer pool2
I1226 13:43:57.714500 98538 net.cpp:228] Setting up conv2
I1226 13:43:57.714608 98538 net.cpp:235] Top shape: 64 256 27 27 (11943936)
I1226 13:43:57.714633 98538 net.cpp:243] Memory required for data: 328293888
I1226 13:43:57.714735 98538 layer_factory.hpp:114] Creating layer relu2
I1226 13:43:57.714818 98538 net.cpp:178] Creating Layer relu2
I1226 13:43:57.714853 98538 net.cpp:612] relu2 <- conv2
I1226 13:43:57.714933 98538 net.cpp:573] relu2 -> conv2 (in-place)
I1226 13:43:57.715165 98538 net.cpp:228] Setting up relu2
I1226 13:43:57.715294 98538 net.cpp:235] Top shape: 64 256 27 27 (11943936)
I1226 13:43:57.715322 98538 net.cpp:243] Memory required for data: 376069632
I1226 13:43:57.715353 98538 layer_factory.hpp:114] Creating layer norm2
I1226 13:43:57.715420 98538 net.cpp:178] Creating Layer norm2
I1226 13:43:57.715472 98538 net.cpp:612] norm2 <- conv2
I1226 13:43:57.715528 98538 net.cpp:586] norm2 -> norm2
I1226 13:43:57.715750 98538 net.cpp:228] Setting up norm2
I1226 13:43:57.715812 98538 net.cpp:235] Top shape: 64 256 27 27 (11943936)
I1226 13:43:57.715836 98538 net.cpp:243] Memory required for data: 423845376
I1226 13:43:57.715898 98538 layer_factory.hpp:114] Creating layer pool2
I1226 13:43:57.715966 98538 net.cpp:178] Creating Layer pool2
I1226 13:43:57.716001 98538 net.cpp:612] pool2 <- norm2
I1226 13:43:57.716051 98538 net.cpp:586] pool2 -> pool2
I1226 13:43:57.716225 98538 net.cpp:228] Setting up pool2
I1226 13:43:57.716306 98538 net.cpp:235] Top shape: 64 256 13 13 (2768896)
I1226 13:43:57.716464 98538 net.cpp:243] Memory required for data: 434920960
I1226 13:43:57.716524 98538 layer_factory.hpp:114] Creating layer conv3
I1226 13:43:57.716671 98538 net.cpp:178] Creating Layer conv3
I1226 13:43:57.716728 98538 net.cpp:612] conv3 <- pool2
I1226 13:43:57.716795 98538 net.cpp:586] conv3 -> conv3
I1226 13:43:57.695361 91110 net.cpp:178] Creating Layer pool2
I1226 13:43:57.708204 91110 net.cpp:612] pool2 <- norm2
I1226 13:43:57.708278 91110 net.cpp:586] pool2 -> pool2
I1226 13:43:57.708420 91110 net.cpp:228] Setting up pool2
I1226 13:43:57.708473 91110 net.cpp:235] Top shape: 64 256 13 13 (2768896)
I1226 13:43:57.708593 91110 net.cpp:243] Memory required for data: 434920960
I1226 13:43:57.708632 91110 layer_factory.hpp:114] Creating layer conv3
I1226 13:43:57.708719 91110 net.cpp:178] Creating Layer conv3
I1226 13:43:57.708758 91110 net.cpp:612] conv3 <- pool2
I1226 13:43:57.708802 91110 net.cpp:586] conv3 -> conv3
I1226 13:43:57.742375 91279 net.cpp:228] Setting up conv2
I1226 13:43:57.742497 91279 net.cpp:235] Top shape: 64 256 27 27 (11943936)
I1226 13:43:57.742530 91279 net.cpp:243] Memory required for data: 328293888
I1226 13:43:57.742638 91279 layer_factory.hpp:114] Creating layer relu2
I1226 13:43:57.742738 91279 net.cpp:178] Creating Layer relu2
I1226 13:43:57.742766 91279 net.cpp:612] relu2 <- conv2
I1226 13:43:57.742846 91279 net.cpp:573] relu2 -> conv2 (in-place)
I1226 13:43:57.742944 91279 net.cpp:228] Setting up relu2
I1226 13:43:57.743016 91279 net.cpp:235] Top shape: 64 256 27 27 (11943936)
I1226 13:43:57.743147 91279 net.cpp:243] Memory required for data: 376069632
I1226 13:43:57.743178 91279 layer_factory.hpp:114] Creating layer norm2
I1226 13:43:57.743254 91279 net.cpp:178] Creating Layer norm2
I1226 13:43:57.743294 91279 net.cpp:612] norm2 <- conv2
I1226 13:43:57.743342 91279 net.cpp:586] norm2 -> norm2
I1226 13:43:57.743429 91279 net.cpp:228] Setting up norm2
I1226 13:43:57.743481 91279 net.cpp:235] Top shape: 64 256 27 27 (11943936)
I1226 13:43:57.743505 91279 net.cpp:243] Memory required for data: 423845376
I1226 13:43:57.743530 91279 layer_factory.hpp:114] Creating layer pool2
I1226 13:43:57.743616 91279 net.cpp:178] Creating Layer pool2
I1226 13:43:57.743649 91279 net.cpp:612] pool2 <- norm2
I1226 13:43:57.743683 91279 net.cpp:586] pool2 -> pool2
I1226 13:43:57.743759 91279 net.cpp:228] Setting up pool2
I1226 13:43:57.743803 91279 net.cpp:235] Top shape: 64 256 13 13 (2768896)
I1226 13:43:57.743932 91279 net.cpp:243] Memory required for data: 434920960
I1226 13:43:57.743965 91279 layer_factory.hpp:114] Creating layer conv3
I1226 13:43:57.744062 91279 net.cpp:178] Creating Layer conv3
I1226 13:43:57.744096 91279 net.cpp:612] conv3 <- pool2
I1226 13:43:57.744137 91279 net.cpp:586] conv3 -> conv3
I1226 13:43:57.798691 85406 net.cpp:228] Setting up conv4
I1226 13:43:57.798800 85406 net.cpp:235] Top shape: 64 384 13 13 (4153344)
I1226 13:43:57.798826 85406 net.cpp:243] Memory required for data: 484761088
I1226 13:43:57.798883 85406 layer_factory.hpp:114] Creating layer relu4
I1226 13:43:57.798956 85406 net.cpp:178] Creating Layer relu4
I1226 13:43:57.799001 85406 net.cpp:612] relu4 <- conv4
I1226 13:43:57.799048 85406 net.cpp:573] relu4 -> conv4 (in-place)
I1226 13:43:57.799151 85406 net.cpp:228] Setting up relu4
I1226 13:43:57.799206 85406 net.cpp:235] Top shape: 64 384 13 13 (4153344)
I1226 13:43:57.799230 85406 net.cpp:243] Memory required for data: 501374464
I1226 13:43:57.799259 85406 layer_factory.hpp:114] Creating layer conv5
I1226 13:43:57.799347 85406 net.cpp:178] Creating Layer conv5
I1226 13:43:57.799388 85406 net.cpp:612] conv5 <- conv4
I1226 13:43:57.799438 85406 net.cpp:586] conv5 -> conv5
I1226 13:43:57.800117 94226 net.cpp:228] Setting up conv2
I1226 13:43:57.800226 94226 net.cpp:235] Top shape: 64 256 27 27 (11943936)
I1226 13:43:57.800251 94226 net.cpp:243] Memory required for data: 328293888
I1226 13:43:57.800323 94226 layer_factory.hpp:114] Creating layer relu2
I1226 13:43:57.800408 94226 net.cpp:178] Creating Layer relu2
I1226 13:43:57.800446 94226 net.cpp:612] relu2 <- conv2
I1226 13:43:57.800509 94226 net.cpp:573] relu2 -> conv2 (in-place)
I1226 13:43:57.800606 94226 net.cpp:228] Setting up relu2
I1226 13:43:57.800674 94226 net.cpp:235] Top shape: 64 256 27 27 (11943936)
I1226 13:43:57.800703 94226 net.cpp:243] Memory required for data: 376069632
I1226 13:43:57.800734 94226 layer_factory.hpp:114] Creating layer norm2
I1226 13:43:57.800783 94226 net.cpp:178] Creating Layer norm2
I1226 13:43:57.800812 94226 net.cpp:612] norm2 <- conv2
I1226 13:43:57.800849 94226 net.cpp:586] norm2 -> norm2
I1226 13:43:57.800940 94226 net.cpp:228] Setting up norm2
I1226 13:43:57.800986 94226 net.cpp:235] Top shape: 64 256 27 27 (11943936)
I1226 13:43:57.801007 94226 net.cpp:243] Memory required for data: 423845376
I1226 13:43:57.801033 94226 layer_factory.hpp:114] Creating layer pool2
I1226 13:43:57.801076 94226 net.cpp:178] Creating Layer pool2
I1226 13:43:57.801102 94226 net.cpp:612] pool2 <- norm2
I1226 13:43:57.801146 94226 net.cpp:586] pool2 -> pool2
I1226 13:43:57.801225 94226 net.cpp:228] Setting up pool2
I1226 13:43:57.801266 94226 net.cpp:235] Top shape: 64 256 13 13 (2768896)
I1226 13:43:57.801396 94226 net.cpp:243] Memory required for data: 434920960
I1226 13:43:57.801437 94226 layer_factory.hpp:114] Creating layer conv3
I1226 13:43:57.801524 94226 net.cpp:178] Creating Layer conv3
I1226 13:43:57.801559 94226 net.cpp:612] conv3 <- pool2
I1226 13:43:57.801610 94226 net.cpp:586] conv3 -> conv3
I1226 13:43:57.821112 92417 net.cpp:228] Setting up conv2
I1226 13:43:57.821223 92417 net.cpp:235] Top shape: 64 256 27 27 (11943936)
I1226 13:43:57.821247 92417 net.cpp:243] Memory required for data: 328293888
I1226 13:43:57.821316 92417 layer_factory.hpp:114] Creating layer relu2
I1226 13:43:57.821380 92417 net.cpp:178] Creating Layer relu2
I1226 13:43:57.821408 92417 net.cpp:612] relu2 <- conv2
I1226 13:43:57.821463 92417 net.cpp:573] relu2 -> conv2 (in-place)
I1226 13:43:57.821597 92417 net.cpp:228] Setting up relu2
I1226 13:43:57.821660 92417 net.cpp:235] Top shape: 64 256 27 27 (11943936)
I1226 13:43:57.821684 92417 net.cpp:243] Memory required for data: 376069632
I1226 13:43:57.821714 92417 layer_factory.hpp:114] Creating layer norm2
I1226 13:43:57.821763 92417 net.cpp:178] Creating Layer norm2
I1226 13:43:57.821787 92417 net.cpp:612] norm2 <- conv2
I1226 13:43:57.821822 92417 net.cpp:586] norm2 -> norm2
I1226 13:43:57.821928 92417 net.cpp:228] Setting up norm2
I1226 13:43:57.821976 92417 net.cpp:235] Top shape: 64 256 27 27 (11943936)
I1226 13:43:57.822008 92417 net.cpp:243] Memory required for data: 423845376
I1226 13:43:57.822036 92417 layer_factory.hpp:114] Creating layer pool2
I1226 13:43:57.822077 92417 net.cpp:178] Creating Layer pool2
I1226 13:43:57.822101 92417 net.cpp:612] pool2 <- norm2
I1226 13:43:57.822181 92417 net.cpp:586] pool2 -> pool2
I1226 13:43:57.822278 92417 net.cpp:228] Setting up pool2
I1226 13:43:57.822327 92417 net.cpp:235] Top shape: 64 256 13 13 (2768896)
I1226 13:43:57.822469 92417 net.cpp:243] Memory required for data: 434920960
I1226 13:43:57.822532 92417 layer_factory.hpp:114] Creating layer conv3
I1226 13:43:57.822623 92417 net.cpp:178] Creating Layer conv3
I1226 13:43:57.822659 92417 net.cpp:612] conv3 <- pool2
I1226 13:43:57.822712 92417 net.cpp:586] conv3 -> conv3
I1226 13:43:57.843250 85984 net.cpp:228] Setting up conv2
I1226 13:43:57.843381 85984 net.cpp:235] Top shape: 64 256 27 27 (11943936)
I1226 13:43:57.843410 85984 net.cpp:243] Memory required for data: 328293888
I1226 13:43:57.843513 85984 layer_factory.hpp:114] Creating layer relu2
I1226 13:43:57.843603 85984 net.cpp:178] Creating Layer relu2
I1226 13:43:57.843642 85984 net.cpp:612] relu2 <- conv2
I1226 13:43:57.843746 85984 net.cpp:573] relu2 -> conv2 (in-place)
I1226 13:43:57.843963 85984 net.cpp:228] Setting up relu2
I1226 13:43:57.844034 85984 net.cpp:235] Top shape: 64 256 27 27 (11943936)
I1226 13:43:57.844058 85984 net.cpp:243] Memory required for data: 376069632
I1226 13:43:57.844086 85984 layer_factory.hpp:114] Creating layer norm2
I1226 13:43:57.844144 85984 net.cpp:178] Creating Layer norm2
I1226 13:43:57.844502 85984 net.cpp:612] norm2 <- conv2
I1226 13:43:57.844571 85984 net.cpp:586] norm2 -> norm2
I1226 13:43:57.844790 85984 net.cpp:228] Setting up norm2
I1226 13:43:57.844852 85984 net.cpp:235] Top shape: 64 256 27 27 (11943936)
I1226 13:43:57.844877 85984 net.cpp:243] Memory required for data: 423845376
I1226 13:43:57.844907 85984 layer_factory.hpp:114] Creating layer pool2
I1226 13:43:57.844979 85984 net.cpp:178] Creating Layer pool2
I1226 13:43:57.845006 85984 net.cpp:612] pool2 <- norm2
I1226 13:43:57.845042 85984 net.cpp:586] pool2 -> pool2
I1226 13:43:57.845136 85984 net.cpp:228] Setting up pool2
I1226 13:43:57.845180 85984 net.cpp:235] Top shape: 64 256 13 13 (2768896)
I1226 13:43:57.845326 85984 net.cpp:243] Memory required for data: 434920960
I1226 13:43:57.845360 85984 layer_factory.hpp:114] Creating layer conv3
I1226 13:43:57.845443 85984 net.cpp:178] Creating Layer conv3
I1226 13:43:57.845479 85984 net.cpp:612] conv3 <- pool2
I1226 13:43:57.845548 85984 net.cpp:586] conv3 -> conv3
I1226 13:43:57.887315 94885 net.cpp:228] Setting up conv1
I1226 13:43:57.887459 94885 net.cpp:235] Top shape: 64 96 55 55 (18585600)
I1226 13:43:57.887512 94885 net.cpp:243] Memory required for data: 113917440
I1226 13:43:57.887681 94885 layer_factory.hpp:114] Creating layer relu1
I1226 13:43:57.887856 94885 net.cpp:178] Creating Layer relu1
I1226 13:43:57.888332 94885 net.cpp:612] relu1 <- conv1
I1226 13:43:57.888520 94885 net.cpp:573] relu1 -> conv1 (in-place)
I1226 13:43:57.888762 94885 net.cpp:228] Setting up relu1
I1226 13:43:57.888881 94885 net.cpp:235] Top shape: 64 96 55 55 (18585600)
I1226 13:43:57.888927 94885 net.cpp:243] Memory required for data: 188259840
I1226 13:43:57.888975 94885 layer_factory.hpp:114] Creating layer norm1
I1226 13:43:57.889158 94885 net.cpp:178] Creating Layer norm1
I1226 13:43:57.889461 94885 net.cpp:612] norm1 <- conv1
I1226 13:43:57.889605 94885 net.cpp:586] norm1 -> norm1
I1226 13:43:57.889948 94885 net.cpp:228] Setting up norm1
I1226 13:43:57.890434 94885 net.cpp:235] Top shape: 64 96 55 55 (18585600)
I1226 13:43:57.890504 94885 net.cpp:243] Memory required for data: 262602240
I1226 13:43:57.890661 94885 layer_factory.hpp:114] Creating layer pool1
I1226 13:43:57.890790 94885 net.cpp:178] Creating Layer pool1
I1226 13:43:57.890879 94885 net.cpp:612] pool1 <- norm1
I1226 13:43:57.890940 94885 net.cpp:586] pool1 -> pool1
I1226 13:43:57.891134 94885 net.cpp:228] Setting up pool1
I1226 13:43:57.891227 94885 net.cpp:235] Top shape: 64 96 27 27 (4478976)
I1226 13:43:57.891260 94885 net.cpp:243] Memory required for data: 280518144
I1226 13:43:57.891335 94885 layer_factory.hpp:114] Creating layer conv2
I1226 13:43:57.891611 94885 net.cpp:178] Creating Layer conv2
I1226 13:43:57.891670 94885 net.cpp:612] conv2 <- pool1
I1226 13:43:57.892166 94885 net.cpp:586] conv2 -> conv2
I1226 13:43:57.947146 85406 net.cpp:228] Setting up conv5
I1226 13:43:57.947257 85406 net.cpp:235] Top shape: 64 256 13 13 (2768896)
I1226 13:43:57.947285 85406 net.cpp:243] Memory required for data: 512450048
I1226 13:43:57.947361 85406 layer_factory.hpp:114] Creating layer relu5
I1226 13:43:57.947428 85406 net.cpp:178] Creating Layer relu5
I1226 13:43:57.947466 85406 net.cpp:612] relu5 <- conv5
I1226 13:43:57.947525 85406 net.cpp:573] relu5 -> conv5 (in-place)
I1226 13:43:57.947659 85406 net.cpp:228] Setting up relu5
I1226 13:43:57.947712 85406 net.cpp:235] Top shape: 64 256 13 13 (2768896)
I1226 13:43:57.947738 85406 net.cpp:243] Memory required for data: 523525632
I1226 13:43:57.947768 85406 layer_factory.hpp:114] Creating layer pool5
I1226 13:43:57.947831 85406 net.cpp:178] Creating Layer pool5
I1226 13:43:57.947861 85406 net.cpp:612] pool5 <- conv5
I1226 13:43:57.947899 85406 net.cpp:586] pool5 -> pool5
I1226 13:43:57.948009 85406 net.cpp:228] Setting up pool5
I1226 13:43:57.948058 85406 net.cpp:235] Top shape: 64 256 6 6 (589824)
I1226 13:43:57.948087 85406 net.cpp:243] Memory required for data: 525884928
I1226 13:43:57.948123 85406 layer_factory.hpp:114] Creating layer fc6
I1226 13:43:57.948201 85406 net.cpp:178] Creating Layer fc6
I1226 13:43:57.948233 85406 net.cpp:612] fc6 <- pool5
I1226 13:43:57.948278 85406 net.cpp:586] fc6 -> fc6
I1226 13:43:58.098398 98538 net.cpp:228] Setting up conv3
I1226 13:43:58.098508 98538 net.cpp:235] Top shape: 64 384 13 13 (4153344)
I1226 13:43:58.098533 98538 net.cpp:243] Memory required for data: 451534336
I1226 13:43:58.098621 98538 layer_factory.hpp:114] Creating layer relu3
I1226 13:43:58.098692 98538 net.cpp:178] Creating Layer relu3
I1226 13:43:58.098728 98538 net.cpp:612] relu3 <- conv3
I1226 13:43:58.098784 98538 net.cpp:573] relu3 -> conv3 (in-place)
I1226 13:43:58.098876 98538 net.cpp:228] Setting up relu3
I1226 13:43:58.098927 98538 net.cpp:235] Top shape: 64 384 13 13 (4153344)
I1226 13:43:58.098950 98538 net.cpp:243] Memory required for data: 468147712
I1226 13:43:58.098979 98538 layer_factory.hpp:114] Creating layer conv4
I1226 13:43:58.099097 98538 net.cpp:178] Creating Layer conv4
I1226 13:43:58.099139 98538 net.cpp:612] conv4 <- conv3
I1226 13:43:58.099182 98538 net.cpp:586] conv4 -> conv4
I1226 13:43:58.102519 91110 net.cpp:228] Setting up conv3
I1226 13:43:58.102625 91110 net.cpp:235] Top shape: 64 384 13 13 (4153344)
I1226 13:43:58.102653 91110 net.cpp:243] Memory required for data: 451534336
I1226 13:43:58.102758 91110 layer_factory.hpp:114] Creating layer relu3
I1226 13:43:58.102854 91110 net.cpp:178] Creating Layer relu3
I1226 13:43:58.102893 91110 net.cpp:612] relu3 <- conv3
I1226 13:43:58.102972 91110 net.cpp:573] relu3 -> conv3 (in-place)
I1226 13:43:58.103096 91110 net.cpp:228] Setting up relu3
I1226 13:43:58.103225 91110 net.cpp:235] Top shape: 64 384 13 13 (4153344)
I1226 13:43:58.103272 91110 net.cpp:243] Memory required for data: 468147712
I1226 13:43:58.103402 91110 layer_factory.hpp:114] Creating layer conv4
I1226 13:43:58.103482 91110 net.cpp:178] Creating Layer conv4
I1226 13:43:58.103516 91110 net.cpp:612] conv4 <- conv3
I1226 13:43:58.103554 91110 net.cpp:586] conv4 -> conv4
I1226 13:43:58.135989 91279 net.cpp:228] Setting up conv3
I1226 13:43:58.136096 91279 net.cpp:235] Top shape: 64 384 13 13 (4153344)
I1226 13:43:58.136126 91279 net.cpp:243] Memory required for data: 451534336
I1226 13:43:58.136239 91279 layer_factory.hpp:114] Creating layer relu3
I1226 13:43:58.136315 91279 net.cpp:178] Creating Layer relu3
I1226 13:43:58.136404 91279 net.cpp:612] relu3 <- conv3
I1226 13:43:58.136489 91279 net.cpp:573] relu3 -> conv3 (in-place)
I1226 13:43:58.136593 91279 net.cpp:228] Setting up relu3
I1226 13:43:58.136744 91279 net.cpp:235] Top shape: 64 384 13 13 (4153344)
I1226 13:43:58.136772 91279 net.cpp:243] Memory required for data: 468147712
I1226 13:43:58.136840 91279 layer_factory.hpp:114] Creating layer conv4
I1226 13:43:58.136945 91279 net.cpp:178] Creating Layer conv4
I1226 13:43:58.136984 91279 net.cpp:612] conv4 <- conv3
I1226 13:43:58.137029 91279 net.cpp:586] conv4 -> conv4
I1226 13:43:58.153918 97018 net.cpp:228] Setting up conv1
I1226 13:43:58.154036 97018 net.cpp:235] Top shape: 64 96 55 55 (18585600)
I1226 13:43:58.154070 97018 net.cpp:243] Memory required for data: 113917440
I1226 13:43:58.154192 97018 layer_factory.hpp:114] Creating layer relu1
I1226 13:43:58.154283 97018 net.cpp:178] Creating Layer relu1
I1226 13:43:58.154320 97018 net.cpp:612] relu1 <- conv1
I1226 13:43:58.154369 97018 net.cpp:573] relu1 -> conv1 (in-place)
I1226 13:43:58.154506 97018 net.cpp:228] Setting up relu1
I1226 13:43:58.154561 97018 net.cpp:235] Top shape: 64 96 55 55 (18585600)
I1226 13:43:58.154588 97018 net.cpp:243] Memory required for data: 188259840
I1226 13:43:58.154619 97018 layer_factory.hpp:114] Creating layer norm1
I1226 13:43:58.154700 97018 net.cpp:178] Creating Layer norm1
I1226 13:43:58.154764 97018 net.cpp:612] norm1 <- conv1
I1226 13:43:58.154810 97018 net.cpp:586] norm1 -> norm1
I1226 13:43:58.154922 97018 net.cpp:228] Setting up norm1
I1226 13:43:58.154971 97018 net.cpp:235] Top shape: 64 96 55 55 (18585600)
I1226 13:43:58.154995 97018 net.cpp:243] Memory required for data: 262602240
I1226 13:43:58.155025 97018 layer_factory.hpp:114] Creating layer pool1
I1226 13:43:58.155098 97018 net.cpp:178] Creating Layer pool1
I1226 13:43:58.155133 97018 net.cpp:612] pool1 <- norm1
I1226 13:43:58.155172 97018 net.cpp:586] pool1 -> pool1
I1226 13:43:58.155282 97018 net.cpp:228] Setting up pool1
I1226 13:43:58.155333 97018 net.cpp:235] Top shape: 64 96 27 27 (4478976)
I1226 13:43:58.155356 97018 net.cpp:243] Memory required for data: 280518144
I1226 13:43:58.155387 97018 layer_factory.hpp:114] Creating layer conv2
I1226 13:43:58.155488 97018 net.cpp:178] Creating Layer conv2
I1226 13:43:58.155527 97018 net.cpp:612] conv2 <- pool1
I1226 13:43:58.155581 97018 net.cpp:586] conv2 -> conv2
I1226 13:43:58.273285 92417 net.cpp:228] Setting up conv3
I1226 13:43:58.273396 92417 net.cpp:235] Top shape: 64 384 13 13 (4153344)
I1226 13:43:58.273422 92417 net.cpp:243] Memory required for data: 451534336
I1226 13:43:58.273533 92417 layer_factory.hpp:114] Creating layer relu3
I1226 13:43:58.273610 92417 net.cpp:178] Creating Layer relu3
I1226 13:43:58.273650 92417 net.cpp:612] relu3 <- conv3
I1226 13:43:58.273689 92417 net.cpp:573] relu3 -> conv3 (in-place)
I1226 13:43:58.273780 92417 net.cpp:228] Setting up relu3
I1226 13:43:58.273834 92417 net.cpp:235] Top shape: 64 384 13 13 (4153344)
I1226 13:43:58.273859 92417 net.cpp:243] Memory required for data: 468147712
I1226 13:43:58.273886 92417 layer_factory.hpp:114] Creating layer conv4
I1226 13:43:58.273962 92417 net.cpp:178] Creating Layer conv4
I1226 13:43:58.273996 92417 net.cpp:612] conv4 <- conv3
I1226 13:43:58.274049 92417 net.cpp:586] conv4 -> conv4
I1226 13:43:58.277559 85984 net.cpp:228] Setting up conv3
I1226 13:43:58.277684 85984 net.cpp:235] Top shape: 64 384 13 13 (4153344)
I1226 13:43:58.277714 85984 net.cpp:243] Memory required for data: 451534336
I1226 13:43:58.277788 85984 layer_factory.hpp:114] Creating layer relu3
I1226 13:43:58.278329 85984 net.cpp:178] Creating Layer relu3
I1226 13:43:58.278388 85984 net.cpp:612] relu3 <- conv3
I1226 13:43:58.278431 85984 net.cpp:573] relu3 -> conv3 (in-place)
I1226 13:43:58.278564 85984 net.cpp:228] Setting up relu3
I1226 13:43:58.278617 85984 net.cpp:235] Top shape: 64 384 13 13 (4153344)
I1226 13:43:58.278641 85984 net.cpp:243] Memory required for data: 468147712
I1226 13:43:58.278695 85984 layer_factory.hpp:114] Creating layer conv4
I1226 13:43:58.278787 85984 net.cpp:178] Creating Layer conv4
I1226 13:43:58.278825 85984 net.cpp:612] conv4 <- conv3
I1226 13:43:58.278867 85984 net.cpp:586] conv4 -> conv4
I1226 13:43:58.269716 94226 net.cpp:228] Setting up conv3
I1226 13:43:58.269821 94226 net.cpp:235] Top shape: 64 384 13 13 (4153344)
I1226 13:43:58.269845 94226 net.cpp:243] Memory required for data: 451534336
I1226 13:43:58.269915 94226 layer_factory.hpp:114] Creating layer relu3
I1226 13:43:58.269987 94226 net.cpp:178] Creating Layer relu3
I1226 13:43:58.270020 94226 net.cpp:612] relu3 <- conv3
I1226 13:43:58.270057 94226 net.cpp:573] relu3 -> conv3 (in-place)
I1226 13:43:58.270148 94226 net.cpp:228] Setting up relu3
I1226 13:43:58.270198 94226 net.cpp:235] Top shape: 64 384 13 13 (4153344)
I1226 13:43:58.270221 94226 net.cpp:243] Memory required for data: 468147712
I1226 13:43:58.270249 94226 layer_factory.hpp:114] Creating layer conv4
I1226 13:43:58.270326 94226 net.cpp:178] Creating Layer conv4
I1226 13:43:58.270355 94226 net.cpp:612] conv4 <- conv3
I1226 13:43:58.270464 94226 net.cpp:586] conv4 -> conv4
I1226 13:43:58.397537 98538 net.cpp:228] Setting up conv4
I1226 13:43:58.397665 98538 net.cpp:235] Top shape: 64 384 13 13 (4153344)
I1226 13:43:58.398052 98538 net.cpp:243] Memory required for data: 484761088
I1226 13:43:58.398219 98538 layer_factory.hpp:114] Creating layer relu4
I1226 13:43:58.398341 98538 net.cpp:178] Creating Layer relu4
I1226 13:43:58.398488 98538 net.cpp:612] relu4 <- conv4
I1226 13:43:58.398533 98538 net.cpp:573] relu4 -> conv4 (in-place)
I1226 13:43:58.398644 98538 net.cpp:228] Setting up relu4
I1226 13:43:58.398739 98538 net.cpp:235] Top shape: 64 384 13 13 (4153344)
I1226 13:43:58.398768 98538 net.cpp:243] Memory required for data: 501374464
I1226 13:43:58.398800 98538 layer_factory.hpp:114] Creating layer conv5
I1226 13:43:58.398910 98538 net.cpp:178] Creating Layer conv5
I1226 13:43:58.398996 98538 net.cpp:612] conv5 <- conv4
I1226 13:43:58.399051 98538 net.cpp:586] conv5 -> conv5
I1226 13:43:58.385488 91110 net.cpp:228] Setting up conv4
I1226 13:43:58.385594 91110 net.cpp:235] Top shape: 64 384 13 13 (4153344)
I1226 13:43:58.385618 91110 net.cpp:243] Memory required for data: 484761088
I1226 13:43:58.385701 91110 layer_factory.hpp:114] Creating layer relu4
I1226 13:43:58.385783 91110 net.cpp:178] Creating Layer relu4
I1226 13:43:58.385823 91110 net.cpp:612] relu4 <- conv4
I1226 13:43:58.385869 91110 net.cpp:573] relu4 -> conv4 (in-place)
I1226 13:43:58.386051 91110 net.cpp:228] Setting up relu4
I1226 13:43:58.386121 91110 net.cpp:235] Top shape: 64 384 13 13 (4153344)
I1226 13:43:58.386144 91110 net.cpp:243] Memory required for data: 501374464
I1226 13:43:58.386173 91110 layer_factory.hpp:114] Creating layer conv5
I1226 13:43:58.386256 91110 net.cpp:178] Creating Layer conv5
I1226 13:43:58.386291 91110 net.cpp:612] conv5 <- conv4
I1226 13:43:58.386345 91110 net.cpp:586] conv5 -> conv5
I1226 13:43:58.497325 91279 net.cpp:228] Setting up conv4
I1226 13:43:58.497434 91279 net.cpp:235] Top shape: 64 384 13 13 (4153344)
I1226 13:43:58.497462 91279 net.cpp:243] Memory required for data: 484761088
I1226 13:43:58.497519 91279 layer_factory.hpp:114] Creating layer relu4
I1226 13:43:58.497586 91279 net.cpp:178] Creating Layer relu4
I1226 13:43:58.497624 91279 net.cpp:612] relu4 <- conv4
I1226 13:43:58.497663 91279 net.cpp:573] relu4 -> conv4 (in-place)
I1226 13:43:58.497735 91279 net.cpp:228] Setting up relu4
I1226 13:43:58.497808 91279 net.cpp:235] Top shape: 64 384 13 13 (4153344)
I1226 13:43:58.497833 91279 net.cpp:243] Memory required for data: 501374464
I1226 13:43:58.497858 91279 layer_factory.hpp:114] Creating layer conv5
I1226 13:43:58.497921 91279 net.cpp:178] Creating Layer conv5
I1226 13:43:58.497948 91279 net.cpp:612] conv5 <- conv4
I1226 13:43:58.497987 91279 net.cpp:586] conv5 -> conv5
I1226 13:43:58.591295 92417 net.cpp:228] Setting up conv4
I1226 13:43:58.591406 92417 net.cpp:235] Top shape: 64 384 13 13 (4153344)
I1226 13:43:58.591434 92417 net.cpp:243] Memory required for data: 484761088
I1226 13:43:58.591513 92417 layer_factory.hpp:114] Creating layer relu4
I1226 13:43:58.591590 92417 net.cpp:178] Creating Layer relu4
I1226 13:43:58.591717 92417 net.cpp:612] relu4 <- conv4
I1226 13:43:58.591759 92417 net.cpp:573] relu4 -> conv4 (in-place)
I1226 13:43:58.591856 92417 net.cpp:228] Setting up relu4
I1226 13:43:58.591951 92417 net.cpp:235] Top shape: 64 384 13 13 (4153344)
I1226 13:43:58.591976 92417 net.cpp:243] Memory required for data: 501374464
I1226 13:43:58.592006 92417 layer_factory.hpp:114] Creating layer conv5
I1226 13:43:58.592097 92417 net.cpp:178] Creating Layer conv5
I1226 13:43:58.592131 92417 net.cpp:612] conv5 <- conv4
I1226 13:43:58.592172 92417 net.cpp:586] conv5 -> conv5
I1226 13:43:58.586683 94226 net.cpp:228] Setting up conv4
I1226 13:43:58.586791 94226 net.cpp:235] Top shape: 64 384 13 13 (4153344)
I1226 13:43:58.586814 94226 net.cpp:243] Memory required for data: 484761088
I1226 13:43:58.586872 94226 layer_factory.hpp:114] Creating layer relu4
I1226 13:43:58.586946 94226 net.cpp:178] Creating Layer relu4
I1226 13:43:58.586978 94226 net.cpp:612] relu4 <- conv4
I1226 13:43:58.587018 94226 net.cpp:573] relu4 -> conv4 (in-place)
I1226 13:43:58.587112 94226 net.cpp:228] Setting up relu4
I1226 13:43:58.587172 94226 net.cpp:235] Top shape: 64 384 13 13 (4153344)
I1226 13:43:58.587196 94226 net.cpp:243] Memory required for data: 501374464
I1226 13:43:58.587225 94226 layer_factory.hpp:114] Creating layer conv5
I1226 13:43:58.587313 94226 net.cpp:178] Creating Layer conv5
I1226 13:43:58.587345 94226 net.cpp:612] conv5 <- conv4
I1226 13:43:58.587412 94226 net.cpp:586] conv5 -> conv5
I1226 13:43:58.614121 85984 net.cpp:228] Setting up conv4
I1226 13:43:58.614270 85984 net.cpp:235] Top shape: 64 384 13 13 (4153344)
I1226 13:43:58.614306 85984 net.cpp:243] Memory required for data: 484761088
I1226 13:43:58.614365 85984 layer_factory.hpp:114] Creating layer relu4
I1226 13:43:58.614784 85984 net.cpp:178] Creating Layer relu4
I1226 13:43:58.614881 85984 net.cpp:612] relu4 <- conv4
I1226 13:43:58.615003 85984 net.cpp:573] relu4 -> conv4 (in-place)
I1226 13:43:58.615182 85984 net.cpp:228] Setting up relu4
I1226 13:43:58.615329 85984 net.cpp:235] Top shape: 64 384 13 13 (4153344)
I1226 13:43:58.615540 85984 net.cpp:243] Memory required for data: 501374464
I1226 13:43:58.615583 85984 layer_factory.hpp:114] Creating layer conv5
I1226 13:43:58.616238 85984 net.cpp:178] Creating Layer conv5
I1226 13:43:58.616361 85984 net.cpp:612] conv5 <- conv4
I1226 13:43:58.616478 85984 net.cpp:586] conv5 -> conv5
I1226 13:43:58.617669 91110 net.cpp:228] Setting up conv5
I1226 13:43:58.617784 91110 net.cpp:235] Top shape: 64 256 13 13 (2768896)
I1226 13:43:58.617812 91110 net.cpp:243] Memory required for data: 512450048
I1226 13:43:58.617887 91110 layer_factory.hpp:114] Creating layer relu5
I1226 13:43:58.617951 91110 net.cpp:178] Creating Layer relu5
I1226 13:43:58.617987 91110 net.cpp:612] relu5 <- conv5
I1226 13:43:58.618028 91110 net.cpp:573] relu5 -> conv5 (in-place)
I1226 13:43:58.618147 91110 net.cpp:228] Setting up relu5
I1226 13:43:58.618207 91110 net.cpp:235] Top shape: 64 256 13 13 (2768896)
I1226 13:43:58.618232 91110 net.cpp:243] Memory required for data: 523525632
I1226 13:43:58.618258 91110 layer_factory.hpp:114] Creating layer pool5
I1226 13:43:58.618309 91110 net.cpp:178] Creating Layer pool5
I1226 13:43:58.618333 91110 net.cpp:612] pool5 <- conv5
I1226 13:43:58.618367 91110 net.cpp:586] pool5 -> pool5
I1226 13:43:58.618544 91110 net.cpp:228] Setting up pool5
I1226 13:43:58.618579 91110 net.cpp:235] Top shape: 64 256 6 6 (589824)
I1226 13:43:58.618602 91110 net.cpp:243] Memory required for data: 525884928
I1226 13:43:58.618633 91110 layer_factory.hpp:114] Creating layer fc6
I1226 13:43:58.618696 91110 net.cpp:178] Creating Layer fc6
I1226 13:43:58.618723 91110 net.cpp:612] fc6 <- pool5
I1226 13:43:58.618760 91110 net.cpp:586] fc6 -> fc6
I1226 13:43:58.656512 98538 net.cpp:228] Setting up conv5
I1226 13:43:58.657052 98538 net.cpp:235] Top shape: 64 256 13 13 (2768896)
I1226 13:43:58.657138 98538 net.cpp:243] Memory required for data: 512450048
I1226 13:43:58.657241 98538 layer_factory.hpp:114] Creating layer relu5
I1226 13:43:58.657380 98538 net.cpp:178] Creating Layer relu5
I1226 13:43:58.657464 98538 net.cpp:612] relu5 <- conv5
I1226 13:43:58.657519 98538 net.cpp:573] relu5 -> conv5 (in-place)
I1226 13:43:58.657865 98538 net.cpp:228] Setting up relu5
I1226 13:43:58.657986 98538 net.cpp:235] Top shape: 64 256 13 13 (2768896)
I1226 13:43:58.658021 98538 net.cpp:243] Memory required for data: 523525632
I1226 13:43:58.658059 98538 layer_factory.hpp:114] Creating layer pool5
I1226 13:43:58.658562 98538 net.cpp:178] Creating Layer pool5
I1226 13:43:58.658826 98538 net.cpp:612] pool5 <- conv5
I1226 13:43:58.658947 98538 net.cpp:586] pool5 -> pool5
I1226 13:43:58.659139 98538 net.cpp:228] Setting up pool5
I1226 13:43:58.659262 98538 net.cpp:235] Top shape: 64 256 6 6 (589824)
I1226 13:43:58.659298 98538 net.cpp:243] Memory required for data: 525884928
I1226 13:43:58.659338 98538 layer_factory.hpp:114] Creating layer fc6
I1226 13:43:58.659730 98538 net.cpp:178] Creating Layer fc6
I1226 13:43:58.659793 98538 net.cpp:612] fc6 <- pool5
I1226 13:43:58.659847 98538 net.cpp:586] fc6 -> fc6
I1226 13:43:58.747112 91279 net.cpp:228] Setting up conv5
I1226 13:43:58.747238 91279 net.cpp:235] Top shape: 64 256 13 13 (2768896)
I1226 13:43:58.747267 91279 net.cpp:243] Memory required for data: 512450048
I1226 13:43:58.747344 91279 layer_factory.hpp:114] Creating layer relu5
I1226 13:43:58.747426 91279 net.cpp:178] Creating Layer relu5
I1226 13:43:58.747459 91279 net.cpp:612] relu5 <- conv5
I1226 13:43:58.747508 91279 net.cpp:573] relu5 -> conv5 (in-place)
I1226 13:43:58.747603 91279 net.cpp:228] Setting up relu5
I1226 13:43:58.747648 91279 net.cpp:235] Top shape: 64 256 13 13 (2768896)
I1226 13:43:58.747678 91279 net.cpp:243] Memory required for data: 523525632
I1226 13:43:58.747706 91279 layer_factory.hpp:114] Creating layer pool5
I1226 13:43:58.747766 91279 net.cpp:178] Creating Layer pool5
I1226 13:43:58.747797 91279 net.cpp:612] pool5 <- conv5
I1226 13:43:58.747843 91279 net.cpp:586] pool5 -> pool5
I1226 13:43:58.747925 91279 net.cpp:228] Setting up pool5
I1226 13:43:58.747972 91279 net.cpp:235] Top shape: 64 256 6 6 (589824)
I1226 13:43:58.747994 91279 net.cpp:243] Memory required for data: 525884928
I1226 13:43:58.748026 91279 layer_factory.hpp:114] Creating layer fc6
I1226 13:43:58.748091 91279 net.cpp:178] Creating Layer fc6
I1226 13:43:58.748119 91279 net.cpp:612] fc6 <- pool5
I1226 13:43:58.748168 91279 net.cpp:586] fc6 -> fc6
I1226 13:43:58.740285 92417 net.cpp:228] Setting up conv5
I1226 13:43:58.740411 92417 net.cpp:235] Top shape: 64 256 13 13 (2768896)
I1226 13:43:58.740438 92417 net.cpp:243] Memory required for data: 512450048
I1226 13:43:58.740537 92417 layer_factory.hpp:114] Creating layer relu5
I1226 13:43:58.740595 92417 net.cpp:178] Creating Layer relu5
I1226 13:43:58.740633 92417 net.cpp:612] relu5 <- conv5
I1226 13:43:58.740696 92417 net.cpp:573] relu5 -> conv5 (in-place)
I1226 13:43:58.740792 92417 net.cpp:228] Setting up relu5
I1226 13:43:58.740842 92417 net.cpp:235] Top shape: 64 256 13 13 (2768896)
I1226 13:43:58.740866 92417 net.cpp:243] Memory required for data: 523525632
I1226 13:43:58.740895 92417 layer_factory.hpp:114] Creating layer pool5
I1226 13:43:58.740962 92417 net.cpp:178] Creating Layer pool5
I1226 13:43:58.740989 92417 net.cpp:612] pool5 <- conv5
I1226 13:43:58.741025 92417 net.cpp:586] pool5 -> pool5
I1226 13:43:58.741120 92417 net.cpp:228] Setting up pool5
I1226 13:43:58.741164 92417 net.cpp:235] Top shape: 64 256 6 6 (589824)
I1226 13:43:58.741186 92417 net.cpp:243] Memory required for data: 525884928
I1226 13:43:58.741214 92417 layer_factory.hpp:114] Creating layer fc6
I1226 13:43:58.741282 92417 net.cpp:178] Creating Layer fc6
I1226 13:43:58.741317 92417 net.cpp:612] fc6 <- pool5
I1226 13:43:58.741365 92417 net.cpp:586] fc6 -> fc6
I1226 13:43:58.735666 94226 net.cpp:228] Setting up conv5
I1226 13:43:58.735792 94226 net.cpp:235] Top shape: 64 256 13 13 (2768896)
I1226 13:43:58.735821 94226 net.cpp:243] Memory required for data: 512450048
I1226 13:43:58.735901 94226 layer_factory.hpp:114] Creating layer relu5
I1226 13:43:58.735965 94226 net.cpp:178] Creating Layer relu5
I1226 13:43:58.736003 94226 net.cpp:612] relu5 <- conv5
I1226 13:43:58.736060 94226 net.cpp:573] relu5 -> conv5 (in-place)
I1226 13:43:58.736171 94226 net.cpp:228] Setting up relu5
I1226 13:43:58.736232 94226 net.cpp:235] Top shape: 64 256 13 13 (2768896)
I1226 13:43:58.736259 94226 net.cpp:243] Memory required for data: 523525632
I1226 13:43:58.736290 94226 layer_factory.hpp:114] Creating layer pool5
I1226 13:43:58.736358 94226 net.cpp:178] Creating Layer pool5
I1226 13:43:58.736418 94226 net.cpp:612] pool5 <- conv5
I1226 13:43:58.736460 94226 net.cpp:586] pool5 -> pool5
I1226 13:43:58.736555 94226 net.cpp:228] Setting up pool5
I1226 13:43:58.736599 94226 net.cpp:235] Top shape: 64 256 6 6 (589824)
I1226 13:43:58.736624 94226 net.cpp:243] Memory required for data: 525884928
I1226 13:43:58.736652 94226 layer_factory.hpp:114] Creating layer fc6
I1226 13:43:58.736722 94226 net.cpp:178] Creating Layer fc6
I1226 13:43:58.736753 94226 net.cpp:612] fc6 <- pool5
I1226 13:43:58.736800 94226 net.cpp:586] fc6 -> fc6
I1226 13:43:58.884778 85984 net.cpp:228] Setting up conv5
I1226 13:43:58.884898 85984 net.cpp:235] Top shape: 64 256 13 13 (2768896)
I1226 13:43:58.884932 85984 net.cpp:243] Memory required for data: 512450048
I1226 13:43:58.885088 85984 layer_factory.hpp:114] Creating layer relu5
I1226 13:43:58.885185 85984 net.cpp:178] Creating Layer relu5
I1226 13:43:58.885426 85984 net.cpp:612] relu5 <- conv5
I1226 13:43:58.885478 85984 net.cpp:573] relu5 -> conv5 (in-place)
I1226 13:43:58.885601 85984 net.cpp:228] Setting up relu5
I1226 13:43:58.885753 85984 net.cpp:235] Top shape: 64 256 13 13 (2768896)
I1226 13:43:58.885787 85984 net.cpp:243] Memory required for data: 523525632
I1226 13:43:58.885825 85984 layer_factory.hpp:114] Creating layer pool5
I1226 13:43:58.885886 85984 net.cpp:178] Creating Layer pool5
I1226 13:43:58.885920 85984 net.cpp:612] pool5 <- conv5
I1226 13:43:58.886164 85984 net.cpp:586] pool5 -> pool5
I1226 13:43:58.886445 85984 net.cpp:228] Setting up pool5
I1226 13:43:58.886584 85984 net.cpp:235] Top shape: 64 256 6 6 (589824)
I1226 13:43:58.886611 85984 net.cpp:243] Memory required for data: 525884928
I1226 13:43:58.886646 85984 layer_factory.hpp:114] Creating layer fc6
I1226 13:43:58.886750 85984 net.cpp:178] Creating Layer fc6
I1226 13:43:58.886955 85984 net.cpp:612] fc6 <- pool5
I1226 13:43:58.887027 85984 net.cpp:586] fc6 -> fc6
I1226 13:43:59.263499 97018 net.cpp:228] Setting up conv2
I1226 13:43:59.263623 97018 net.cpp:235] Top shape: 64 256 27 27 (11943936)
I1226 13:43:59.263659 97018 net.cpp:243] Memory required for data: 328293888
I1226 13:43:59.263779 97018 layer_factory.hpp:114] Creating layer relu2
I1226 13:43:59.263989 97018 net.cpp:178] Creating Layer relu2
I1226 13:43:59.264045 97018 net.cpp:612] relu2 <- conv2
I1226 13:43:59.264091 97018 net.cpp:573] relu2 -> conv2 (in-place)
I1226 13:43:59.264194 97018 net.cpp:228] Setting up relu2
I1226 13:43:59.264251 97018 net.cpp:235] Top shape: 64 256 27 27 (11943936)
I1226 13:43:59.264277 97018 net.cpp:243] Memory required for data: 376069632
I1226 13:43:59.264312 97018 layer_factory.hpp:114] Creating layer norm2
I1226 13:43:59.264391 97018 net.cpp:178] Creating Layer norm2
I1226 13:43:59.264431 97018 net.cpp:612] norm2 <- conv2
I1226 13:43:59.264475 97018 net.cpp:586] norm2 -> norm2
I1226 13:43:59.264569 97018 net.cpp:228] Setting up norm2
I1226 13:43:59.264623 97018 net.cpp:235] Top shape: 64 256 27 27 (11943936)
I1226 13:43:59.264648 97018 net.cpp:243] Memory required for data: 423845376
I1226 13:43:59.264679 97018 layer_factory.hpp:114] Creating layer pool2
I1226 13:43:59.264775 97018 net.cpp:178] Creating Layer pool2
I1226 13:43:59.264816 97018 net.cpp:612] pool2 <- norm2
I1226 13:43:59.264868 97018 net.cpp:586] pool2 -> pool2
I1226 13:43:59.264966 97018 net.cpp:228] Setting up pool2
I1226 13:43:59.265019 97018 net.cpp:235] Top shape: 64 256 13 13 (2768896)
I1226 13:43:59.265159 97018 net.cpp:243] Memory required for data: 434920960
I1226 13:43:59.265198 97018 layer_factory.hpp:114] Creating layer conv3
I1226 13:43:59.265311 97018 net.cpp:178] Creating Layer conv3
I1226 13:43:59.265348 97018 net.cpp:612] conv3 <- pool2
I1226 13:43:59.265394 97018 net.cpp:586] conv3 -> conv3
I1226 13:43:59.370255 94885 net.cpp:228] Setting up conv2
I1226 13:43:59.370385 94885 net.cpp:235] Top shape: 64 256 27 27 (11943936)
I1226 13:43:59.370424 94885 net.cpp:243] Memory required for data: 328293888
I1226 13:43:59.370523 94885 layer_factory.hpp:114] Creating layer relu2
I1226 13:43:59.370610 94885 net.cpp:178] Creating Layer relu2
I1226 13:43:59.370651 94885 net.cpp:612] relu2 <- conv2
I1226 13:43:59.370717 94885 net.cpp:573] relu2 -> conv2 (in-place)
I1226 13:43:59.374629 94885 net.cpp:228] Setting up relu2
I1226 13:43:59.374809 94885 net.cpp:235] Top shape: 64 256 27 27 (11943936)
I1226 13:43:59.374944 94885 net.cpp:243] Memory required for data: 376069632
I1226 13:43:59.375120 94885 layer_factory.hpp:114] Creating layer norm2
I1226 13:43:59.375231 94885 net.cpp:178] Creating Layer norm2
I1226 13:43:59.375283 94885 net.cpp:612] norm2 <- conv2
I1226 13:43:59.375339 94885 net.cpp:586] norm2 -> norm2
I1226 13:43:59.375468 94885 net.cpp:228] Setting up norm2
I1226 13:43:59.375536 94885 net.cpp:235] Top shape: 64 256 27 27 (11943936)
I1226 13:43:59.375569 94885 net.cpp:243] Memory required for data: 423845376
I1226 13:43:59.375608 94885 layer_factory.hpp:114] Creating layer pool2
I1226 13:43:59.375725 94885 net.cpp:178] Creating Layer pool2
I1226 13:43:59.375919 94885 net.cpp:612] pool2 <- norm2
I1226 13:43:59.375983 94885 net.cpp:586] pool2 -> pool2
I1226 13:43:59.376170 94885 net.cpp:228] Setting up pool2
I1226 13:43:59.376338 94885 net.cpp:235] Top shape: 64 256 13 13 (2768896)
I1226 13:43:59.376888 94885 net.cpp:243] Memory required for data: 434920960
I1226 13:43:59.376976 94885 layer_factory.hpp:114] Creating layer conv3
I1226 13:43:59.377224 94885 net.cpp:178] Creating Layer conv3
I1226 13:43:59.377286 94885 net.cpp:612] conv3 <- pool2
I1226 13:43:59.377352 94885 net.cpp:586] conv3 -> conv3
I1226 13:44:00.297349 97018 net.cpp:228] Setting up conv3
I1226 13:44:00.297473 97018 net.cpp:235] Top shape: 64 384 13 13 (4153344)
I1226 13:44:00.297510 97018 net.cpp:243] Memory required for data: 451534336
I1226 13:44:00.297598 97018 layer_factory.hpp:114] Creating layer relu3
I1226 13:44:00.297806 97018 net.cpp:178] Creating Layer relu3
I1226 13:44:00.297868 97018 net.cpp:612] relu3 <- conv3
I1226 13:44:00.297921 97018 net.cpp:573] relu3 -> conv3 (in-place)
I1226 13:44:00.298043 97018 net.cpp:228] Setting up relu3
I1226 13:44:00.298110 97018 net.cpp:235] Top shape: 64 384 13 13 (4153344)
I1226 13:44:00.298138 97018 net.cpp:243] Memory required for data: 468147712
I1226 13:44:00.298180 97018 layer_factory.hpp:114] Creating layer conv4
I1226 13:44:00.298290 97018 net.cpp:178] Creating Layer conv4
I1226 13:44:00.298331 97018 net.cpp:612] conv4 <- conv3
I1226 13:44:00.298380 97018 net.cpp:586] conv4 -> conv4
I1226 13:44:00.673702 94885 net.cpp:228] Setting up conv3
I1226 13:44:00.673851 94885 net.cpp:235] Top shape: 64 384 13 13 (4153344)
I1226 13:44:00.673900 94885 net.cpp:243] Memory required for data: 451534336
I1226 13:44:00.674051 94885 layer_factory.hpp:114] Creating layer relu3
I1226 13:44:00.674151 94885 net.cpp:178] Creating Layer relu3
I1226 13:44:00.674201 94885 net.cpp:612] relu3 <- conv3
I1226 13:44:00.674262 94885 net.cpp:573] relu3 -> conv3 (in-place)
I1226 13:44:00.674401 94885 net.cpp:228] Setting up relu3
I1226 13:44:00.674487 94885 net.cpp:235] Top shape: 64 384 13 13 (4153344)
I1226 13:44:00.674527 94885 net.cpp:243] Memory required for data: 468147712
I1226 13:44:00.674574 94885 layer_factory.hpp:114] Creating layer conv4
I1226 13:44:00.674715 94885 net.cpp:178] Creating Layer conv4
I1226 13:44:00.674775 94885 net.cpp:612] conv4 <- conv3
I1226 13:44:00.674897 94885 net.cpp:586] conv4 -> conv4
I1226 13:44:01.175091 97018 net.cpp:228] Setting up conv4
I1226 13:44:01.175248 97018 net.cpp:235] Top shape: 64 384 13 13 (4153344)
I1226 13:44:01.175299 97018 net.cpp:243] Memory required for data: 484761088
I1226 13:44:01.175371 97018 layer_factory.hpp:114] Creating layer relu4
I1226 13:44:01.175464 97018 net.cpp:178] Creating Layer relu4
I1226 13:44:01.175516 97018 net.cpp:612] relu4 <- conv4
I1226 13:44:01.175588 97018 net.cpp:573] relu4 -> conv4 (in-place)
I1226 13:44:01.175717 97018 net.cpp:228] Setting up relu4
I1226 13:44:01.175819 97018 net.cpp:235] Top shape: 64 384 13 13 (4153344)
I1226 13:44:01.175858 97018 net.cpp:243] Memory required for data: 501374464
I1226 13:44:01.175894 97018 layer_factory.hpp:114] Creating layer conv5
I1226 13:44:01.175994 97018 net.cpp:178] Creating Layer conv5
I1226 13:44:01.176040 97018 net.cpp:612] conv5 <- conv4
I1226 13:44:01.176106 97018 net.cpp:586] conv5 -> conv5
I1226 13:44:01.552480 94885 net.cpp:228] Setting up conv4
I1226 13:44:01.552618 94885 net.cpp:235] Top shape: 64 384 13 13 (4153344)
I1226 13:44:01.552659 94885 net.cpp:243] Memory required for data: 484761088
I1226 13:44:01.552748 94885 layer_factory.hpp:114] Creating layer relu4
I1226 13:44:01.552985 94885 net.cpp:178] Creating Layer relu4
I1226 13:44:01.553036 94885 net.cpp:612] relu4 <- conv4
I1226 13:44:01.553119 94885 net.cpp:573] relu4 -> conv4 (in-place)
I1226 13:44:01.553288 94885 net.cpp:228] Setting up relu4
I1226 13:44:01.553376 94885 net.cpp:235] Top shape: 64 384 13 13 (4153344)
I1226 13:44:01.553423 94885 net.cpp:243] Memory required for data: 501374464
I1226 13:44:01.553472 94885 layer_factory.hpp:114] Creating layer conv5
I1226 13:44:01.553597 94885 net.cpp:178] Creating Layer conv5
I1226 13:44:01.553659 94885 net.cpp:612] conv5 <- conv4
I1226 13:44:01.553745 94885 net.cpp:586] conv5 -> conv5
I1226 13:44:01.803935 97018 net.cpp:228] Setting up conv5
I1226 13:44:01.804056 97018 net.cpp:235] Top shape: 64 256 13 13 (2768896)
I1226 13:44:01.804091 97018 net.cpp:243] Memory required for data: 512450048
I1226 13:44:01.804172 97018 layer_factory.hpp:114] Creating layer relu5
I1226 13:44:01.804270 97018 net.cpp:178] Creating Layer relu5
I1226 13:44:01.804317 97018 net.cpp:612] relu5 <- conv5
I1226 13:44:01.804363 97018 net.cpp:573] relu5 -> conv5 (in-place)
I1226 13:44:01.804471 97018 net.cpp:228] Setting up relu5
I1226 13:44:01.804525 97018 net.cpp:235] Top shape: 64 256 13 13 (2768896)
I1226 13:44:01.804551 97018 net.cpp:243] Memory required for data: 523525632
I1226 13:44:01.804582 97018 layer_factory.hpp:114] Creating layer pool5
I1226 13:44:01.804657 97018 net.cpp:178] Creating Layer pool5
I1226 13:44:01.804697 97018 net.cpp:612] pool5 <- conv5
I1226 13:44:01.804762 97018 net.cpp:586] pool5 -> pool5
I1226 13:44:01.804872 97018 net.cpp:228] Setting up pool5
I1226 13:44:01.804929 97018 net.cpp:235] Top shape: 64 256 6 6 (589824)
I1226 13:44:01.804952 97018 net.cpp:243] Memory required for data: 525884928
I1226 13:44:01.804982 97018 layer_factory.hpp:114] Creating layer fc6
I1226 13:44:01.805052 97018 net.cpp:178] Creating Layer fc6
I1226 13:44:01.805088 97018 net.cpp:612] fc6 <- pool5
I1226 13:44:01.805141 97018 net.cpp:586] fc6 -> fc6
I1226 13:44:02.181713 94885 net.cpp:228] Setting up conv5
I1226 13:44:02.181862 94885 net.cpp:235] Top shape: 64 256 13 13 (2768896)
I1226 13:44:02.181911 94885 net.cpp:243] Memory required for data: 512450048
I1226 13:44:02.182039 94885 layer_factory.hpp:114] Creating layer relu5
I1226 13:44:02.182171 94885 net.cpp:178] Creating Layer relu5
I1226 13:44:02.182242 94885 net.cpp:612] relu5 <- conv5
I1226 13:44:02.182307 94885 net.cpp:573] relu5 -> conv5 (in-place)
I1226 13:44:02.182459 94885 net.cpp:228] Setting up relu5
I1226 13:44:02.182560 94885 net.cpp:235] Top shape: 64 256 13 13 (2768896)
I1226 13:44:02.182605 94885 net.cpp:243] Memory required for data: 523525632
I1226 13:44:02.182672 94885 layer_factory.hpp:114] Creating layer pool5
I1226 13:44:02.182756 94885 net.cpp:178] Creating Layer pool5
I1226 13:44:02.182814 94885 net.cpp:612] pool5 <- conv5
I1226 13:44:02.182912 94885 net.cpp:586] pool5 -> pool5
I1226 13:44:02.183068 94885 net.cpp:228] Setting up pool5
I1226 13:44:02.183151 94885 net.cpp:235] Top shape: 64 256 6 6 (589824)
I1226 13:44:02.183189 94885 net.cpp:243] Memory required for data: 525884928
I1226 13:44:02.183233 94885 layer_factory.hpp:114] Creating layer fc6
I1226 13:44:02.183310 94885 net.cpp:178] Creating Layer fc6
I1226 13:44:02.183358 94885 net.cpp:612] fc6 <- pool5
I1226 13:44:02.183435 94885 net.cpp:586] fc6 -> fc6
I1226 13:44:03.018816 85406 net.cpp:228] Setting up fc6
I1226 13:44:03.018929 85406 net.cpp:235] Top shape: 64 4096 (262144)
I1226 13:44:03.018983 85406 net.cpp:243] Memory required for data: 526933504
I1226 13:44:03.019049 85406 layer_factory.hpp:114] Creating layer relu6
I1226 13:44:03.019143 85406 net.cpp:178] Creating Layer relu6
I1226 13:44:03.019193 85406 net.cpp:612] relu6 <- fc6
I1226 13:44:03.019235 85406 net.cpp:573] relu6 -> fc6 (in-place)
I1226 13:44:03.019318 85406 net.cpp:228] Setting up relu6
I1226 13:44:03.019359 85406 net.cpp:235] Top shape: 64 4096 (262144)
I1226 13:44:03.019493 85406 net.cpp:243] Memory required for data: 527982080
I1226 13:44:03.019534 85406 layer_factory.hpp:114] Creating layer drop6
I1226 13:44:03.019631 85406 net.cpp:178] Creating Layer drop6
I1226 13:44:03.019671 85406 net.cpp:612] drop6 <- fc6
I1226 13:44:03.019711 85406 net.cpp:573] drop6 -> fc6 (in-place)
I1226 13:44:03.019773 85406 net.cpp:228] Setting up drop6
I1226 13:44:03.019812 85406 net.cpp:235] Top shape: 64 4096 (262144)
I1226 13:44:03.019847 85406 net.cpp:243] Memory required for data: 529030656
I1226 13:44:03.019877 85406 layer_factory.hpp:114] Creating layer fc7
I1226 13:44:03.019947 85406 net.cpp:178] Creating Layer fc7
I1226 13:44:03.019974 85406 net.cpp:612] fc7 <- fc6
I1226 13:44:03.020064 85406 net.cpp:586] fc7 -> fc7
I1226 13:44:03.869889 92417 net.cpp:228] Setting up fc6
I1226 13:44:03.869999 92417 net.cpp:235] Top shape: 64 4096 (262144)
I1226 13:44:03.870026 92417 net.cpp:243] Memory required for data: 526933504
I1226 13:44:03.870081 92417 layer_factory.hpp:114] Creating layer relu6
I1226 13:44:03.870178 92417 net.cpp:178] Creating Layer relu6
I1226 13:44:03.870288 92417 net.cpp:612] relu6 <- fc6
I1226 13:44:03.870332 92417 net.cpp:573] relu6 -> fc6 (in-place)
I1226 13:44:03.870419 92417 net.cpp:228] Setting up relu6
I1226 13:44:03.870596 92417 net.cpp:235] Top shape: 64 4096 (262144)
I1226 13:44:03.870626 92417 net.cpp:243] Memory required for data: 527982080
I1226 13:44:03.870659 92417 layer_factory.hpp:114] Creating layer drop6
I1226 13:44:03.870736 92417 net.cpp:178] Creating Layer drop6
I1226 13:44:03.870770 92417 net.cpp:612] drop6 <- fc6
I1226 13:44:03.870810 92417 net.cpp:573] drop6 -> fc6 (in-place)
I1226 13:44:03.870875 92417 net.cpp:228] Setting up drop6
I1226 13:44:03.870911 92417 net.cpp:235] Top shape: 64 4096 (262144)
I1226 13:44:03.870934 92417 net.cpp:243] Memory required for data: 529030656
I1226 13:44:03.870961 92417 layer_factory.hpp:114] Creating layer fc7
I1226 13:44:03.871016 92417 net.cpp:178] Creating Layer fc7
I1226 13:44:03.871042 92417 net.cpp:612] fc7 <- fc6
I1226 13:44:03.871104 92417 net.cpp:586] fc7 -> fc7
I1226 13:44:03.949949 94226 net.cpp:228] Setting up fc6
I1226 13:44:03.950068 94226 net.cpp:235] Top shape: 64 4096 (262144)
I1226 13:44:03.950103 94226 net.cpp:243] Memory required for data: 526933504
I1226 13:44:03.950165 94226 layer_factory.hpp:114] Creating layer relu6
I1226 13:44:03.950261 94226 net.cpp:178] Creating Layer relu6
I1226 13:44:03.950311 94226 net.cpp:612] relu6 <- fc6
I1226 13:44:03.950361 94226 net.cpp:573] relu6 -> fc6 (in-place)
I1226 13:44:03.950497 94226 net.cpp:228] Setting up relu6
I1226 13:44:03.950664 94226 net.cpp:235] Top shape: 64 4096 (262144)
I1226 13:44:03.950700 94226 net.cpp:243] Memory required for data: 527982080
I1226 13:44:03.950738 94226 layer_factory.hpp:114] Creating layer drop6
I1226 13:44:03.950814 94226 net.cpp:178] Creating Layer drop6
I1226 13:44:03.950853 94226 net.cpp:612] drop6 <- fc6
I1226 13:44:03.950896 94226 net.cpp:573] drop6 -> fc6 (in-place)
I1226 13:44:03.950964 94226 net.cpp:228] Setting up drop6
I1226 13:44:03.951009 94226 net.cpp:235] Top shape: 64 4096 (262144)
I1226 13:44:03.951035 94226 net.cpp:243] Memory required for data: 529030656
I1226 13:44:03.951066 94226 layer_factory.hpp:114] Creating layer fc7
I1226 13:44:03.951126 94226 net.cpp:178] Creating Layer fc7
I1226 13:44:03.951158 94226 net.cpp:612] fc7 <- fc6
I1226 13:44:03.951213 94226 net.cpp:586] fc7 -> fc7
I1226 13:44:04.244132 91110 net.cpp:228] Setting up fc6
I1226 13:44:04.244243 91110 net.cpp:235] Top shape: 64 4096 (262144)
I1226 13:44:04.244279 91110 net.cpp:243] Memory required for data: 526933504
I1226 13:44:04.244340 91110 layer_factory.hpp:114] Creating layer relu6
I1226 13:44:04.244402 91110 net.cpp:178] Creating Layer relu6
I1226 13:44:04.244531 91110 net.cpp:612] relu6 <- fc6
I1226 13:44:04.244571 91110 net.cpp:573] relu6 -> fc6 (in-place)
I1226 13:44:04.244662 91110 net.cpp:228] Setting up relu6
I1226 13:44:04.244807 91110 net.cpp:235] Top shape: 64 4096 (262144)
I1226 13:44:04.244837 91110 net.cpp:243] Memory required for data: 527982080
I1226 13:44:04.244869 91110 layer_factory.hpp:114] Creating layer drop6
I1226 13:44:04.244940 91110 net.cpp:178] Creating Layer drop6
I1226 13:44:04.244969 91110 net.cpp:612] drop6 <- fc6
I1226 13:44:04.245007 91110 net.cpp:573] drop6 -> fc6 (in-place)
I1226 13:44:04.245095 91110 net.cpp:228] Setting up drop6
I1226 13:44:04.245133 91110 net.cpp:235] Top shape: 64 4096 (262144)
I1226 13:44:04.245158 91110 net.cpp:243] Memory required for data: 529030656
I1226 13:44:04.245185 91110 layer_factory.hpp:114] Creating layer fc7
I1226 13:44:04.245244 91110 net.cpp:178] Creating Layer fc7
I1226 13:44:04.245270 91110 net.cpp:612] fc7 <- fc6
I1226 13:44:04.245322 91110 net.cpp:586] fc7 -> fc7
I1226 13:44:04.294308 91279 net.cpp:228] Setting up fc6
I1226 13:44:04.294435 91279 net.cpp:235] Top shape: 64 4096 (262144)
I1226 13:44:04.294468 91279 net.cpp:243] Memory required for data: 526933504
I1226 13:44:04.294525 91279 layer_factory.hpp:114] Creating layer relu6
I1226 13:44:04.294622 91279 net.cpp:178] Creating Layer relu6
I1226 13:44:04.294754 91279 net.cpp:612] relu6 <- fc6
I1226 13:44:04.294800 91279 net.cpp:573] relu6 -> fc6 (in-place)
I1226 13:44:04.294884 91279 net.cpp:228] Setting up relu6
I1226 13:44:04.295030 91279 net.cpp:235] Top shape: 64 4096 (262144)
I1226 13:44:04.295061 91279 net.cpp:243] Memory required for data: 527982080
I1226 13:44:04.295105 91279 layer_factory.hpp:114] Creating layer drop6
I1226 13:44:04.295192 91279 net.cpp:178] Creating Layer drop6
I1226 13:44:04.295253 91279 net.cpp:612] drop6 <- fc6
I1226 13:44:04.295292 91279 net.cpp:573] drop6 -> fc6 (in-place)
I1226 13:44:04.295353 91279 net.cpp:228] Setting up drop6
I1226 13:44:04.295477 91279 net.cpp:235] Top shape: 64 4096 (262144)
I1226 13:44:04.295503 91279 net.cpp:243] Memory required for data: 529030656
I1226 13:44:04.295531 91279 layer_factory.hpp:114] Creating layer fc7
I1226 13:44:04.295634 91279 net.cpp:178] Creating Layer fc7
I1226 13:44:04.295678 91279 net.cpp:612] fc7 <- fc6
I1226 13:44:04.295747 91279 net.cpp:586] fc7 -> fc7
I1226 13:44:04.302523 98538 net.cpp:228] Setting up fc6
I1226 13:44:04.302642 98538 net.cpp:235] Top shape: 64 4096 (262144)
I1226 13:44:04.302677 98538 net.cpp:243] Memory required for data: 526933504
I1226 13:44:04.302784 98538 layer_factory.hpp:114] Creating layer relu6
I1226 13:44:04.302893 98538 net.cpp:178] Creating Layer relu6
I1226 13:44:04.302947 98538 net.cpp:612] relu6 <- fc6
I1226 13:44:04.303014 98538 net.cpp:573] relu6 -> fc6 (in-place)
I1226 13:44:04.303166 98538 net.cpp:228] Setting up relu6
I1226 13:44:04.303385 98538 net.cpp:235] Top shape: 64 4096 (262144)
I1226 13:44:04.303455 98538 net.cpp:243] Memory required for data: 527982080
I1226 13:44:04.303511 98538 layer_factory.hpp:114] Creating layer drop6
I1226 13:44:04.303586 98538 net.cpp:178] Creating Layer drop6
I1226 13:44:04.303643 98538 net.cpp:612] drop6 <- fc6
I1226 13:44:04.303714 98538 net.cpp:573] drop6 -> fc6 (in-place)
I1226 13:44:04.303792 98538 net.cpp:228] Setting up drop6
I1226 13:44:04.303846 98538 net.cpp:235] Top shape: 64 4096 (262144)
I1226 13:44:04.303889 98538 net.cpp:243] Memory required for data: 529030656
I1226 13:44:04.303974 98538 layer_factory.hpp:114] Creating layer fc7
I1226 13:44:04.304054 98538 net.cpp:178] Creating Layer fc7
I1226 13:44:04.304129 98538 net.cpp:612] fc7 <- fc6
I1226 13:44:04.304226 98538 net.cpp:586] fc7 -> fc7
I1226 13:44:04.481752 85984 net.cpp:228] Setting up fc6
I1226 13:44:04.481891 85984 net.cpp:235] Top shape: 64 4096 (262144)
I1226 13:44:04.481930 85984 net.cpp:243] Memory required for data: 526933504
I1226 13:44:04.482010 85984 layer_factory.hpp:114] Creating layer relu6
I1226 13:44:04.482120 85984 net.cpp:178] Creating Layer relu6
I1226 13:44:04.482172 85984 net.cpp:612] relu6 <- fc6
I1226 13:44:04.482220 85984 net.cpp:573] relu6 -> fc6 (in-place)
I1226 13:44:04.482318 85984 net.cpp:228] Setting up relu6
I1226 13:44:04.482494 85984 net.cpp:235] Top shape: 64 4096 (262144)
I1226 13:44:04.482533 85984 net.cpp:243] Memory required for data: 527982080
I1226 13:44:04.482584 85984 layer_factory.hpp:114] Creating layer drop6
I1226 13:44:04.482647 85984 net.cpp:178] Creating Layer drop6
I1226 13:44:04.482720 85984 net.cpp:612] drop6 <- fc6
I1226 13:44:04.482772 85984 net.cpp:573] drop6 -> fc6 (in-place)
I1226 13:44:04.482853 85984 net.cpp:228] Setting up drop6
I1226 13:44:04.482915 85984 net.cpp:235] Top shape: 64 4096 (262144)
I1226 13:44:04.482945 85984 net.cpp:243] Memory required for data: 529030656
I1226 13:44:04.482983 85984 layer_factory.hpp:114] Creating layer fc7
I1226 13:44:04.483084 85984 net.cpp:178] Creating Layer fc7
I1226 13:44:04.483130 85984 net.cpp:612] fc7 <- fc6
I1226 13:44:04.483178 85984 net.cpp:586] fc7 -> fc7
I1226 13:44:05.274713 85406 net.cpp:228] Setting up fc7
I1226 13:44:05.274827 85406 net.cpp:235] Top shape: 64 4096 (262144)
I1226 13:44:05.274858 85406 net.cpp:243] Memory required for data: 530079232
I1226 13:44:05.274914 85406 layer_factory.hpp:114] Creating layer relu7
I1226 13:44:05.274993 85406 net.cpp:178] Creating Layer relu7
I1226 13:44:05.275025 85406 net.cpp:612] relu7 <- fc7
I1226 13:44:05.275094 85406 net.cpp:573] relu7 -> fc7 (in-place)
I1226 13:44:05.275183 85406 net.cpp:228] Setting up relu7
I1226 13:44:05.275224 85406 net.cpp:235] Top shape: 64 4096 (262144)
I1226 13:44:05.275262 85406 net.cpp:243] Memory required for data: 531127808
I1226 13:44:05.275291 85406 layer_factory.hpp:114] Creating layer drop7
I1226 13:44:05.275331 85406 net.cpp:178] Creating Layer drop7
I1226 13:44:05.275357 85406 net.cpp:612] drop7 <- fc7
I1226 13:44:05.275401 85406 net.cpp:573] drop7 -> fc7 (in-place)
I1226 13:44:05.275450 85406 net.cpp:228] Setting up drop7
I1226 13:44:05.275485 85406 net.cpp:235] Top shape: 64 4096 (262144)
I1226 13:44:05.275507 85406 net.cpp:243] Memory required for data: 532176384
I1226 13:44:05.275534 85406 layer_factory.hpp:114] Creating layer fc8
I1226 13:44:05.275635 85406 net.cpp:178] Creating Layer fc8
I1226 13:44:05.275670 85406 net.cpp:612] fc8 <- fc7
I1226 13:44:05.275710 85406 net.cpp:586] fc8 -> fc8
I1226 13:44:05.842836 85406 net.cpp:228] Setting up fc8
I1226 13:44:05.842954 85406 net.cpp:235] Top shape: 64 1000 (64000)
I1226 13:44:05.842988 85406 net.cpp:243] Memory required for data: 532432384
I1226 13:44:05.843046 85406 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 13:44:05.843133 85406 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 13:44:05.843183 85406 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 13:44:05.843238 85406 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 13:44:05.843328 85406 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 13:44:05.843533 85406 net.cpp:228] Setting up fc8_fc8_0_split
I1226 13:44:05.843612 85406 net.cpp:235] Top shape: 64 1000 (64000)
I1226 13:44:05.843649 85406 net.cpp:235] Top shape: 64 1000 (64000)
I1226 13:44:05.843673 85406 net.cpp:243] Memory required for data: 532944384
I1226 13:44:05.843718 85406 layer_factory.hpp:114] Creating layer accuracy
I1226 13:44:05.843780 85406 net.cpp:178] Creating Layer accuracy
I1226 13:44:05.843814 85406 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 13:44:05.843858 85406 net.cpp:612] accuracy <- label_data_1_split_0
I1226 13:44:05.843900 85406 net.cpp:586] accuracy -> accuracy
I1226 13:44:05.843947 85406 net.cpp:228] Setting up accuracy
I1226 13:44:05.843987 85406 net.cpp:235] Top shape: (1)
I1226 13:44:05.844010 85406 net.cpp:243] Memory required for data: 532944388
I1226 13:44:05.844038 85406 layer_factory.hpp:114] Creating layer loss
I1226 13:44:05.844089 85406 net.cpp:178] Creating Layer loss
I1226 13:44:05.844115 85406 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 13:44:05.844174 85406 net.cpp:612] loss <- label_data_1_split_1
I1226 13:44:05.844208 85406 net.cpp:586] loss -> loss
I1226 13:44:05.844274 85406 layer_factory.hpp:114] Creating layer loss
I1226 13:44:05.868654 85406 net.cpp:228] Setting up loss
I1226 13:44:05.868773 85406 net.cpp:235] Top shape: (1)
I1226 13:44:05.868813 85406 net.cpp:238]     with loss weight 1
I1226 13:44:05.869062 85406 net.cpp:243] Memory required for data: 532944392
I1226 13:44:05.869122 85406 net.cpp:305] loss needs backward computation.
I1226 13:44:05.869163 85406 net.cpp:307] accuracy does not need backward computation.
I1226 13:44:05.869207 85406 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 13:44:05.869252 85406 net.cpp:305] fc8 needs backward computation.
I1226 13:44:05.869294 85406 net.cpp:305] drop7 needs backward computation.
I1226 13:44:05.869335 85406 net.cpp:305] relu7 needs backward computation.
I1226 13:44:05.869365 85406 net.cpp:305] fc7 needs backward computation.
I1226 13:44:05.869396 85406 net.cpp:305] drop6 needs backward computation.
I1226 13:44:05.869433 85406 net.cpp:305] relu6 needs backward computation.
I1226 13:44:05.869462 85406 net.cpp:305] fc6 needs backward computation.
I1226 13:44:05.869493 85406 net.cpp:305] pool5 needs backward computation.
I1226 13:44:05.869529 85406 net.cpp:305] relu5 needs backward computation.
I1226 13:44:05.869570 85406 net.cpp:305] conv5 needs backward computation.
I1226 13:44:05.869627 85406 net.cpp:305] relu4 needs backward computation.
I1226 13:44:05.869662 85406 net.cpp:305] conv4 needs backward computation.
I1226 13:44:05.869693 85406 net.cpp:305] relu3 needs backward computation.
I1226 13:44:05.869729 85406 net.cpp:305] conv3 needs backward computation.
I1226 13:44:05.869771 85406 net.cpp:305] pool2 needs backward computation.
I1226 13:44:05.869812 85406 net.cpp:305] norm2 needs backward computation.
I1226 13:44:05.869843 85406 net.cpp:305] relu2 needs backward computation.
I1226 13:44:05.869881 85406 net.cpp:305] conv2 needs backward computation.
I1226 13:44:05.869922 85406 net.cpp:305] pool1 needs backward computation.
I1226 13:44:05.869953 85406 net.cpp:305] norm1 needs backward computation.
I1226 13:44:05.869984 85406 net.cpp:305] relu1 needs backward computation.
I1226 13:44:05.870021 85406 net.cpp:305] conv1 needs backward computation.
I1226 13:44:05.870054 85406 net.cpp:307] label_data_1_split does not need backward computation.
I1226 13:44:05.870088 85406 net.cpp:307] data does not need backward computation.
I1226 13:44:05.870117 85406 net.cpp:349] This network produces output accuracy
I1226 13:44:05.870157 85406 net.cpp:349] This network produces output loss
I1226 13:44:05.870275 85406 net.cpp:363] Network initialization done.
I1226 13:44:05.870749 85406 solver.cpp:107] Solver scaffolding done.
I1226 13:44:05.870970 85406 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 13:44:06.176761 92417 net.cpp:228] Setting up fc7
I1226 13:44:06.176874 92417 net.cpp:235] Top shape: 64 4096 (262144)
I1226 13:44:06.176905 92417 net.cpp:243] Memory required for data: 530079232
I1226 13:44:06.176959 92417 layer_factory.hpp:114] Creating layer relu7
I1226 13:44:06.177034 92417 net.cpp:178] Creating Layer relu7
I1226 13:44:06.177067 92417 net.cpp:612] relu7 <- fc7
I1226 13:44:06.177130 92417 net.cpp:573] relu7 -> fc7 (in-place)
I1226 13:44:06.177253 92417 net.cpp:228] Setting up relu7
I1226 13:44:06.177356 92417 net.cpp:235] Top shape: 64 4096 (262144)
I1226 13:44:06.177381 92417 net.cpp:243] Memory required for data: 531127808
I1226 13:44:06.177410 92417 layer_factory.hpp:114] Creating layer drop7
I1226 13:44:06.177448 92417 net.cpp:178] Creating Layer drop7
I1226 13:44:06.177532 92417 net.cpp:612] drop7 <- fc7
I1226 13:44:06.177569 92417 net.cpp:573] drop7 -> fc7 (in-place)
I1226 13:44:06.177613 92417 net.cpp:228] Setting up drop7
I1226 13:44:06.177721 92417 net.cpp:235] Top shape: 64 4096 (262144)
I1226 13:44:06.177743 92417 net.cpp:243] Memory required for data: 532176384
I1226 13:44:06.177770 92417 layer_factory.hpp:114] Creating layer fc8
I1226 13:44:06.177845 92417 net.cpp:178] Creating Layer fc8
I1226 13:44:06.177881 92417 net.cpp:612] fc8 <- fc7
I1226 13:44:06.177922 92417 net.cpp:586] fc8 -> fc8
I1226 13:44:06.279958 94226 net.cpp:228] Setting up fc7
I1226 13:44:06.280079 94226 net.cpp:235] Top shape: 64 4096 (262144)
I1226 13:44:06.280112 94226 net.cpp:243] Memory required for data: 530079232
I1226 13:44:06.280201 94226 layer_factory.hpp:114] Creating layer relu7
I1226 13:44:06.280278 94226 net.cpp:178] Creating Layer relu7
I1226 13:44:06.280320 94226 net.cpp:612] relu7 <- fc7
I1226 13:44:06.280441 94226 net.cpp:573] relu7 -> fc7 (in-place)
I1226 13:44:06.280561 94226 net.cpp:228] Setting up relu7
I1226 13:44:06.280634 94226 net.cpp:235] Top shape: 64 4096 (262144)
I1226 13:44:06.280666 94226 net.cpp:243] Memory required for data: 531127808
I1226 13:44:06.280702 94226 layer_factory.hpp:114] Creating layer drop7
I1226 13:44:06.280748 94226 net.cpp:178] Creating Layer drop7
I1226 13:44:06.280781 94226 net.cpp:612] drop7 <- fc7
I1226 13:44:06.280818 94226 net.cpp:573] drop7 -> fc7 (in-place)
I1226 13:44:06.280869 94226 net.cpp:228] Setting up drop7
I1226 13:44:06.280907 94226 net.cpp:235] Top shape: 64 4096 (262144)
I1226 13:44:06.280931 94226 net.cpp:243] Memory required for data: 532176384
I1226 13:44:06.280962 94226 layer_factory.hpp:114] Creating layer fc8
I1226 13:44:06.281038 94226 net.cpp:178] Creating Layer fc8
I1226 13:44:06.281071 94226 net.cpp:612] fc8 <- fc7
I1226 13:44:06.281112 94226 net.cpp:586] fc8 -> fc8
I1226 13:44:06.493907 91110 net.cpp:228] Setting up fc7
I1226 13:44:06.494019 91110 net.cpp:235] Top shape: 64 4096 (262144)
I1226 13:44:06.494047 91110 net.cpp:243] Memory required for data: 530079232
I1226 13:44:06.494133 91110 layer_factory.hpp:114] Creating layer relu7
I1226 13:44:06.494199 91110 net.cpp:178] Creating Layer relu7
I1226 13:44:06.494231 91110 net.cpp:612] relu7 <- fc7
I1226 13:44:06.494279 91110 net.cpp:573] relu7 -> fc7 (in-place)
I1226 13:44:06.494387 91110 net.cpp:228] Setting up relu7
I1226 13:44:06.494436 91110 net.cpp:235] Top shape: 64 4096 (262144)
I1226 13:44:06.494460 91110 net.cpp:243] Memory required for data: 531127808
I1226 13:44:06.494490 91110 layer_factory.hpp:114] Creating layer drop7
I1226 13:44:06.494529 91110 net.cpp:178] Creating Layer drop7
I1226 13:44:06.494562 91110 net.cpp:612] drop7 <- fc7
I1226 13:44:06.494598 91110 net.cpp:573] drop7 -> fc7 (in-place)
I1226 13:44:06.494643 91110 net.cpp:228] Setting up drop7
I1226 13:44:06.494681 91110 net.cpp:235] Top shape: 64 4096 (262144)
I1226 13:44:06.494710 91110 net.cpp:243] Memory required for data: 532176384
I1226 13:44:06.494737 91110 layer_factory.hpp:114] Creating layer fc8
I1226 13:44:06.494791 91110 net.cpp:178] Creating Layer fc8
I1226 13:44:06.494823 91110 net.cpp:612] fc8 <- fc7
I1226 13:44:06.494874 91110 net.cpp:586] fc8 -> fc8
I1226 13:44:06.589990 91279 net.cpp:228] Setting up fc7
I1226 13:44:06.590103 91279 net.cpp:235] Top shape: 64 4096 (262144)
I1226 13:44:06.590132 91279 net.cpp:243] Memory required for data: 530079232
I1226 13:44:06.590248 91279 layer_factory.hpp:114] Creating layer relu7
I1226 13:44:06.590322 91279 net.cpp:178] Creating Layer relu7
I1226 13:44:06.590365 91279 net.cpp:612] relu7 <- fc7
I1226 13:44:06.590423 91279 net.cpp:573] relu7 -> fc7 (in-place)
I1226 13:44:06.590611 91279 net.cpp:228] Setting up relu7
I1226 13:44:06.590662 91279 net.cpp:235] Top shape: 64 4096 (262144)
I1226 13:44:06.590687 91279 net.cpp:243] Memory required for data: 531127808
I1226 13:44:06.590714 91279 layer_factory.hpp:114] Creating layer drop7
I1226 13:44:06.590751 91279 net.cpp:178] Creating Layer drop7
I1226 13:44:06.590776 91279 net.cpp:612] drop7 <- fc7
I1226 13:44:06.590812 91279 net.cpp:573] drop7 -> fc7 (in-place)
I1226 13:44:06.590862 91279 net.cpp:228] Setting up drop7
I1226 13:44:06.590903 91279 net.cpp:235] Top shape: 64 4096 (262144)
I1226 13:44:06.590929 91279 net.cpp:243] Memory required for data: 532176384
I1226 13:44:06.590966 91279 layer_factory.hpp:114] Creating layer fc8
I1226 13:44:06.591037 91279 net.cpp:178] Creating Layer fc8
I1226 13:44:06.591073 91279 net.cpp:612] fc8 <- fc7
I1226 13:44:06.591119 91279 net.cpp:586] fc8 -> fc8
I1226 13:44:06.604495 98538 net.cpp:228] Setting up fc7
I1226 13:44:06.604619 98538 net.cpp:235] Top shape: 64 4096 (262144)
I1226 13:44:06.604650 98538 net.cpp:243] Memory required for data: 530079232
I1226 13:44:06.604713 98538 layer_factory.hpp:114] Creating layer relu7
I1226 13:44:06.604817 98538 net.cpp:178] Creating Layer relu7
I1226 13:44:06.604869 98538 net.cpp:612] relu7 <- fc7
I1226 13:44:06.604912 98538 net.cpp:573] relu7 -> fc7 (in-place)
I1226 13:44:06.605006 98538 net.cpp:228] Setting up relu7
I1226 13:44:06.605089 98538 net.cpp:235] Top shape: 64 4096 (262144)
I1226 13:44:06.605121 98538 net.cpp:243] Memory required for data: 531127808
I1226 13:44:06.605159 98538 layer_factory.hpp:114] Creating layer drop7
I1226 13:44:06.605201 98538 net.cpp:178] Creating Layer drop7
I1226 13:44:06.605240 98538 net.cpp:612] drop7 <- fc7
I1226 13:44:06.605279 98538 net.cpp:573] drop7 -> fc7 (in-place)
I1226 13:44:06.605331 98538 net.cpp:228] Setting up drop7
I1226 13:44:06.605377 98538 net.cpp:235] Top shape: 64 4096 (262144)
I1226 13:44:06.605401 98538 net.cpp:243] Memory required for data: 532176384
I1226 13:44:06.605429 98538 layer_factory.hpp:114] Creating layer fc8
I1226 13:44:06.605514 98538 net.cpp:178] Creating Layer fc8
I1226 13:44:06.605551 98538 net.cpp:612] fc8 <- fc7
I1226 13:44:06.605592 98538 net.cpp:586] fc8 -> fc8
I1226 13:44:06.743015 92417 net.cpp:228] Setting up fc8
I1226 13:44:06.743322 92417 net.cpp:235] Top shape: 64 1000 (64000)
I1226 13:44:06.743365 92417 net.cpp:243] Memory required for data: 532432384
I1226 13:44:06.743419 92417 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 13:44:06.743557 92417 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 13:44:06.743703 92417 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 13:44:06.743757 92417 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 13:44:06.743815 92417 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 13:44:06.743899 92417 net.cpp:228] Setting up fc8_fc8_0_split
I1226 13:44:06.743949 92417 net.cpp:235] Top shape: 64 1000 (64000)
I1226 13:44:06.743981 92417 net.cpp:235] Top shape: 64 1000 (64000)
I1226 13:44:06.744004 92417 net.cpp:243] Memory required for data: 532944384
I1226 13:44:06.744060 92417 layer_factory.hpp:114] Creating layer accuracy
I1226 13:44:06.744230 92417 net.cpp:178] Creating Layer accuracy
I1226 13:44:06.744269 92417 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 13:44:06.744323 92417 net.cpp:612] accuracy <- label_data_1_split_0
I1226 13:44:06.744362 92417 net.cpp:586] accuracy -> accuracy
I1226 13:44:06.744410 92417 net.cpp:228] Setting up accuracy
I1226 13:44:06.744455 92417 net.cpp:235] Top shape: (1)
I1226 13:44:06.744505 92417 net.cpp:243] Memory required for data: 532944388
I1226 13:44:06.744534 92417 layer_factory.hpp:114] Creating layer loss
I1226 13:44:06.744580 92417 net.cpp:178] Creating Layer loss
I1226 13:44:06.744612 92417 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 13:44:06.744649 92417 net.cpp:612] loss <- label_data_1_split_1
I1226 13:44:06.744684 92417 net.cpp:586] loss -> loss
I1226 13:44:06.744742 92417 layer_factory.hpp:114] Creating layer loss
I1226 13:44:06.775776 85984 net.cpp:228] Setting up fc7
I1226 13:44:06.775895 85984 net.cpp:235] Top shape: 64 4096 (262144)
I1226 13:44:06.775933 85984 net.cpp:243] Memory required for data: 530079232
I1226 13:44:06.776016 85984 layer_factory.hpp:114] Creating layer relu7
I1226 13:44:06.776110 85984 net.cpp:178] Creating Layer relu7
I1226 13:44:06.776273 85984 net.cpp:612] relu7 <- fc7
I1226 13:44:06.776345 85984 net.cpp:573] relu7 -> fc7 (in-place)
I1226 13:44:06.776461 85984 net.cpp:228] Setting up relu7
I1226 13:44:06.776526 85984 net.cpp:235] Top shape: 64 4096 (262144)
I1226 13:44:06.776556 85984 net.cpp:243] Memory required for data: 531127808
I1226 13:44:06.776592 85984 layer_factory.hpp:114] Creating layer drop7
I1226 13:44:06.776639 85984 net.cpp:178] Creating Layer drop7
I1226 13:44:06.776705 85984 net.cpp:612] drop7 <- fc7
I1226 13:44:06.776757 85984 net.cpp:573] drop7 -> fc7 (in-place)
I1226 13:44:06.776814 85984 net.cpp:228] Setting up drop7
I1226 13:44:06.776859 85984 net.cpp:235] Top shape: 64 4096 (262144)
I1226 13:44:06.776891 85984 net.cpp:243] Memory required for data: 532176384
I1226 13:44:06.776928 85984 layer_factory.hpp:114] Creating layer fc8
I1226 13:44:06.777034 85984 net.cpp:178] Creating Layer fc8
I1226 13:44:06.777083 85984 net.cpp:612] fc8 <- fc7
I1226 13:44:06.777140 85984 net.cpp:586] fc8 -> fc8
I1226 13:44:06.776408 92417 net.cpp:228] Setting up loss
I1226 13:44:06.776551 92417 net.cpp:235] Top shape: (1)
I1226 13:44:06.776718 92417 net.cpp:238]     with loss weight 1
I1226 13:44:06.776875 92417 net.cpp:243] Memory required for data: 532944392
I1226 13:44:06.776924 92417 net.cpp:305] loss needs backward computation.
I1226 13:44:06.776969 92417 net.cpp:307] accuracy does not need backward computation.
I1226 13:44:06.777024 92417 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 13:44:06.777068 92417 net.cpp:305] fc8 needs backward computation.
I1226 13:44:06.777098 92417 net.cpp:305] drop7 needs backward computation.
I1226 13:44:06.777128 92417 net.cpp:305] relu7 needs backward computation.
I1226 13:44:06.777164 92417 net.cpp:305] fc7 needs backward computation.
I1226 13:44:06.777195 92417 net.cpp:305] drop6 needs backward computation.
I1226 13:44:06.777236 92417 net.cpp:305] relu6 needs backward computation.
I1226 13:44:06.777272 92417 net.cpp:305] fc6 needs backward computation.
I1226 13:44:06.777308 92417 net.cpp:305] pool5 needs backward computation.
I1226 13:44:06.777348 92417 net.cpp:305] relu5 needs backward computation.
I1226 13:44:06.777379 92417 net.cpp:305] conv5 needs backward computation.
I1226 13:44:06.777410 92417 net.cpp:305] relu4 needs backward computation.
I1226 13:44:06.777446 92417 net.cpp:305] conv4 needs backward computation.
I1226 13:44:06.777498 92417 net.cpp:305] relu3 needs backward computation.
I1226 13:44:06.777530 92417 net.cpp:305] conv3 needs backward computation.
I1226 13:44:06.777562 92417 net.cpp:305] pool2 needs backward computation.
I1226 13:44:06.777601 92417 net.cpp:305] norm2 needs backward computation.
I1226 13:44:06.777633 92417 net.cpp:305] relu2 needs backward computation.
I1226 13:44:06.777670 92417 net.cpp:305] conv2 needs backward computation.
I1226 13:44:06.777703 92417 net.cpp:305] pool1 needs backward computation.
I1226 13:44:06.777739 92417 net.cpp:305] norm1 needs backward computation.
I1226 13:44:06.777770 92417 net.cpp:305] relu1 needs backward computation.
I1226 13:44:06.777801 92417 net.cpp:305] conv1 needs backward computation.
I1226 13:44:06.777833 92417 net.cpp:307] label_data_1_split does not need backward computation.
I1226 13:44:06.777868 92417 net.cpp:307] data does not need backward computation.
I1226 13:44:06.777901 92417 net.cpp:349] This network produces output accuracy
I1226 13:44:06.777936 92417 net.cpp:349] This network produces output loss
I1226 13:44:06.778043 92417 net.cpp:363] Network initialization done.
I1226 13:44:06.778523 92417 solver.cpp:107] Solver scaffolding done.
I1226 13:44:06.778745 92417 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 13:44:06.858587 94226 net.cpp:228] Setting up fc8
I1226 13:44:06.858723 94226 net.cpp:235] Top shape: 64 1000 (64000)
I1226 13:44:06.858757 94226 net.cpp:243] Memory required for data: 532432384
I1226 13:44:06.858820 94226 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 13:44:06.858939 94226 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 13:44:06.858984 94226 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 13:44:06.859042 94226 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 13:44:06.859096 94226 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 13:44:06.859218 94226 net.cpp:228] Setting up fc8_fc8_0_split
I1226 13:44:06.859408 94226 net.cpp:235] Top shape: 64 1000 (64000)
I1226 13:44:06.859448 94226 net.cpp:235] Top shape: 64 1000 (64000)
I1226 13:44:06.859472 94226 net.cpp:243] Memory required for data: 532944384
I1226 13:44:06.859505 94226 layer_factory.hpp:114] Creating layer accuracy
I1226 13:44:06.859606 94226 net.cpp:178] Creating Layer accuracy
I1226 13:44:06.859786 94226 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 13:44:06.859823 94226 net.cpp:612] accuracy <- label_data_1_split_0
I1226 13:44:06.859861 94226 net.cpp:586] accuracy -> accuracy
I1226 13:44:06.859913 94226 net.cpp:228] Setting up accuracy
I1226 13:44:06.859952 94226 net.cpp:235] Top shape: (1)
I1226 13:44:06.859975 94226 net.cpp:243] Memory required for data: 532944388
I1226 13:44:06.860005 94226 layer_factory.hpp:114] Creating layer loss
I1226 13:44:06.860051 94226 net.cpp:178] Creating Layer loss
I1226 13:44:06.860077 94226 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 13:44:06.860106 94226 net.cpp:612] loss <- label_data_1_split_1
I1226 13:44:06.860163 94226 net.cpp:586] loss -> loss
I1226 13:44:06.860229 94226 layer_factory.hpp:114] Creating layer loss
I1226 13:44:06.887222 94226 net.cpp:228] Setting up loss
I1226 13:44:06.887322 94226 net.cpp:235] Top shape: (1)
I1226 13:44:06.887477 94226 net.cpp:238]     with loss weight 1
I1226 13:44:06.887603 94226 net.cpp:243] Memory required for data: 532944392
I1226 13:44:06.887639 94226 net.cpp:305] loss needs backward computation.
I1226 13:44:06.887679 94226 net.cpp:307] accuracy does not need backward computation.
I1226 13:44:06.887717 94226 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 13:44:06.887751 94226 net.cpp:305] fc8 needs backward computation.
I1226 13:44:06.887786 94226 net.cpp:305] drop7 needs backward computation.
I1226 13:44:06.887817 94226 net.cpp:305] relu7 needs backward computation.
I1226 13:44:06.887850 94226 net.cpp:305] fc7 needs backward computation.
I1226 13:44:06.887881 94226 net.cpp:305] drop6 needs backward computation.
I1226 13:44:06.887910 94226 net.cpp:305] relu6 needs backward computation.
I1226 13:44:06.887941 94226 net.cpp:305] fc6 needs backward computation.
I1226 13:44:06.887972 94226 net.cpp:305] pool5 needs backward computation.
I1226 13:44:06.888006 94226 net.cpp:305] relu5 needs backward computation.
I1226 13:44:06.888037 94226 net.cpp:305] conv5 needs backward computation.
I1226 13:44:06.888070 94226 net.cpp:305] relu4 needs backward computation.
I1226 13:44:06.888101 94226 net.cpp:305] conv4 needs backward computation.
I1226 13:44:06.888134 94226 net.cpp:305] relu3 needs backward computation.
I1226 13:44:06.888165 94226 net.cpp:305] conv3 needs backward computation.
I1226 13:44:06.888198 94226 net.cpp:305] pool2 needs backward computation.
I1226 13:44:06.888231 94226 net.cpp:305] norm2 needs backward computation.
I1226 13:44:06.888263 94226 net.cpp:305] relu2 needs backward computation.
I1226 13:44:06.888295 94226 net.cpp:305] conv2 needs backward computation.
I1226 13:44:06.888329 94226 net.cpp:305] pool1 needs backward computation.
I1226 13:44:06.888360 94226 net.cpp:305] norm1 needs backward computation.
I1226 13:44:06.888411 94226 net.cpp:305] relu1 needs backward computation.
I1226 13:44:06.888440 94226 net.cpp:305] conv1 needs backward computation.
I1226 13:44:06.888474 94226 net.cpp:307] label_data_1_split does not need backward computation.
I1226 13:44:06.888506 94226 net.cpp:307] data does not need backward computation.
I1226 13:44:06.888537 94226 net.cpp:349] This network produces output accuracy
I1226 13:44:06.888574 94226 net.cpp:349] This network produces output loss
I1226 13:44:06.888676 94226 net.cpp:363] Network initialization done.
I1226 13:44:06.889106 94226 solver.cpp:107] Solver scaffolding done.
I1226 13:44:06.889297 94226 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 13:44:07.050827 91110 net.cpp:228] Setting up fc8
I1226 13:44:07.050972 91110 net.cpp:235] Top shape: 64 1000 (64000)
I1226 13:44:07.051000 91110 net.cpp:243] Memory required for data: 532432384
I1226 13:44:07.051112 91110 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 13:44:07.051177 91110 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 13:44:07.051208 91110 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 13:44:07.051276 91110 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 13:44:07.051326 91110 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 13:44:07.051409 91110 net.cpp:228] Setting up fc8_fc8_0_split
I1226 13:44:07.051458 91110 net.cpp:235] Top shape: 64 1000 (64000)
I1226 13:44:07.051501 91110 net.cpp:235] Top shape: 64 1000 (64000)
I1226 13:44:07.051524 91110 net.cpp:243] Memory required for data: 532944384
I1226 13:44:07.051553 91110 layer_factory.hpp:114] Creating layer accuracy
I1226 13:44:07.051606 91110 net.cpp:178] Creating Layer accuracy
I1226 13:44:07.051641 91110 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 13:44:07.051672 91110 net.cpp:612] accuracy <- label_data_1_split_0
I1226 13:44:07.051707 91110 net.cpp:586] accuracy -> accuracy
I1226 13:44:07.051750 91110 net.cpp:228] Setting up accuracy
I1226 13:44:07.051790 91110 net.cpp:235] Top shape: (1)
I1226 13:44:07.051815 91110 net.cpp:243] Memory required for data: 532944388
I1226 13:44:07.051841 91110 layer_factory.hpp:114] Creating layer loss
I1226 13:44:07.051892 91110 net.cpp:178] Creating Layer loss
I1226 13:44:07.051918 91110 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 13:44:07.051945 91110 net.cpp:612] loss <- label_data_1_split_1
I1226 13:44:07.051978 91110 net.cpp:586] loss -> loss
I1226 13:44:07.052050 91110 layer_factory.hpp:114] Creating layer loss
I1226 13:44:07.089336 91110 net.cpp:228] Setting up loss
I1226 13:44:07.089454 91110 net.cpp:235] Top shape: (1)
I1226 13:44:07.089617 91110 net.cpp:238]     with loss weight 1
I1226 13:44:07.089771 91110 net.cpp:243] Memory required for data: 532944392
I1226 13:44:07.089825 91110 net.cpp:305] loss needs backward computation.
I1226 13:44:07.089885 91110 net.cpp:307] accuracy does not need backward computation.
I1226 13:44:07.089937 91110 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 13:44:07.089979 91110 net.cpp:305] fc8 needs backward computation.
I1226 13:44:07.090020 91110 net.cpp:305] drop7 needs backward computation.
I1226 13:44:07.090051 91110 net.cpp:305] relu7 needs backward computation.
I1226 13:44:07.090109 91110 net.cpp:305] fc7 needs backward computation.
I1226 13:44:07.090142 91110 net.cpp:305] drop6 needs backward computation.
I1226 13:44:07.090173 91110 net.cpp:305] relu6 needs backward computation.
I1226 13:44:07.090203 91110 net.cpp:305] fc6 needs backward computation.
I1226 13:44:07.090234 91110 net.cpp:305] pool5 needs backward computation.
I1226 13:44:07.090265 91110 net.cpp:305] relu5 needs backward computation.
I1226 13:44:07.090296 91110 net.cpp:305] conv5 needs backward computation.
I1226 13:44:07.090332 91110 net.cpp:305] relu4 needs backward computation.
I1226 13:44:07.090363 91110 net.cpp:305] conv4 needs backward computation.
I1226 13:44:07.090399 91110 net.cpp:305] relu3 needs backward computation.
I1226 13:44:07.090430 91110 net.cpp:305] conv3 needs backward computation.
I1226 13:44:07.090471 91110 net.cpp:305] pool2 needs backward computation.
I1226 13:44:07.090515 91110 net.cpp:305] norm2 needs backward computation.
I1226 13:44:07.090554 91110 net.cpp:305] relu2 needs backward computation.
I1226 13:44:07.090591 91110 net.cpp:305] conv2 needs backward computation.
I1226 13:44:07.090633 91110 net.cpp:305] pool1 needs backward computation.
I1226 13:44:07.090688 91110 net.cpp:305] norm1 needs backward computation.
I1226 13:44:07.090730 91110 net.cpp:305] relu1 needs backward computation.
I1226 13:44:07.090770 91110 net.cpp:305] conv1 needs backward computation.
I1226 13:44:07.090811 91110 net.cpp:307] label_data_1_split does not need backward computation.
I1226 13:44:07.090855 91110 net.cpp:307] data does not need backward computation.
I1226 13:44:07.090885 91110 net.cpp:349] This network produces output accuracy
I1226 13:44:07.090924 91110 net.cpp:349] This network produces output loss
I1226 13:44:07.091032 91110 net.cpp:363] Network initialization done.
I1226 13:44:07.091563 91110 solver.cpp:107] Solver scaffolding done.
I1226 13:44:07.091812 91110 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 13:44:07.159045 91279 net.cpp:228] Setting up fc8
I1226 13:44:07.159158 91279 net.cpp:235] Top shape: 64 1000 (64000)
I1226 13:44:07.159186 91279 net.cpp:243] Memory required for data: 532432384
I1226 13:44:07.159272 91279 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 13:44:07.159353 91279 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 13:44:07.159392 91279 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 13:44:07.159440 91279 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 13:44:07.159497 91279 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 13:44:07.159582 91279 net.cpp:228] Setting up fc8_fc8_0_split
I1226 13:44:07.159631 91279 net.cpp:235] Top shape: 64 1000 (64000)
I1226 13:44:07.159660 91279 net.cpp:235] Top shape: 64 1000 (64000)
I1226 13:44:07.159682 91279 net.cpp:243] Memory required for data: 532944384
I1226 13:44:07.159711 91279 layer_factory.hpp:114] Creating layer accuracy
I1226 13:44:07.159770 91279 net.cpp:178] Creating Layer accuracy
I1226 13:44:07.159801 91279 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 13:44:07.159831 91279 net.cpp:612] accuracy <- label_data_1_split_0
I1226 13:44:07.159873 91279 net.cpp:586] accuracy -> accuracy
I1226 13:44:07.159914 91279 net.cpp:228] Setting up accuracy
I1226 13:44:07.159948 91279 net.cpp:235] Top shape: (1)
I1226 13:44:07.159970 91279 net.cpp:243] Memory required for data: 532944388
I1226 13:44:07.160004 91279 layer_factory.hpp:114] Creating layer loss
I1226 13:44:07.160048 91279 net.cpp:178] Creating Layer loss
I1226 13:44:07.160080 91279 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 13:44:07.160109 91279 net.cpp:612] loss <- label_data_1_split_1
I1226 13:44:07.160142 91279 net.cpp:586] loss -> loss
I1226 13:44:07.160200 91279 layer_factory.hpp:114] Creating layer loss
I1226 13:44:07.170029 98538 net.cpp:228] Setting up fc8
I1226 13:44:07.170181 98538 net.cpp:235] Top shape: 64 1000 (64000)
I1226 13:44:07.170210 98538 net.cpp:243] Memory required for data: 532432384
I1226 13:44:07.170269 98538 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 13:44:07.170358 98538 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 13:44:07.170405 98538 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 13:44:07.170464 98538 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 13:44:07.170516 98538 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 13:44:07.170604 98538 net.cpp:228] Setting up fc8_fc8_0_split
I1226 13:44:07.170650 98538 net.cpp:235] Top shape: 64 1000 (64000)
I1226 13:44:07.170681 98538 net.cpp:235] Top shape: 64 1000 (64000)
I1226 13:44:07.170709 98538 net.cpp:243] Memory required for data: 532944384
I1226 13:44:07.170738 98538 layer_factory.hpp:114] Creating layer accuracy
I1226 13:44:07.170800 98538 net.cpp:178] Creating Layer accuracy
I1226 13:44:07.170831 98538 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 13:44:07.170871 98538 net.cpp:612] accuracy <- label_data_1_split_0
I1226 13:44:07.170909 98538 net.cpp:586] accuracy -> accuracy
I1226 13:44:07.170953 98538 net.cpp:228] Setting up accuracy
I1226 13:44:07.170990 98538 net.cpp:235] Top shape: (1)
I1226 13:44:07.171018 98538 net.cpp:243] Memory required for data: 532944388
I1226 13:44:07.171046 98538 layer_factory.hpp:114] Creating layer loss
I1226 13:44:07.171114 98538 net.cpp:178] Creating Layer loss
I1226 13:44:07.171147 98538 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 13:44:07.171176 98538 net.cpp:612] loss <- label_data_1_split_1
I1226 13:44:07.171217 98538 net.cpp:586] loss -> loss
I1226 13:44:07.171277 98538 layer_factory.hpp:114] Creating layer loss
I1226 13:44:07.186250 91279 net.cpp:228] Setting up loss
I1226 13:44:07.186365 91279 net.cpp:235] Top shape: (1)
I1226 13:44:07.186527 91279 net.cpp:238]     with loss weight 1
I1226 13:44:07.186681 91279 net.cpp:243] Memory required for data: 532944392
I1226 13:44:07.186729 91279 net.cpp:305] loss needs backward computation.
I1226 13:44:07.186770 91279 net.cpp:307] accuracy does not need backward computation.
I1226 13:44:07.186807 91279 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 13:44:07.186847 91279 net.cpp:305] fc8 needs backward computation.
I1226 13:44:07.186880 91279 net.cpp:305] drop7 needs backward computation.
I1226 13:44:07.186909 91279 net.cpp:305] relu7 needs backward computation.
I1226 13:44:07.186945 91279 net.cpp:305] fc7 needs backward computation.
I1226 13:44:07.186975 91279 net.cpp:305] drop6 needs backward computation.
I1226 13:44:07.187005 91279 net.cpp:305] relu6 needs backward computation.
I1226 13:44:07.187037 91279 net.cpp:305] fc6 needs backward computation.
I1226 13:44:07.187079 91279 net.cpp:305] pool5 needs backward computation.
I1226 13:44:07.187120 91279 net.cpp:305] relu5 needs backward computation.
I1226 13:44:07.187157 91279 net.cpp:305] conv5 needs backward computation.
I1226 13:44:07.187188 91279 net.cpp:305] relu4 needs backward computation.
I1226 13:44:07.187252 91279 net.cpp:305] conv4 needs backward computation.
I1226 13:44:07.187291 91279 net.cpp:305] relu3 needs backward computation.
I1226 13:44:07.187327 91279 net.cpp:305] conv3 needs backward computation.
I1226 13:44:07.187360 91279 net.cpp:305] pool2 needs backward computation.
I1226 13:44:07.187391 91279 net.cpp:305] norm2 needs backward computation.
I1226 13:44:07.187422 91279 net.cpp:305] relu2 needs backward computation.
I1226 13:44:07.187463 91279 net.cpp:305] conv2 needs backward computation.
I1226 13:44:07.187492 91279 net.cpp:305] pool1 needs backward computation.
I1226 13:44:07.187528 91279 net.cpp:305] norm1 needs backward computation.
I1226 13:44:07.187568 91279 net.cpp:305] relu1 needs backward computation.
I1226 13:44:07.187597 91279 net.cpp:305] conv1 needs backward computation.
I1226 13:44:07.187630 91279 net.cpp:307] label_data_1_split does not need backward computation.
I1226 13:44:07.187669 91279 net.cpp:307] data does not need backward computation.
I1226 13:44:07.187705 91279 net.cpp:349] This network produces output accuracy
I1226 13:44:07.187739 91279 net.cpp:349] This network produces output loss
I1226 13:44:07.187842 91279 net.cpp:363] Network initialization done.
I1226 13:44:07.188335 91279 solver.cpp:107] Solver scaffolding done.
I1226 13:44:07.188560 91279 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 13:44:07.203740 98538 net.cpp:228] Setting up loss
I1226 13:44:07.203855 98538 net.cpp:235] Top shape: (1)
I1226 13:44:07.204025 98538 net.cpp:238]     with loss weight 1
I1226 13:44:07.204190 98538 net.cpp:243] Memory required for data: 532944392
I1226 13:44:07.204241 98538 net.cpp:305] loss needs backward computation.
I1226 13:44:07.204284 98538 net.cpp:307] accuracy does not need backward computation.
I1226 13:44:07.204319 98538 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 13:44:07.204365 98538 net.cpp:305] fc8 needs backward computation.
I1226 13:44:07.204397 98538 net.cpp:305] drop7 needs backward computation.
I1226 13:44:07.204427 98538 net.cpp:305] relu7 needs backward computation.
I1226 13:44:07.204455 98538 net.cpp:305] fc7 needs backward computation.
I1226 13:44:07.204485 98538 net.cpp:305] drop6 needs backward computation.
I1226 13:44:07.204514 98538 net.cpp:305] relu6 needs backward computation.
I1226 13:44:07.204551 98538 net.cpp:305] fc6 needs backward computation.
I1226 13:44:07.204582 98538 net.cpp:305] pool5 needs backward computation.
I1226 13:44:07.204618 98538 net.cpp:305] relu5 needs backward computation.
I1226 13:44:07.204648 98538 net.cpp:305] conv5 needs backward computation.
I1226 13:44:07.204684 98538 net.cpp:305] relu4 needs backward computation.
I1226 13:44:07.204715 98538 net.cpp:305] conv4 needs backward computation.
I1226 13:44:07.204751 98538 net.cpp:305] relu3 needs backward computation.
I1226 13:44:07.204779 98538 net.cpp:305] conv3 needs backward computation.
I1226 13:44:07.204810 98538 net.cpp:305] pool2 needs backward computation.
I1226 13:44:07.204852 98538 net.cpp:305] norm2 needs backward computation.
I1226 13:44:07.204888 98538 net.cpp:305] relu2 needs backward computation.
I1226 13:44:07.204926 98538 net.cpp:305] conv2 needs backward computation.
I1226 13:44:07.204962 98538 net.cpp:305] pool1 needs backward computation.
I1226 13:44:07.204993 98538 net.cpp:305] norm1 needs backward computation.
I1226 13:44:07.205029 98538 net.cpp:305] relu1 needs backward computation.
I1226 13:44:07.205083 98538 net.cpp:305] conv1 needs backward computation.
I1226 13:44:07.205118 98538 net.cpp:307] label_data_1_split does not need backward computation.
I1226 13:44:07.205153 98538 net.cpp:307] data does not need backward computation.
I1226 13:44:07.205183 98538 net.cpp:349] This network produces output accuracy
I1226 13:44:07.205216 98538 net.cpp:349] This network produces output loss
I1226 13:44:07.205338 98538 net.cpp:363] Network initialization done.
I1226 13:44:07.205783 98538 solver.cpp:107] Solver scaffolding done.
I1226 13:44:07.205998 98538 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 13:44:07.349498 85984 net.cpp:228] Setting up fc8
I1226 13:44:07.349622 85984 net.cpp:235] Top shape: 64 1000 (64000)
I1226 13:44:07.349686 85984 net.cpp:243] Memory required for data: 532432384
I1226 13:44:07.349786 85984 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 13:44:07.349897 85984 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 13:44:07.349959 85984 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 13:44:07.350033 85984 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 13:44:07.350100 85984 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 13:44:07.350236 85984 net.cpp:228] Setting up fc8_fc8_0_split
I1226 13:44:07.350318 85984 net.cpp:235] Top shape: 64 1000 (64000)
I1226 13:44:07.350370 85984 net.cpp:235] Top shape: 64 1000 (64000)
I1226 13:44:07.350404 85984 net.cpp:243] Memory required for data: 532944384
I1226 13:44:07.350445 85984 layer_factory.hpp:114] Creating layer accuracy
I1226 13:44:07.350538 85984 net.cpp:178] Creating Layer accuracy
I1226 13:44:07.350595 85984 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 13:44:07.350642 85984 net.cpp:612] accuracy <- label_data_1_split_0
I1226 13:44:07.350733 85984 net.cpp:586] accuracy -> accuracy
I1226 13:44:07.350819 85984 net.cpp:228] Setting up accuracy
I1226 13:44:07.350883 85984 net.cpp:235] Top shape: (1)
I1226 13:44:07.350925 85984 net.cpp:243] Memory required for data: 532944388
I1226 13:44:07.350970 85984 layer_factory.hpp:114] Creating layer loss
I1226 13:44:07.351045 85984 net.cpp:178] Creating Layer loss
I1226 13:44:07.351097 85984 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 13:44:07.351142 85984 net.cpp:612] loss <- label_data_1_split_1
I1226 13:44:07.351192 85984 net.cpp:586] loss -> loss
I1226 13:44:07.351289 85984 layer_factory.hpp:114] Creating layer loss
I1226 13:44:07.380218 85984 net.cpp:228] Setting up loss
I1226 13:44:07.380332 85984 net.cpp:235] Top shape: (1)
I1226 13:44:07.380497 85984 net.cpp:238]     with loss weight 1
I1226 13:44:07.380652 85984 net.cpp:243] Memory required for data: 532944392
I1226 13:44:07.380729 85984 net.cpp:305] loss needs backward computation.
I1226 13:44:07.380770 85984 net.cpp:307] accuracy does not need backward computation.
I1226 13:44:07.380815 85984 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 13:44:07.380848 85984 net.cpp:305] fc8 needs backward computation.
I1226 13:44:07.380879 85984 net.cpp:305] drop7 needs backward computation.
I1226 13:44:07.380910 85984 net.cpp:305] relu7 needs backward computation.
I1226 13:44:07.380947 85984 net.cpp:305] fc7 needs backward computation.
I1226 13:44:07.380977 85984 net.cpp:305] drop6 needs backward computation.
I1226 13:44:07.381007 85984 net.cpp:305] relu6 needs backward computation.
I1226 13:44:07.381043 85984 net.cpp:305] fc6 needs backward computation.
I1226 13:44:07.381073 85984 net.cpp:305] pool5 needs backward computation.
I1226 13:44:07.381111 85984 net.cpp:305] relu5 needs backward computation.
I1226 13:44:07.381146 85984 net.cpp:305] conv5 needs backward computation.
I1226 13:44:07.381183 85984 net.cpp:305] relu4 needs backward computation.
I1226 13:44:07.381222 85984 net.cpp:305] conv4 needs backward computation.
I1226 13:44:07.381253 85984 net.cpp:305] relu3 needs backward computation.
I1226 13:44:07.381289 85984 net.cpp:305] conv3 needs backward computation.
I1226 13:44:07.381322 85984 net.cpp:305] pool2 needs backward computation.
I1226 13:44:07.381366 85984 net.cpp:305] norm2 needs backward computation.
I1226 13:44:07.381397 85984 net.cpp:305] relu2 needs backward computation.
I1226 13:44:07.381428 85984 net.cpp:305] conv2 needs backward computation.
I1226 13:44:07.381464 85984 net.cpp:305] pool1 needs backward computation.
I1226 13:44:07.381495 85984 net.cpp:305] norm1 needs backward computation.
I1226 13:44:07.381531 85984 net.cpp:305] relu1 needs backward computation.
I1226 13:44:07.381561 85984 net.cpp:305] conv1 needs backward computation.
I1226 13:44:07.381623 85984 net.cpp:307] label_data_1_split does not need backward computation.
I1226 13:44:07.381678 85984 net.cpp:307] data does not need backward computation.
I1226 13:44:07.381716 85984 net.cpp:349] This network produces output accuracy
I1226 13:44:07.381772 85984 net.cpp:349] This network produces output loss
I1226 13:44:07.381878 85984 net.cpp:363] Network initialization done.
I1226 13:44:07.382480 85984 solver.cpp:107] Solver scaffolding done.
I1226 13:44:07.382753 85984 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 13:44:09.929697 85406 caffe.cpp:376] Configuring multinode setup
I1226 13:44:09.933419 85406 caffe.cpp:386] Starting parameter server in mpi environment
I1226 13:44:09.995589 92417 caffe.cpp:376] Configuring multinode setup
I1226 13:44:09.997184 92417 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 13:44:10.197266 94226 caffe.cpp:376] Configuring multinode setup
I1226 13:44:10.198936 94226 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 13:44:10.449872 91279 caffe.cpp:376] Configuring multinode setup
I1226 13:44:10.451339 91279 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 13:44:10.503213 98538 caffe.cpp:376] Configuring multinode setup
I1226 13:44:10.504649 98538 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 13:44:10.615455 85984 caffe.cpp:376] Configuring multinode setup
I1226 13:44:10.616942 85984 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 13:44:11.138417 91110 caffe.cpp:376] Configuring multinode setup
I1226 13:44:11.139839 91110 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 13:44:31.235699 97018 net.cpp:228] Setting up fc6
I1226 13:44:31.236027 97018 net.cpp:235] Top shape: 64 4096 (262144)
I1226 13:44:31.236222 97018 net.cpp:243] Memory required for data: 526933504
I1226 13:44:31.236294 97018 layer_factory.hpp:114] Creating layer relu6
I1226 13:44:31.236424 97018 net.cpp:178] Creating Layer relu6
I1226 13:44:31.236479 97018 net.cpp:612] relu6 <- fc6
I1226 13:44:31.236526 97018 net.cpp:573] relu6 -> fc6 (in-place)
I1226 13:44:31.236680 97018 net.cpp:228] Setting up relu6
I1226 13:44:31.236778 97018 net.cpp:235] Top shape: 64 4096 (262144)
I1226 13:44:31.236809 97018 net.cpp:243] Memory required for data: 527982080
I1226 13:44:31.236872 97018 layer_factory.hpp:114] Creating layer drop6
I1226 13:44:31.236958 97018 net.cpp:178] Creating Layer drop6
I1226 13:44:31.236995 97018 net.cpp:612] drop6 <- fc6
I1226 13:44:31.237064 97018 net.cpp:573] drop6 -> fc6 (in-place)
I1226 13:44:31.237130 97018 net.cpp:228] Setting up drop6
I1226 13:44:31.237175 97018 net.cpp:235] Top shape: 64 4096 (262144)
I1226 13:44:31.237207 97018 net.cpp:243] Memory required for data: 529030656
I1226 13:44:31.237244 97018 layer_factory.hpp:114] Creating layer fc7
I1226 13:44:31.237310 97018 net.cpp:178] Creating Layer fc7
I1226 13:44:31.237344 97018 net.cpp:612] fc7 <- fc6
I1226 13:44:31.237392 97018 net.cpp:586] fc7 -> fc7
I1226 13:44:31.577175 94885 net.cpp:228] Setting up fc6
I1226 13:44:31.577445 94885 net.cpp:235] Top shape: 64 4096 (262144)
I1226 13:44:31.577503 94885 net.cpp:243] Memory required for data: 526933504
I1226 13:44:31.577596 94885 layer_factory.hpp:114] Creating layer relu6
I1226 13:44:31.577883 94885 net.cpp:178] Creating Layer relu6
I1226 13:44:31.577955 94885 net.cpp:612] relu6 <- fc6
I1226 13:44:31.578019 94885 net.cpp:573] relu6 -> fc6 (in-place)
I1226 13:44:31.578150 94885 net.cpp:228] Setting up relu6
I1226 13:44:31.578243 94885 net.cpp:235] Top shape: 64 4096 (262144)
I1226 13:44:31.578286 94885 net.cpp:243] Memory required for data: 527982080
I1226 13:44:31.578333 94885 layer_factory.hpp:114] Creating layer drop6
I1226 13:44:31.578409 94885 net.cpp:178] Creating Layer drop6
I1226 13:44:31.578469 94885 net.cpp:612] drop6 <- fc6
I1226 13:44:31.578553 94885 net.cpp:573] drop6 -> fc6 (in-place)
I1226 13:44:31.578649 94885 net.cpp:228] Setting up drop6
I1226 13:44:31.578729 94885 net.cpp:235] Top shape: 64 4096 (262144)
I1226 13:44:31.578769 94885 net.cpp:243] Memory required for data: 529030656
I1226 13:44:31.578847 94885 layer_factory.hpp:114] Creating layer fc7
I1226 13:44:31.578943 94885 net.cpp:178] Creating Layer fc7
I1226 13:44:31.578996 94885 net.cpp:612] fc7 <- fc6
I1226 13:44:31.579059 94885 net.cpp:586] fc7 -> fc7
I1226 13:44:44.312003 97018 net.cpp:228] Setting up fc7
I1226 13:44:44.312121 97018 net.cpp:235] Top shape: 64 4096 (262144)
I1226 13:44:44.312156 97018 net.cpp:243] Memory required for data: 530079232
I1226 13:44:44.312222 97018 layer_factory.hpp:114] Creating layer relu7
I1226 13:44:44.312399 97018 net.cpp:178] Creating Layer relu7
I1226 13:44:44.312451 97018 net.cpp:612] relu7 <- fc7
I1226 13:44:44.312496 97018 net.cpp:573] relu7 -> fc7 (in-place)
I1226 13:44:44.312595 97018 net.cpp:228] Setting up relu7
I1226 13:44:44.312650 97018 net.cpp:235] Top shape: 64 4096 (262144)
I1226 13:44:44.312678 97018 net.cpp:243] Memory required for data: 531127808
I1226 13:44:44.312708 97018 layer_factory.hpp:114] Creating layer drop7
I1226 13:44:44.312788 97018 net.cpp:178] Creating Layer drop7
I1226 13:44:44.312819 97018 net.cpp:612] drop7 <- fc7
I1226 13:44:44.312860 97018 net.cpp:573] drop7 -> fc7 (in-place)
I1226 13:44:44.312922 97018 net.cpp:228] Setting up drop7
I1226 13:44:44.312963 97018 net.cpp:235] Top shape: 64 4096 (262144)
I1226 13:44:44.312988 97018 net.cpp:243] Memory required for data: 532176384
I1226 13:44:44.313015 97018 layer_factory.hpp:114] Creating layer fc8
I1226 13:44:44.313091 97018 net.cpp:178] Creating Layer fc8
I1226 13:44:44.313127 97018 net.cpp:612] fc8 <- fc7
I1226 13:44:44.313187 97018 net.cpp:586] fc8 -> fc8
I1226 13:44:44.641553 94885 net.cpp:228] Setting up fc7
I1226 13:44:44.641705 94885 net.cpp:235] Top shape: 64 4096 (262144)
I1226 13:44:44.641752 94885 net.cpp:243] Memory required for data: 530079232
I1226 13:44:44.641911 94885 layer_factory.hpp:114] Creating layer relu7
I1226 13:44:44.642227 94885 net.cpp:178] Creating Layer relu7
I1226 13:44:44.642462 94885 net.cpp:612] relu7 <- fc7
I1226 13:44:44.642534 94885 net.cpp:573] relu7 -> fc7 (in-place)
I1226 13:44:44.642709 94885 net.cpp:228] Setting up relu7
I1226 13:44:44.642977 94885 net.cpp:235] Top shape: 64 4096 (262144)
I1226 13:44:44.643144 94885 net.cpp:243] Memory required for data: 531127808
I1226 13:44:44.643234 94885 layer_factory.hpp:114] Creating layer drop7
I1226 13:44:44.643462 94885 net.cpp:178] Creating Layer drop7
I1226 13:44:44.643509 94885 net.cpp:612] drop7 <- fc7
I1226 13:44:44.643730 94885 net.cpp:573] drop7 -> fc7 (in-place)
I1226 13:44:44.643803 94885 net.cpp:228] Setting up drop7
I1226 13:44:44.643893 94885 net.cpp:235] Top shape: 64 4096 (262144)
I1226 13:44:44.643929 94885 net.cpp:243] Memory required for data: 532176384
I1226 13:44:44.643970 94885 layer_factory.hpp:114] Creating layer fc8
I1226 13:44:44.644081 94885 net.cpp:178] Creating Layer fc8
I1226 13:44:44.644130 94885 net.cpp:612] fc8 <- fc7
I1226 13:44:44.644187 94885 net.cpp:586] fc8 -> fc8
I1226 13:44:47.507786 97018 net.cpp:228] Setting up fc8
I1226 13:44:47.507906 97018 net.cpp:235] Top shape: 64 1000 (64000)
I1226 13:44:47.507943 97018 net.cpp:243] Memory required for data: 532432384
I1226 13:44:47.508007 97018 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 13:44:47.508090 97018 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 13:44:47.508138 97018 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 13:44:47.508213 97018 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 13:44:47.508275 97018 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 13:44:47.508388 97018 net.cpp:228] Setting up fc8_fc8_0_split
I1226 13:44:47.508450 97018 net.cpp:235] Top shape: 64 1000 (64000)
I1226 13:44:47.508486 97018 net.cpp:235] Top shape: 64 1000 (64000)
I1226 13:44:47.508510 97018 net.cpp:243] Memory required for data: 532944384
I1226 13:44:47.508541 97018 layer_factory.hpp:114] Creating layer accuracy
I1226 13:44:47.508606 97018 net.cpp:178] Creating Layer accuracy
I1226 13:44:47.508643 97018 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 13:44:47.508677 97018 net.cpp:612] accuracy <- label_data_1_split_0
I1226 13:44:47.508755 97018 net.cpp:586] accuracy -> accuracy
I1226 13:44:47.508828 97018 net.cpp:228] Setting up accuracy
I1226 13:44:47.508872 97018 net.cpp:235] Top shape: (1)
I1226 13:44:47.508905 97018 net.cpp:243] Memory required for data: 532944388
I1226 13:44:47.508945 97018 layer_factory.hpp:114] Creating layer loss
I1226 13:44:47.509109 97018 net.cpp:178] Creating Layer loss
I1226 13:44:47.509158 97018 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 13:44:47.509202 97018 net.cpp:612] loss <- label_data_1_split_1
I1226 13:44:47.509239 97018 net.cpp:586] loss -> loss
I1226 13:44:47.509306 97018 layer_factory.hpp:114] Creating layer loss
I1226 13:44:47.541115 97018 net.cpp:228] Setting up loss
I1226 13:44:47.541275 97018 net.cpp:235] Top shape: (1)
I1226 13:44:47.541333 97018 net.cpp:238]     with loss weight 1
I1226 13:44:47.541558 97018 net.cpp:243] Memory required for data: 532944392
I1226 13:44:47.541611 97018 net.cpp:305] loss needs backward computation.
I1226 13:44:47.541659 97018 net.cpp:307] accuracy does not need backward computation.
I1226 13:44:47.541694 97018 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 13:44:47.541762 97018 net.cpp:305] fc8 needs backward computation.
I1226 13:44:47.541800 97018 net.cpp:305] drop7 needs backward computation.
I1226 13:44:47.541828 97018 net.cpp:305] relu7 needs backward computation.
I1226 13:44:47.541857 97018 net.cpp:305] fc7 needs backward computation.
I1226 13:44:47.541887 97018 net.cpp:305] drop6 needs backward computation.
I1226 13:44:47.541914 97018 net.cpp:305] relu6 needs backward computation.
I1226 13:44:47.541941 97018 net.cpp:305] fc6 needs backward computation.
I1226 13:44:47.541971 97018 net.cpp:305] pool5 needs backward computation.
I1226 13:44:47.542001 97018 net.cpp:305] relu5 needs backward computation.
I1226 13:44:47.542031 97018 net.cpp:305] conv5 needs backward computation.
I1226 13:44:47.542062 97018 net.cpp:305] relu4 needs backward computation.
I1226 13:44:47.542091 97018 net.cpp:305] conv4 needs backward computation.
I1226 13:44:47.542121 97018 net.cpp:305] relu3 needs backward computation.
I1226 13:44:47.542155 97018 net.cpp:305] conv3 needs backward computation.
I1226 13:44:47.542187 97018 net.cpp:305] pool2 needs backward computation.
I1226 13:44:47.542223 97018 net.cpp:305] norm2 needs backward computation.
I1226 13:44:47.542423 97018 net.cpp:305] relu2 needs backward computation.
I1226 13:44:47.542454 97018 net.cpp:305] conv2 needs backward computation.
I1226 13:44:47.542487 97018 net.cpp:305] pool1 needs backward computation.
I1226 13:44:47.542520 97018 net.cpp:305] norm1 needs backward computation.
I1226 13:44:47.542564 97018 net.cpp:305] relu1 needs backward computation.
I1226 13:44:47.542593 97018 net.cpp:305] conv1 needs backward computation.
I1226 13:44:47.542628 97018 net.cpp:307] label_data_1_split does not need backward computation.
I1226 13:44:47.542660 97018 net.cpp:307] data does not need backward computation.
I1226 13:44:47.542696 97018 net.cpp:349] This network produces output accuracy
I1226 13:44:47.542757 97018 net.cpp:349] This network produces output loss
I1226 13:44:47.542858 97018 net.cpp:363] Network initialization done.
I1226 13:44:47.543321 97018 solver.cpp:107] Solver scaffolding done.
I1226 13:44:47.543543 97018 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 13:44:47.832803 94885 net.cpp:228] Setting up fc8
I1226 13:44:47.832959 94885 net.cpp:235] Top shape: 64 1000 (64000)
I1226 13:44:47.833014 94885 net.cpp:243] Memory required for data: 532432384
I1226 13:44:47.833098 94885 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 13:44:47.833343 94885 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 13:44:47.833415 94885 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 13:44:47.833477 94885 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 13:44:47.833544 94885 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 13:44:47.833695 94885 net.cpp:228] Setting up fc8_fc8_0_split
I1226 13:44:47.833767 94885 net.cpp:235] Top shape: 64 1000 (64000)
I1226 13:44:47.833813 94885 net.cpp:235] Top shape: 64 1000 (64000)
I1226 13:44:47.833881 94885 net.cpp:243] Memory required for data: 532944384
I1226 13:44:47.833930 94885 layer_factory.hpp:114] Creating layer accuracy
I1226 13:44:47.834005 94885 net.cpp:178] Creating Layer accuracy
I1226 13:44:47.834059 94885 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 13:44:47.834105 94885 net.cpp:612] accuracy <- label_data_1_split_0
I1226 13:44:47.834182 94885 net.cpp:586] accuracy -> accuracy
I1226 13:44:47.834270 94885 net.cpp:228] Setting up accuracy
I1226 13:44:47.834345 94885 net.cpp:235] Top shape: (1)
I1226 13:44:47.834384 94885 net.cpp:243] Memory required for data: 532944388
I1226 13:44:47.834442 94885 layer_factory.hpp:114] Creating layer loss
I1226 13:44:47.834642 94885 net.cpp:178] Creating Layer loss
I1226 13:44:47.834708 94885 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 13:44:47.834758 94885 net.cpp:612] loss <- label_data_1_split_1
I1226 13:44:47.834847 94885 net.cpp:586] loss -> loss
I1226 13:44:47.834954 94885 layer_factory.hpp:114] Creating layer loss
I1226 13:44:47.870801 94885 net.cpp:228] Setting up loss
I1226 13:44:47.870929 94885 net.cpp:235] Top shape: (1)
I1226 13:44:47.870970 94885 net.cpp:238]     with loss weight 1
I1226 13:44:47.871119 94885 net.cpp:243] Memory required for data: 532944392
I1226 13:44:47.871167 94885 net.cpp:305] loss needs backward computation.
I1226 13:44:47.871206 94885 net.cpp:307] accuracy does not need backward computation.
I1226 13:44:47.871243 94885 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 13:44:47.871286 94885 net.cpp:305] fc8 needs backward computation.
I1226 13:44:47.871318 94885 net.cpp:305] drop7 needs backward computation.
I1226 13:44:47.871348 94885 net.cpp:305] relu7 needs backward computation.
I1226 13:44:47.871377 94885 net.cpp:305] fc7 needs backward computation.
I1226 13:44:47.871414 94885 net.cpp:305] drop6 needs backward computation.
I1226 13:44:47.871476 94885 net.cpp:305] relu6 needs backward computation.
I1226 13:44:47.871510 94885 net.cpp:305] fc6 needs backward computation.
I1226 13:44:47.871551 94885 net.cpp:305] pool5 needs backward computation.
I1226 13:44:47.871582 94885 net.cpp:305] relu5 needs backward computation.
I1226 13:44:47.871610 94885 net.cpp:305] conv5 needs backward computation.
I1226 13:44:47.871641 94885 net.cpp:305] relu4 needs backward computation.
I1226 13:44:47.871707 94885 net.cpp:305] conv4 needs backward computation.
I1226 13:44:47.871738 94885 net.cpp:305] relu3 needs backward computation.
I1226 13:44:47.871769 94885 net.cpp:305] conv3 needs backward computation.
I1226 13:44:47.871801 94885 net.cpp:305] pool2 needs backward computation.
I1226 13:44:47.871892 94885 net.cpp:305] norm2 needs backward computation.
I1226 13:44:47.871925 94885 net.cpp:305] relu2 needs backward computation.
I1226 13:44:47.871954 94885 net.cpp:305] conv2 needs backward computation.
I1226 13:44:47.872011 94885 net.cpp:305] pool1 needs backward computation.
I1226 13:44:47.872051 94885 net.cpp:305] norm1 needs backward computation.
I1226 13:44:47.872081 94885 net.cpp:305] relu1 needs backward computation.
I1226 13:44:47.872112 94885 net.cpp:305] conv1 needs backward computation.
I1226 13:44:47.872155 94885 net.cpp:307] label_data_1_split does not need backward computation.
I1226 13:44:47.872196 94885 net.cpp:307] data does not need backward computation.
I1226 13:44:47.872233 94885 net.cpp:349] This network produces output accuracy
I1226 13:44:47.872268 94885 net.cpp:349] This network produces output loss
I1226 13:44:47.872360 94885 net.cpp:363] Network initialization done.
I1226 13:44:47.872817 94885 solver.cpp:107] Solver scaffolding done.
I1226 13:44:47.873067 94885 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 13:44:52.098259 97018 caffe.cpp:376] Configuring multinode setup
I1226 13:44:52.100148 97018 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 13:44:52.430106 94885 caffe.cpp:376] Configuring multinode setup
I1226 13:44:52.431958 94885 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 13:44:52.420963 97018 SynchronousNode.cpp:675] [7] [proc 7] solving
I1226 13:44:52.426157 85984 SynchronousNode.cpp:675] [5] [proc 5] solving
I1226 13:44:52.415024 94226 SynchronousNode.cpp:675] [0] [proc 0] solving
I1226 13:44:52.432185 94885 SynchronousNode.cpp:675] [4] [proc 4] solving
I1226 13:44:52.425595 92417 SynchronousNode.cpp:675] [1] [proc 1] solving
I1226 13:44:52.431018 98538 SynchronousNode.cpp:675] [3] [proc 3] solving
I1226 13:44:52.412742 91110 SynchronousNode.cpp:675] [6] [proc 6] solving
I1226 13:44:52.421100 97018 solver.cpp:354] Solving AlexNet
I1226 13:44:52.421151 97018 solver.cpp:355] Learning Rate Policy: step
I1226 13:44:52.426435 85984 solver.cpp:354] Solving AlexNet
I1226 13:44:52.426483 85984 solver.cpp:355] Learning Rate Policy: step
I1226 13:44:52.415308 94226 solver.cpp:354] Solving AlexNet
I1226 13:44:52.415362 94226 solver.cpp:355] Learning Rate Policy: step
I1226 13:44:52.432292 94885 solver.cpp:354] Solving AlexNet
I1226 13:44:52.425875 92417 solver.cpp:354] Solving AlexNet
I1226 13:44:52.425921 92417 solver.cpp:355] Learning Rate Policy: step
I1226 13:44:52.431365 98538 solver.cpp:354] Solving AlexNet
I1226 13:44:52.441606 91279 SynchronousNode.cpp:675] [2] [proc 2] solving
I1226 13:44:52.413020 91110 solver.cpp:354] Solving AlexNet
I1226 13:44:52.413115 91110 solver.cpp:355] Learning Rate Policy: step
I1226 13:44:52.432330 94885 solver.cpp:355] Learning Rate Policy: step
I1226 13:44:52.431443 98538 solver.cpp:355] Learning Rate Policy: step
I1226 13:44:52.441922 91279 solver.cpp:354] Solving AlexNet
I1226 13:44:52.441968 91279 solver.cpp:355] Learning Rate Policy: step
I1226 13:44:52.424389 97087 SynchronousNode.cpp:293] [7] Comm thread started 1 0
I1226 13:44:52.439882 94959 SynchronousNode.cpp:293] [4] Comm thread started 0 1
I1226 13:44:52.456655 92490 SynchronousNode.cpp:293] [1] Comm thread started 0 0
I1226 13:44:52.463071 86057 SynchronousNode.cpp:293] [5] Comm thread started 0 0
I1226 13:44:52.468920 98611 SynchronousNode.cpp:293] [3] Comm thread started 1 0
I1226 13:44:52.479665 91351 SynchronousNode.cpp:293] [2] Comm thread started 1 0
I1226 13:44:52.451022 91179 SynchronousNode.cpp:293] [6] Comm thread started 1 0
I1226 13:44:52.454164 94297 SynchronousNode.cpp:293] [0] Comm thread started 0 1
I1226 13:44:52.471364 94959 SynchronousNode.cpp:479] [4] initialized root of cluster with nodes: 9 and the total iter size is: 4
I1226 13:44:52.457430 94297 SynchronousNode.cpp:479] [0] initialized root of cluster with nodes: 9 and the total iter size is: 4
I1226 13:44:52.775092 94226 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 13:44:52.775194 94226 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 13:44:53.164700 94226 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 13:44:53.164770 94226 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 13:44:53.164808 94226 solver.cpp:291] [0] Iteration 1, loss = 3.33347
I1226 13:44:53.164861 94226 solver.cpp:317]     Train net output #0: accuracy = 0.296875
I1226 13:44:53.164911 94226 solver.cpp:317]     Train net output #1: loss = 3.33347 (* 1 = 3.33347 loss)
I1226 13:44:53.624439 92490 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 13:44:53.640393 91351 SynchronousNode.cpp:246] [2] PROFILING END[WaitingToSync]
I1226 13:44:53.624564 92490 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 13:44:53.640483 91351 SynchronousNode.cpp:247] [2] PROFILING BEGIN[Forward]
I1226 13:44:53.641290 98611 SynchronousNode.cpp:246] [3] PROFILING END[WaitingToSync]
I1226 13:44:53.641374 98611 SynchronousNode.cpp:247] [3] PROFILING BEGIN[Forward]
I1226 13:44:53.756263 91279 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 13:44:53.756372 91279 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 13:44:53.746877 98538 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 13:44:53.746960 98538 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 13:44:53.746996 92417 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 13:44:53.747103 92417 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 13:44:53.862576 92417 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 13:44:53.862658 92417 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 13:44:53.862707 92417 solver.cpp:291] [1] Iteration 1, loss = 2.67267
I1226 13:44:53.862785 92417 solver.cpp:317]     Train net output #0: accuracy = 0.359375
I1226 13:44:53.862990 92417 solver.cpp:317]     Train net output #1: loss = 2.67267 (* 1 = 2.67267 loss)
I1226 13:44:53.965070 91279 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 13:44:53.965162 91279 MultiSolver.cpp:109] [2] PROFILING BEGIN[WaitingToSync]
I1226 13:44:53.965252 91279 solver.cpp:291] [2] Iteration 1, loss = 3.3201
I1226 13:44:53.965348 91279 solver.cpp:317]     Train net output #0: accuracy = 0.234375
I1226 13:44:53.965435 91279 solver.cpp:317]     Train net output #1: loss = 3.3201 (* 1 = 3.3201 loss)
I1226 13:44:53.958220 98538 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 13:44:53.958307 98538 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 13:44:53.958356 98538 solver.cpp:291] [3] Iteration 1, loss = 2.95315
I1226 13:44:53.958449 98538 solver.cpp:317]     Train net output #0: accuracy = 0.453125
I1226 13:44:53.958531 98538 solver.cpp:317]     Train net output #1: loss = 2.95315 (* 1 = 2.95315 loss)
I1226 13:44:54.021069 94885 MultiSolver.cpp:92] [4] PROFILING END[Forward]
I1226 13:44:54.021162 94885 MultiSolver.cpp:94] [4] PROFILING BEGIN[Backward]
I1226 13:44:55.270045 94226 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 13:44:55.270133 94226 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 13:44:55.307096 91179 SynchronousNode.cpp:246] [6] PROFILING END[WaitingToSync]
I1226 13:44:55.307171 91179 SynchronousNode.cpp:247] [6] PROFILING BEGIN[Forward]
I1226 13:44:55.315639 97087 SynchronousNode.cpp:246] [7] PROFILING END[WaitingToSync]
I1226 13:44:55.315718 97087 SynchronousNode.cpp:247] [7] PROFILING BEGIN[Forward]
I1226 13:44:55.320566 86057 SynchronousNode.cpp:246] [5] PROFILING END[WaitingToSync]
I1226 13:44:55.320653 86057 SynchronousNode.cpp:247] [5] PROFILING BEGIN[Forward]
I1226 13:44:55.412616 94226 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 13:44:55.412693 94226 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 13:44:55.412804 94226 solver.cpp:291] [0] Iteration 2, loss = 3.6589
I1226 13:44:55.412885 94226 solver.cpp:317]     Train net output #0: accuracy = 0.359375
I1226 13:44:55.412969 94226 solver.cpp:317]     Train net output #1: loss = 3.6589 (* 1 = 3.6589 loss)
I1226 13:44:55.438513 85984 MultiSolver.cpp:92] [5] PROFILING END[Forward]
I1226 13:44:55.438616 85984 MultiSolver.cpp:94] [5] PROFILING BEGIN[Backward]
I1226 13:44:55.529386 91110 MultiSolver.cpp:92] [6] PROFILING END[Forward]
I1226 13:44:55.529494 91110 MultiSolver.cpp:94] [6] PROFILING BEGIN[Backward]
I1226 13:44:55.576504 85984 MultiSolver.cpp:108] [5] PROFILING END[Backward]
I1226 13:44:55.576581 85984 MultiSolver.cpp:109] [5] PROFILING BEGIN[WaitingToSync]
I1226 13:44:55.576622 85984 solver.cpp:291] [5] Iteration 1, loss = 3.15525
I1226 13:44:55.576735 85984 solver.cpp:317]     Train net output #0: accuracy = 0.3125
I1226 13:44:55.576805 85984 solver.cpp:317]     Train net output #1: loss = 3.15525 (* 1 = 3.15525 loss)
I1226 13:44:55.611068 91351 SynchronousNode.cpp:246] [2] PROFILING END[WaitingToSync]
I1226 13:44:55.595150 92490 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 13:44:55.595235 92490 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 13:44:55.611160 91351 SynchronousNode.cpp:247] [2] PROFILING BEGIN[Forward]
I1226 13:44:55.605890 98611 SynchronousNode.cpp:246] [3] PROFILING END[WaitingToSync]
I1226 13:44:55.605970 98611 SynchronousNode.cpp:247] [3] PROFILING BEGIN[Forward]
I1226 13:44:55.607695 92417 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 13:44:55.607796 92417 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 13:44:55.623925 91279 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 13:44:55.624048 91279 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 13:44:55.628444 98538 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 13:44:55.628531 98538 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 13:44:55.696166 92417 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 13:44:55.696252 92417 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 13:44:55.696295 92417 solver.cpp:291] [1] Iteration 2, loss = 3.4125
I1226 13:44:55.696360 92417 solver.cpp:317]     Train net output #0: accuracy = 0.265625
I1226 13:44:55.696426 92417 solver.cpp:317]     Train net output #1: loss = 3.4125 (* 1 = 3.4125 loss)
I1226 13:44:55.765242 91279 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 13:44:55.765331 91279 MultiSolver.cpp:109] [2] PROFILING BEGIN[WaitingToSync]
I1226 13:44:55.765380 91279 solver.cpp:291] [2] Iteration 2, loss = 3.34469
I1226 13:44:55.765453 91279 solver.cpp:317]     Train net output #0: accuracy = 0.34375
I1226 13:44:55.765530 91279 solver.cpp:317]     Train net output #1: loss = 3.34469 (* 1 = 3.34469 loss)
I1226 13:44:55.874508 98538 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 13:44:55.874594 98538 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 13:44:55.874644 98538 solver.cpp:291] [3] Iteration 2, loss = 2.55652
I1226 13:44:55.874721 98538 solver.cpp:317]     Train net output #0: accuracy = 0.421875
I1226 13:44:55.874796 98538 solver.cpp:317]     Train net output #1: loss = 2.55652 (* 1 = 2.55652 loss)
I1226 13:44:55.929945 94885 MultiSolver.cpp:108] [4] PROFILING END[Backward]
I1226 13:44:55.930037 94885 MultiSolver.cpp:109] [4] PROFILING BEGIN[WaitingToSync]
I1226 13:44:55.930088 94885 solver.cpp:291] [4] Iteration 1, loss = 3.11224
I1226 13:44:55.930164 94885 solver.cpp:317]     Train net output #0: accuracy = 0.28125
I1226 13:44:55.930335 94885 solver.cpp:317]     Train net output #1: loss = 3.11224 (* 1 = 3.11224 loss)
I1226 13:44:55.920519 91110 MultiSolver.cpp:108] [6] PROFILING END[Backward]
I1226 13:44:55.920603 91110 MultiSolver.cpp:109] [6] PROFILING BEGIN[WaitingToSync]
I1226 13:44:55.920650 91110 solver.cpp:291] [6] Iteration 1, loss = 3.54186
I1226 13:44:55.920739 91110 solver.cpp:317]     Train net output #0: accuracy = 0.28125
I1226 13:44:55.920815 91110 solver.cpp:317]     Train net output #1: loss = 3.54186 (* 1 = 3.54186 loss)
I1226 13:44:55.945025 97018 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 13:44:55.945140 97018 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 13:44:56.752429 94226 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 13:44:56.752552 94226 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 13:44:56.845768 94226 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 13:44:56.845850 94226 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 13:44:56.845918 94226 solver.cpp:291] [0] Iteration 3, loss = 3.47351
I1226 13:44:56.845989 94226 solver.cpp:317]     Train net output #0: accuracy = 0.234375
I1226 13:44:56.846070 94226 solver.cpp:317]     Train net output #1: loss = 3.47351 (* 1 = 3.47351 loss)
I1226 13:44:56.939384 97018 MultiSolver.cpp:108] [7] PROFILING END[Backward]
I1226 13:44:56.939474 97018 MultiSolver.cpp:109] [7] PROFILING BEGIN[WaitingToSync]
I1226 13:44:56.939532 97018 solver.cpp:291] [7] Iteration 1, loss = 3.00562
I1226 13:44:56.939716 97018 solver.cpp:317]     Train net output #0: accuracy = 0.40625
I1226 13:44:56.939817 97018 solver.cpp:317]     Train net output #1: loss = 3.00562 (* 1 = 3.00562 loss)
I1226 13:44:57.248781 92490 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 13:44:57.264780 91351 SynchronousNode.cpp:246] [2] PROFILING END[WaitingToSync]
I1226 13:44:57.248869 92490 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 13:44:57.264878 91351 SynchronousNode.cpp:247] [2] PROFILING BEGIN[Forward]
I1226 13:44:57.261358 92417 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 13:44:57.277336 91279 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 13:44:57.261503 92417 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 13:44:57.277446 91279 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 13:44:57.267470 98611 SynchronousNode.cpp:246] [3] PROFILING END[WaitingToSync]
I1226 13:44:57.267556 98611 SynchronousNode.cpp:247] [3] PROFILING BEGIN[Forward]
I1226 13:44:57.280191 98538 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 13:44:57.280303 98538 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 13:44:57.349613 92417 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 13:44:57.349742 92417 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 13:44:57.349798 92417 solver.cpp:291] [1] Iteration 3, loss = 3.70071
I1226 13:44:57.349869 92417 solver.cpp:317]     Train net output #0: accuracy = 0.234375
I1226 13:44:57.349951 92417 solver.cpp:317]     Train net output #1: loss = 3.70071 (* 1 = 3.70071 loss)
I1226 13:44:57.421515 91279 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 13:44:57.421600 91279 MultiSolver.cpp:109] [2] PROFILING BEGIN[WaitingToSync]
I1226 13:44:57.421650 91279 solver.cpp:291] [2] Iteration 3, loss = 2.80661
I1226 13:44:57.421722 91279 solver.cpp:317]     Train net output #0: accuracy = 0.421875
I1226 13:44:57.421797 91279 solver.cpp:317]     Train net output #1: loss = 2.80661 (* 1 = 2.80661 loss)
I1226 13:44:57.423135 98538 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 13:44:57.423224 98538 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 13:44:57.423279 98538 solver.cpp:291] [3] Iteration 3, loss = 2.77806
I1226 13:44:57.423352 98538 solver.cpp:317]     Train net output #0: accuracy = 0.4375
I1226 13:44:57.423427 98538 solver.cpp:317]     Train net output #1: loss = 2.77806 (* 1 = 2.77806 loss)
I1226 13:44:58.426829 94226 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 13:44:58.426941 94226 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 13:44:58.517278 94226 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 13:44:58.517354 94226 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 13:44:58.517452 94226 solver.cpp:291] [0] Iteration 4, loss = 3.37978
I1226 13:44:58.517521 94226 solver.cpp:317]     Train net output #0: accuracy = 0.265625
I1226 13:44:58.517606 94226 solver.cpp:317]     Train net output #1: loss = 3.37978 (* 1 = 3.37978 loss)
I1226 13:44:58.892693 92490 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 13:44:58.908579 91351 SynchronousNode.cpp:246] [2] PROFILING END[WaitingToSync]
I1226 13:44:58.908661 91351 SynchronousNode.cpp:247] [2] PROFILING BEGIN[Forward]
I1226 13:44:58.892779 92490 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 13:44:58.905273 92417 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 13:44:58.905375 92417 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 13:44:58.921385 91279 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 13:44:58.921494 91279 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 13:44:58.916410 98611 SynchronousNode.cpp:246] [3] PROFILING END[WaitingToSync]
I1226 13:44:58.916492 98611 SynchronousNode.cpp:247] [3] PROFILING BEGIN[Forward]
I1226 13:44:58.930284 98538 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 13:44:58.930398 98538 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 13:44:58.999536 92417 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 13:44:58.999627 92417 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 13:44:58.999677 92417 solver.cpp:291] [1] Iteration 4, loss = 3.54607
I1226 13:44:58.999752 92417 solver.cpp:317]     Train net output #0: accuracy = 0.234375
I1226 13:44:58.999826 92417 solver.cpp:317]     Train net output #1: loss = 3.54607 (* 1 = 3.54607 loss)
I1226 13:44:59.080149 91279 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 13:44:59.080281 91279 MultiSolver.cpp:109] [2] PROFILING BEGIN[WaitingToSync]
I1226 13:44:59.080343 91279 solver.cpp:291] [2] Iteration 4, loss = 3.5632
I1226 13:44:59.080413 91279 solver.cpp:317]     Train net output #0: accuracy = 0.25
I1226 13:44:59.080498 91279 solver.cpp:317]     Train net output #1: loss = 3.5632 (* 1 = 3.5632 loss)
I1226 13:44:59.076207 98538 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 13:44:59.076294 98538 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 13:44:59.076342 98538 solver.cpp:291] [3] Iteration 4, loss = 3.54603
I1226 13:44:59.076418 98538 solver.cpp:317]     Train net output #0: accuracy = 0.34375
I1226 13:44:59.076493 98538 solver.cpp:317]     Train net output #1: loss = 3.54603 (* 1 = 3.54603 loss)
I1226 13:45:00.073554 94226 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 13:45:00.073663 94226 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 13:45:00.164832 94226 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 13:45:00.164917 94226 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 13:45:00.164955 94226 solver.cpp:291] [0] Iteration 5, loss = 3.37725
I1226 13:45:00.165025 94226 solver.cpp:317]     Train net output #0: accuracy = 0.265625
I1226 13:45:00.165107 94226 solver.cpp:317]     Train net output #1: loss = 3.37725 (* 1 = 3.37725 loss)
I1226 13:45:00.544258 92490 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 13:45:00.560187 91351 SynchronousNode.cpp:246] [2] PROFILING END[WaitingToSync]
I1226 13:45:00.544348 92490 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 13:45:00.560304 91351 SynchronousNode.cpp:247] [2] PROFILING BEGIN[Forward]
I1226 13:45:00.556963 92417 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 13:45:00.557070 92417 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 13:45:00.573920 91279 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 13:45:00.574033 91279 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 13:45:00.576400 98611 SynchronousNode.cpp:246] [3] PROFILING END[WaitingToSync]
I1226 13:45:00.576485 98611 SynchronousNode.cpp:247] [3] PROFILING BEGIN[Forward]
I1226 13:45:00.589421 98538 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 13:45:00.589537 98538 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 13:45:00.647891 92417 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 13:45:00.647971 92417 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 13:45:00.648012 92417 solver.cpp:291] [1] Iteration 5, loss = 3.1951
I1226 13:45:00.648077 92417 solver.cpp:317]     Train net output #0: accuracy = 0.40625
I1226 13:45:00.648144 92417 solver.cpp:317]     Train net output #1: loss = 3.1951 (* 1 = 3.1951 loss)
I1226 13:45:00.717702 91279 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 13:45:00.717799 91279 MultiSolver.cpp:109] [2] PROFILING BEGIN[WaitingToSync]
I1226 13:45:00.717851 91279 solver.cpp:291] [2] Iteration 5, loss = 3.05489
I1226 13:45:00.717923 91279 solver.cpp:317]     Train net output #0: accuracy = 0.421875
I1226 13:45:00.718000 91279 solver.cpp:317]     Train net output #1: loss = 3.05489 (* 1 = 3.05489 loss)
I1226 13:45:00.733109 98538 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 13:45:00.733201 98538 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 13:45:00.733252 98538 solver.cpp:291] [3] Iteration 5, loss = 3.07583
I1226 13:45:00.733324 98538 solver.cpp:317]     Train net output #0: accuracy = 0.390625
I1226 13:45:00.733398 98538 solver.cpp:317]     Train net output #1: loss = 3.07583 (* 1 = 3.07583 loss)
I1226 13:45:01.833422 94226 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 13:45:01.833536 94226 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 13:45:01.922756 94885 MultiSolver.cpp:92] [4] PROFILING END[Forward]
I1226 13:45:01.924337 94885 MultiSolver.cpp:94] [4] PROFILING BEGIN[Backward]
I1226 13:45:01.938700 94226 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 13:45:01.938815 94226 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 13:45:01.938856 94226 solver.cpp:291] [0] Iteration 6, loss = 2.58953
I1226 13:45:01.938935 94226 solver.cpp:317]     Train net output #0: accuracy = 0.328125
I1226 13:45:01.939016 94226 solver.cpp:317]     Train net output #1: loss = 2.58953 (* 1 = 2.58953 loss)
I1226 13:45:02.297220 92490 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 13:45:02.313136 91351 SynchronousNode.cpp:246] [2] PROFILING END[WaitingToSync]
I1226 13:45:02.297305 92490 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 13:45:02.313246 91351 SynchronousNode.cpp:247] [2] PROFILING BEGIN[Forward]
I1226 13:45:02.326449 91279 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 13:45:02.326566 91279 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 13:45:02.312386 92417 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 13:45:02.317911 98611 SynchronousNode.cpp:246] [3] PROFILING END[WaitingToSync]
I1226 13:45:02.312525 92417 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 13:45:02.317999 98611 SynchronousNode.cpp:247] [3] PROFILING BEGIN[Forward]
I1226 13:45:02.330350 98538 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 13:45:02.330463 98538 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 13:45:02.395640 92417 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 13:45:02.395735 92417 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 13:45:02.395787 92417 solver.cpp:291] [1] Iteration 6, loss = 3.51582
I1226 13:45:02.395860 92417 solver.cpp:317]     Train net output #0: accuracy = 0.203125
I1226 13:45:02.395936 92417 solver.cpp:317]     Train net output #1: loss = 3.51582 (* 1 = 3.51582 loss)
I1226 13:45:02.471063 91279 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 13:45:02.471154 91279 MultiSolver.cpp:109] [2] PROFILING BEGIN[WaitingToSync]
I1226 13:45:02.471249 91279 solver.cpp:291] [2] Iteration 6, loss = 3.55966
I1226 13:45:02.471326 91279 solver.cpp:317]     Train net output #0: accuracy = 0.234375
I1226 13:45:02.471416 91279 solver.cpp:317]     Train net output #1: loss = 3.55966 (* 1 = 3.55966 loss)
I1226 13:45:02.472352 98538 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 13:45:02.472473 98538 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 13:45:02.472528 98538 solver.cpp:291] [3] Iteration 6, loss = 2.87021
I1226 13:45:02.472630 98538 solver.cpp:317]     Train net output #0: accuracy = 0.34375
I1226 13:45:02.472839 98538 solver.cpp:317]     Train net output #1: loss = 2.87021 (* 1 = 2.87021 loss)
I1226 13:45:02.746845 86057 SynchronousNode.cpp:246] [5] PROFILING END[WaitingToSync]
I1226 13:45:02.733423 91179 SynchronousNode.cpp:246] [6] PROFILING END[WaitingToSync]
I1226 13:45:02.746932 86057 SynchronousNode.cpp:247] [5] PROFILING BEGIN[Forward]
I1226 13:45:02.733505 91179 SynchronousNode.cpp:247] [6] PROFILING BEGIN[Forward]
I1226 13:45:02.759341 85984 MultiSolver.cpp:92] [5] PROFILING END[Forward]
I1226 13:45:02.759454 85984 MultiSolver.cpp:94] [5] PROFILING BEGIN[Backward]
I1226 13:45:02.746206 91110 MultiSolver.cpp:92] [6] PROFILING END[Forward]
I1226 13:45:02.746318 91110 MultiSolver.cpp:94] [6] PROFILING BEGIN[Backward]
I1226 13:45:02.821565 94885 MultiSolver.cpp:108] [4] PROFILING END[Backward]
I1226 13:45:02.821655 94885 MultiSolver.cpp:109] [4] PROFILING BEGIN[WaitingToSync]
I1226 13:45:02.821698 94885 solver.cpp:291] [4] Iteration 2, loss = 3.85255
I1226 13:45:02.821763 94885 solver.cpp:317]     Train net output #0: accuracy = 0.15625
I1226 13:45:02.821866 94885 solver.cpp:317]     Train net output #1: loss = 3.85255 (* 1 = 3.85255 loss)
I1226 13:45:02.834697 97087 SynchronousNode.cpp:246] [7] PROFILING END[WaitingToSync]
I1226 13:45:02.836163 97087 SynchronousNode.cpp:247] [7] PROFILING BEGIN[Forward]
I1226 13:45:02.848323 85984 MultiSolver.cpp:108] [5] PROFILING END[Backward]
I1226 13:45:02.848405 85984 MultiSolver.cpp:109] [5] PROFILING BEGIN[WaitingToSync]
I1226 13:45:02.848444 85984 solver.cpp:291] [5] Iteration 2, loss = 3.42268
I1226 13:45:02.848508 85984 solver.cpp:317]     Train net output #0: accuracy = 0.3125
I1226 13:45:02.848574 85984 solver.cpp:317]     Train net output #1: loss = 3.42268 (* 1 = 3.42268 loss)
I1226 13:45:02.897811 91110 MultiSolver.cpp:108] [6] PROFILING END[Backward]
I1226 13:45:02.897898 91110 MultiSolver.cpp:109] [6] PROFILING BEGIN[WaitingToSync]
I1226 13:45:02.897948 91110 solver.cpp:291] [6] Iteration 2, loss = 2.87949
I1226 13:45:02.898023 91110 solver.cpp:317]     Train net output #0: accuracy = 0.421875
I1226 13:45:02.898123 91110 solver.cpp:317]     Train net output #1: loss = 2.87949 (* 1 = 2.87949 loss)
I1226 13:45:02.907949 97018 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 13:45:02.908071 97018 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 13:45:03.474364 94226 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 13:45:03.474529 94226 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 13:45:03.568236 94226 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 13:45:03.568341 94226 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 13:45:03.568575 94226 solver.cpp:291] [0] Iteration 7, loss = 3.28283
I1226 13:45:03.568658 94226 solver.cpp:317]     Train net output #0: accuracy = 0.359375
I1226 13:45:03.568737 94226 solver.cpp:317]     Train net output #1: loss = 3.28283 (* 1 = 3.28283 loss)
I1226 13:45:03.820240 97018 MultiSolver.cpp:108] [7] PROFILING END[Backward]
I1226 13:45:03.820333 97018 MultiSolver.cpp:109] [7] PROFILING BEGIN[WaitingToSync]
I1226 13:45:03.820385 97018 solver.cpp:291] [7] Iteration 2, loss = 3.11394
I1226 13:45:03.820456 97018 solver.cpp:317]     Train net output #0: accuracy = 0.328125
I1226 13:45:03.820533 97018 solver.cpp:317]     Train net output #1: loss = 3.11394 (* 1 = 3.11394 loss)
I1226 13:45:03.945240 92490 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 13:45:03.961141 91351 SynchronousNode.cpp:246] [2] PROFILING END[WaitingToSync]
I1226 13:45:03.945332 92490 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 13:45:03.961257 91351 SynchronousNode.cpp:247] [2] PROFILING BEGIN[Forward]
I1226 13:45:03.973817 91279 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 13:45:03.973932 91279 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 13:45:03.959260 92417 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 13:45:03.959424 92417 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 13:45:03.976352 98611 SynchronousNode.cpp:246] [3] PROFILING END[WaitingToSync]
I1226 13:45:03.976434 98611 SynchronousNode.cpp:247] [3] PROFILING BEGIN[Forward]
I1226 13:45:03.988867 98538 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 13:45:03.988983 98538 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 13:45:04.049926 92417 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 13:45:04.050015 92417 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 13:45:04.050056 92417 solver.cpp:291] [1] Iteration 7, loss = 2.90372
I1226 13:45:04.050122 92417 solver.cpp:317]     Train net output #0: accuracy = 0.390625
I1226 13:45:04.050186 92417 solver.cpp:317]     Train net output #1: loss = 2.90372 (* 1 = 2.90372 loss)
I1226 13:45:04.118566 91279 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 13:45:04.118655 91279 MultiSolver.cpp:109] [2] PROFILING BEGIN[WaitingToSync]
I1226 13:45:04.118712 91279 solver.cpp:291] [2] Iteration 7, loss = 2.61556
I1226 13:45:04.118785 91279 solver.cpp:317]     Train net output #0: accuracy = 0.375
I1226 13:45:04.118862 91279 solver.cpp:317]     Train net output #1: loss = 2.61556 (* 1 = 2.61556 loss)
I1226 13:45:04.134872 98538 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 13:45:04.134961 98538 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 13:45:04.135010 98538 solver.cpp:291] [3] Iteration 7, loss = 3.4724
I1226 13:45:04.135118 98538 solver.cpp:317]     Train net output #0: accuracy = 0.34375
I1226 13:45:04.135196 98538 solver.cpp:317]     Train net output #1: loss = 3.4724 (* 1 = 3.4724 loss)
I1226 13:45:05.123430 94226 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 13:45:05.123546 94226 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 13:45:05.213733 94226 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 13:45:05.213811 94226 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 13:45:05.213858 94226 solver.cpp:291] [0] Iteration 8, loss = 3.20431
I1226 13:45:05.213928 94226 solver.cpp:317]     Train net output #0: accuracy = 0.28125
I1226 13:45:05.214012 94226 solver.cpp:317]     Train net output #1: loss = 3.20431 (* 1 = 3.20431 loss)
I1226 13:45:05.599416 92490 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 13:45:05.615353 91351 SynchronousNode.cpp:246] [2] PROFILING END[WaitingToSync]
I1226 13:45:05.615437 91351 SynchronousNode.cpp:247] [2] PROFILING BEGIN[Forward]
I1226 13:45:05.599534 92490 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 13:45:05.612004 92417 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 13:45:05.627984 91279 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 13:45:05.612108 92417 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 13:45:05.628094 91279 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 13:45:05.621605 98611 SynchronousNode.cpp:246] [3] PROFILING END[WaitingToSync]
I1226 13:45:05.621690 98611 SynchronousNode.cpp:247] [3] PROFILING BEGIN[Forward]
I1226 13:45:05.634310 98538 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 13:45:05.634424 98538 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 13:45:05.698665 92417 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 13:45:05.698753 92417 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 13:45:05.698828 92417 solver.cpp:291] [1] Iteration 8, loss = 3.14943
I1226 13:45:05.698901 92417 solver.cpp:317]     Train net output #0: accuracy = 0.34375
I1226 13:45:05.698973 92417 solver.cpp:317]     Train net output #1: loss = 3.14943 (* 1 = 3.14943 loss)
I1226 13:45:05.772635 91279 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 13:45:05.772732 91279 MultiSolver.cpp:109] [2] PROFILING BEGIN[WaitingToSync]
I1226 13:45:05.772785 91279 solver.cpp:291] [2] Iteration 8, loss = 3.62536
I1226 13:45:05.772857 91279 solver.cpp:317]     Train net output #0: accuracy = 0.296875
I1226 13:45:05.772934 91279 solver.cpp:317]     Train net output #1: loss = 3.62536 (* 1 = 3.62536 loss)
I1226 13:45:05.775163 98538 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 13:45:05.775259 98538 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 13:45:05.775310 98538 solver.cpp:291] [3] Iteration 8, loss = 3.36614
I1226 13:45:05.775383 98538 solver.cpp:317]     Train net output #0: accuracy = 0.265625
I1226 13:45:05.775459 98538 solver.cpp:317]     Train net output #1: loss = 3.36614 (* 1 = 3.36614 loss)
I1226 13:45:06.779347 94226 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 13:45:06.779494 94226 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 13:45:06.874297 94226 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 13:45:06.874416 94226 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 13:45:06.874516 94226 solver.cpp:291] [0] Iteration 9, loss = 3.93454
I1226 13:45:06.874599 94226 solver.cpp:317]     Train net output #0: accuracy = 0.203125
I1226 13:45:06.874681 94226 solver.cpp:317]     Train net output #1: loss = 3.93454 (* 1 = 3.93454 loss)
I1226 13:45:07.250542 92490 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 13:45:07.266438 91351 SynchronousNode.cpp:246] [2] PROFILING END[WaitingToSync]
I1226 13:45:07.250628 92490 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 13:45:07.266523 91351 SynchronousNode.cpp:247] [2] PROFILING BEGIN[Forward]
I1226 13:45:07.263083 92417 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 13:45:07.279014 91279 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 13:45:07.263191 92417 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 13:45:07.279126 91279 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 13:45:07.278537 98611 SynchronousNode.cpp:246] [3] PROFILING END[WaitingToSync]
I1226 13:45:07.278625 98611 SynchronousNode.cpp:247] [3] PROFILING BEGIN[Forward]
I1226 13:45:07.291263 98538 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 13:45:07.291378 98538 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 13:45:07.351970 92417 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 13:45:07.352049 92417 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 13:45:07.352092 92417 solver.cpp:291] [1] Iteration 9, loss = 3.62082
I1226 13:45:07.352159 92417 solver.cpp:317]     Train net output #0: accuracy = 0.3125
I1226 13:45:07.352223 92417 solver.cpp:317]     Train net output #1: loss = 3.62082 (* 1 = 3.62082 loss)
I1226 13:45:07.421335 91279 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 13:45:07.421424 91279 MultiSolver.cpp:109] [2] PROFILING BEGIN[WaitingToSync]
I1226 13:45:07.421475 91279 solver.cpp:291] [2] Iteration 9, loss = 3.40417
I1226 13:45:07.421550 91279 solver.cpp:317]     Train net output #0: accuracy = 0.359375
I1226 13:45:07.421627 91279 solver.cpp:317]     Train net output #1: loss = 3.40417 (* 1 = 3.40417 loss)
I1226 13:45:07.433367 98538 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 13:45:07.433461 98538 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 13:45:07.433542 98538 solver.cpp:291] [3] Iteration 9, loss = 3.24262
I1226 13:45:07.433614 98538 solver.cpp:317]     Train net output #0: accuracy = 0.375
I1226 13:45:07.433837 98538 solver.cpp:317]     Train net output #1: loss = 3.24262 (* 1 = 3.24262 loss)
I1226 13:45:08.592039 94226 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 13:45:08.592165 94226 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 13:45:08.685523 94885 MultiSolver.cpp:92] [4] PROFILING END[Forward]
I1226 13:45:08.685641 94885 MultiSolver.cpp:94] [4] PROFILING BEGIN[Backward]
I1226 13:45:08.683810 94226 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 13:45:08.683889 94226 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 13:45:08.683934 94226 solver.cpp:291] [0] Iteration 10, loss = 3.04481
I1226 13:45:08.684005 94226 solver.cpp:317]     Train net output #0: accuracy = 0.328125
I1226 13:45:08.684087 94226 solver.cpp:317]     Train net output #1: loss = 3.04481 (* 1 = 3.04481 loss)
I1226 13:45:09.059831 92490 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 13:45:09.075721 91351 SynchronousNode.cpp:246] [2] PROFILING END[WaitingToSync]
I1226 13:45:09.059921 92490 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 13:45:09.075808 91351 SynchronousNode.cpp:247] [2] PROFILING BEGIN[Forward]
I1226 13:45:09.072310 92417 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 13:45:09.072419 92417 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 13:45:09.079414 98611 SynchronousNode.cpp:246] [3] PROFILING END[WaitingToSync]
I1226 13:45:09.079496 98611 SynchronousNode.cpp:247] [3] PROFILING BEGIN[Forward]
I1226 13:45:09.090420 91279 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 13:45:09.090533 91279 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 13:45:09.092900 98538 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 13:45:09.093016 98538 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 13:45:09.157866 92417 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 13:45:09.157984 92417 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 13:45:09.158030 92417 solver.cpp:291] [1] Iteration 10, loss = 3.68288
I1226 13:45:09.158095 92417 solver.cpp:317]     Train net output #0: accuracy = 0.328125
I1226 13:45:09.158171 92417 solver.cpp:317]     Train net output #1: loss = 3.68288 (* 1 = 3.68288 loss)
I1226 13:45:09.235755 91279 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 13:45:09.235851 91279 MultiSolver.cpp:109] [2] PROFILING BEGIN[WaitingToSync]
I1226 13:45:09.235903 91279 solver.cpp:291] [2] Iteration 10, loss = 3.16351
I1226 13:45:09.235975 91279 solver.cpp:317]     Train net output #0: accuracy = 0.359375
I1226 13:45:09.236053 91279 solver.cpp:317]     Train net output #1: loss = 3.16351 (* 1 = 3.16351 loss)
I1226 13:45:09.285511 98538 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 13:45:09.285605 98538 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 13:45:09.285656 98538 solver.cpp:291] [3] Iteration 10, loss = 3.08199
I1226 13:45:09.285729 98538 solver.cpp:317]     Train net output #0: accuracy = 0.453125
I1226 13:45:09.285805 98538 solver.cpp:317]     Train net output #1: loss = 3.08199 (* 1 = 3.08199 loss)
I1226 13:45:09.624173 86057 SynchronousNode.cpp:246] [5] PROFILING END[WaitingToSync]
I1226 13:45:09.610740 91179 SynchronousNode.cpp:246] [6] PROFILING END[WaitingToSync]
I1226 13:45:09.610821 91179 SynchronousNode.cpp:247] [6] PROFILING BEGIN[Forward]
I1226 13:45:09.624261 86057 SynchronousNode.cpp:247] [5] PROFILING BEGIN[Forward]
I1226 13:45:09.624543 97087 SynchronousNode.cpp:246] [7] PROFILING END[WaitingToSync]
I1226 13:45:09.624632 97087 SynchronousNode.cpp:247] [7] PROFILING BEGIN[Forward]
I1226 13:45:09.636632 85984 MultiSolver.cpp:92] [5] PROFILING END[Forward]
I1226 13:45:09.636786 85984 MultiSolver.cpp:94] [5] PROFILING BEGIN[Backward]
I1226 13:45:09.623651 91110 MultiSolver.cpp:92] [6] PROFILING END[Forward]
I1226 13:45:09.623766 91110 MultiSolver.cpp:94] [6] PROFILING BEGIN[Backward]
I1226 13:45:09.698215 97018 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 13:45:09.698333 97018 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 13:45:09.757444 85984 MultiSolver.cpp:108] [5] PROFILING END[Backward]
I1226 13:45:09.757526 85984 MultiSolver.cpp:109] [5] PROFILING BEGIN[WaitingToSync]
I1226 13:45:09.757566 85984 solver.cpp:291] [5] Iteration 3, loss = 3.15931
I1226 13:45:09.757632 85984 solver.cpp:317]     Train net output #0: accuracy = 0.265625
I1226 13:45:09.757726 85984 solver.cpp:317]     Train net output #1: loss = 3.15931 (* 1 = 3.15931 loss)
I1226 13:45:09.785043 91110 MultiSolver.cpp:108] [6] PROFILING END[Backward]
I1226 13:45:09.785168 91110 MultiSolver.cpp:109] [6] PROFILING BEGIN[WaitingToSync]
I1226 13:45:09.785215 91110 solver.cpp:291] [6] Iteration 3, loss = 2.97001
I1226 13:45:09.785290 91110 solver.cpp:317]     Train net output #0: accuracy = 0.34375
I1226 13:45:09.785367 91110 solver.cpp:317]     Train net output #1: loss = 2.97001 (* 1 = 2.97001 loss)
I1226 13:45:09.952188 94885 MultiSolver.cpp:108] [4] PROFILING END[Backward]
I1226 13:45:09.952276 94885 MultiSolver.cpp:109] [4] PROFILING BEGIN[WaitingToSync]
I1226 13:45:09.952325 94885 solver.cpp:291] [4] Iteration 3, loss = 3.18905
I1226 13:45:09.952399 94885 solver.cpp:317]     Train net output #0: accuracy = 0.28125
I1226 13:45:09.952476 94885 solver.cpp:317]     Train net output #1: loss = 3.18905 (* 1 = 3.18905 loss)
I1226 13:45:10.246649 94226 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 13:45:10.246770 94226 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 13:45:10.342173 94226 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 13:45:10.342252 94226 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 13:45:10.342291 94226 solver.cpp:291] [0] Iteration 11, loss = 3.61995
I1226 13:45:10.342360 94226 solver.cpp:317]     Train net output #0: accuracy = 0.3125
I1226 13:45:10.342473 94226 solver.cpp:317]     Train net output #1: loss = 3.61995 (* 1 = 3.61995 loss)
I1226 13:45:10.612118 97018 MultiSolver.cpp:108] [7] PROFILING END[Backward]
I1226 13:45:10.612210 97018 MultiSolver.cpp:109] [7] PROFILING BEGIN[WaitingToSync]
I1226 13:45:10.612263 97018 solver.cpp:291] [7] Iteration 3, loss = 3.24604
I1226 13:45:10.612334 97018 solver.cpp:317]     Train net output #0: accuracy = 0.25
I1226 13:45:10.612411 97018 solver.cpp:317]     Train net output #1: loss = 3.24604 (* 1 = 3.24604 loss)
I1226 13:45:10.738448 92490 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 13:45:10.754382 91351 SynchronousNode.cpp:246] [2] PROFILING END[WaitingToSync]
I1226 13:45:10.738572 92490 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 13:45:10.754467 91351 SynchronousNode.cpp:247] [2] PROFILING BEGIN[Forward]
I1226 13:45:10.751013 92417 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 13:45:10.766840 91279 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 13:45:10.751111 92417 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 13:45:10.766949 91279 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 13:45:10.764226 98611 SynchronousNode.cpp:246] [3] PROFILING END[WaitingToSync]
I1226 13:45:10.764313 98611 SynchronousNode.cpp:247] [3] PROFILING BEGIN[Forward]
I1226 13:45:10.776837 98538 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 13:45:10.776952 98538 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 13:45:10.843881 92417 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 13:45:10.843971 92417 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 13:45:10.844127 92417 solver.cpp:291] [1] Iteration 11, loss = 3.40602
I1226 13:45:10.844205 92417 solver.cpp:317]     Train net output #0: accuracy = 0.265625
I1226 13:45:10.844285 92417 solver.cpp:317]     Train net output #1: loss = 3.40602 (* 1 = 3.40602 loss)
I1226 13:45:10.911998 91279 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 13:45:10.912088 91279 MultiSolver.cpp:109] [2] PROFILING BEGIN[WaitingToSync]
I1226 13:45:10.912298 91279 solver.cpp:291] [2] Iteration 11, loss = 3.24147
I1226 13:45:10.912382 91279 solver.cpp:317]     Train net output #0: accuracy = 0.34375
I1226 13:45:10.912459 91279 solver.cpp:317]     Train net output #1: loss = 3.24147 (* 1 = 3.24147 loss)
I1226 13:45:10.918123 98538 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 13:45:10.918215 98538 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 13:45:10.918382 98538 solver.cpp:291] [3] Iteration 11, loss = 3.47675
I1226 13:45:10.918459 98538 solver.cpp:317]     Train net output #0: accuracy = 0.328125
I1226 13:45:10.918541 98538 solver.cpp:317]     Train net output #1: loss = 3.47675 (* 1 = 3.47675 loss)
I1226 13:45:11.927203 94226 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 13:45:11.927312 94226 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 13:45:12.021332 94226 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 13:45:12.021438 94226 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 13:45:12.021484 94226 solver.cpp:291] [0] Iteration 12, loss = 3.02552
I1226 13:45:12.021553 94226 solver.cpp:317]     Train net output #0: accuracy = 0.34375
I1226 13:45:12.021637 94226 solver.cpp:317]     Train net output #1: loss = 3.02552 (* 1 = 3.02552 loss)
I1226 13:45:12.400979 92490 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 13:45:12.401065 92490 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 13:45:12.416862 91351 SynchronousNode.cpp:246] [2] PROFILING END[WaitingToSync]
I1226 13:45:12.416945 91351 SynchronousNode.cpp:247] [2] PROFILING BEGIN[Forward]
I1226 13:45:12.413630 92417 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 13:45:12.429453 91279 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 13:45:12.413745 92417 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 13:45:12.429566 91279 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 13:45:12.421150 98611 SynchronousNode.cpp:246] [3] PROFILING END[WaitingToSync]
I1226 13:45:12.421232 98611 SynchronousNode.cpp:247] [3] PROFILING BEGIN[Forward]
I1226 13:45:12.430466 98538 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 13:45:12.430573 98538 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 13:45:12.498608 92417 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 13:45:12.498697 92417 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 13:45:12.498747 92417 solver.cpp:291] [1] Iteration 12, loss = 3.0261
I1226 13:45:12.498823 92417 solver.cpp:317]     Train net output #0: accuracy = 0.296875
I1226 13:45:12.498898 92417 solver.cpp:317]     Train net output #1: loss = 3.0261 (* 1 = 3.0261 loss)
I1226 13:45:12.577005 91279 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 13:45:12.577096 91279 MultiSolver.cpp:109] [2] PROFILING BEGIN[WaitingToSync]
I1226 13:45:12.577152 91279 solver.cpp:291] [2] Iteration 12, loss = 3.32345
I1226 13:45:12.577255 91279 solver.cpp:317]     Train net output #0: accuracy = 0.375
I1226 13:45:12.577335 91279 solver.cpp:317]     Train net output #1: loss = 3.32345 (* 1 = 3.32345 loss)
I1226 13:45:12.573948 98538 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 13:45:12.574039 98538 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 13:45:12.574121 98538 solver.cpp:291] [3] Iteration 12, loss = 3.50051
I1226 13:45:12.574196 98538 solver.cpp:317]     Train net output #0: accuracy = 0.3125
I1226 13:45:12.574272 98538 solver.cpp:317]     Train net output #1: loss = 3.50051 (* 1 = 3.50051 loss)
I1226 13:45:13.590173 94226 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 13:45:13.590284 94226 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 13:45:13.681787 94226 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 13:45:13.681867 94226 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 13:45:13.681951 94226 solver.cpp:291] [0] Iteration 13, loss = 3.30549
I1226 13:45:13.682060 94226 solver.cpp:317]     Train net output #0: accuracy = 0.3125
I1226 13:45:13.682324 94226 solver.cpp:317]     Train net output #1: loss = 3.30549 (* 1 = 3.30549 loss)
I1226 13:45:14.056439 92490 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 13:45:14.072371 91351 SynchronousNode.cpp:246] [2] PROFILING END[WaitingToSync]
I1226 13:45:14.056563 92490 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 13:45:14.072456 91351 SynchronousNode.cpp:247] [2] PROFILING BEGIN[Forward]
I1226 13:45:14.069072 92417 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 13:45:14.085037 91279 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 13:45:14.069231 92417 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 13:45:14.085149 91279 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 13:45:14.079516 98611 SynchronousNode.cpp:246] [3] PROFILING END[WaitingToSync]
I1226 13:45:14.079601 98611 SynchronousNode.cpp:247] [3] PROFILING BEGIN[Forward]
I1226 13:45:14.092178 98538 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 13:45:14.092289 98538 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 13:45:14.156370 92417 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 13:45:14.156450 92417 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 13:45:14.156532 92417 solver.cpp:291] [1] Iteration 13, loss = 3.06029
I1226 13:45:14.156599 92417 solver.cpp:317]     Train net output #0: accuracy = 0.375
I1226 13:45:14.156666 92417 solver.cpp:317]     Train net output #1: loss = 3.06029 (* 1 = 3.06029 loss)
I1226 13:45:14.232866 91279 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 13:45:14.232959 91279 MultiSolver.cpp:109] [2] PROFILING BEGIN[WaitingToSync]
I1226 13:45:14.233017 91279 solver.cpp:291] [2] Iteration 13, loss = 3.38601
I1226 13:45:14.233090 91279 solver.cpp:317]     Train net output #0: accuracy = 0.234375
I1226 13:45:14.233166 91279 solver.cpp:317]     Train net output #1: loss = 3.38601 (* 1 = 3.38601 loss)
I1226 13:45:14.234941 98538 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 13:45:14.235029 98538 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 13:45:14.235146 98538 solver.cpp:291] [3] Iteration 13, loss = 3.08104
I1226 13:45:14.235220 98538 solver.cpp:317]     Train net output #0: accuracy = 0.359375
I1226 13:45:14.235307 98538 solver.cpp:317]     Train net output #1: loss = 3.08104 (* 1 = 3.08104 loss)
I1226 13:45:15.250860 94226 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 13:45:15.251008 94226 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 13:45:15.342366 94226 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 13:45:15.342514 94226 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 13:45:15.342671 94226 solver.cpp:291] [0] Iteration 14, loss = 2.62997
I1226 13:45:15.342754 94226 solver.cpp:317]     Train net output #0: accuracy = 0.4375
I1226 13:45:15.342838 94226 solver.cpp:317]     Train net output #1: loss = 2.62997 (* 1 = 2.62997 loss)
I1226 13:45:15.563223 94885 MultiSolver.cpp:92] [4] PROFILING END[Forward]
I1226 13:45:15.563350 94885 MultiSolver.cpp:94] [4] PROFILING BEGIN[Backward]
I1226 13:45:15.713886 92490 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 13:45:15.729775 91351 SynchronousNode.cpp:246] [2] PROFILING END[WaitingToSync]
I1226 13:45:15.713974 92490 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 13:45:15.729871 91351 SynchronousNode.cpp:247] [2] PROFILING BEGIN[Forward]
I1226 13:45:15.726316 92417 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 13:45:15.742311 91279 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 13:45:15.726429 92417 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 13:45:15.742419 91279 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 13:45:15.740831 98611 SynchronousNode.cpp:246] [3] PROFILING END[WaitingToSync]
I1226 13:45:15.740918 98611 SynchronousNode.cpp:247] [3] PROFILING BEGIN[Forward]
I1226 13:45:15.753619 98538 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 13:45:15.753741 98538 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 13:45:15.820634 92417 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 13:45:15.820724 92417 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 13:45:15.820775 92417 solver.cpp:291] [1] Iteration 14, loss = 3.68133
I1226 13:45:15.820850 92417 solver.cpp:317]     Train net output #0: accuracy = 0.234375
I1226 13:45:15.820925 92417 solver.cpp:317]     Train net output #1: loss = 3.68133 (* 1 = 3.68133 loss)
I1226 13:45:15.884317 91279 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 13:45:15.884408 91279 MultiSolver.cpp:109] [2] PROFILING BEGIN[WaitingToSync]
I1226 13:45:15.884466 91279 solver.cpp:291] [2] Iteration 14, loss = 3.49975
I1226 13:45:15.884539 91279 solver.cpp:317]     Train net output #0: accuracy = 0.25
I1226 13:45:15.884616 91279 solver.cpp:317]     Train net output #1: loss = 3.49975 (* 1 = 3.49975 loss)
I1226 13:45:15.892772 98538 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 13:45:15.892880 98538 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 13:45:15.892926 98538 solver.cpp:291] [3] Iteration 14, loss = 3.21146
I1226 13:45:15.893000 98538 solver.cpp:317]     Train net output #0: accuracy = 0.328125
I1226 13:45:15.893112 98538 solver.cpp:317]     Train net output #1: loss = 3.21146 (* 1 = 3.21146 loss)
I1226 13:45:16.459182 94885 MultiSolver.cpp:108] [4] PROFILING END[Backward]
I1226 13:45:16.459383 94885 MultiSolver.cpp:109] [4] PROFILING BEGIN[WaitingToSync]
I1226 13:45:16.459431 94885 solver.cpp:291] [4] Iteration 4, loss = 3.91722
I1226 13:45:16.459504 94885 solver.cpp:317]     Train net output #0: accuracy = 0.203125
I1226 13:45:16.459579 94885 solver.cpp:317]     Train net output #1: loss = 3.91722 (* 1 = 3.91722 loss)
I1226 13:45:16.541592 86057 SynchronousNode.cpp:246] [5] PROFILING END[WaitingToSync]
I1226 13:45:16.541704 86057 SynchronousNode.cpp:247] [5] PROFILING BEGIN[Forward]
I1226 13:45:16.528194 91179 SynchronousNode.cpp:246] [6] PROFILING END[WaitingToSync]
I1226 13:45:16.528275 91179 SynchronousNode.cpp:247] [6] PROFILING BEGIN[Forward]
I1226 13:45:16.539587 97087 SynchronousNode.cpp:246] [7] PROFILING END[WaitingToSync]
I1226 13:45:16.539674 97087 SynchronousNode.cpp:247] [7] PROFILING BEGIN[Forward]
I1226 13:45:16.554080 85984 MultiSolver.cpp:92] [5] PROFILING END[Forward]
I1226 13:45:16.554193 85984 MultiSolver.cpp:94] [5] PROFILING BEGIN[Backward]
I1226 13:45:16.540994 91110 MultiSolver.cpp:92] [6] PROFILING END[Forward]
I1226 13:45:16.541152 91110 MultiSolver.cpp:94] [6] PROFILING BEGIN[Backward]
I1226 13:45:16.612452 97018 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 13:45:16.612570 97018 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 13:45:16.665624 85984 MultiSolver.cpp:108] [5] PROFILING END[Backward]
I1226 13:45:16.665735 85984 MultiSolver.cpp:109] [5] PROFILING BEGIN[WaitingToSync]
I1226 13:45:16.665776 85984 solver.cpp:291] [5] Iteration 4, loss = 3.21118
I1226 13:45:16.665843 85984 solver.cpp:317]     Train net output #0: accuracy = 0.3125
I1226 13:45:16.665915 85984 solver.cpp:317]     Train net output #1: loss = 3.21118 (* 1 = 3.21118 loss)
I1226 13:45:16.700281 91110 MultiSolver.cpp:108] [6] PROFILING END[Backward]
I1226 13:45:16.700364 91110 MultiSolver.cpp:109] [6] PROFILING BEGIN[WaitingToSync]
I1226 13:45:16.700412 91110 solver.cpp:291] [6] Iteration 4, loss = 3.33603
I1226 13:45:16.700486 91110 solver.cpp:317]     Train net output #0: accuracy = 0.3125
I1226 13:45:16.700562 91110 solver.cpp:317]     Train net output #1: loss = 3.33603 (* 1 = 3.33603 loss)
I1226 13:45:16.879911 94226 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 13:45:16.880112 94226 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 13:45:16.973234 94226 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 13:45:16.973354 94226 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 13:45:16.973420 94226 solver.cpp:291] [0] Iteration 15, loss = 3.57972
I1226 13:45:16.973498 94226 solver.cpp:317]     Train net output #0: accuracy = 0.265625
I1226 13:45:16.973578 94226 solver.cpp:317]     Train net output #1: loss = 3.57972 (* 1 = 3.57972 loss)
I1226 13:45:17.355263 92490 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 13:45:17.371158 91351 SynchronousNode.cpp:246] [2] PROFILING END[WaitingToSync]
I1226 13:45:17.355350 92490 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 13:45:17.371269 91351 SynchronousNode.cpp:247] [2] PROFILING BEGIN[Forward]
I1226 13:45:17.367692 92417 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 13:45:17.367794 92417 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 13:45:17.383819 91279 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 13:45:17.383931 91279 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 13:45:17.387394 98611 SynchronousNode.cpp:246] [3] PROFILING END[WaitingToSync]
I1226 13:45:17.387481 98611 SynchronousNode.cpp:247] [3] PROFILING BEGIN[Forward]
I1226 13:45:17.400157 98538 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 13:45:17.400275 98538 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 13:45:17.458539 92417 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 13:45:17.458616 92417 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 13:45:17.458662 92417 solver.cpp:291] [1] Iteration 15, loss = 3.52418
I1226 13:45:17.458726 92417 solver.cpp:317]     Train net output #0: accuracy = 0.296875
I1226 13:45:17.458791 92417 solver.cpp:317]     Train net output #1: loss = 3.52418 (* 1 = 3.52418 loss)
I1226 13:45:17.528642 91279 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 13:45:17.528733 91279 MultiSolver.cpp:109] [2] PROFILING BEGIN[WaitingToSync]
I1226 13:45:17.528787 91279 solver.cpp:291] [2] Iteration 15, loss = 3.22184
I1226 13:45:17.528862 91279 solver.cpp:317]     Train net output #0: accuracy = 0.34375
I1226 13:45:17.528939 91279 solver.cpp:317]     Train net output #1: loss = 3.22184 (* 1 = 3.22184 loss)
I1226 13:45:17.524488 97018 MultiSolver.cpp:108] [7] PROFILING END[Backward]
I1226 13:45:17.524583 97018 MultiSolver.cpp:109] [7] PROFILING BEGIN[WaitingToSync]
I1226 13:45:17.524636 97018 solver.cpp:291] [7] Iteration 4, loss = 3.40192
I1226 13:45:17.524706 97018 solver.cpp:317]     Train net output #0: accuracy = 0.25
I1226 13:45:17.524806 97018 solver.cpp:317]     Train net output #1: loss = 3.40192 (* 1 = 3.40192 loss)
I1226 13:45:17.540216 98538 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 13:45:17.540304 98538 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 13:45:17.540352 98538 solver.cpp:291] [3] Iteration 15, loss = 3.42593
I1226 13:45:17.540426 98538 solver.cpp:317]     Train net output #0: accuracy = 0.3125
I1226 13:45:17.540501 98538 solver.cpp:317]     Train net output #1: loss = 3.42593 (* 1 = 3.42593 loss)
I1226 13:45:18.534512 94226 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 13:45:18.534628 94226 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 13:45:18.626027 94226 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 13:45:18.626160 94226 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 13:45:18.626204 94226 solver.cpp:291] [0] Iteration 16, loss = 3.17629
I1226 13:45:18.626315 94226 solver.cpp:317]     Train net output #0: accuracy = 0.28125
I1226 13:45:18.626580 94226 solver.cpp:317]     Train net output #1: loss = 3.17629 (* 1 = 3.17629 loss)
I1226 13:45:19.001137 92490 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 13:45:19.016978 91351 SynchronousNode.cpp:246] [2] PROFILING END[WaitingToSync]
I1226 13:45:19.001224 92490 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 13:45:19.017066 91351 SynchronousNode.cpp:247] [2] PROFILING BEGIN[Forward]
I1226 13:45:19.013723 92417 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 13:45:19.013830 92417 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 13:45:19.031999 91279 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 13:45:19.032161 91279 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 13:45:19.024531 98611 SynchronousNode.cpp:246] [3] PROFILING END[WaitingToSync]
I1226 13:45:19.024616 98611 SynchronousNode.cpp:247] [3] PROFILING BEGIN[Forward]
I1226 13:45:19.037164 98538 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 13:45:19.037281 98538 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 13:45:19.102638 92417 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 13:45:19.102725 92417 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 13:45:19.102802 92417 solver.cpp:291] [1] Iteration 16, loss = 3.36996
I1226 13:45:19.103066 92417 solver.cpp:317]     Train net output #0: accuracy = 0.296875
I1226 13:45:19.103149 92417 solver.cpp:317]     Train net output #1: loss = 3.36996 (* 1 = 3.36996 loss)
I1226 13:45:19.173889 91279 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 13:45:19.173979 91279 MultiSolver.cpp:109] [2] PROFILING BEGIN[WaitingToSync]
I1226 13:45:19.174031 91279 solver.cpp:291] [2] Iteration 16, loss = 3.15639
I1226 13:45:19.174104 91279 solver.cpp:317]     Train net output #0: accuracy = 0.375
I1226 13:45:19.174181 91279 solver.cpp:317]     Train net output #1: loss = 3.15639 (* 1 = 3.15639 loss)
I1226 13:45:19.185189 98538 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 13:45:19.185274 98538 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 13:45:19.185323 98538 solver.cpp:291] [3] Iteration 16, loss = 3.32234
I1226 13:45:19.185397 98538 solver.cpp:317]     Train net output #0: accuracy = 0.328125
I1226 13:45:19.185475 98538 solver.cpp:317]     Train net output #1: loss = 3.32234 (* 1 = 3.32234 loss)
I1226 13:45:20.171159 94226 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 13:45:20.171267 94226 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 13:45:20.263077 94226 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 13:45:20.263157 94226 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 13:45:20.263196 94226 solver.cpp:291] [0] Iteration 17, loss = 3.82844
I1226 13:45:20.263265 94226 solver.cpp:317]     Train net output #0: accuracy = 0.296875
I1226 13:45:20.263346 94226 solver.cpp:317]     Train net output #1: loss = 3.82844 (* 1 = 3.82844 loss)
I1226 13:45:20.658390 92490 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 13:45:20.674248 91351 SynchronousNode.cpp:246] [2] PROFILING END[WaitingToSync]
I1226 13:45:20.658605 92490 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 13:45:20.674412 91351 SynchronousNode.cpp:247] [2] PROFILING BEGIN[Forward]
I1226 13:45:20.671195 92417 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 13:45:20.671345 92417 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 13:45:20.688541 91279 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 13:45:20.688661 91279 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 13:45:20.678449 98611 SynchronousNode.cpp:246] [3] PROFILING END[WaitingToSync]
I1226 13:45:20.678634 98611 SynchronousNode.cpp:247] [3] PROFILING BEGIN[Forward]
I1226 13:45:20.693025 98538 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 13:45:20.693167 98538 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 13:45:20.757339 92417 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 13:45:20.757421 92417 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 13:45:20.757465 92417 solver.cpp:291] [1] Iteration 17, loss = 3.29362
I1226 13:45:20.757576 92417 solver.cpp:317]     Train net output #0: accuracy = 0.328125
I1226 13:45:20.757643 92417 solver.cpp:317]     Train net output #1: loss = 3.29362 (* 1 = 3.29362 loss)
I1226 13:45:20.833119 91279 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 13:45:20.833245 91279 MultiSolver.cpp:109] [2] PROFILING BEGIN[WaitingToSync]
I1226 13:45:20.833304 91279 solver.cpp:291] [2] Iteration 17, loss = 3.21265
I1226 13:45:20.833379 91279 solver.cpp:317]     Train net output #0: accuracy = 0.3125
I1226 13:45:20.833456 91279 solver.cpp:317]     Train net output #1: loss = 3.21265 (* 1 = 3.21265 loss)
I1226 13:45:20.834012 98538 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 13:45:20.834156 98538 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 13:45:20.834213 98538 solver.cpp:291] [3] Iteration 17, loss = 3.29461
I1226 13:45:20.834288 98538 solver.cpp:317]     Train net output #0: accuracy = 0.34375
I1226 13:45:20.834370 98538 solver.cpp:317]     Train net output #1: loss = 3.29461 (* 1 = 3.29461 loss)
I1226 13:45:21.834291 94226 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 13:45:21.834439 94226 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 13:45:21.928736 94226 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 13:45:21.928825 94226 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 13:45:21.928864 94226 solver.cpp:291] [0] Iteration 18, loss = 3.61356
I1226 13:45:21.928931 94226 solver.cpp:317]     Train net output #0: accuracy = 0.25
I1226 13:45:21.929013 94226 solver.cpp:317]     Train net output #1: loss = 3.61356 (* 1 = 3.61356 loss)
I1226 13:45:22.318025 92490 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 13:45:22.333892 91351 SynchronousNode.cpp:246] [2] PROFILING END[WaitingToSync]
I1226 13:45:22.318114 92490 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 13:45:22.333979 91351 SynchronousNode.cpp:247] [2] PROFILING BEGIN[Forward]
I1226 13:45:22.330579 92417 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 13:45:22.330682 92417 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 13:45:22.336923 98611 SynchronousNode.cpp:246] [3] PROFILING END[WaitingToSync]
I1226 13:45:22.347486 91279 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 13:45:22.337008 98611 SynchronousNode.cpp:247] [3] PROFILING BEGIN[Forward]
I1226 13:45:22.347633 91279 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 13:45:22.350164 98538 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 13:45:22.350322 98538 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 13:45:22.414522 92417 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 13:45:22.414604 92417 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 13:45:22.414646 92417 solver.cpp:291] [1] Iteration 18, loss = 2.96417
I1226 13:45:22.414712 92417 solver.cpp:317]     Train net output #0: accuracy = 0.375
I1226 13:45:22.414783 92417 solver.cpp:317]     Train net output #1: loss = 2.96417 (* 1 = 2.96417 loss)
I1226 13:45:22.441534 94885 MultiSolver.cpp:92] [4] PROFILING END[Forward]
I1226 13:45:22.441644 94885 MultiSolver.cpp:94] [4] PROFILING BEGIN[Backward]
I1226 13:45:22.492151 91279 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 13:45:22.493261 91279 MultiSolver.cpp:109] [2] PROFILING BEGIN[WaitingToSync]
I1226 13:45:22.493343 91279 solver.cpp:291] [2] Iteration 18, loss = 3.43972
I1226 13:45:22.493424 91279 solver.cpp:317]     Train net output #0: accuracy = 0.359375
I1226 13:45:22.493507 91279 solver.cpp:317]     Train net output #1: loss = 3.43972 (* 1 = 3.43972 loss)
I1226 13:45:22.492548 98538 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 13:45:22.493894 98538 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 13:45:22.493974 98538 solver.cpp:291] [3] Iteration 18, loss = 3.40996
I1226 13:45:22.494051 98538 solver.cpp:317]     Train net output #0: accuracy = 0.234375
I1226 13:45:22.494153 98538 solver.cpp:317]     Train net output #1: loss = 3.40996 (* 1 = 3.40996 loss)
I1226 13:45:23.337110 94885 MultiSolver.cpp:108] [4] PROFILING END[Backward]
I1226 13:45:23.337324 94885 MultiSolver.cpp:109] [4] PROFILING BEGIN[WaitingToSync]
I1226 13:45:23.337373 94885 solver.cpp:291] [4] Iteration 5, loss = 2.56202
I1226 13:45:23.337445 94885 solver.cpp:317]     Train net output #0: accuracy = 0.421875
I1226 13:45:23.337512 94885 solver.cpp:317]     Train net output #1: loss = 2.56202 (* 1 = 2.56202 loss)
I1226 13:45:23.366266 86057 SynchronousNode.cpp:246] [5] PROFILING END[WaitingToSync]
I1226 13:45:23.352880 91179 SynchronousNode.cpp:246] [6] PROFILING END[WaitingToSync]
I1226 13:45:23.367277 86057 SynchronousNode.cpp:247] [5] PROFILING BEGIN[Forward]
I1226 13:45:23.353886 91179 SynchronousNode.cpp:247] [6] PROFILING BEGIN[Forward]
I1226 13:45:23.362522 97087 SynchronousNode.cpp:246] [7] PROFILING END[WaitingToSync]
I1226 13:45:23.362604 97087 SynchronousNode.cpp:247] [7] PROFILING BEGIN[Forward]
I1226 13:45:23.365880 91110 MultiSolver.cpp:92] [6] PROFILING END[Forward]
I1226 13:45:23.365990 91110 MultiSolver.cpp:94] [6] PROFILING BEGIN[Backward]
I1226 13:45:23.380620 85984 MultiSolver.cpp:92] [5] PROFILING END[Forward]
I1226 13:45:23.380758 85984 MultiSolver.cpp:94] [5] PROFILING BEGIN[Backward]
I1226 13:45:23.435667 97018 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 13:45:23.435813 97018 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 13:45:23.508837 85984 MultiSolver.cpp:108] [5] PROFILING END[Backward]
I1226 13:45:23.508931 85984 MultiSolver.cpp:109] [5] PROFILING BEGIN[WaitingToSync]
I1226 13:45:23.508971 85984 solver.cpp:291] [5] Iteration 5, loss = 3.01252
I1226 13:45:23.509037 85984 solver.cpp:317]     Train net output #0: accuracy = 0.34375
I1226 13:45:23.509109 85984 solver.cpp:317]     Train net output #1: loss = 3.01252 (* 1 = 3.01252 loss)
I1226 13:45:23.499596 94226 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 13:45:23.500890 94226 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 13:45:23.546371 91110 MultiSolver.cpp:108] [6] PROFILING END[Backward]
I1226 13:45:23.546455 91110 MultiSolver.cpp:109] [6] PROFILING BEGIN[WaitingToSync]
I1226 13:45:23.546504 91110 solver.cpp:291] [6] Iteration 5, loss = 3.64387
I1226 13:45:23.546577 91110 solver.cpp:317]     Train net output #0: accuracy = 0.28125
I1226 13:45:23.546653 91110 solver.cpp:317]     Train net output #1: loss = 3.64387 (* 1 = 3.64387 loss)
I1226 13:45:23.595662 94226 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 13:45:23.595737 94226 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 13:45:23.595782 94226 solver.cpp:291] [0] Iteration 19, loss = 3.04491
I1226 13:45:23.595849 94226 solver.cpp:317]     Train net output #0: accuracy = 0.390625
I1226 13:45:23.595932 94226 solver.cpp:317]     Train net output #1: loss = 3.04491 (* 1 = 3.04491 loss)
I1226 13:45:23.988708 91351 SynchronousNode.cpp:246] [2] PROFILING END[WaitingToSync]
I1226 13:45:23.988795 91351 SynchronousNode.cpp:247] [2] PROFILING BEGIN[Forward]
I1226 13:45:23.972846 92490 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 13:45:23.974259 92490 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 13:45:23.985569 92417 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 13:45:23.985668 92417 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 13:45:24.001662 91279 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 13:45:24.001834 91279 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 13:45:24.005287 98611 SynchronousNode.cpp:246] [3] PROFILING END[WaitingToSync]
I1226 13:45:24.005373 98611 SynchronousNode.cpp:247] [3] PROFILING BEGIN[Forward]
I1226 13:45:24.018376 98538 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 13:45:24.018487 98538 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 13:45:24.074518 92417 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 13:45:24.074597 92417 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 13:45:24.074683 92417 solver.cpp:291] [1] Iteration 19, loss = 3.39078
I1226 13:45:24.074759 92417 solver.cpp:317]     Train net output #0: accuracy = 0.265625
I1226 13:45:24.074828 92417 solver.cpp:317]     Train net output #1: loss = 3.39078 (* 1 = 3.39078 loss)
I1226 13:45:24.146119 91279 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 13:45:24.146247 91279 MultiSolver.cpp:109] [2] PROFILING BEGIN[WaitingToSync]
I1226 13:45:24.146302 91279 solver.cpp:291] [2] Iteration 19, loss = 2.72676
I1226 13:45:24.146387 91279 solver.cpp:317]     Train net output #0: accuracy = 0.390625
I1226 13:45:24.146472 91279 solver.cpp:317]     Train net output #1: loss = 2.72676 (* 1 = 2.72676 loss)
I1226 13:45:24.160804 98538 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 13:45:24.160904 98538 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 13:45:24.160950 98538 solver.cpp:291] [3] Iteration 19, loss = 3.47241
I1226 13:45:24.161028 98538 solver.cpp:317]     Train net output #0: accuracy = 0.265625
I1226 13:45:24.161140 98538 solver.cpp:317]     Train net output #1: loss = 3.47241 (* 1 = 3.47241 loss)
I1226 13:45:24.346396 97018 MultiSolver.cpp:108] [7] PROFILING END[Backward]
I1226 13:45:24.346542 97018 MultiSolver.cpp:109] [7] PROFILING BEGIN[WaitingToSync]
I1226 13:45:24.346674 97018 solver.cpp:291] [7] Iteration 5, loss = 2.87472
I1226 13:45:24.346827 97018 solver.cpp:317]     Train net output #0: accuracy = 0.296875
I1226 13:45:24.346905 97018 solver.cpp:317]     Train net output #1: loss = 2.87472 (* 1 = 2.87472 loss)
I1226 13:45:25.172061 94226 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 13:45:25.172286 94226 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 13:45:25.263648 94226 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 13:45:25.263720 94226 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 13:45:25.263753 94226 solver.cpp:291] [0] Iteration 20, loss = 3.16158
I1226 13:45:25.263811 94226 solver.cpp:317]     Train net output #0: accuracy = 0.296875
I1226 13:45:25.263880 94226 solver.cpp:317]     Train net output #1: loss = 3.16158 (* 1 = 3.16158 loss)
I1226 13:45:25.660135 92490 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 13:45:25.660224 92490 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 13:45:25.675954 91351 SynchronousNode.cpp:246] [2] PROFILING END[WaitingToSync]
I1226 13:45:25.676039 91351 SynchronousNode.cpp:247] [2] PROFILING BEGIN[Forward]
I1226 13:45:25.672760 92417 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 13:45:25.672920 92417 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 13:45:25.688179 98611 SynchronousNode.cpp:246] [3] PROFILING END[WaitingToSync]
I1226 13:45:25.688259 98611 SynchronousNode.cpp:247] [3] PROFILING BEGIN[Forward]
I1226 13:45:25.708714 91279 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 13:45:25.708822 91279 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 13:45:25.703114 98538 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 13:45:25.703228 98538 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 13:45:25.763571 92417 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 13:45:25.763653 92417 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 13:45:25.763696 92417 solver.cpp:291] [1] Iteration 20, loss = 3.27914
I1226 13:45:25.763761 92417 solver.cpp:317]     Train net output #0: accuracy = 0.328125
I1226 13:45:25.763826 92417 solver.cpp:317]     Train net output #1: loss = 3.27914 (* 1 = 3.27914 loss)
I1226 13:45:25.846232 98538 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 13:45:25.846325 98538 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 13:45:25.846376 98538 solver.cpp:291] [3] Iteration 20, loss = 3.18355
I1226 13:45:25.846448 98538 solver.cpp:317]     Train net output #0: accuracy = 0.3125
I1226 13:45:25.846524 98538 solver.cpp:317]     Train net output #1: loss = 3.18355 (* 1 = 3.18355 loss)
I1226 13:45:25.929671 91279 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 13:45:25.929764 91279 MultiSolver.cpp:109] [2] PROFILING BEGIN[WaitingToSync]
I1226 13:45:25.929816 91279 solver.cpp:291] [2] Iteration 20, loss = 3.33328
I1226 13:45:25.929891 91279 solver.cpp:317]     Train net output #0: accuracy = 0.34375
I1226 13:45:25.929975 91279 solver.cpp:317]     Train net output #1: loss = 3.33328 (* 1 = 3.33328 loss)
I1226 13:45:26.861877 94226 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 13:45:26.862036 94226 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 13:45:26.951901 94226 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 13:45:26.951975 94226 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 13:45:26.952009 94226 solver.cpp:291] [0] Iteration 21, loss = 3.24082
I1226 13:45:26.952064 94226 solver.cpp:317]     Train net output #0: accuracy = 0.28125
I1226 13:45:26.952113 94226 solver.cpp:317]     Train net output #1: loss = 3.24082 (* 1 = 3.24082 loss)
I1226 13:45:27.332458 92490 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 13:45:27.348299 91351 SynchronousNode.cpp:246] [2] PROFILING END[WaitingToSync]
I1226 13:45:27.348378 91351 SynchronousNode.cpp:247] [2] PROFILING BEGIN[Forward]
I1226 13:45:27.332583 92490 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 13:45:27.360752 91279 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 13:45:27.345098 92417 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 13:45:27.360875 91279 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 13:45:27.345209 92417 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 13:45:27.361363 98611 SynchronousNode.cpp:246] [3] PROFILING END[WaitingToSync]
I1226 13:45:27.361444 98611 SynchronousNode.cpp:247] [3] PROFILING BEGIN[Forward]
I1226 13:45:27.383190 98538 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 13:45:27.383285 98538 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 13:45:27.448546 92417 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 13:45:27.448628 92417 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 13:45:27.448670 92417 solver.cpp:291] [1] Iteration 21, loss = 3.18755
I1226 13:45:27.448734 92417 solver.cpp:317]     Train net output #0: accuracy = 0.265625
I1226 13:45:27.448799 92417 solver.cpp:317]     Train net output #1: loss = 3.18755 (* 1 = 3.18755 loss)
I1226 13:45:27.500519 91279 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 13:45:27.500608 91279 MultiSolver.cpp:109] [2] PROFILING BEGIN[WaitingToSync]
I1226 13:45:27.500658 91279 solver.cpp:291] [2] Iteration 21, loss = 3.1736
I1226 13:45:27.500730 91279 solver.cpp:317]     Train net output #0: accuracy = 0.359375
I1226 13:45:27.500804 91279 solver.cpp:317]     Train net output #1: loss = 3.1736 (* 1 = 3.1736 loss)
I1226 13:45:27.664497 98538 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 13:45:27.664592 98538 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 13:45:27.664649 98538 solver.cpp:291] [3] Iteration 21, loss = 3.69818
I1226 13:45:27.664722 98538 solver.cpp:317]     Train net output #0: accuracy = 0.21875
I1226 13:45:27.665164 98538 solver.cpp:317]     Train net output #1: loss = 3.69818 (* 1 = 3.69818 loss)
I1226 13:45:28.518020 94226 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 13:45:28.518126 94226 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 13:45:28.609910 94226 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 13:45:28.610030 94226 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 13:45:28.610065 94226 solver.cpp:291] [0] Iteration 22, loss = 2.94135
I1226 13:45:28.610119 94226 solver.cpp:317]     Train net output #0: accuracy = 0.34375
I1226 13:45:28.610169 94226 solver.cpp:317]     Train net output #1: loss = 2.94135 (* 1 = 2.94135 loss)
I1226 13:45:29.003317 92490 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 13:45:29.019142 91351 SynchronousNode.cpp:246] [2] PROFILING END[WaitingToSync]
I1226 13:45:29.003406 92490 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 13:45:29.019258 91351 SynchronousNode.cpp:247] [2] PROFILING BEGIN[Forward]
I1226 13:45:29.015730 92417 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 13:45:29.015852 92417 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 13:45:29.022251 98611 SynchronousNode.cpp:246] [3] PROFILING END[WaitingToSync]
I1226 13:45:29.032763 91279 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 13:45:29.022337 98611 SynchronousNode.cpp:247] [3] PROFILING BEGIN[Forward]
I1226 13:45:29.032881 91279 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 13:45:29.035511 98538 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 13:45:29.035637 98538 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 13:45:29.105816 92417 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 13:45:29.105903 92417 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 13:45:29.105952 92417 solver.cpp:291] [1] Iteration 22, loss = 3.1896
I1226 13:45:29.106025 92417 solver.cpp:317]     Train net output #0: accuracy = 0.328125
I1226 13:45:29.106101 92417 solver.cpp:317]     Train net output #1: loss = 3.1896 (* 1 = 3.1896 loss)
I1226 13:45:29.177667 91279 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 13:45:29.177759 91279 MultiSolver.cpp:109] [2] PROFILING BEGIN[WaitingToSync]
I1226 13:45:29.177809 91279 solver.cpp:291] [2] Iteration 22, loss = 3.24789
I1226 13:45:29.177882 91279 solver.cpp:317]     Train net output #0: accuracy = 0.25
I1226 13:45:29.177959 91279 solver.cpp:317]     Train net output #1: loss = 3.24789 (* 1 = 3.24789 loss)
I1226 13:45:29.178869 98538 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 13:45:29.178964 98538 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 13:45:29.179011 98538 solver.cpp:291] [3] Iteration 22, loss = 3.54035
I1226 13:45:29.179118 98538 solver.cpp:317]     Train net output #0: accuracy = 0.25
I1226 13:45:29.179198 98538 solver.cpp:317]     Train net output #1: loss = 3.54035 (* 1 = 3.54035 loss)
I1226 13:45:29.307436 94885 MultiSolver.cpp:92] [4] PROFILING END[Forward]
I1226 13:45:29.307595 94885 MultiSolver.cpp:94] [4] PROFILING BEGIN[Backward]
I1226 13:45:30.176023 94226 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 13:45:30.176177 94226 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 13:45:30.202553 94885 MultiSolver.cpp:108] [4] PROFILING END[Backward]
I1226 13:45:30.202651 94885 MultiSolver.cpp:109] [4] PROFILING BEGIN[WaitingToSync]
I1226 13:45:30.202690 94885 solver.cpp:291] [4] Iteration 6, loss = 3.24351
I1226 13:45:30.202756 94885 solver.cpp:317]     Train net output #0: accuracy = 0.390625
I1226 13:45:30.202857 94885 solver.cpp:317]     Train net output #1: loss = 3.24351 (* 1 = 3.24351 loss)
I1226 13:45:30.251335 86057 SynchronousNode.cpp:246] [5] PROFILING END[WaitingToSync]
I1226 13:45:30.251420 86057 SynchronousNode.cpp:247] [5] PROFILING BEGIN[Forward]
I1226 13:45:30.237988 91179 SynchronousNode.cpp:246] [6] PROFILING END[WaitingToSync]
I1226 13:45:30.238092 91179 SynchronousNode.cpp:247] [6] PROFILING BEGIN[Forward]
I1226 13:45:30.246489 97087 SynchronousNode.cpp:246] [7] PROFILING END[WaitingToSync]
I1226 13:45:30.246569 97087 SynchronousNode.cpp:247] [7] PROFILING BEGIN[Forward]
I1226 13:45:30.250934 91110 MultiSolver.cpp:92] [6] PROFILING END[Forward]
I1226 13:45:30.251046 91110 MultiSolver.cpp:94] [6] PROFILING BEGIN[Backward]
I1226 13:45:30.265801 85984 MultiSolver.cpp:92] [5] PROFILING END[Forward]
I1226 13:45:30.265911 85984 MultiSolver.cpp:94] [5] PROFILING BEGIN[Backward]
I1226 13:45:30.270800 94226 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 13:45:30.270871 94226 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 13:45:30.270907 94226 solver.cpp:291] [0] Iteration 23, loss = 3.28132
I1226 13:45:30.270961 94226 solver.cpp:317]     Train net output #0: accuracy = 0.3125
I1226 13:45:30.271013 94226 solver.cpp:317]     Train net output #1: loss = 3.28132 (* 1 = 3.28132 loss)
I1226 13:45:30.320701 97018 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 13:45:30.320854 97018 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 13:45:30.384356 85984 MultiSolver.cpp:108] [5] PROFILING END[Backward]
I1226 13:45:30.384462 85984 MultiSolver.cpp:109] [5] PROFILING BEGIN[WaitingToSync]
I1226 13:45:30.384505 85984 solver.cpp:291] [5] Iteration 6, loss = 3.21609
I1226 13:45:30.384569 85984 solver.cpp:317]     Train net output #0: accuracy = 0.25
I1226 13:45:30.384636 85984 solver.cpp:317]     Train net output #1: loss = 3.21609 (* 1 = 3.21609 loss)
I1226 13:45:30.403481 91110 MultiSolver.cpp:108] [6] PROFILING END[Backward]
I1226 13:45:30.403564 91110 MultiSolver.cpp:109] [6] PROFILING BEGIN[WaitingToSync]
I1226 13:45:30.403612 91110 solver.cpp:291] [6] Iteration 6, loss = 2.96425
I1226 13:45:30.403687 91110 solver.cpp:317]     Train net output #0: accuracy = 0.34375
I1226 13:45:30.403761 91110 solver.cpp:317]     Train net output #1: loss = 2.96425 (* 1 = 2.96425 loss)
I1226 13:45:30.653661 92490 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 13:45:30.669427 91351 SynchronousNode.cpp:246] [2] PROFILING END[WaitingToSync]
I1226 13:45:30.653749 92490 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 13:45:30.669510 91351 SynchronousNode.cpp:247] [2] PROFILING BEGIN[Forward]
I1226 13:45:30.666250 92417 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 13:45:30.666363 92417 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 13:45:30.683260 91279 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 13:45:30.683373 91279 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 13:45:30.681195 98611 SynchronousNode.cpp:246] [3] PROFILING END[WaitingToSync]
I1226 13:45:30.681280 98611 SynchronousNode.cpp:247] [3] PROFILING BEGIN[Forward]
I1226 13:45:30.695206 98538 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 13:45:30.695335 98538 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 13:45:30.760773 92417 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 13:45:30.760862 92417 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 13:45:30.760911 92417 solver.cpp:291] [1] Iteration 23, loss = 3.10248
I1226 13:45:30.760987 92417 solver.cpp:317]     Train net output #0: accuracy = 0.375
I1226 13:45:30.761061 92417 solver.cpp:317]     Train net output #1: loss = 3.10248 (* 1 = 3.10248 loss)
I1226 13:45:30.830601 91279 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 13:45:30.830690 91279 MultiSolver.cpp:109] [2] PROFILING BEGIN[WaitingToSync]
I1226 13:45:30.830796 91279 solver.cpp:291] [2] Iteration 23, loss = 2.92011
I1226 13:45:30.830873 91279 solver.cpp:317]     Train net output #0: accuracy = 0.375
I1226 13:45:30.830958 91279 solver.cpp:317]     Train net output #1: loss = 2.92011 (* 1 = 2.92011 loss)
I1226 13:45:30.837167 98538 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 13:45:30.837255 98538 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 13:45:30.837340 98538 solver.cpp:291] [3] Iteration 23, loss = 3.46081
I1226 13:45:30.837411 98538 solver.cpp:317]     Train net output #0: accuracy = 0.265625
I1226 13:45:30.837492 98538 solver.cpp:317]     Train net output #1: loss = 3.46081 (* 1 = 3.46081 loss)
I1226 13:45:31.231108 97018 MultiSolver.cpp:108] [7] PROFILING END[Backward]
I1226 13:45:31.231299 97018 MultiSolver.cpp:109] [7] PROFILING BEGIN[WaitingToSync]
I1226 13:45:31.231359 97018 solver.cpp:291] [7] Iteration 6, loss = 3.03879
I1226 13:45:31.231441 97018 solver.cpp:317]     Train net output #0: accuracy = 0.328125
I1226 13:45:31.231520 97018 solver.cpp:317]     Train net output #1: loss = 3.03879 (* 1 = 3.03879 loss)
I1226 13:45:31.838634 94226 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 13:45:31.838731 94226 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 13:45:31.933521 94226 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 13:45:31.933610 94226 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 13:45:31.933645 94226 solver.cpp:291] [0] Iteration 24, loss = 3.66876
I1226 13:45:31.933698 94226 solver.cpp:317]     Train net output #0: accuracy = 0.3125
I1226 13:45:31.933749 94226 solver.cpp:317]     Train net output #1: loss = 3.66876 (* 1 = 3.66876 loss)
I1226 13:45:32.311249 92490 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 13:45:32.311337 92490 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 13:45:32.327077 91351 SynchronousNode.cpp:246] [2] PROFILING END[WaitingToSync]
I1226 13:45:32.327162 91351 SynchronousNode.cpp:247] [2] PROFILING BEGIN[Forward]
I1226 13:45:32.323851 92417 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 13:45:32.339592 91279 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 13:45:32.324009 92417 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 13:45:32.339712 91279 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 13:45:32.341583 98611 SynchronousNode.cpp:246] [3] PROFILING END[WaitingToSync]
I1226 13:45:32.341670 98611 SynchronousNode.cpp:247] [3] PROFILING BEGIN[Forward]
I1226 13:45:32.355224 98538 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 13:45:32.355337 98538 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 13:45:32.415068 92417 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 13:45:32.415184 92417 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 13:45:32.415320 92417 solver.cpp:291] [1] Iteration 24, loss = 3.4751
I1226 13:45:32.415392 92417 solver.cpp:317]     Train net output #0: accuracy = 0.296875
I1226 13:45:32.415457 92417 solver.cpp:317]     Train net output #1: loss = 3.4751 (* 1 = 3.4751 loss)
I1226 13:45:32.485615 91279 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 13:45:32.485707 91279 MultiSolver.cpp:109] [2] PROFILING BEGIN[WaitingToSync]
I1226 13:45:32.485756 91279 solver.cpp:291] [2] Iteration 24, loss = 3.80879
I1226 13:45:32.485831 91279 solver.cpp:317]     Train net output #0: accuracy = 0.265625
I1226 13:45:32.485906 91279 solver.cpp:317]     Train net output #1: loss = 3.80879 (* 1 = 3.80879 loss)
I1226 13:45:32.498900 98538 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 13:45:32.498998 98538 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 13:45:32.499114 98538 solver.cpp:291] [3] Iteration 24, loss = 3.45046
I1226 13:45:32.499191 98538 solver.cpp:317]     Train net output #0: accuracy = 0.359375
I1226 13:45:32.499269 98538 solver.cpp:317]     Train net output #1: loss = 3.45046 (* 1 = 3.45046 loss)
I1226 13:45:33.504361 94226 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 13:45:33.504504 94226 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 13:45:33.620658 94226 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 13:45:33.620734 94226 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 13:45:33.620811 94226 solver.cpp:291] [0] Iteration 25, loss = 3.70299
I1226 13:45:33.620867 94226 solver.cpp:317]     Train net output #0: accuracy = 0.21875
I1226 13:45:33.620918 94226 solver.cpp:317]     Train net output #1: loss = 3.70299 (* 1 = 3.70299 loss)
I1226 13:45:33.976328 92490 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 13:45:33.976416 92490 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 13:45:33.992099 91351 SynchronousNode.cpp:246] [2] PROFILING END[WaitingToSync]
I1226 13:45:33.992185 91351 SynchronousNode.cpp:247] [2] PROFILING BEGIN[Forward]
I1226 13:45:33.990519 98611 SynchronousNode.cpp:246] [3] PROFILING END[WaitingToSync]
I1226 13:45:33.990613 98611 SynchronousNode.cpp:247] [3] PROFILING BEGIN[Forward]
I1226 13:45:33.988951 92417 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 13:45:34.004600 91279 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 13:45:33.989064 92417 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 13:45:34.004712 91279 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 13:45:34.002549 98538 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 13:45:34.002681 98538 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 13:45:34.071941 92417 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 13:45:34.072024 92417 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 13:45:34.072068 92417 solver.cpp:291] [1] Iteration 25, loss = 3.30583
I1226 13:45:34.072134 92417 solver.cpp:317]     Train net output #0: accuracy = 0.296875
I1226 13:45:34.072199 92417 solver.cpp:317]     Train net output #1: loss = 3.30583 (* 1 = 3.30583 loss)
I1226 13:45:34.151501 91279 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 13:45:34.151592 91279 MultiSolver.cpp:109] [2] PROFILING BEGIN[WaitingToSync]
I1226 13:45:34.151643 91279 solver.cpp:291] [2] Iteration 25, loss = 3.39509
I1226 13:45:34.151716 91279 solver.cpp:317]     Train net output #0: accuracy = 0.296875
I1226 13:45:34.151793 91279 solver.cpp:317]     Train net output #1: loss = 3.39509 (* 1 = 3.39509 loss)
I1226 13:45:34.152505 98538 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 13:45:34.152593 98538 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 13:45:34.152647 98538 solver.cpp:291] [3] Iteration 25, loss = 3.11603
I1226 13:45:34.152720 98538 solver.cpp:317]     Train net output #0: accuracy = 0.28125
I1226 13:45:34.152796 98538 solver.cpp:317]     Train net output #1: loss = 3.11603 (* 1 = 3.11603 loss)
I1226 13:45:35.160006 94226 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 13:45:35.160110 94226 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 13:45:35.251075 94226 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 13:45:35.251153 94226 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 13:45:35.251188 94226 solver.cpp:291] [0] Iteration 26, loss = 3.5029
I1226 13:45:35.251241 94226 solver.cpp:317]     Train net output #0: accuracy = 0.296875
I1226 13:45:35.251291 94226 solver.cpp:317]     Train net output #1: loss = 3.5029 (* 1 = 3.5029 loss)
I1226 13:45:35.636667 92490 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 13:45:35.652425 91351 SynchronousNode.cpp:246] [2] PROFILING END[WaitingToSync]
I1226 13:45:35.636757 92490 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 13:45:35.652513 91351 SynchronousNode.cpp:247] [2] PROFILING BEGIN[Forward]
I1226 13:45:35.649314 92417 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 13:45:35.665134 91279 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 13:45:35.649417 92417 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 13:45:35.665271 91279 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 13:45:35.665226 98611 SynchronousNode.cpp:246] [3] PROFILING END[WaitingToSync]
I1226 13:45:35.665310 98611 SynchronousNode.cpp:247] [3] PROFILING BEGIN[Forward]
I1226 13:45:35.677660 98538 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 13:45:35.677775 98538 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 13:45:35.740809 92417 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 13:45:35.740903 92417 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 13:45:35.740944 92417 solver.cpp:291] [1] Iteration 26, loss = 3.04143
I1226 13:45:35.741009 92417 solver.cpp:317]     Train net output #0: accuracy = 0.390625
I1226 13:45:35.741073 92417 solver.cpp:317]     Train net output #1: loss = 3.04143 (* 1 = 3.04143 loss)
I1226 13:45:35.812366 91279 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 13:45:35.812460 91279 MultiSolver.cpp:109] [2] PROFILING BEGIN[WaitingToSync]
I1226 13:45:35.812518 91279 solver.cpp:291] [2] Iteration 26, loss = 2.97056
I1226 13:45:35.812590 91279 solver.cpp:317]     Train net output #0: accuracy = 0.296875
I1226 13:45:35.812675 91279 solver.cpp:317]     Train net output #1: loss = 2.97056 (* 1 = 2.97056 loss)
I1226 13:45:35.819191 98538 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 13:45:35.819280 98538 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 13:45:35.819329 98538 solver.cpp:291] [3] Iteration 26, loss = 3.43522
I1226 13:45:35.819402 98538 solver.cpp:317]     Train net output #0: accuracy = 0.3125
I1226 13:45:35.819478 98538 solver.cpp:317]     Train net output #1: loss = 3.43522 (* 1 = 3.43522 loss)
I1226 13:45:36.188163 94885 MultiSolver.cpp:92] [4] PROFILING END[Forward]
I1226 13:45:36.188524 94885 MultiSolver.cpp:94] [4] PROFILING BEGIN[Backward]
I1226 13:45:36.827348 94226 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 13:45:36.827493 94226 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 13:45:36.919797 94226 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 13:45:36.919869 94226 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 13:45:36.919903 94226 solver.cpp:291] [0] Iteration 27, loss = 3.17762
I1226 13:45:36.919956 94226 solver.cpp:317]     Train net output #0: accuracy = 0.34375
I1226 13:45:36.920007 94226 solver.cpp:317]     Train net output #1: loss = 3.17762 (* 1 = 3.17762 loss)
I1226 13:45:37.083933 94885 MultiSolver.cpp:108] [4] PROFILING END[Backward]
I1226 13:45:37.084017 94885 MultiSolver.cpp:109] [4] PROFILING BEGIN[WaitingToSync]
I1226 13:45:37.084058 94885 solver.cpp:291] [4] Iteration 7, loss = 3.69655
I1226 13:45:37.084121 94885 solver.cpp:317]     Train net output #0: accuracy = 0.234375
I1226 13:45:37.084188 94885 solver.cpp:317]     Train net output #1: loss = 3.69655 (* 1 = 3.69655 loss)
I1226 13:45:37.100308 86057 SynchronousNode.cpp:246] [5] PROFILING END[WaitingToSync]
I1226 13:45:37.100392 86057 SynchronousNode.cpp:247] [5] PROFILING BEGIN[Forward]
I1226 13:45:37.086974 91179 SynchronousNode.cpp:246] [6] PROFILING END[WaitingToSync]
I1226 13:45:37.087052 91179 SynchronousNode.cpp:247] [6] PROFILING BEGIN[Forward]
I1226 13:45:37.095469 97087 SynchronousNode.cpp:246] [7] PROFILING END[WaitingToSync]
I1226 13:45:37.095803 97087 SynchronousNode.cpp:247] [7] PROFILING BEGIN[Forward]
I1226 13:45:37.112813 85984 MultiSolver.cpp:92] [5] PROFILING END[Forward]
I1226 13:45:37.112913 85984 MultiSolver.cpp:94] [5] PROFILING BEGIN[Backward]
I1226 13:45:37.099961 91110 MultiSolver.cpp:92] [6] PROFILING END[Forward]
I1226 13:45:37.100108 91110 MultiSolver.cpp:94] [6] PROFILING BEGIN[Backward]
I1226 13:45:37.170466 97018 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 13:45:37.170583 97018 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 13:45:37.214385 85984 MultiSolver.cpp:108] [5] PROFILING END[Backward]
I1226 13:45:37.214464 85984 MultiSolver.cpp:109] [5] PROFILING BEGIN[WaitingToSync]
I1226 13:45:37.214532 85984 solver.cpp:291] [5] Iteration 7, loss = 2.93395
I1226 13:45:37.214598 85984 solver.cpp:317]     Train net output #0: accuracy = 0.328125
I1226 13:45:37.214826 85984 solver.cpp:317]     Train net output #1: loss = 2.93395 (* 1 = 2.93395 loss)
I1226 13:45:37.264459 91110 MultiSolver.cpp:108] [6] PROFILING END[Backward]
I1226 13:45:37.264544 91110 MultiSolver.cpp:109] [6] PROFILING BEGIN[WaitingToSync]
I1226 13:45:37.264600 91110 solver.cpp:291] [6] Iteration 7, loss = 2.957
I1226 13:45:37.264672 91110 solver.cpp:317]     Train net output #0: accuracy = 0.34375
I1226 13:45:37.264747 91110 solver.cpp:317]     Train net output #1: loss = 2.957 (* 1 = 2.957 loss)
I1226 13:45:37.308303 92490 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 13:45:37.324111 91351 SynchronousNode.cpp:246] [2] PROFILING END[WaitingToSync]
I1226 13:45:37.324199 91351 SynchronousNode.cpp:247] [2] PROFILING BEGIN[Forward]
I1226 13:45:37.308393 92490 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 13:45:37.336638 91279 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 13:45:37.336747 91279 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 13:45:37.330232 98611 SynchronousNode.cpp:246] [3] PROFILING END[WaitingToSync]
I1226 13:45:37.330318 98611 SynchronousNode.cpp:247] [3] PROFILING BEGIN[Forward]
I1226 13:45:37.327002 92417 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 13:45:37.327100 92417 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 13:45:37.342782 98538 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 13:45:37.342892 98538 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 13:45:37.484580 91279 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 13:45:37.484668 91279 MultiSolver.cpp:109] [2] PROFILING BEGIN[WaitingToSync]
I1226 13:45:37.484719 91279 solver.cpp:291] [2] Iteration 27, loss = 3.11905
I1226 13:45:37.484793 91279 solver.cpp:317]     Train net output #0: accuracy = 0.265625
I1226 13:45:37.484869 91279 solver.cpp:317]     Train net output #1: loss = 3.11905 (* 1 = 3.11905 loss)
I1226 13:45:37.489027 98538 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 13:45:37.489151 98538 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 13:45:37.489202 98538 solver.cpp:291] [3] Iteration 27, loss = 3.22014
I1226 13:45:37.489274 98538 solver.cpp:317]     Train net output #0: accuracy = 0.21875
I1226 13:45:37.489349 98538 solver.cpp:317]     Train net output #1: loss = 3.22014 (* 1 = 3.22014 loss)
I1226 13:45:37.520284 92417 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 13:45:37.520381 92417 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 13:45:37.520431 92417 solver.cpp:291] [1] Iteration 27, loss = 3.43921
I1226 13:45:37.520555 92417 solver.cpp:317]     Train net output #0: accuracy = 0.359375
I1226 13:45:37.520639 92417 solver.cpp:317]     Train net output #1: loss = 3.43921 (* 1 = 3.43921 loss)
I1226 13:45:38.082679 97018 MultiSolver.cpp:108] [7] PROFILING END[Backward]
I1226 13:45:38.082783 97018 MultiSolver.cpp:109] [7] PROFILING BEGIN[WaitingToSync]
I1226 13:45:38.082839 97018 solver.cpp:291] [7] Iteration 7, loss = 3.19627
I1226 13:45:38.082911 97018 solver.cpp:317]     Train net output #0: accuracy = 0.34375
I1226 13:45:38.082994 97018 solver.cpp:317]     Train net output #1: loss = 3.19627 (* 1 = 3.19627 loss)
I1226 13:45:38.486059 94226 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 13:45:38.486161 94226 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 13:45:38.581756 94226 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 13:45:38.581826 94226 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 13:45:38.581859 94226 solver.cpp:291] [0] Iteration 28, loss = 3.44966
I1226 13:45:38.581914 94226 solver.cpp:317]     Train net output #0: accuracy = 0.25
I1226 13:45:38.581964 94226 solver.cpp:317]     Train net output #1: loss = 3.44966 (* 1 = 3.44966 loss)
I1226 13:45:38.955665 92490 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 13:45:38.971397 91351 SynchronousNode.cpp:246] [2] PROFILING END[WaitingToSync]
I1226 13:45:38.971482 91351 SynchronousNode.cpp:247] [2] PROFILING BEGIN[Forward]
I1226 13:45:38.955751 92490 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 13:45:38.968096 92417 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 13:45:38.983815 91279 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 13:45:38.968210 92417 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 13:45:38.983925 91279 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 13:45:38.982501 98611 SynchronousNode.cpp:246] [3] PROFILING END[WaitingToSync]
I1226 13:45:38.982584 98611 SynchronousNode.cpp:247] [3] PROFILING BEGIN[Forward]
I1226 13:45:38.995173 98538 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 13:45:38.995282 98538 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 13:45:39.057948 92417 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 13:45:39.058064 92417 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 13:45:39.058146 92417 solver.cpp:291] [1] Iteration 28, loss = 3.21381
I1226 13:45:39.058375 92417 solver.cpp:317]     Train net output #0: accuracy = 0.3125
I1226 13:45:39.058440 92417 solver.cpp:317]     Train net output #1: loss = 3.21381 (* 1 = 3.21381 loss)
I1226 13:45:39.127513 91279 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 13:45:39.127606 91279 MultiSolver.cpp:109] [2] PROFILING BEGIN[WaitingToSync]
I1226 13:45:39.127656 91279 solver.cpp:291] [2] Iteration 28, loss = 3.07256
I1226 13:45:39.127732 91279 solver.cpp:317]     Train net output #0: accuracy = 0.296875
I1226 13:45:39.127809 91279 solver.cpp:317]     Train net output #1: loss = 3.07256 (* 1 = 3.07256 loss)
I1226 13:45:39.137666 98538 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 13:45:39.137758 98538 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 13:45:39.137810 98538 solver.cpp:291] [3] Iteration 28, loss = 3.53505
I1226 13:45:39.137887 98538 solver.cpp:317]     Train net output #0: accuracy = 0.265625
I1226 13:45:39.137962 98538 solver.cpp:317]     Train net output #1: loss = 3.53505 (* 1 = 3.53505 loss)
I1226 13:45:40.133138 94226 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 13:45:40.133239 94226 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 13:45:40.224189 94226 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 13:45:40.224304 94226 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 13:45:40.224356 94226 solver.cpp:291] [0] Iteration 29, loss = 3.52882
I1226 13:45:40.224458 94226 solver.cpp:317]     Train net output #0: accuracy = 0.28125
I1226 13:45:40.224509 94226 solver.cpp:317]     Train net output #1: loss = 3.52882 (* 1 = 3.52882 loss)
I1226 13:45:40.615429 92490 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 13:45:40.631258 91351 SynchronousNode.cpp:246] [2] PROFILING END[WaitingToSync]
I1226 13:45:40.615543 92490 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 13:45:40.631345 91351 SynchronousNode.cpp:247] [2] PROFILING BEGIN[Forward]
I1226 13:45:40.628274 92417 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 13:45:40.643889 91279 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 13:45:40.644013 91279 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 13:45:40.628376 92417 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 13:45:40.637145 98611 SynchronousNode.cpp:246] [3] PROFILING END[WaitingToSync]
I1226 13:45:40.637230 98611 SynchronousNode.cpp:247] [3] PROFILING BEGIN[Forward]
I1226 13:45:40.649636 98538 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 13:45:40.649745 98538 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 13:45:40.716862 92417 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 13:45:40.716977 92417 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 13:45:40.717031 92417 solver.cpp:291] [1] Iteration 29, loss = 3.29932
I1226 13:45:40.717106 92417 solver.cpp:317]     Train net output #0: accuracy = 0.265625
I1226 13:45:40.717192 92417 solver.cpp:317]     Train net output #1: loss = 3.29932 (* 1 = 3.29932 loss)
I1226 13:45:40.787112 91279 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 13:45:40.787245 91279 MultiSolver.cpp:109] [2] PROFILING BEGIN[WaitingToSync]
I1226 13:45:40.787297 91279 solver.cpp:291] [2] Iteration 29, loss = 3.07034
I1226 13:45:40.787370 91279 solver.cpp:317]     Train net output #0: accuracy = 0.375
I1226 13:45:40.787448 91279 solver.cpp:317]     Train net output #1: loss = 3.07034 (* 1 = 3.07034 loss)
I1226 13:45:40.788837 98538 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 13:45:40.788933 98538 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 13:45:40.788985 98538 solver.cpp:291] [3] Iteration 29, loss = 3.3333
I1226 13:45:40.789059 98538 solver.cpp:317]     Train net output #0: accuracy = 0.25
I1226 13:45:40.789163 98538 solver.cpp:317]     Train net output #1: loss = 3.3333 (* 1 = 3.3333 loss)
I1226 13:45:41.799541 94226 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 13:45:41.799644 94226 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 13:45:41.892261 94226 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 13:45:41.892405 94226 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 13:45:41.892442 94226 solver.cpp:291] [0] Iteration 30, loss = 3.47222
I1226 13:45:41.892496 94226 solver.cpp:317]     Train net output #0: accuracy = 0.3125
I1226 13:45:41.892547 94226 solver.cpp:317]     Train net output #1: loss = 3.47222 (* 1 = 3.47222 loss)
I1226 13:45:42.289201 91351 SynchronousNode.cpp:246] [2] PROFILING END[WaitingToSync]
I1226 13:45:42.273437 92490 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 13:45:42.289319 91351 SynchronousNode.cpp:247] [2] PROFILING BEGIN[Forward]
I1226 13:45:42.273648 92490 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 13:45:42.285958 92417 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 13:45:42.301851 91279 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 13:45:42.286056 92417 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 13:45:42.301962 91279 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 13:45:42.292300 98611 SynchronousNode.cpp:246] [3] PROFILING END[WaitingToSync]
I1226 13:45:42.292384 98611 SynchronousNode.cpp:247] [3] PROFILING BEGIN[Forward]
I1226 13:45:42.305022 98538 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 13:45:42.305164 98538 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 13:45:42.371744 92417 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 13:45:42.371836 92417 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 13:45:42.371888 92417 solver.cpp:291] [1] Iteration 30, loss = 2.91209
I1226 13:45:42.371963 92417 solver.cpp:317]     Train net output #0: accuracy = 0.328125
I1226 13:45:42.372040 92417 solver.cpp:317]     Train net output #1: loss = 2.91209 (* 1 = 2.91209 loss)
I1226 13:45:42.443809 91279 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 13:45:42.443907 91279 MultiSolver.cpp:109] [2] PROFILING BEGIN[WaitingToSync]
I1226 13:45:42.443958 91279 solver.cpp:291] [2] Iteration 30, loss = 3.42989
I1226 13:45:42.444028 91279 solver.cpp:317]     Train net output #0: accuracy = 0.296875
I1226 13:45:42.444105 91279 solver.cpp:317]     Train net output #1: loss = 3.42989 (* 1 = 3.42989 loss)
I1226 13:45:42.444561 98538 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 13:45:42.444649 98538 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 13:45:42.444705 98538 solver.cpp:291] [3] Iteration 30, loss = 3.2427
I1226 13:45:42.444777 98538 solver.cpp:317]     Train net output #0: accuracy = 0.359375
I1226 13:45:42.444852 98538 solver.cpp:317]     Train net output #1: loss = 3.2427 (* 1 = 3.2427 loss)
I1226 13:45:43.123603 94885 MultiSolver.cpp:92] [4] PROFILING END[Forward]
I1226 13:45:43.123715 94885 MultiSolver.cpp:94] [4] PROFILING BEGIN[Backward]
I1226 13:45:43.465929 94226 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 13:45:43.466027 94226 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 13:45:43.559989 94226 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 13:45:43.560061 94226 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 13:45:43.560096 94226 solver.cpp:291] [0] Iteration 31, loss = 3.41101
I1226 13:45:43.560149 94226 solver.cpp:317]     Train net output #0: accuracy = 0.3125
I1226 13:45:43.560199 94226 solver.cpp:317]     Train net output #1: loss = 3.41101 (* 1 = 3.41101 loss)
I1226 13:45:43.932420 92490 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 13:45:43.932535 92490 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 13:45:43.948191 91351 SynchronousNode.cpp:246] [2] PROFILING END[WaitingToSync]
I1226 13:45:43.948518 91351 SynchronousNode.cpp:247] [2] PROFILING BEGIN[Forward]
I1226 13:45:43.960851 91279 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 13:45:43.960965 91279 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 13:45:43.946071 92417 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 13:45:43.946172 92417 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 13:45:43.957803 98611 SynchronousNode.cpp:246] [3] PROFILING END[WaitingToSync]
I1226 13:45:43.957888 98611 SynchronousNode.cpp:247] [3] PROFILING BEGIN[Forward]
I1226 13:45:43.970427 98538 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 13:45:43.970538 98538 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 13:45:44.018754 94885 MultiSolver.cpp:108] [4] PROFILING END[Backward]
I1226 13:45:44.018985 94885 MultiSolver.cpp:109] [4] PROFILING BEGIN[WaitingToSync]
I1226 13:45:44.019037 94885 solver.cpp:291] [4] Iteration 8, loss = 3.32401
I1226 13:45:44.019135 94885 solver.cpp:317]     Train net output #0: accuracy = 0.3125
I1226 13:45:44.019212 94885 solver.cpp:317]     Train net output #1: loss = 3.32401 (* 1 = 3.32401 loss)
I1226 13:45:44.033131 92417 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 13:45:44.033210 92417 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 13:45:44.033278 92417 solver.cpp:291] [1] Iteration 31, loss = 3.16162
I1226 13:45:44.033341 92417 solver.cpp:317]     Train net output #0: accuracy = 0.296875
I1226 13:45:44.033419 92417 solver.cpp:317]     Train net output #1: loss = 3.16162 (* 1 = 3.16162 loss)
I1226 13:45:44.078297 86057 SynchronousNode.cpp:246] [5] PROFILING END[WaitingToSync]
I1226 13:45:44.064939 91179 SynchronousNode.cpp:246] [6] PROFILING END[WaitingToSync]
I1226 13:45:44.078387 86057 SynchronousNode.cpp:247] [5] PROFILING BEGIN[Forward]
I1226 13:45:44.065024 91179 SynchronousNode.cpp:247] [6] PROFILING BEGIN[Forward]
I1226 13:45:44.073488 97087 SynchronousNode.cpp:246] [7] PROFILING END[WaitingToSync]
I1226 13:45:44.073575 97087 SynchronousNode.cpp:247] [7] PROFILING BEGIN[Forward]
I1226 13:45:44.102432 91279 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 13:45:44.102527 91279 MultiSolver.cpp:109] [2] PROFILING BEGIN[WaitingToSync]
I1226 13:45:44.102586 91279 solver.cpp:291] [2] Iteration 31, loss = 3.41977
I1226 13:45:44.102658 91279 solver.cpp:317]     Train net output #0: accuracy = 0.34375
I1226 13:45:44.102903 91279 solver.cpp:317]     Train net output #1: loss = 3.41977 (* 1 = 3.41977 loss)
I1226 13:45:44.090934 85984 MultiSolver.cpp:92] [5] PROFILING END[Forward]
I1226 13:45:44.091039 85984 MultiSolver.cpp:94] [5] PROFILING BEGIN[Backward]
I1226 13:45:44.077823 91110 MultiSolver.cpp:92] [6] PROFILING END[Forward]
I1226 13:45:44.077940 91110 MultiSolver.cpp:94] [6] PROFILING BEGIN[Backward]
I1226 13:45:44.119408 98538 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 13:45:44.119495 98538 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 13:45:44.119549 98538 solver.cpp:291] [3] Iteration 31, loss = 3.0795
I1226 13:45:44.119652 98538 solver.cpp:317]     Train net output #0: accuracy = 0.3125
I1226 13:45:44.119900 98538 solver.cpp:317]     Train net output #1: loss = 3.0795 (* 1 = 3.0795 loss)
I1226 13:45:44.146425 97018 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 13:45:44.146541 97018 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 13:45:44.190948 85984 MultiSolver.cpp:108] [5] PROFILING END[Backward]
I1226 13:45:44.191030 85984 MultiSolver.cpp:109] [5] PROFILING BEGIN[WaitingToSync]
I1226 13:45:44.191072 85984 solver.cpp:291] [5] Iteration 8, loss = 2.97117
I1226 13:45:44.191138 85984 solver.cpp:317]     Train net output #0: accuracy = 0.359375
I1226 13:45:44.191205 85984 solver.cpp:317]     Train net output #1: loss = 2.97117 (* 1 = 2.97117 loss)
I1226 13:45:44.239722 91110 MultiSolver.cpp:108] [6] PROFILING END[Backward]
I1226 13:45:44.239809 91110 MultiSolver.cpp:109] [6] PROFILING BEGIN[WaitingToSync]
I1226 13:45:44.239857 91110 solver.cpp:291] [6] Iteration 8, loss = 3.59665
I1226 13:45:44.239930 91110 solver.cpp:317]     Train net output #0: accuracy = 0.296875
I1226 13:45:44.240006 91110 solver.cpp:317]     Train net output #1: loss = 3.59665 (* 1 = 3.59665 loss)
I1226 13:45:45.054893 97018 MultiSolver.cpp:108] [7] PROFILING END[Backward]
I1226 13:45:45.054986 97018 MultiSolver.cpp:109] [7] PROFILING BEGIN[WaitingToSync]
I1226 13:45:45.055039 97018 solver.cpp:291] [7] Iteration 8, loss = 3.2069
I1226 13:45:45.055114 97018 solver.cpp:317]     Train net output #0: accuracy = 0.296875
I1226 13:45:45.055188 97018 solver.cpp:317]     Train net output #1: loss = 3.2069 (* 1 = 3.2069 loss)
I1226 13:45:45.116753 94226 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 13:45:45.116859 94226 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 13:45:45.227087 94226 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 13:45:45.227159 94226 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 13:45:45.227254 94226 solver.cpp:291] [0] Iteration 32, loss = 2.97732
I1226 13:45:45.227308 94226 solver.cpp:317]     Train net output #0: accuracy = 0.390625
I1226 13:45:45.227358 94226 solver.cpp:317]     Train net output #1: loss = 2.97732 (* 1 = 2.97732 loss)
I1226 13:45:45.599889 92490 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 13:45:45.615600 91351 SynchronousNode.cpp:246] [2] PROFILING END[WaitingToSync]
I1226 13:45:45.599977 92490 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 13:45:45.615687 91351 SynchronousNode.cpp:247] [2] PROFILING BEGIN[Forward]
I1226 13:45:45.614401 98611 SynchronousNode.cpp:246] [3] PROFILING END[WaitingToSync]
I1226 13:45:45.614492 98611 SynchronousNode.cpp:247] [3] PROFILING BEGIN[Forward]
I1226 13:45:45.612422 92417 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 13:45:45.628198 91279 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 13:45:45.612567 92417 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 13:45:45.628350 91279 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 13:45:45.625339 98538 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 13:45:45.625457 98538 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 13:45:45.701361 92417 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 13:45:45.701449 92417 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 13:45:45.701532 92417 solver.cpp:291] [1] Iteration 32, loss = 3.3348
I1226 13:45:45.701608 92417 solver.cpp:317]     Train net output #0: accuracy = 0.3125
I1226 13:45:45.701692 92417 solver.cpp:317]     Train net output #1: loss = 3.3348 (* 1 = 3.3348 loss)
I1226 13:45:45.770381 91279 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 13:45:45.770476 91279 MultiSolver.cpp:109] [2] PROFILING BEGIN[WaitingToSync]
I1226 13:45:45.770525 91279 solver.cpp:291] [2] Iteration 32, loss = 3.07558
I1226 13:45:45.770599 91279 solver.cpp:317]     Train net output #0: accuracy = 0.328125
I1226 13:45:45.770678 91279 solver.cpp:317]     Train net output #1: loss = 3.07558 (* 1 = 3.07558 loss)
I1226 13:45:45.765476 98538 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 13:45:45.765611 98538 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 13:45:45.765663 98538 solver.cpp:291] [3] Iteration 32, loss = 3.41721
I1226 13:45:45.765739 98538 solver.cpp:317]     Train net output #0: accuracy = 0.28125
I1226 13:45:45.765820 98538 solver.cpp:317]     Train net output #1: loss = 3.41721 (* 1 = 3.41721 loss)
I1226 13:45:46.788015 94226 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 13:45:46.788172 94226 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 13:45:46.876806 94226 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 13:45:46.876878 94226 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 13:45:46.876914 94226 solver.cpp:291] [0] Iteration 33, loss = 3.29599
I1226 13:45:46.876967 94226 solver.cpp:317]     Train net output #0: accuracy = 0.328125
I1226 13:45:46.877017 94226 solver.cpp:317]     Train net output #1: loss = 3.29599 (* 1 = 3.29599 loss)
I1226 13:45:47.253556 92490 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 13:45:47.269320 91351 SynchronousNode.cpp:246] [2] PROFILING END[WaitingToSync]
I1226 13:45:47.253643 92490 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 13:45:47.269404 91351 SynchronousNode.cpp:247] [2] PROFILING BEGIN[Forward]
I1226 13:45:47.266288 92417 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 13:45:47.266433 92417 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 13:45:47.282105 91279 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 13:45:47.282263 91279 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 13:45:47.282449 98611 SynchronousNode.cpp:246] [3] PROFILING END[WaitingToSync]
I1226 13:45:47.282531 98611 SynchronousNode.cpp:247] [3] PROFILING BEGIN[Forward]
I1226 13:45:47.295114 98538 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 13:45:47.295233 98538 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 13:45:47.357625 92417 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 13:45:47.357700 92417 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 13:45:47.357741 92417 solver.cpp:291] [1] Iteration 33, loss = 3.23303
I1226 13:45:47.357805 92417 solver.cpp:317]     Train net output #0: accuracy = 0.375
I1226 13:45:47.357869 92417 solver.cpp:317]     Train net output #1: loss = 3.23303 (* 1 = 3.23303 loss)
I1226 13:45:47.424692 91279 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 13:45:47.424785 91279 MultiSolver.cpp:109] [2] PROFILING BEGIN[WaitingToSync]
I1226 13:45:47.424836 91279 solver.cpp:291] [2] Iteration 33, loss = 2.57422
I1226 13:45:47.424909 91279 solver.cpp:317]     Train net output #0: accuracy = 0.4375
I1226 13:45:47.424986 91279 solver.cpp:317]     Train net output #1: loss = 2.57422 (* 1 = 2.57422 loss)
I1226 13:45:47.435191 98538 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 13:45:47.435284 98538 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 13:45:47.435340 98538 solver.cpp:291] [3] Iteration 33, loss = 3.04385
I1226 13:45:47.435415 98538 solver.cpp:317]     Train net output #0: accuracy = 0.34375
I1226 13:45:47.435490 98538 solver.cpp:317]     Train net output #1: loss = 3.04385 (* 1 = 3.04385 loss)
I1226 13:45:48.448014 94226 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 13:45:48.448117 94226 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 13:45:48.541311 94226 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 13:45:48.541453 94226 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 13:45:48.541487 94226 solver.cpp:291] [0] Iteration 34, loss = 3.0449
I1226 13:45:48.541543 94226 solver.cpp:317]     Train net output #0: accuracy = 0.390625
I1226 13:45:48.541594 94226 solver.cpp:317]     Train net output #1: loss = 3.0449 (* 1 = 3.0449 loss)
I1226 13:45:48.919123 92490 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 13:45:48.934850 91351 SynchronousNode.cpp:246] [2] PROFILING END[WaitingToSync]
I1226 13:45:48.919211 92490 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 13:45:48.934938 91351 SynchronousNode.cpp:247] [2] PROFILING BEGIN[Forward]
I1226 13:45:48.947350 91279 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 13:45:48.947460 91279 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 13:45:48.931813 92417 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 13:45:48.931926 92417 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 13:45:48.941162 98611 SynchronousNode.cpp:246] [3] PROFILING END[WaitingToSync]
I1226 13:45:48.941247 98611 SynchronousNode.cpp:247] [3] PROFILING BEGIN[Forward]
I1226 13:45:48.953716 98538 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 13:45:48.953824 98538 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 13:45:49.021440 92417 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 13:45:49.021618 92417 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 13:45:49.021678 92417 solver.cpp:291] [1] Iteration 34, loss = 3.59168
I1226 13:45:49.021759 92417 solver.cpp:317]     Train net output #0: accuracy = 0.25
I1226 13:45:49.021839 92417 solver.cpp:317]     Train net output #1: loss = 3.59168 (* 1 = 3.59168 loss)
I1226 13:45:49.095983 91279 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 13:45:49.096074 91279 MultiSolver.cpp:109] [2] PROFILING BEGIN[WaitingToSync]
I1226 13:45:49.096128 91279 solver.cpp:291] [2] Iteration 34, loss = 3.14174
I1226 13:45:49.096201 91279 solver.cpp:317]     Train net output #0: accuracy = 0.34375
I1226 13:45:49.096319 91279 solver.cpp:317]     Train net output #1: loss = 3.14174 (* 1 = 3.14174 loss)
I1226 13:45:49.093966 98538 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 13:45:49.094055 98538 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 13:45:49.094167 98538 solver.cpp:291] [3] Iteration 34, loss = 3.12682
I1226 13:45:49.094244 98538 solver.cpp:317]     Train net output #0: accuracy = 0.328125
I1226 13:45:49.094321 98538 solver.cpp:317]     Train net output #1: loss = 3.12682 (* 1 = 3.12682 loss)
I1226 13:45:50.180559 94885 MultiSolver.cpp:92] [4] PROFILING END[Forward]
I1226 13:45:50.180663 94885 MultiSolver.cpp:94] [4] PROFILING BEGIN[Backward]
I1226 13:45:50.170459 94226 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 13:45:50.170555 94226 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 13:45:50.259860 94226 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 13:45:50.260280 94226 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 13:45:50.260408 94226 solver.cpp:291] [0] Iteration 35, loss = 3.19883
I1226 13:45:50.260617 94226 solver.cpp:317]     Train net output #0: accuracy = 0.328125
I1226 13:45:50.260668 94226 solver.cpp:317]     Train net output #1: loss = 3.19883 (* 1 = 3.19883 loss)
I1226 13:45:50.646215 92490 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 13:45:50.661901 91351 SynchronousNode.cpp:246] [2] PROFILING END[WaitingToSync]
I1226 13:45:50.661984 91351 SynchronousNode.cpp:247] [2] PROFILING BEGIN[Forward]
I1226 13:45:50.646303 92490 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 13:45:50.658848 92417 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 13:45:50.674684 91279 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 13:45:50.658962 92417 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 13:45:50.674793 91279 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 13:45:50.679323 98611 SynchronousNode.cpp:246] [3] PROFILING END[WaitingToSync]
I1226 13:45:50.679400 98611 SynchronousNode.cpp:247] [3] PROFILING BEGIN[Forward]
I1226 13:45:50.691908 98538 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 13:45:50.692036 98538 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 13:45:50.746716 92417 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 13:45:50.746912 92417 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 13:45:50.746958 92417 solver.cpp:291] [1] Iteration 35, loss = 3.92523
I1226 13:45:50.747045 92417 solver.cpp:317]     Train net output #0: accuracy = 0.21875
I1226 13:45:50.747273 92417 solver.cpp:317]     Train net output #1: loss = 3.92523 (* 1 = 3.92523 loss)
I1226 13:45:50.816956 91279 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 13:45:50.817046 91279 MultiSolver.cpp:109] [2] PROFILING BEGIN[WaitingToSync]
I1226 13:45:50.817095 91279 solver.cpp:291] [2] Iteration 35, loss = 3.28282
I1226 13:45:50.817170 91279 solver.cpp:317]     Train net output #0: accuracy = 0.28125
I1226 13:45:50.817288 91279 solver.cpp:317]     Train net output #1: loss = 3.28282 (* 1 = 3.28282 loss)
I1226 13:45:50.837352 98538 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 13:45:50.837466 98538 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 13:45:50.837517 98538 solver.cpp:291] [3] Iteration 35, loss = 3.97854
I1226 13:45:50.837590 98538 solver.cpp:317]     Train net output #0: accuracy = 0.21875
I1226 13:45:50.837671 98538 solver.cpp:317]     Train net output #1: loss = 3.97854 (* 1 = 3.97854 loss)
I1226 13:45:50.990298 86057 SynchronousNode.cpp:246] [5] PROFILING END[WaitingToSync]
I1226 13:45:50.990381 86057 SynchronousNode.cpp:247] [5] PROFILING BEGIN[Forward]
I1226 13:45:50.976986 91179 SynchronousNode.cpp:246] [6] PROFILING END[WaitingToSync]
I1226 13:45:50.977095 91179 SynchronousNode.cpp:247] [6] PROFILING BEGIN[Forward]
I1226 13:45:50.997865 97087 SynchronousNode.cpp:246] [7] PROFILING END[WaitingToSync]
I1226 13:45:51.002959 85984 MultiSolver.cpp:92] [5] PROFILING END[Forward]
I1226 13:45:50.997953 97087 SynchronousNode.cpp:247] [7] PROFILING BEGIN[Forward]
I1226 13:45:51.003065 85984 MultiSolver.cpp:94] [5] PROFILING BEGIN[Backward]
I1226 13:45:50.989837 91110 MultiSolver.cpp:92] [6] PROFILING END[Forward]
I1226 13:45:50.989948 91110 MultiSolver.cpp:94] [6] PROFILING BEGIN[Backward]
I1226 13:45:51.077579 94885 MultiSolver.cpp:108] [4] PROFILING END[Backward]
I1226 13:45:51.077668 94885 MultiSolver.cpp:109] [4] PROFILING BEGIN[WaitingToSync]
I1226 13:45:51.077713 94885 solver.cpp:291] [4] Iteration 9, loss = 2.80848
I1226 13:45:51.077778 94885 solver.cpp:317]     Train net output #0: accuracy = 0.390625
I1226 13:45:51.077888 94885 solver.cpp:317]     Train net output #1: loss = 2.80848 (* 1 = 2.80848 loss)
I1226 13:45:51.071388 97018 MultiSolver.cpp:92] [7] PROFILING END[Forward]
I1226 13:45:51.071506 97018 MultiSolver.cpp:94] [7] PROFILING BEGIN[Backward]
I1226 13:45:51.122745 85984 MultiSolver.cpp:108] [5] PROFILING END[Backward]
I1226 13:45:51.122841 85984 MultiSolver.cpp:109] [5] PROFILING BEGIN[WaitingToSync]
I1226 13:45:51.122885 85984 solver.cpp:291] [5] Iteration 9, loss = 3.12314
I1226 13:45:51.122951 85984 solver.cpp:317]     Train net output #0: accuracy = 0.28125
I1226 13:45:51.123023 85984 solver.cpp:317]     Train net output #1: loss = 3.12314 (* 1 = 3.12314 loss)
I1226 13:45:51.152323 91110 MultiSolver.cpp:108] [6] PROFILING END[Backward]
I1226 13:45:51.152410 91110 MultiSolver.cpp:109] [6] PROFILING BEGIN[WaitingToSync]
I1226 13:45:51.152462 91110 solver.cpp:291] [6] Iteration 9, loss = 3.25566
I1226 13:45:51.152534 91110 solver.cpp:317]     Train net output #0: accuracy = 0.3125
I1226 13:45:51.152609 91110 solver.cpp:317]     Train net output #1: loss = 3.25566 (* 1 = 3.25566 loss)
I1226 13:45:51.807585 94226 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 13:45:51.807689 94226 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 13:45:51.901979 94226 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 13:45:51.902053 94226 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 13:45:51.902086 94226 solver.cpp:291] [0] Iteration 36, loss = 3.58338
I1226 13:45:51.902140 94226 solver.cpp:317]     Train net output #0: accuracy = 0.25
I1226 13:45:51.902189 94226 solver.cpp:317]     Train net output #1: loss = 3.58338 (* 1 = 3.58338 loss)
I1226 13:45:51.982077 97018 MultiSolver.cpp:108] [7] PROFILING END[Backward]
I1226 13:45:51.982167 97018 MultiSolver.cpp:109] [7] PROFILING BEGIN[WaitingToSync]
I1226 13:45:51.982220 97018 solver.cpp:291] [7] Iteration 9, loss = 3.35334
I1226 13:45:51.982291 97018 solver.cpp:317]     Train net output #0: accuracy = 0.296875
I1226 13:45:51.982368 97018 solver.cpp:317]     Train net output #1: loss = 3.35334 (* 1 = 3.35334 loss)
I1226 13:45:52.287392 92490 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 13:45:52.287508 92490 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 13:45:52.303129 91351 SynchronousNode.cpp:246] [2] PROFILING END[WaitingToSync]
I1226 13:45:52.303243 91351 SynchronousNode.cpp:247] [2] PROFILING BEGIN[Forward]
I1226 13:45:52.300068 92417 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 13:45:52.300225 92417 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 13:45:52.316886 91279 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 13:45:52.317034 91279 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 13:45:52.308297 98611 SynchronousNode.cpp:246] [3] PROFILING END[WaitingToSync]
I1226 13:45:52.308383 98611 SynchronousNode.cpp:247] [3] PROFILING BEGIN[Forward]
I1226 13:45:52.322278 98538 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 13:45:52.322397 98538 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 13:45:52.387441 92417 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 13:45:52.387569 92417 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 13:45:52.387611 92417 solver.cpp:291] [1] Iteration 36, loss = 3.75755
I1226 13:45:52.387676 92417 solver.cpp:317]     Train net output #0: accuracy = 0.25
I1226 13:45:52.387740 92417 solver.cpp:317]     Train net output #1: loss = 3.75755 (* 1 = 3.75755 loss)
I1226 13:45:52.463665 91279 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 13:45:52.463758 91279 MultiSolver.cpp:109] [2] PROFILING BEGIN[WaitingToSync]
I1226 13:45:52.463809 91279 solver.cpp:291] [2] Iteration 36, loss = 3.27805
I1226 13:45:52.463883 91279 solver.cpp:317]     Train net output #0: accuracy = 0.265625
I1226 13:45:52.463959 91279 solver.cpp:317]     Train net output #1: loss = 3.27805 (* 1 = 3.27805 loss)
I1226 13:45:52.467667 98538 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 13:45:52.467756 98538 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 13:45:52.467808 98538 solver.cpp:291] [3] Iteration 36, loss = 3.30647
I1226 13:45:52.467881 98538 solver.cpp:317]     Train net output #0: accuracy = 0.296875
I1226 13:45:52.467958 98538 solver.cpp:317]     Train net output #1: loss = 3.30647 (* 1 = 3.30647 loss)
I1226 13:45:53.460961 94226 MultiSolver.cpp:92] [0] PROFILING END[Forward]
I1226 13:45:53.461066 94226 MultiSolver.cpp:94] [0] PROFILING BEGIN[Backward]
I1226 13:45:53.555012 94226 MultiSolver.cpp:108] [0] PROFILING END[Backward]
I1226 13:45:53.555728 94226 MultiSolver.cpp:109] [0] PROFILING BEGIN[WaitingToSync]
I1226 13:45:53.555778 94226 solver.cpp:291] [0] Iteration 37, loss = 3.1706
I1226 13:45:53.555835 94226 solver.cpp:317]     Train net output #0: accuracy = 0.296875
I1226 13:45:53.555886 94226 solver.cpp:317]     Train net output #1: loss = 3.1706 (* 1 = 3.1706 loss)
I1226 13:45:53.933555 92490 SynchronousNode.cpp:246] [1] PROFILING END[WaitingToSync]
I1226 13:45:53.933640 92490 SynchronousNode.cpp:247] [1] PROFILING BEGIN[Forward]
I1226 13:45:53.949300 91351 SynchronousNode.cpp:246] [2] PROFILING END[WaitingToSync]
I1226 13:45:53.949637 91351 SynchronousNode.cpp:247] [2] PROFILING BEGIN[Forward]
I1226 13:45:53.945967 92417 MultiSolver.cpp:92] [1] PROFILING END[Forward]
I1226 13:45:53.961827 91279 MultiSolver.cpp:92] [2] PROFILING END[Forward]
I1226 13:45:53.946074 92417 MultiSolver.cpp:94] [1] PROFILING BEGIN[Backward]
I1226 13:45:53.961936 91279 MultiSolver.cpp:94] [2] PROFILING BEGIN[Backward]
I1226 13:45:53.959600 98611 SynchronousNode.cpp:246] [3] PROFILING END[WaitingToSync]
I1226 13:45:53.960220 98611 SynchronousNode.cpp:247] [3] PROFILING BEGIN[Forward]
I1226 13:45:53.972443 98538 MultiSolver.cpp:92] [3] PROFILING END[Forward]
I1226 13:45:53.972554 98538 MultiSolver.cpp:94] [3] PROFILING BEGIN[Backward]
I1226 13:45:54.037528 92417 MultiSolver.cpp:108] [1] PROFILING END[Backward]
I1226 13:45:54.038188 92417 MultiSolver.cpp:109] [1] PROFILING BEGIN[WaitingToSync]
I1226 13:45:54.038249 92417 solver.cpp:291] [1] Iteration 37, loss = 3.21209
I1226 13:45:54.038319 92417 solver.cpp:317]     Train net output #0: accuracy = 0.328125
I1226 13:45:54.038390 92417 solver.cpp:317]     Train net output #1: loss = 3.21209 (* 1 = 3.21209 loss)
I1226 13:45:54.103451 91279 MultiSolver.cpp:108] [2] PROFILING END[Backward]
I1226 13:45:54.103548 91279 MultiSolver.cpp:109] [2] PROFILING BEGIN[WaitingToSync]
I1226 13:45:54.103601 91279 solver.cpp:291] [2] Iteration 37, loss = 3.65815
I1226 13:45:54.103674 91279 solver.cpp:317]     Train net output #0: accuracy = 0.25
I1226 13:45:54.103760 91279 solver.cpp:317]     Train net output #1: loss = 3.65815 (* 1 = 3.65815 loss)
I1226 13:45:54.125748 98538 MultiSolver.cpp:108] [3] PROFILING END[Backward]
I1226 13:45:54.125836 98538 MultiSolver.cpp:109] [3] PROFILING BEGIN[WaitingToSync]
I1226 13:45:54.125886 98538 solver.cpp:291] [3] Iteration 37, loss = 3.01961
I1226 13:45:54.125959 98538 solver.cpp:317]     Train net output #0: accuracy = 0.375
I1226 13:45:54.126034 98538 solver.cpp:317]     Train net output #1: loss = 3.01961 (* 1 = 3.01961 loss)
User defined signal 2
