Dec 26 07:40:43 2016 89761 3 10.1 NIOS_DEBUG: stdin_fd set to -1
Dec 26 07:40:43 2016 89761 3 10.1 NIOS_DEBUG: fds[0] has a value of -1
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 07:40:46.593484 86513 mpiutil.cpp:166] Process rank 7 from number of 9 processes running on knl-node041
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 07:40:46.594480 88308 mpiutil.cpp:166] Process rank 2 from number of 9 processes running on knl-node053
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 07:40:46.596473 89800 mpiutil.cpp:166] Process rank 6 from number of 9 processes running on knl-node005
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 07:40:46.592419 93793 mpiutil.cpp:166] Process rank 1 from number of 9 processes running on knl-node079
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 07:40:46.590566 89665 mpiutil.cpp:166] Process rank 3 from number of 9 processes running on knl-node038
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 07:40:46.596125 89771 mpiutil.cpp:166] Process rank 0 from number of 9 processes running on knl-node025
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 07:40:43.327610 89666 mpiutil.cpp:166] Process rank 5 from number of 9 processes running on knl-node048
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 07:40:46.587795 92408 mpiutil.cpp:166] Process rank 8 from number of 9 processes running on knl-node084
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1226 07:40:46.577217 90196 mpiutil.cpp:166] Process rank 4 from number of 9 processes running on knl-node029
I1226 07:40:46.605816 93793 caffe.cpp:316] Use CPU.
I1226 07:40:46.605083 89665 caffe.cpp:316] Use CPU.
I1226 07:40:46.608906 88308 caffe.cpp:316] Use CPU.
I1226 07:40:46.607092 93793 solver.cpp:105] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_2.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 07:40:46.608144 93793 solver.cpp:140] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_2.prototxt
I1226 07:40:46.609990 88308 solver.cpp:105] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_3.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 07:40:46.610891 88308 solver.cpp:140] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_3.prototxt
I1226 07:40:46.612242 89800 caffe.cpp:316] Use CPU.
I1226 07:40:46.606246 89665 solver.cpp:105] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_4.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 07:40:46.607225 89665 solver.cpp:140] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_4.prototxt
I1226 07:40:46.603384 92408 caffe.cpp:316] Use CPU.
I1226 07:40:46.594022 90196 caffe.cpp:316] Use CPU.
I1226 07:40:46.610556 86513 caffe.cpp:316] Use CPU.
I1226 07:40:46.613435 89800 solver.cpp:105] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_7.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 07:40:46.614296 89800 solver.cpp:140] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_7.prototxt
I1226 07:40:46.604782 92408 solver.cpp:105] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_dummy.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 07:40:46.595109 90196 solver.cpp:105] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_5.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 07:40:46.614114 89771 caffe.cpp:316] Use CPU.
I1226 07:40:46.605921 92408 solver.cpp:140] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_dummy.prototxt
I1226 07:40:46.595916 90196 solver.cpp:140] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_5.prototxt
I1226 07:40:46.611708 86513 solver.cpp:105] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_8.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 07:40:46.612732 86513 solver.cpp:140] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_8.prototxt
I1226 07:40:43.346446 89666 caffe.cpp:316] Use CPU.
I1226 07:40:46.615241 89771 solver.cpp:105] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_1.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 07:40:46.616148 89771 solver.cpp:140] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_1.prototxt
I1226 07:40:43.347746 89666 solver.cpp:105] Initializing solver from parameters: 
train_net: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_6.prototxt"
base_lr: 0.0003
display: 1
max_iter: 200000
lr_policy: "step"
gamma: 0.1
momentum: 0
weight_decay: 0.0005
stepsize: 100000
snapshot: 5000
snapshot_prefix: "/export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024_25000/"
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
I1226 07:40:43.348819 89666 solver.cpp:140] Creating training net from train_net file: /export/data1/stanford/hazy/gordon_bell/caffenet_knl/split_prototxt/8_nodes_lmdb_config_1/train-val/train_val_lmdb_6.prototxt
I1226 07:40:46.638232 89665 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 07:40:46.638310 89665 cpu_info.cpp:455] Total number of sockets: 1
I1226 07:40:46.642310 88308 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 07:40:46.642386 88308 cpu_info.cpp:455] Total number of sockets: 1
I1226 07:40:46.638334 89665 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 07:40:46.638352 89665 cpu_info.cpp:461] Total number of processors: 272
I1226 07:40:46.638371 89665 cpu_info.cpp:464] GPU is used: no
I1226 07:40:46.638391 89665 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 07:40:46.642410 88308 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 07:40:46.642431 88308 cpu_info.cpp:461] Total number of processors: 272
I1226 07:40:46.642451 88308 cpu_info.cpp:464] GPU is used: no
I1226 07:40:46.642470 88308 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 07:40:46.638438 89665 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 07:40:46.642491 88308 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 07:40:46.645681 89800 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 07:40:46.645757 89800 cpu_info.cpp:455] Total number of sockets: 1
I1226 07:40:46.645779 89800 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 07:40:46.645798 89800 cpu_info.cpp:461] Total number of processors: 272
I1226 07:40:46.645817 89800 cpu_info.cpp:464] GPU is used: no
I1226 07:40:46.645838 89800 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 07:40:46.626587 90196 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 07:40:46.626659 90196 cpu_info.cpp:455] Total number of sockets: 1
I1226 07:40:46.645884 89800 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 07:40:46.626682 90196 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 07:40:46.626703 90196 cpu_info.cpp:461] Total number of processors: 272
I1226 07:40:46.626724 90196 cpu_info.cpp:464] GPU is used: no
I1226 07:40:46.626744 90196 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 07:40:46.641671 93793 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 07:40:46.641760 93793 cpu_info.cpp:455] Total number of sockets: 1
I1226 07:40:46.626763 90196 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 07:40:46.641809 93793 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 07:40:46.641836 93793 cpu_info.cpp:461] Total number of processors: 272
I1226 07:40:46.641858 93793 cpu_info.cpp:464] GPU is used: no
I1226 07:40:46.641881 93793 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 07:40:46.641903 93793 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 07:40:46.643833 86513 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 07:40:46.643908 86513 cpu_info.cpp:455] Total number of sockets: 1
I1226 07:40:46.643932 86513 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 07:40:46.643952 86513 cpu_info.cpp:461] Total number of processors: 272
I1226 07:40:46.643972 86513 cpu_info.cpp:464] GPU is used: no
I1226 07:40:46.643992 86513 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 07:40:46.644012 86513 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 07:40:46.647148 89771 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 07:40:46.647223 89771 cpu_info.cpp:455] Total number of sockets: 1
I1226 07:40:46.647248 89771 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 07:40:46.647269 89771 cpu_info.cpp:461] Total number of processors: 272
I1226 07:40:46.647289 89771 cpu_info.cpp:464] GPU is used: no
I1226 07:40:46.647308 89771 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 07:40:46.647328 89771 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 07:40:46.640034 92408 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 07:40:46.640143 92408 cpu_info.cpp:455] Total number of sockets: 1
I1226 07:40:46.640187 92408 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 07:40:46.640224 92408 cpu_info.cpp:461] Total number of processors: 272
I1226 07:40:46.640255 92408 cpu_info.cpp:464] GPU is used: no
I1226 07:40:46.640286 92408 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 07:40:46.640316 92408 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 07:40:43.381212 89666 cpu_info.cpp:452] Processor speed [MHz]: 1400
I1226 07:40:43.381283 89666 cpu_info.cpp:455] Total number of sockets: 1
I1226 07:40:43.381306 89666 cpu_info.cpp:458] Total number of CPU cores: 68
I1226 07:40:43.381358 89666 cpu_info.cpp:461] Total number of processors: 272
I1226 07:40:43.381381 89666 cpu_info.cpp:464] GPU is used: no
I1226 07:40:43.381402 89666 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I1226 07:40:43.381425 89666 cpu_info.cpp:470] OpenMP thread bind allowed: no
I1226 07:40:46.662946 93793 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 07:40:46.663301 93793 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 07:40:46.665449 93793 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_1"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 07:40:46.657318 90196 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 07:40:46.657649 90196 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 07:40:46.659235 90196 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_4"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 07:40:46.671465 92408 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 07:40:43.420653 89666 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 07:40:46.671877 92408 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 07:40:46.686177 88308 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 07:40:43.421047 89666 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 07:40:46.674214 92408 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "DummyData"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  dummy_data_param {
    data_filler {
      type: "gaussian"
      std: 1
    }
    data_filler {
      type: "constant"
      value: 0
    }
    shape {
      dim: 64
      dim: 3
      dim: 227
      dim: 227
    }
    shape {
      dim: 64
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 07:40:46.686485 88308 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 07:40:46.686450 89665 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 07:40:43.423499 89666 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_5"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 07:40:46.688202 88308 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_2"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 07:40:46.686862 89665 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 07:40:46.688407 89665 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_3"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 07:40:46.694203 86513 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 07:40:46.694528 86513 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 07:40:46.696080 86513 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_7"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 07:40:46.721388 89771 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 07:40:46.722189 89800 cpu_info.cpp:473] Number of OpenMP threads: 66
I1226 07:40:46.721838 89771 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 07:40:46.722548 89800 net.cpp:500] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1226 07:40:46.724232 89800 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_6"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 07:40:46.723922 89771 net.cpp:136] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
engine: "MKL2017"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_0"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    engine: MKL2017
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: MKL2017
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
    engine: MKL2017
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
  relu_param {
    engine: MKL2017
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    engine: MKL2017
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I1226 07:40:46.734083 93793 layer_factory.hpp:114] Creating layer data
I1226 07:40:46.748777 89800 layer_factory.hpp:114] Creating layer data
I1226 07:40:46.763919 89800 net.cpp:178] Creating Layer data
I1226 07:40:46.764025 89800 net.cpp:586] data -> data
I1226 07:40:46.764109 89800 net.cpp:586] data -> label
I1226 07:40:46.771777 93793 net.cpp:178] Creating Layer data
I1226 07:40:46.771948 93793 net.cpp:586] data -> data
I1226 07:40:46.772071 93793 net.cpp:586] data -> label
I1226 07:40:46.780697 86513 layer_factory.hpp:114] Creating layer data
I1226 07:40:46.789692 89802 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_6
I1226 07:40:43.521466 89666 layer_factory.hpp:114] Creating layer data
I1226 07:40:46.777117 90196 layer_factory.hpp:114] Creating layer data
I1226 07:40:46.792927 93795 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_1
I1226 07:40:46.797538 89800 data_layer.cpp:80] output data size: 64,3,227,227
I1226 07:40:46.797562 89771 layer_factory.hpp:114] Creating layer data
I1226 07:40:46.796268 88308 layer_factory.hpp:114] Creating layer data
I1226 07:40:46.801033 93793 data_layer.cpp:80] output data size: 64,3,227,227
I1226 07:40:46.804097 89665 layer_factory.hpp:114] Creating layer data
I1226 07:40:43.541651 89666 net.cpp:178] Creating Layer data
I1226 07:40:43.541743 89666 net.cpp:586] data -> data
I1226 07:40:43.541826 89666 net.cpp:586] data -> label
I1226 07:40:46.809418 86513 net.cpp:178] Creating Layer data
I1226 07:40:46.809504 86513 net.cpp:586] data -> data
I1226 07:40:46.809582 86513 net.cpp:586] data -> label
I1226 07:40:46.803805 92408 layer_factory.hpp:114] Creating layer data
I1226 07:40:46.804016 92408 net.cpp:178] Creating Layer data
I1226 07:40:46.804124 92408 net.cpp:586] data -> data
I1226 07:40:46.804261 92408 net.cpp:586] data -> label
I1226 07:40:46.815615 89771 net.cpp:178] Creating Layer data
I1226 07:40:46.815701 89771 net.cpp:586] data -> data
I1226 07:40:46.815778 89771 net.cpp:586] data -> label
I1226 07:40:46.818393 88308 net.cpp:178] Creating Layer data
I1226 07:40:46.818478 88308 net.cpp:586] data -> data
I1226 07:40:46.818575 88308 net.cpp:586] data -> label
I1226 07:40:46.803150 90196 net.cpp:178] Creating Layer data
I1226 07:40:46.803237 90196 net.cpp:586] data -> data
I1226 07:40:46.803339 90196 net.cpp:586] data -> label
I1226 07:40:46.821164 89665 net.cpp:178] Creating Layer data
I1226 07:40:46.821254 89665 net.cpp:586] data -> data
I1226 07:40:46.821332 89665 net.cpp:586] data -> label
I1226 07:40:43.566550 89668 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_5
I1226 07:40:46.834053 86515 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_7
I1226 07:40:46.837683 89774 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_0
I1226 07:40:43.574256 89666 data_layer.cpp:80] output data size: 64,3,227,227
I1226 07:40:46.842434 88310 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_2
I1226 07:40:46.841799 86513 data_layer.cpp:80] output data size: 64,3,227,227
I1226 07:40:46.827365 90198 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_4
I1226 07:40:46.841800 89667 db_lmdb.cpp:72] Opened lmdb /export/data1/stanford/hazy/zjian/gorden_bell/ILSVRC2012_lmdb/db_8/ilsvrc12_train_lmdb_3
I1226 07:40:46.850533 88308 data_layer.cpp:80] output data size: 64,3,227,227
I1226 07:40:46.843374 92408 net.cpp:228] Setting up data
I1226 07:40:46.843513 92408 net.cpp:235] Top shape: 64 3 227 227 (9893568)
I1226 07:40:46.843559 92408 net.cpp:235] Top shape: 64 1 1 1 (64)
I1226 07:40:46.843585 92408 net.cpp:243] Memory required for data: 39574528
I1226 07:40:46.843639 92408 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 07:40:46.843740 92408 net.cpp:178] Creating Layer label_data_1_split
I1226 07:40:46.843781 92408 net.cpp:612] label_data_1_split <- label
I1226 07:40:46.843974 92408 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 07:40:46.844055 92408 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 07:40:46.835191 90196 data_layer.cpp:80] output data size: 64,3,227,227
I1226 07:40:46.850406 89665 data_layer.cpp:80] output data size: 64,3,227,227
I1226 07:40:46.856312 89771 data_layer.cpp:80] output data size: 64,3,227,227
I1226 07:40:46.856266 92408 net.cpp:228] Setting up label_data_1_split
I1226 07:40:46.856379 92408 net.cpp:235] Top shape: 64 1 1 1 (64)
I1226 07:40:46.856416 92408 net.cpp:235] Top shape: 64 1 1 1 (64)
I1226 07:40:46.856441 92408 net.cpp:243] Memory required for data: 39575040
I1226 07:40:46.856478 92408 layer_factory.hpp:114] Creating layer conv1
I1226 07:40:46.856626 92408 net.cpp:178] Creating Layer conv1
I1226 07:40:46.856668 92408 net.cpp:612] conv1 <- data
I1226 07:40:46.856715 92408 net.cpp:586] conv1 -> conv1
I1226 07:40:46.896020 89800 net.cpp:228] Setting up data
I1226 07:40:46.896140 89800 net.cpp:235] Top shape: 64 3 227 227 (9893568)
I1226 07:40:46.896179 89800 net.cpp:235] Top shape: 64 (64)
I1226 07:40:46.896205 89800 net.cpp:243] Memory required for data: 39574528
I1226 07:40:46.896242 89800 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 07:40:46.896338 89800 net.cpp:178] Creating Layer label_data_1_split
I1226 07:40:46.896492 89800 net.cpp:612] label_data_1_split <- label
I1226 07:40:46.896545 89800 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 07:40:46.896607 89800 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 07:40:46.910101 89800 net.cpp:228] Setting up label_data_1_split
I1226 07:40:46.910220 89800 net.cpp:235] Top shape: 64 (64)
I1226 07:40:46.910256 89800 net.cpp:235] Top shape: 64 (64)
I1226 07:40:46.910296 89800 net.cpp:243] Memory required for data: 39575040
I1226 07:40:46.910336 89800 layer_factory.hpp:114] Creating layer conv1
I1226 07:40:46.910563 89800 net.cpp:178] Creating Layer conv1
I1226 07:40:46.910924 89800 net.cpp:612] conv1 <- data
I1226 07:40:46.911011 89800 net.cpp:586] conv1 -> conv1
I1226 07:40:46.940091 86513 net.cpp:228] Setting up data
I1226 07:40:46.940209 86513 net.cpp:235] Top shape: 64 3 227 227 (9893568)
I1226 07:40:46.940245 86513 net.cpp:235] Top shape: 64 (64)
I1226 07:40:46.940297 86513 net.cpp:243] Memory required for data: 39574528
I1226 07:40:46.940366 86513 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 07:40:46.940451 86513 net.cpp:178] Creating Layer label_data_1_split
I1226 07:40:46.940593 86513 net.cpp:612] label_data_1_split <- label
I1226 07:40:46.940644 86513 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 07:40:46.940697 86513 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 07:40:43.676005 89666 net.cpp:228] Setting up data
I1226 07:40:43.676180 89666 net.cpp:235] Top shape: 64 3 227 227 (9893568)
I1226 07:40:43.676229 89666 net.cpp:235] Top shape: 64 (64)
I1226 07:40:43.676342 89666 net.cpp:243] Memory required for data: 39574528
I1226 07:40:43.676383 89666 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 07:40:43.676491 89666 net.cpp:178] Creating Layer label_data_1_split
I1226 07:40:43.676676 89666 net.cpp:612] label_data_1_split <- label
I1226 07:40:43.676728 89666 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 07:40:43.677121 89666 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 07:40:46.946120 88308 net.cpp:228] Setting up data
I1226 07:40:46.946252 88308 net.cpp:235] Top shape: 64 3 227 227 (9893568)
I1226 07:40:46.946290 88308 net.cpp:235] Top shape: 64 (64)
I1226 07:40:46.946336 88308 net.cpp:243] Memory required for data: 39574528
I1226 07:40:46.946501 88308 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 07:40:46.946714 88308 net.cpp:178] Creating Layer label_data_1_split
I1226 07:40:46.946871 88308 net.cpp:612] label_data_1_split <- label
I1226 07:40:46.946938 88308 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 07:40:46.947000 88308 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 07:40:46.934795 90196 net.cpp:228] Setting up data
I1226 07:40:46.935292 90196 net.cpp:235] Top shape: 64 3 227 227 (9893568)
I1226 07:40:46.935376 90196 net.cpp:235] Top shape: 64 (64)
I1226 07:40:46.935408 90196 net.cpp:243] Memory required for data: 39574528
I1226 07:40:46.935475 90196 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 07:40:46.935614 90196 net.cpp:178] Creating Layer label_data_1_split
I1226 07:40:46.935791 90196 net.cpp:612] label_data_1_split <- label
I1226 07:40:46.935845 90196 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 07:40:46.935899 90196 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 07:40:46.955291 89771 net.cpp:228] Setting up data
I1226 07:40:46.955433 89771 net.cpp:235] Top shape: 64 3 227 227 (9893568)
I1226 07:40:46.955538 89771 net.cpp:235] Top shape: 64 (64)
I1226 07:40:46.956073 89771 net.cpp:243] Memory required for data: 39574528
I1226 07:40:46.956181 89771 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 07:40:46.956311 89771 net.cpp:178] Creating Layer label_data_1_split
I1226 07:40:46.956581 89771 net.cpp:612] label_data_1_split <- label
I1226 07:40:46.956656 89771 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 07:40:46.953999 86513 net.cpp:228] Setting up label_data_1_split
I1226 07:40:46.956722 89771 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 07:40:46.954133 86513 net.cpp:235] Top shape: 64 (64)
I1226 07:40:46.954170 86513 net.cpp:235] Top shape: 64 (64)
I1226 07:40:46.954195 86513 net.cpp:243] Memory required for data: 39575040
I1226 07:40:46.954277 86513 layer_factory.hpp:114] Creating layer conv1
I1226 07:40:46.954406 86513 net.cpp:178] Creating Layer conv1
I1226 07:40:46.954449 86513 net.cpp:612] conv1 <- data
I1226 07:40:46.954493 86513 net.cpp:586] conv1 -> conv1
I1226 07:40:46.952885 89665 net.cpp:228] Setting up data
I1226 07:40:46.953013 89665 net.cpp:235] Top shape: 64 3 227 227 (9893568)
I1226 07:40:46.953127 89665 net.cpp:235] Top shape: 64 (64)
I1226 07:40:46.953310 89665 net.cpp:243] Memory required for data: 39574528
I1226 07:40:46.953357 89665 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 07:40:46.953660 89665 net.cpp:178] Creating Layer label_data_1_split
I1226 07:40:46.953851 89665 net.cpp:612] label_data_1_split <- label
I1226 07:40:46.953912 89665 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 07:40:46.954365 89665 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 07:40:46.966400 88308 net.cpp:228] Setting up label_data_1_split
I1226 07:40:46.966512 88308 net.cpp:235] Top shape: 64 (64)
I1226 07:40:46.966595 88308 net.cpp:235] Top shape: 64 (64)
I1226 07:40:46.966670 88308 net.cpp:243] Memory required for data: 39575040
I1226 07:40:46.966720 88308 layer_factory.hpp:114] Creating layer conv1
I1226 07:40:46.966858 88308 net.cpp:178] Creating Layer conv1
I1226 07:40:46.966989 88308 net.cpp:612] conv1 <- data
I1226 07:40:46.967034 88308 net.cpp:586] conv1 -> conv1
I1226 07:40:43.700731 89666 net.cpp:228] Setting up label_data_1_split
I1226 07:40:43.700896 89666 net.cpp:235] Top shape: 64 (64)
I1226 07:40:43.700942 89666 net.cpp:235] Top shape: 64 (64)
I1226 07:40:43.700968 89666 net.cpp:243] Memory required for data: 39575040
I1226 07:40:43.701004 89666 layer_factory.hpp:114] Creating layer conv1
I1226 07:40:43.701187 89666 net.cpp:178] Creating Layer conv1
I1226 07:40:43.701249 89666 net.cpp:612] conv1 <- data
I1226 07:40:43.701297 89666 net.cpp:586] conv1 -> conv1
I1226 07:40:46.954077 90196 net.cpp:228] Setting up label_data_1_split
I1226 07:40:46.954205 90196 net.cpp:235] Top shape: 64 (64)
I1226 07:40:46.954246 90196 net.cpp:235] Top shape: 64 (64)
I1226 07:40:46.954270 90196 net.cpp:243] Memory required for data: 39575040
I1226 07:40:46.954423 90196 layer_factory.hpp:114] Creating layer conv1
I1226 07:40:46.954553 90196 net.cpp:178] Creating Layer conv1
I1226 07:40:46.954596 90196 net.cpp:612] conv1 <- data
I1226 07:40:46.954640 90196 net.cpp:586] conv1 -> conv1
I1226 07:40:46.978366 89771 net.cpp:228] Setting up label_data_1_split
I1226 07:40:46.978516 89771 net.cpp:235] Top shape: 64 (64)
I1226 07:40:46.978559 89771 net.cpp:235] Top shape: 64 (64)
I1226 07:40:46.978605 89771 net.cpp:243] Memory required for data: 39575040
I1226 07:40:46.978663 89771 layer_factory.hpp:114] Creating layer conv1
I1226 07:40:46.978816 89771 net.cpp:178] Creating Layer conv1
I1226 07:40:46.978860 89771 net.cpp:612] conv1 <- data
I1226 07:40:46.978912 89771 net.cpp:586] conv1 -> conv1
I1226 07:40:46.990600 89665 net.cpp:228] Setting up label_data_1_split
I1226 07:40:46.990712 89665 net.cpp:235] Top shape: 64 (64)
I1226 07:40:46.990753 89665 net.cpp:235] Top shape: 64 (64)
I1226 07:40:46.990788 89665 net.cpp:243] Memory required for data: 39575040
I1226 07:40:46.990859 89665 layer_factory.hpp:114] Creating layer conv1
I1226 07:40:46.990980 89665 net.cpp:178] Creating Layer conv1
I1226 07:40:46.991186 89665 net.cpp:612] conv1 <- data
I1226 07:40:46.991243 89665 net.cpp:586] conv1 -> conv1
I1226 07:40:47.067893 93793 net.cpp:228] Setting up data
I1226 07:40:47.068094 93793 net.cpp:235] Top shape: 64 3 227 227 (9893568)
I1226 07:40:47.068303 93793 net.cpp:235] Top shape: 64 (64)
I1226 07:40:47.068367 93793 net.cpp:243] Memory required for data: 39574528
I1226 07:40:47.068414 93793 layer_factory.hpp:114] Creating layer label_data_1_split
I1226 07:40:47.068775 93793 net.cpp:178] Creating Layer label_data_1_split
I1226 07:40:47.068984 93793 net.cpp:612] label_data_1_split <- label
I1226 07:40:47.069600 93793 net.cpp:586] label_data_1_split -> label_data_1_split_0
I1226 07:40:47.069779 93793 net.cpp:586] label_data_1_split -> label_data_1_split_1
I1226 07:40:47.102864 93793 net.cpp:228] Setting up label_data_1_split
I1226 07:40:47.102978 93793 net.cpp:235] Top shape: 64 (64)
I1226 07:40:47.103020 93793 net.cpp:235] Top shape: 64 (64)
I1226 07:40:47.103055 93793 net.cpp:243] Memory required for data: 39575040
I1226 07:40:47.103415 93793 layer_factory.hpp:114] Creating layer conv1
I1226 07:40:47.103669 93793 net.cpp:178] Creating Layer conv1
I1226 07:40:47.103773 93793 net.cpp:612] conv1 <- data
I1226 07:40:47.103905 93793 net.cpp:586] conv1 -> conv1
I1226 07:40:47.135476 89800 net.cpp:228] Setting up conv1
I1226 07:40:47.135597 89800 net.cpp:235] Top shape: 64 96 55 55 (18585600)
I1226 07:40:47.135632 89800 net.cpp:243] Memory required for data: 113917440
I1226 07:40:47.135751 89800 layer_factory.hpp:114] Creating layer relu1
I1226 07:40:47.135854 89800 net.cpp:178] Creating Layer relu1
I1226 07:40:47.135902 89800 net.cpp:612] relu1 <- conv1
I1226 07:40:47.135947 89800 net.cpp:573] relu1 -> conv1 (in-place)
I1226 07:40:47.136075 89800 net.cpp:228] Setting up relu1
I1226 07:40:47.136128 89800 net.cpp:235] Top shape: 64 96 55 55 (18585600)
I1226 07:40:47.136154 89800 net.cpp:243] Memory required for data: 188259840
I1226 07:40:47.136185 89800 layer_factory.hpp:114] Creating layer norm1
I1226 07:40:47.136270 89800 net.cpp:178] Creating Layer norm1
I1226 07:40:47.136303 89800 net.cpp:612] norm1 <- conv1
I1226 07:40:47.136353 89800 net.cpp:586] norm1 -> norm1
I1226 07:40:47.136492 89800 net.cpp:228] Setting up norm1
I1226 07:40:47.136553 89800 net.cpp:235] Top shape: 64 96 55 55 (18585600)
I1226 07:40:47.136579 89800 net.cpp:243] Memory required for data: 262602240
I1226 07:40:47.136610 89800 layer_factory.hpp:114] Creating layer pool1
I1226 07:40:47.136684 89800 net.cpp:178] Creating Layer pool1
I1226 07:40:47.136720 89800 net.cpp:612] pool1 <- norm1
I1226 07:40:47.136770 89800 net.cpp:586] pool1 -> pool1
I1226 07:40:47.136873 89800 net.cpp:228] Setting up pool1
I1226 07:40:47.136917 89800 net.cpp:235] Top shape: 64 96 27 27 (4478976)
I1226 07:40:47.136941 89800 net.cpp:243] Memory required for data: 280518144
I1226 07:40:47.136970 89800 layer_factory.hpp:114] Creating layer conv2
I1226 07:40:47.137051 89800 net.cpp:178] Creating Layer conv2
I1226 07:40:47.137085 89800 net.cpp:612] conv2 <- pool1
I1226 07:40:47.137127 89800 net.cpp:586] conv2 -> conv2
I1226 07:40:47.160928 86513 net.cpp:228] Setting up conv1
I1226 07:40:47.161039 86513 net.cpp:235] Top shape: 64 96 55 55 (18585600)
I1226 07:40:47.161069 86513 net.cpp:243] Memory required for data: 113917440
I1226 07:40:47.161555 86513 layer_factory.hpp:114] Creating layer relu1
I1226 07:40:47.161727 86513 net.cpp:178] Creating Layer relu1
I1226 07:40:47.161808 86513 net.cpp:612] relu1 <- conv1
I1226 07:40:47.161856 86513 net.cpp:573] relu1 -> conv1 (in-place)
I1226 07:40:47.161972 86513 net.cpp:228] Setting up relu1
I1226 07:40:47.162029 86513 net.cpp:235] Top shape: 64 96 55 55 (18585600)
I1226 07:40:47.162061 86513 net.cpp:243] Memory required for data: 188259840
I1226 07:40:47.162091 86513 layer_factory.hpp:114] Creating layer norm1
I1226 07:40:47.164607 89771 net.cpp:228] Setting up conv1
I1226 07:40:47.162165 86513 net.cpp:178] Creating Layer norm1
I1226 07:40:47.164724 89771 net.cpp:235] Top shape: 64 96 55 55 (18585600)
I1226 07:40:47.164757 89771 net.cpp:243] Memory required for data: 113917440
I1226 07:40:47.162210 86513 net.cpp:612] norm1 <- conv1
I1226 07:40:47.164873 89771 layer_factory.hpp:114] Creating layer relu1
I1226 07:40:47.162250 86513 net.cpp:586] norm1 -> norm1
I1226 07:40:47.164968 89771 net.cpp:178] Creating Layer relu1
I1226 07:40:47.162415 86513 net.cpp:228] Setting up norm1
I1226 07:40:47.165004 89771 net.cpp:612] relu1 <- conv1
I1226 07:40:47.162497 86513 net.cpp:235] Top shape: 64 96 55 55 (18585600)
I1226 07:40:47.162526 86513 net.cpp:243] Memory required for data: 262602240
I1226 07:40:47.165045 89771 net.cpp:573] relu1 -> conv1 (in-place)
I1226 07:40:47.162556 86513 layer_factory.hpp:114] Creating layer pool1
I1226 07:40:47.165159 89771 net.cpp:228] Setting up relu1
I1226 07:40:47.165222 89771 net.cpp:235] Top shape: 64 96 55 55 (18585600)
I1226 07:40:47.165249 89771 net.cpp:243] Memory required for data: 188259840
I1226 07:40:47.165280 89771 layer_factory.hpp:114] Creating layer norm1
I1226 07:40:47.162842 86513 net.cpp:178] Creating Layer pool1
I1226 07:40:47.165357 89771 net.cpp:178] Creating Layer norm1
I1226 07:40:47.165392 89771 net.cpp:612] norm1 <- conv1
I1226 07:40:47.162883 86513 net.cpp:612] pool1 <- norm1
I1226 07:40:47.165441 89771 net.cpp:586] norm1 -> norm1
I1226 07:40:47.162948 86513 net.cpp:586] pool1 -> pool1
I1226 07:40:47.165611 89771 net.cpp:228] Setting up norm1
I1226 07:40:47.163069 86513 net.cpp:228] Setting up pool1
I1226 07:40:47.165698 89771 net.cpp:235] Top shape: 64 96 55 55 (18585600)
I1226 07:40:47.165729 89771 net.cpp:243] Memory required for data: 262602240
I1226 07:40:47.163137 86513 net.cpp:235] Top shape: 64 96 27 27 (4478976)
I1226 07:40:47.165763 89771 layer_factory.hpp:114] Creating layer pool1
I1226 07:40:47.163166 86513 net.cpp:243] Memory required for data: 280518144
I1226 07:40:47.163197 86513 layer_factory.hpp:114] Creating layer conv2
I1226 07:40:47.165843 89771 net.cpp:178] Creating Layer pool1
I1226 07:40:47.163274 86513 net.cpp:178] Creating Layer conv2
I1226 07:40:47.165875 89771 net.cpp:612] pool1 <- norm1
I1226 07:40:47.165927 89771 net.cpp:586] pool1 -> pool1
I1226 07:40:47.166038 89771 net.cpp:228] Setting up pool1
I1226 07:40:47.166098 89771 net.cpp:235] Top shape: 64 96 27 27 (4478976)
I1226 07:40:47.166122 89771 net.cpp:243] Memory required for data: 280518144
I1226 07:40:47.166153 89771 layer_factory.hpp:114] Creating layer conv2
I1226 07:40:47.166244 89771 net.cpp:178] Creating Layer conv2
I1226 07:40:47.166292 89771 net.cpp:612] conv2 <- pool1
I1226 07:40:47.166345 89771 net.cpp:586] conv2 -> conv2
I1226 07:40:47.163341 86513 net.cpp:612] conv2 <- pool1
I1226 07:40:47.163938 86513 net.cpp:586] conv2 -> conv2
I1226 07:40:47.198211 88308 net.cpp:228] Setting up conv1
I1226 07:40:47.198330 88308 net.cpp:235] Top shape: 64 96 55 55 (18585600)
I1226 07:40:47.198357 88308 net.cpp:243] Memory required for data: 113917440
I1226 07:40:47.198462 88308 layer_factory.hpp:114] Creating layer relu1
I1226 07:40:47.198551 88308 net.cpp:178] Creating Layer relu1
I1226 07:40:47.198617 88308 net.cpp:612] relu1 <- conv1
I1226 07:40:47.198657 88308 net.cpp:573] relu1 -> conv1 (in-place)
I1226 07:40:47.198760 88308 net.cpp:228] Setting up relu1
I1226 07:40:47.198806 88308 net.cpp:235] Top shape: 64 96 55 55 (18585600)
I1226 07:40:47.198829 88308 net.cpp:243] Memory required for data: 188259840
I1226 07:40:47.198858 88308 layer_factory.hpp:114] Creating layer norm1
I1226 07:40:47.198922 88308 net.cpp:178] Creating Layer norm1
I1226 07:40:47.198952 88308 net.cpp:612] norm1 <- conv1
I1226 07:40:47.198988 88308 net.cpp:586] norm1 -> norm1
I1226 07:40:47.199095 88308 net.cpp:228] Setting up norm1
I1226 07:40:47.199141 88308 net.cpp:235] Top shape: 64 96 55 55 (18585600)
I1226 07:40:47.199165 88308 net.cpp:243] Memory required for data: 262602240
I1226 07:40:47.199194 88308 layer_factory.hpp:114] Creating layer pool1
I1226 07:40:47.199265 88308 net.cpp:178] Creating Layer pool1
I1226 07:40:47.199297 88308 net.cpp:612] pool1 <- norm1
I1226 07:40:47.199338 88308 net.cpp:586] pool1 -> pool1
I1226 07:40:47.199434 88308 net.cpp:228] Setting up pool1
I1226 07:40:47.199476 88308 net.cpp:235] Top shape: 64 96 27 27 (4478976)
I1226 07:40:47.199498 88308 net.cpp:243] Memory required for data: 280518144
I1226 07:40:47.199524 88308 layer_factory.hpp:114] Creating layer conv2
I1226 07:40:47.199618 88308 net.cpp:178] Creating Layer conv2
I1226 07:40:47.199663 88308 net.cpp:612] conv2 <- pool1
I1226 07:40:47.199703 88308 net.cpp:586] conv2 -> conv2
I1226 07:40:43.934774 89666 net.cpp:228] Setting up conv1
I1226 07:40:43.934888 89666 net.cpp:235] Top shape: 64 96 55 55 (18585600)
I1226 07:40:43.934917 89666 net.cpp:243] Memory required for data: 113917440
I1226 07:40:43.935025 89666 layer_factory.hpp:114] Creating layer relu1
I1226 07:40:43.935119 89666 net.cpp:178] Creating Layer relu1
I1226 07:40:43.935159 89666 net.cpp:612] relu1 <- conv1
I1226 07:40:43.935200 89666 net.cpp:573] relu1 -> conv1 (in-place)
I1226 07:40:43.935312 89666 net.cpp:228] Setting up relu1
I1226 07:40:43.935360 89666 net.cpp:235] Top shape: 64 96 55 55 (18585600)
I1226 07:40:43.935389 89666 net.cpp:243] Memory required for data: 188259840
I1226 07:40:43.935417 89666 layer_factory.hpp:114] Creating layer norm1
I1226 07:40:43.935487 89666 net.cpp:178] Creating Layer norm1
I1226 07:40:43.935520 89666 net.cpp:612] norm1 <- conv1
I1226 07:40:43.935564 89666 net.cpp:586] norm1 -> norm1
I1226 07:40:43.935684 89666 net.cpp:228] Setting up norm1
I1226 07:40:43.935735 89666 net.cpp:235] Top shape: 64 96 55 55 (18585600)
I1226 07:40:43.935758 89666 net.cpp:243] Memory required for data: 262602240
I1226 07:40:43.935787 89666 layer_factory.hpp:114] Creating layer pool1
I1226 07:40:43.935847 89666 net.cpp:178] Creating Layer pool1
I1226 07:40:43.935883 89666 net.cpp:612] pool1 <- norm1
I1226 07:40:43.935925 89666 net.cpp:586] pool1 -> pool1
I1226 07:40:43.936019 89666 net.cpp:228] Setting up pool1
I1226 07:40:43.936064 89666 net.cpp:235] Top shape: 64 96 27 27 (4478976)
I1226 07:40:43.936092 89666 net.cpp:243] Memory required for data: 280518144
I1226 07:40:43.936125 89666 layer_factory.hpp:114] Creating layer conv2
I1226 07:40:43.936208 89666 net.cpp:178] Creating Layer conv2
I1226 07:40:43.936240 89666 net.cpp:612] conv2 <- pool1
I1226 07:40:43.936281 89666 net.cpp:586] conv2 -> conv2
I1226 07:40:47.188117 90196 net.cpp:228] Setting up conv1
I1226 07:40:47.188235 90196 net.cpp:235] Top shape: 64 96 55 55 (18585600)
I1226 07:40:47.188267 90196 net.cpp:243] Memory required for data: 113917440
I1226 07:40:47.196068 90196 layer_factory.hpp:114] Creating layer relu1
I1226 07:40:47.196238 90196 net.cpp:178] Creating Layer relu1
I1226 07:40:47.196298 90196 net.cpp:612] relu1 <- conv1
I1226 07:40:47.196346 90196 net.cpp:573] relu1 -> conv1 (in-place)
I1226 07:40:47.196496 90196 net.cpp:228] Setting up relu1
I1226 07:40:47.196588 90196 net.cpp:235] Top shape: 64 96 55 55 (18585600)
I1226 07:40:47.196616 90196 net.cpp:243] Memory required for data: 188259840
I1226 07:40:47.196650 90196 layer_factory.hpp:114] Creating layer norm1
I1226 07:40:47.196837 90196 net.cpp:178] Creating Layer norm1
I1226 07:40:47.196884 90196 net.cpp:612] norm1 <- conv1
I1226 07:40:47.196923 90196 net.cpp:586] norm1 -> norm1
I1226 07:40:47.197022 90196 net.cpp:228] Setting up norm1
I1226 07:40:47.197091 90196 net.cpp:235] Top shape: 64 96 55 55 (18585600)
I1226 07:40:47.197118 90196 net.cpp:243] Memory required for data: 262602240
I1226 07:40:47.197146 90196 layer_factory.hpp:114] Creating layer pool1
I1226 07:40:47.197204 90196 net.cpp:178] Creating Layer pool1
I1226 07:40:47.197232 90196 net.cpp:612] pool1 <- norm1
I1226 07:40:47.197279 90196 net.cpp:586] pool1 -> pool1
I1226 07:40:47.197387 90196 net.cpp:228] Setting up pool1
I1226 07:40:47.197434 90196 net.cpp:235] Top shape: 64 96 27 27 (4478976)
I1226 07:40:47.197486 90196 net.cpp:243] Memory required for data: 280518144
I1226 07:40:47.197545 90196 layer_factory.hpp:114] Creating layer conv2
I1226 07:40:47.197623 90196 net.cpp:178] Creating Layer conv2
I1226 07:40:47.197677 90196 net.cpp:612] conv2 <- pool1
I1226 07:40:47.197727 90196 net.cpp:586] conv2 -> conv2
I1226 07:40:47.227766 89665 net.cpp:228] Setting up conv1
I1226 07:40:47.227922 89665 net.cpp:235] Top shape: 64 96 55 55 (18585600)
I1226 07:40:47.227960 89665 net.cpp:243] Memory required for data: 113917440
I1226 07:40:47.228106 89665 layer_factory.hpp:114] Creating layer relu1
I1226 07:40:47.228327 89665 net.cpp:178] Creating Layer relu1
I1226 07:40:47.228514 89665 net.cpp:612] relu1 <- conv1
I1226 07:40:47.228569 89665 net.cpp:573] relu1 -> conv1 (in-place)
I1226 07:40:47.228695 89665 net.cpp:228] Setting up relu1
I1226 07:40:47.228783 89665 net.cpp:235] Top shape: 64 96 55 55 (18585600)
I1226 07:40:47.228840 89665 net.cpp:243] Memory required for data: 188259840
I1226 07:40:47.228883 89665 layer_factory.hpp:114] Creating layer norm1
I1226 07:40:47.228957 89665 net.cpp:178] Creating Layer norm1
I1226 07:40:47.228992 89665 net.cpp:612] norm1 <- conv1
I1226 07:40:47.229050 89665 net.cpp:586] norm1 -> norm1
I1226 07:40:47.229163 89665 net.cpp:228] Setting up norm1
I1226 07:40:47.229224 89665 net.cpp:235] Top shape: 64 96 55 55 (18585600)
I1226 07:40:47.229255 89665 net.cpp:243] Memory required for data: 262602240
I1226 07:40:47.229295 89665 layer_factory.hpp:114] Creating layer pool1
I1226 07:40:47.229370 89665 net.cpp:178] Creating Layer pool1
I1226 07:40:47.229410 89665 net.cpp:612] pool1 <- norm1
I1226 07:40:47.229454 89665 net.cpp:586] pool1 -> pool1
I1226 07:40:47.229571 89665 net.cpp:228] Setting up pool1
I1226 07:40:47.229624 89665 net.cpp:235] Top shape: 64 96 27 27 (4478976)
I1226 07:40:47.229655 89665 net.cpp:243] Memory required for data: 280518144
I1226 07:40:47.229691 89665 layer_factory.hpp:114] Creating layer conv2
I1226 07:40:47.229800 89665 net.cpp:178] Creating Layer conv2
I1226 07:40:47.229877 89665 net.cpp:612] conv2 <- pool1
I1226 07:40:47.229930 89665 net.cpp:586] conv2 -> conv2
I1226 07:40:47.227030 92408 net.cpp:228] Setting up conv1
I1226 07:40:47.227161 92408 net.cpp:235] Top shape: 64 96 55 55 (18585600)
I1226 07:40:47.227210 92408 net.cpp:243] Memory required for data: 113917440
I1226 07:40:47.227366 92408 layer_factory.hpp:114] Creating layer relu1
I1226 07:40:47.227481 92408 net.cpp:178] Creating Layer relu1
I1226 07:40:47.227533 92408 net.cpp:612] relu1 <- conv1
I1226 07:40:47.227586 92408 net.cpp:573] relu1 -> conv1 (in-place)
I1226 07:40:47.227735 92408 net.cpp:228] Setting up relu1
I1226 07:40:47.227807 92408 net.cpp:235] Top shape: 64 96 55 55 (18585600)
I1226 07:40:47.227872 92408 net.cpp:243] Memory required for data: 188259840
I1226 07:40:47.227915 92408 layer_factory.hpp:114] Creating layer norm1
I1226 07:40:47.227996 92408 net.cpp:178] Creating Layer norm1
I1226 07:40:47.228050 92408 net.cpp:612] norm1 <- conv1
I1226 07:40:47.228097 92408 net.cpp:586] norm1 -> norm1
I1226 07:40:47.228195 92408 net.cpp:228] Setting up norm1
I1226 07:40:47.228252 92408 net.cpp:235] Top shape: 64 96 55 55 (18585600)
I1226 07:40:47.228287 92408 net.cpp:243] Memory required for data: 262602240
I1226 07:40:47.228320 92408 layer_factory.hpp:114] Creating layer pool1
I1226 07:40:47.228405 92408 net.cpp:178] Creating Layer pool1
I1226 07:40:47.228451 92408 net.cpp:612] pool1 <- norm1
I1226 07:40:47.228504 92408 net.cpp:586] pool1 -> pool1
I1226 07:40:47.228624 92408 net.cpp:228] Setting up pool1
I1226 07:40:47.228677 92408 net.cpp:235] Top shape: 64 96 27 27 (4478976)
I1226 07:40:47.228701 92408 net.cpp:243] Memory required for data: 280518144
I1226 07:40:47.228731 92408 layer_factory.hpp:114] Creating layer conv2
I1226 07:40:47.228817 92408 net.cpp:178] Creating Layer conv2
I1226 07:40:47.228890 92408 net.cpp:612] conv2 <- pool1
I1226 07:40:47.228951 92408 net.cpp:586] conv2 -> conv2
I1226 07:40:47.516080 89771 net.cpp:228] Setting up conv2
I1226 07:40:47.516190 89771 net.cpp:235] Top shape: 64 256 27 27 (11943936)
I1226 07:40:47.516217 89771 net.cpp:243] Memory required for data: 328293888
I1226 07:40:47.516291 89771 layer_factory.hpp:114] Creating layer relu2
I1226 07:40:47.516376 89771 net.cpp:178] Creating Layer relu2
I1226 07:40:47.516410 89771 net.cpp:612] relu2 <- conv2
I1226 07:40:47.516453 89771 net.cpp:573] relu2 -> conv2 (in-place)
I1226 07:40:47.516590 89771 net.cpp:228] Setting up relu2
I1226 07:40:47.516660 89771 net.cpp:235] Top shape: 64 256 27 27 (11943936)
I1226 07:40:47.516690 89771 net.cpp:243] Memory required for data: 376069632
I1226 07:40:47.516724 89771 layer_factory.hpp:114] Creating layer norm2
I1226 07:40:47.516798 89771 net.cpp:178] Creating Layer norm2
I1226 07:40:47.516829 89771 net.cpp:612] norm2 <- conv2
I1226 07:40:47.516871 89771 net.cpp:586] norm2 -> norm2
I1226 07:40:47.516965 89771 net.cpp:228] Setting up norm2
I1226 07:40:47.517024 89771 net.cpp:235] Top shape: 64 256 27 27 (11943936)
I1226 07:40:47.517047 89771 net.cpp:243] Memory required for data: 423845376
I1226 07:40:47.517077 89771 layer_factory.hpp:114] Creating layer pool2
I1226 07:40:47.517135 89771 net.cpp:178] Creating Layer pool2
I1226 07:40:47.517163 89771 net.cpp:612] pool2 <- norm2
I1226 07:40:47.517201 89771 net.cpp:586] pool2 -> pool2
I1226 07:40:47.517300 89771 net.cpp:228] Setting up pool2
I1226 07:40:47.517343 89771 net.cpp:235] Top shape: 64 256 13 13 (2768896)
I1226 07:40:47.517460 89771 net.cpp:243] Memory required for data: 434920960
I1226 07:40:47.517524 89771 layer_factory.hpp:114] Creating layer conv3
I1226 07:40:47.517613 89771 net.cpp:178] Creating Layer conv3
I1226 07:40:47.517652 89771 net.cpp:612] conv3 <- pool2
I1226 07:40:47.517716 89771 net.cpp:586] conv3 -> conv3
I1226 07:40:47.517503 86513 net.cpp:228] Setting up conv2
I1226 07:40:47.517655 86513 net.cpp:235] Top shape: 64 256 27 27 (11943936)
I1226 07:40:47.517710 86513 net.cpp:243] Memory required for data: 328293888
I1226 07:40:47.518604 86513 layer_factory.hpp:114] Creating layer relu2
I1226 07:40:47.518750 86513 net.cpp:178] Creating Layer relu2
I1226 07:40:47.518846 86513 net.cpp:612] relu2 <- conv2
I1226 07:40:47.518901 86513 net.cpp:573] relu2 -> conv2 (in-place)
I1226 07:40:47.519052 86513 net.cpp:228] Setting up relu2
I1226 07:40:47.519299 86513 net.cpp:235] Top shape: 64 256 27 27 (11943936)
I1226 07:40:47.519399 86513 net.cpp:243] Memory required for data: 376069632
I1226 07:40:47.519443 86513 layer_factory.hpp:114] Creating layer norm2
I1226 07:40:47.519585 86513 net.cpp:178] Creating Layer norm2
I1226 07:40:47.519634 86513 net.cpp:612] norm2 <- conv2
I1226 07:40:47.519713 86513 net.cpp:586] norm2 -> norm2
I1226 07:40:47.519872 86513 net.cpp:228] Setting up norm2
I1226 07:40:47.519938 86513 net.cpp:235] Top shape: 64 256 27 27 (11943936)
I1226 07:40:47.519969 86513 net.cpp:243] Memory required for data: 423845376
I1226 07:40:47.520006 86513 layer_factory.hpp:114] Creating layer pool2
I1226 07:40:47.520500 86513 net.cpp:178] Creating Layer pool2
I1226 07:40:47.520608 86513 net.cpp:612] pool2 <- norm2
I1226 07:40:47.520668 86513 net.cpp:586] pool2 -> pool2
I1226 07:40:47.520843 86513 net.cpp:228] Setting up pool2
I1226 07:40:47.520910 86513 net.cpp:235] Top shape: 64 256 13 13 (2768896)
I1226 07:40:47.521076 86513 net.cpp:243] Memory required for data: 434920960
I1226 07:40:47.521136 86513 layer_factory.hpp:114] Creating layer conv3
I1226 07:40:47.521284 86513 net.cpp:178] Creating Layer conv3
I1226 07:40:47.521353 86513 net.cpp:612] conv3 <- pool2
I1226 07:40:47.521420 86513 net.cpp:586] conv3 -> conv3
I1226 07:40:47.528571 89800 net.cpp:228] Setting up conv2
I1226 07:40:47.528681 89800 net.cpp:235] Top shape: 64 256 27 27 (11943936)
I1226 07:40:47.528708 89800 net.cpp:243] Memory required for data: 328293888
I1226 07:40:47.528781 89800 layer_factory.hpp:114] Creating layer relu2
I1226 07:40:47.528867 89800 net.cpp:178] Creating Layer relu2
I1226 07:40:47.528908 89800 net.cpp:612] relu2 <- conv2
I1226 07:40:47.528952 89800 net.cpp:573] relu2 -> conv2 (in-place)
I1226 07:40:47.529044 89800 net.cpp:228] Setting up relu2
I1226 07:40:47.529090 89800 net.cpp:235] Top shape: 64 256 27 27 (11943936)
I1226 07:40:47.529119 89800 net.cpp:243] Memory required for data: 376069632
I1226 07:40:47.529147 89800 layer_factory.hpp:114] Creating layer norm2
I1226 07:40:47.529197 89800 net.cpp:178] Creating Layer norm2
I1226 07:40:47.529238 89800 net.cpp:612] norm2 <- conv2
I1226 07:40:47.529286 89800 net.cpp:586] norm2 -> norm2
I1226 07:40:47.529372 89800 net.cpp:228] Setting up norm2
I1226 07:40:47.529449 89800 net.cpp:235] Top shape: 64 256 27 27 (11943936)
I1226 07:40:47.529474 89800 net.cpp:243] Memory required for data: 423845376
I1226 07:40:47.529505 89800 layer_factory.hpp:114] Creating layer pool2
I1226 07:40:47.529559 89800 net.cpp:178] Creating Layer pool2
I1226 07:40:47.529599 89800 net.cpp:612] pool2 <- norm2
I1226 07:40:47.529638 89800 net.cpp:586] pool2 -> pool2
I1226 07:40:47.529717 89800 net.cpp:228] Setting up pool2
I1226 07:40:47.529759 89800 net.cpp:235] Top shape: 64 256 13 13 (2768896)
I1226 07:40:47.529881 89800 net.cpp:243] Memory required for data: 434920960
I1226 07:40:47.529917 89800 layer_factory.hpp:114] Creating layer conv3
I1226 07:40:47.530009 89800 net.cpp:178] Creating Layer conv3
I1226 07:40:47.530047 89800 net.cpp:612] conv3 <- pool2
I1226 07:40:47.530086 89800 net.cpp:586] conv3 -> conv3
I1226 07:40:47.573679 89665 net.cpp:228] Setting up conv2
I1226 07:40:47.573796 89665 net.cpp:235] Top shape: 64 256 27 27 (11943936)
I1226 07:40:47.573851 89665 net.cpp:243] Memory required for data: 328293888
I1226 07:40:47.573951 89665 layer_factory.hpp:114] Creating layer relu2
I1226 07:40:47.574029 89665 net.cpp:178] Creating Layer relu2
I1226 07:40:47.574067 89665 net.cpp:612] relu2 <- conv2
I1226 07:40:47.574146 89665 net.cpp:573] relu2 -> conv2 (in-place)
I1226 07:40:47.574251 89665 net.cpp:228] Setting up relu2
I1226 07:40:47.574311 89665 net.cpp:235] Top shape: 64 256 27 27 (11943936)
I1226 07:40:47.574350 89665 net.cpp:243] Memory required for data: 376069632
I1226 07:40:47.574385 89665 layer_factory.hpp:114] Creating layer norm2
I1226 07:40:47.574442 89665 net.cpp:178] Creating Layer norm2
I1226 07:40:47.574475 89665 net.cpp:612] norm2 <- conv2
I1226 07:40:47.574543 89665 net.cpp:586] norm2 -> norm2
I1226 07:40:47.574641 89665 net.cpp:228] Setting up norm2
I1226 07:40:47.574698 89665 net.cpp:235] Top shape: 64 256 27 27 (11943936)
I1226 07:40:47.574728 89665 net.cpp:243] Memory required for data: 423845376
I1226 07:40:47.574762 89665 layer_factory.hpp:114] Creating layer pool2
I1226 07:40:47.574865 89665 net.cpp:178] Creating Layer pool2
I1226 07:40:47.574901 89665 net.cpp:612] pool2 <- norm2
I1226 07:40:47.574942 89665 net.cpp:586] pool2 -> pool2
I1226 07:40:47.575049 89665 net.cpp:228] Setting up pool2
I1226 07:40:47.575100 89665 net.cpp:235] Top shape: 64 256 13 13 (2768896)
I1226 07:40:47.575237 89665 net.cpp:243] Memory required for data: 434920960
I1226 07:40:47.575281 89665 layer_factory.hpp:114] Creating layer conv3
I1226 07:40:47.575387 89665 net.cpp:178] Creating Layer conv3
I1226 07:40:47.575428 89665 net.cpp:612] conv3 <- pool2
I1226 07:40:47.575485 89665 net.cpp:586] conv3 -> conv3
I1226 07:40:47.575896 90196 net.cpp:228] Setting up conv2
I1226 07:40:47.576007 90196 net.cpp:235] Top shape: 64 256 27 27 (11943936)
I1226 07:40:47.576033 90196 net.cpp:243] Memory required for data: 328293888
I1226 07:40:47.576133 90196 layer_factory.hpp:114] Creating layer relu2
I1226 07:40:47.576198 90196 net.cpp:178] Creating Layer relu2
I1226 07:40:47.576234 90196 net.cpp:612] relu2 <- conv2
I1226 07:40:47.576289 90196 net.cpp:573] relu2 -> conv2 (in-place)
I1226 07:40:47.576385 90196 net.cpp:228] Setting up relu2
I1226 07:40:47.576426 90196 net.cpp:235] Top shape: 64 256 27 27 (11943936)
I1226 07:40:47.576474 90196 net.cpp:243] Memory required for data: 376069632
I1226 07:40:47.576508 90196 layer_factory.hpp:114] Creating layer norm2
I1226 07:40:47.576560 90196 net.cpp:178] Creating Layer norm2
I1226 07:40:47.576596 90196 net.cpp:612] norm2 <- conv2
I1226 07:40:47.576633 90196 net.cpp:586] norm2 -> norm2
I1226 07:40:47.576733 90196 net.cpp:228] Setting up norm2
I1226 07:40:47.576781 90196 net.cpp:235] Top shape: 64 256 27 27 (11943936)
I1226 07:40:47.576803 90196 net.cpp:243] Memory required for data: 423845376
I1226 07:40:47.576831 90196 layer_factory.hpp:114] Creating layer pool2
I1226 07:40:47.576884 90196 net.cpp:178] Creating Layer pool2
I1226 07:40:47.576925 90196 net.cpp:612] pool2 <- norm2
I1226 07:40:47.576966 90196 net.cpp:586] pool2 -> pool2
I1226 07:40:47.577040 90196 net.cpp:228] Setting up pool2
I1226 07:40:47.577080 90196 net.cpp:235] Top shape: 64 256 13 13 (2768896)
I1226 07:40:47.577206 90196 net.cpp:243] Memory required for data: 434920960
I1226 07:40:47.577241 90196 layer_factory.hpp:114] Creating layer conv3
I1226 07:40:47.577324 90196 net.cpp:178] Creating Layer conv3
I1226 07:40:47.577360 90196 net.cpp:612] conv3 <- pool2
I1226 07:40:47.577404 90196 net.cpp:586] conv3 -> conv3
I1226 07:40:47.596541 88308 net.cpp:228] Setting up conv2
I1226 07:40:47.596684 88308 net.cpp:235] Top shape: 64 256 27 27 (11943936)
I1226 07:40:47.596717 88308 net.cpp:243] Memory required for data: 328293888
I1226 07:40:47.596796 88308 layer_factory.hpp:114] Creating layer relu2
I1226 07:40:47.596894 88308 net.cpp:178] Creating Layer relu2
I1226 07:40:47.597106 88308 net.cpp:612] relu2 <- conv2
I1226 07:40:47.597165 88308 net.cpp:573] relu2 -> conv2 (in-place)
I1226 07:40:47.597328 88308 net.cpp:228] Setting up relu2
I1226 07:40:47.597383 88308 net.cpp:235] Top shape: 64 256 27 27 (11943936)
I1226 07:40:47.597406 88308 net.cpp:243] Memory required for data: 376069632
I1226 07:40:47.597831 88308 layer_factory.hpp:114] Creating layer norm2
I1226 07:40:47.597913 88308 net.cpp:178] Creating Layer norm2
I1226 07:40:47.598019 88308 net.cpp:612] norm2 <- conv2
I1226 07:40:47.598089 88308 net.cpp:586] norm2 -> norm2
I1226 07:40:47.598234 88308 net.cpp:228] Setting up norm2
I1226 07:40:47.598301 88308 net.cpp:235] Top shape: 64 256 27 27 (11943936)
I1226 07:40:47.598327 88308 net.cpp:243] Memory required for data: 423845376
I1226 07:40:47.598357 88308 layer_factory.hpp:114] Creating layer pool2
I1226 07:40:47.598417 88308 net.cpp:178] Creating Layer pool2
I1226 07:40:47.598446 88308 net.cpp:612] pool2 <- norm2
I1226 07:40:47.598482 88308 net.cpp:586] pool2 -> pool2
I1226 07:40:47.598637 88308 net.cpp:228] Setting up pool2
I1226 07:40:47.598692 88308 net.cpp:235] Top shape: 64 256 13 13 (2768896)
I1226 07:40:47.599057 88308 net.cpp:243] Memory required for data: 434920960
I1226 07:40:47.599098 88308 layer_factory.hpp:114] Creating layer conv3
I1226 07:40:47.599212 88308 net.cpp:178] Creating Layer conv3
I1226 07:40:47.599261 88308 net.cpp:612] conv3 <- pool2
I1226 07:40:47.599311 88308 net.cpp:586] conv3 -> conv3
I1226 07:40:44.346052 89666 net.cpp:228] Setting up conv2
I1226 07:40:44.346355 89666 net.cpp:235] Top shape: 64 256 27 27 (11943936)
I1226 07:40:44.346400 89666 net.cpp:243] Memory required for data: 328293888
I1226 07:40:44.347152 89666 layer_factory.hpp:114] Creating layer relu2
I1226 07:40:44.347302 89666 net.cpp:178] Creating Layer relu2
I1226 07:40:44.347353 89666 net.cpp:612] relu2 <- conv2
I1226 07:40:44.347425 89666 net.cpp:573] relu2 -> conv2 (in-place)
I1226 07:40:44.347558 89666 net.cpp:228] Setting up relu2
I1226 07:40:44.347647 89666 net.cpp:235] Top shape: 64 256 27 27 (11943936)
I1226 07:40:44.347676 89666 net.cpp:243] Memory required for data: 376069632
I1226 07:40:44.347721 89666 layer_factory.hpp:114] Creating layer norm2
I1226 07:40:44.347803 89666 net.cpp:178] Creating Layer norm2
I1226 07:40:44.347851 89666 net.cpp:612] norm2 <- conv2
I1226 07:40:44.348016 89666 net.cpp:586] norm2 -> norm2
I1226 07:40:44.348196 89666 net.cpp:228] Setting up norm2
I1226 07:40:44.348254 89666 net.cpp:235] Top shape: 64 256 27 27 (11943936)
I1226 07:40:44.348278 89666 net.cpp:243] Memory required for data: 423845376
I1226 07:40:44.348309 89666 layer_factory.hpp:114] Creating layer pool2
I1226 07:40:44.348716 89666 net.cpp:178] Creating Layer pool2
I1226 07:40:44.348790 89666 net.cpp:612] pool2 <- norm2
I1226 07:40:44.348853 89666 net.cpp:586] pool2 -> pool2
I1226 07:40:44.349027 89666 net.cpp:228] Setting up pool2
I1226 07:40:44.349123 89666 net.cpp:235] Top shape: 64 256 13 13 (2768896)
I1226 07:40:44.349261 89666 net.cpp:243] Memory required for data: 434920960
I1226 07:40:44.349299 89666 layer_factory.hpp:114] Creating layer conv3
I1226 07:40:44.349411 89666 net.cpp:178] Creating Layer conv3
I1226 07:40:44.349453 89666 net.cpp:612] conv3 <- pool2
I1226 07:40:44.349503 89666 net.cpp:586] conv3 -> conv3
I1226 07:40:47.763855 93793 net.cpp:228] Setting up conv1
I1226 07:40:47.763973 93793 net.cpp:235] Top shape: 64 96 55 55 (18585600)
I1226 07:40:47.764004 93793 net.cpp:243] Memory required for data: 113917440
I1226 07:40:47.764119 93793 layer_factory.hpp:114] Creating layer relu1
I1226 07:40:47.764300 93793 net.cpp:178] Creating Layer relu1
I1226 07:40:47.764348 93793 net.cpp:612] relu1 <- conv1
I1226 07:40:47.764392 93793 net.cpp:573] relu1 -> conv1 (in-place)
I1226 07:40:47.764511 93793 net.cpp:228] Setting up relu1
I1226 07:40:47.764560 93793 net.cpp:235] Top shape: 64 96 55 55 (18585600)
I1226 07:40:47.764585 93793 net.cpp:243] Memory required for data: 188259840
I1226 07:40:47.764616 93793 layer_factory.hpp:114] Creating layer norm1
I1226 07:40:47.764695 93793 net.cpp:178] Creating Layer norm1
I1226 07:40:47.764730 93793 net.cpp:612] norm1 <- conv1
I1226 07:40:47.764785 93793 net.cpp:586] norm1 -> norm1
I1226 07:40:47.769490 93793 net.cpp:228] Setting up norm1
I1226 07:40:47.769572 93793 net.cpp:235] Top shape: 64 96 55 55 (18585600)
I1226 07:40:47.769599 93793 net.cpp:243] Memory required for data: 262602240
I1226 07:40:47.769636 93793 layer_factory.hpp:114] Creating layer pool1
I1226 07:40:47.769721 93793 net.cpp:178] Creating Layer pool1
I1226 07:40:47.769829 93793 net.cpp:612] pool1 <- norm1
I1226 07:40:47.769888 93793 net.cpp:586] pool1 -> pool1
I1226 07:40:47.770036 93793 net.cpp:228] Setting up pool1
I1226 07:40:47.770115 93793 net.cpp:235] Top shape: 64 96 27 27 (4478976)
I1226 07:40:47.770143 93793 net.cpp:243] Memory required for data: 280518144
I1226 07:40:47.770175 93793 layer_factory.hpp:114] Creating layer conv2
I1226 07:40:47.770311 93793 net.cpp:178] Creating Layer conv2
I1226 07:40:47.770352 93793 net.cpp:612] conv2 <- pool1
I1226 07:40:47.770421 93793 net.cpp:586] conv2 -> conv2
I1226 07:40:47.889976 89771 net.cpp:228] Setting up conv3
I1226 07:40:47.890084 89771 net.cpp:235] Top shape: 64 384 13 13 (4153344)
I1226 07:40:47.890110 89771 net.cpp:243] Memory required for data: 451534336
I1226 07:40:47.890184 89771 layer_factory.hpp:114] Creating layer relu3
I1226 07:40:47.890269 89771 net.cpp:178] Creating Layer relu3
I1226 07:40:47.890306 89771 net.cpp:612] relu3 <- conv3
I1226 07:40:47.890348 89771 net.cpp:573] relu3 -> conv3 (in-place)
I1226 07:40:47.890465 89771 net.cpp:228] Setting up relu3
I1226 07:40:47.890559 89771 net.cpp:235] Top shape: 64 384 13 13 (4153344)
I1226 07:40:47.890589 89771 net.cpp:243] Memory required for data: 468147712
I1226 07:40:47.890624 89771 layer_factory.hpp:114] Creating layer conv4
I1226 07:40:47.890724 89771 net.cpp:178] Creating Layer conv4
I1226 07:40:47.890759 89771 net.cpp:612] conv4 <- conv3
I1226 07:40:47.890815 89771 net.cpp:586] conv4 -> conv4
I1226 07:40:47.900825 89800 net.cpp:228] Setting up conv3
I1226 07:40:47.900935 89800 net.cpp:235] Top shape: 64 384 13 13 (4153344)
I1226 07:40:47.900987 89800 net.cpp:243] Memory required for data: 451534336
I1226 07:40:47.901082 89800 layer_factory.hpp:114] Creating layer relu3
I1226 07:40:47.901223 89800 net.cpp:178] Creating Layer relu3
I1226 07:40:47.901319 89800 net.cpp:612] relu3 <- conv3
I1226 07:40:47.901428 89800 net.cpp:573] relu3 -> conv3 (in-place)
I1226 07:40:47.901875 89800 net.cpp:228] Setting up relu3
I1226 07:40:47.901967 89800 net.cpp:235] Top shape: 64 384 13 13 (4153344)
I1226 07:40:47.901996 89800 net.cpp:243] Memory required for data: 468147712
I1226 07:40:47.902125 89800 layer_factory.hpp:114] Creating layer conv4
I1226 07:40:47.902233 89800 net.cpp:178] Creating Layer conv4
I1226 07:40:47.902287 89800 net.cpp:612] conv4 <- conv3
I1226 07:40:47.902354 89800 net.cpp:586] conv4 -> conv4
I1226 07:40:47.929378 86513 net.cpp:228] Setting up conv3
I1226 07:40:47.929494 86513 net.cpp:235] Top shape: 64 384 13 13 (4153344)
I1226 07:40:47.929530 86513 net.cpp:243] Memory required for data: 451534336
I1226 07:40:47.929628 86513 layer_factory.hpp:114] Creating layer relu3
I1226 07:40:47.929719 86513 net.cpp:178] Creating Layer relu3
I1226 07:40:47.929764 86513 net.cpp:612] relu3 <- conv3
I1226 07:40:47.929814 86513 net.cpp:573] relu3 -> conv3 (in-place)
I1226 07:40:47.929930 86513 net.cpp:228] Setting up relu3
I1226 07:40:47.929981 86513 net.cpp:235] Top shape: 64 384 13 13 (4153344)
I1226 07:40:47.930011 86513 net.cpp:243] Memory required for data: 468147712
I1226 07:40:47.930047 86513 layer_factory.hpp:114] Creating layer conv4
I1226 07:40:47.930156 86513 net.cpp:178] Creating Layer conv4
I1226 07:40:47.930207 86513 net.cpp:612] conv4 <- conv3
I1226 07:40:47.930270 86513 net.cpp:586] conv4 -> conv4
I1226 07:40:47.948001 90196 net.cpp:228] Setting up conv3
I1226 07:40:47.948114 90196 net.cpp:235] Top shape: 64 384 13 13 (4153344)
I1226 07:40:47.948142 90196 net.cpp:243] Memory required for data: 451534336
I1226 07:40:47.948231 90196 layer_factory.hpp:114] Creating layer relu3
I1226 07:40:47.948298 90196 net.cpp:178] Creating Layer relu3
I1226 07:40:47.948329 90196 net.cpp:612] relu3 <- conv3
I1226 07:40:47.948390 90196 net.cpp:573] relu3 -> conv3 (in-place)
I1226 07:40:47.948506 90196 net.cpp:228] Setting up relu3
I1226 07:40:47.948557 90196 net.cpp:235] Top shape: 64 384 13 13 (4153344)
I1226 07:40:47.948581 90196 net.cpp:243] Memory required for data: 468147712
I1226 07:40:47.948609 90196 layer_factory.hpp:114] Creating layer conv4
I1226 07:40:47.948690 90196 net.cpp:178] Creating Layer conv4
I1226 07:40:47.948725 90196 net.cpp:612] conv4 <- conv3
I1226 07:40:47.948765 90196 net.cpp:586] conv4 -> conv4
I1226 07:40:47.992593 89665 net.cpp:228] Setting up conv3
I1226 07:40:47.992722 89665 net.cpp:235] Top shape: 64 384 13 13 (4153344)
I1226 07:40:47.993363 89665 net.cpp:243] Memory required for data: 451534336
I1226 07:40:47.993655 89665 layer_factory.hpp:114] Creating layer relu3
I1226 07:40:47.993787 89665 net.cpp:178] Creating Layer relu3
I1226 07:40:47.993852 89665 net.cpp:612] relu3 <- conv3
I1226 07:40:47.993908 89665 net.cpp:573] relu3 -> conv3 (in-place)
I1226 07:40:47.994000 89665 net.cpp:228] Setting up relu3
I1226 07:40:47.994050 89665 net.cpp:235] Top shape: 64 384 13 13 (4153344)
I1226 07:40:47.994073 89665 net.cpp:243] Memory required for data: 468147712
I1226 07:40:47.994103 89665 layer_factory.hpp:114] Creating layer conv4
I1226 07:40:47.994199 89665 net.cpp:178] Creating Layer conv4
I1226 07:40:47.994266 89665 net.cpp:612] conv4 <- conv3
I1226 07:40:47.994328 89665 net.cpp:586] conv4 -> conv4
I1226 07:40:48.001472 88308 net.cpp:228] Setting up conv3
I1226 07:40:48.001652 88308 net.cpp:235] Top shape: 64 384 13 13 (4153344)
I1226 07:40:48.001839 88308 net.cpp:243] Memory required for data: 451534336
I1226 07:40:48.001950 88308 layer_factory.hpp:114] Creating layer relu3
I1226 07:40:48.002046 88308 net.cpp:178] Creating Layer relu3
I1226 07:40:48.002089 88308 net.cpp:612] relu3 <- conv3
I1226 07:40:48.002148 88308 net.cpp:573] relu3 -> conv3 (in-place)
I1226 07:40:48.002380 88308 net.cpp:228] Setting up relu3
I1226 07:40:48.002486 88308 net.cpp:235] Top shape: 64 384 13 13 (4153344)
I1226 07:40:48.002671 88308 net.cpp:243] Memory required for data: 468147712
I1226 07:40:48.002713 88308 layer_factory.hpp:114] Creating layer conv4
I1226 07:40:48.003367 88308 net.cpp:178] Creating Layer conv4
I1226 07:40:48.003448 88308 net.cpp:612] conv4 <- conv3
I1226 07:40:48.003532 88308 net.cpp:586] conv4 -> conv4
I1226 07:40:48.029510 92408 net.cpp:228] Setting up conv2
I1226 07:40:48.029634 92408 net.cpp:235] Top shape: 64 256 27 27 (11943936)
I1226 07:40:48.029673 92408 net.cpp:243] Memory required for data: 328293888
I1226 07:40:48.029758 92408 layer_factory.hpp:114] Creating layer relu2
I1226 07:40:48.029898 92408 net.cpp:178] Creating Layer relu2
I1226 07:40:48.029973 92408 net.cpp:612] relu2 <- conv2
I1226 07:40:48.030045 92408 net.cpp:573] relu2 -> conv2 (in-place)
I1226 07:40:48.030153 92408 net.cpp:228] Setting up relu2
I1226 07:40:48.030217 92408 net.cpp:235] Top shape: 64 256 27 27 (11943936)
I1226 07:40:48.030246 92408 net.cpp:243] Memory required for data: 376069632
I1226 07:40:48.030280 92408 layer_factory.hpp:114] Creating layer norm2
I1226 07:40:48.030362 92408 net.cpp:178] Creating Layer norm2
I1226 07:40:48.030405 92408 net.cpp:612] norm2 <- conv2
I1226 07:40:48.030448 92408 net.cpp:586] norm2 -> norm2
I1226 07:40:48.030570 92408 net.cpp:228] Setting up norm2
I1226 07:40:48.030630 92408 net.cpp:235] Top shape: 64 256 27 27 (11943936)
I1226 07:40:48.030663 92408 net.cpp:243] Memory required for data: 423845376
I1226 07:40:48.030694 92408 layer_factory.hpp:114] Creating layer pool2
I1226 07:40:48.030760 92408 net.cpp:178] Creating Layer pool2
I1226 07:40:48.030802 92408 net.cpp:612] pool2 <- norm2
I1226 07:40:48.030866 92408 net.cpp:586] pool2 -> pool2
I1226 07:40:48.030972 92408 net.cpp:228] Setting up pool2
I1226 07:40:48.031029 92408 net.cpp:235] Top shape: 64 256 13 13 (2768896)
I1226 07:40:48.031158 92408 net.cpp:243] Memory required for data: 434920960
I1226 07:40:48.031199 92408 layer_factory.hpp:114] Creating layer conv3
I1226 07:40:48.031306 92408 net.cpp:178] Creating Layer conv3
I1226 07:40:48.031345 92408 net.cpp:612] conv3 <- pool2
I1226 07:40:48.031412 92408 net.cpp:586] conv3 -> conv3
I1226 07:40:44.833992 89666 net.cpp:228] Setting up conv3
I1226 07:40:44.834130 89666 net.cpp:235] Top shape: 64 384 13 13 (4153344)
I1226 07:40:44.834161 89666 net.cpp:243] Memory required for data: 451534336
I1226 07:40:44.834291 89666 layer_factory.hpp:114] Creating layer relu3
I1226 07:40:44.834372 89666 net.cpp:178] Creating Layer relu3
I1226 07:40:44.834409 89666 net.cpp:612] relu3 <- conv3
I1226 07:40:44.834489 89666 net.cpp:573] relu3 -> conv3 (in-place)
I1226 07:40:44.834745 89666 net.cpp:228] Setting up relu3
I1226 07:40:44.834801 89666 net.cpp:235] Top shape: 64 384 13 13 (4153344)
I1226 07:40:44.834827 89666 net.cpp:243] Memory required for data: 468147712
I1226 07:40:44.834856 89666 layer_factory.hpp:114] Creating layer conv4
I1226 07:40:44.834975 89666 net.cpp:178] Creating Layer conv4
I1226 07:40:44.835012 89666 net.cpp:612] conv4 <- conv3
I1226 07:40:44.835062 89666 net.cpp:586] conv4 -> conv4
I1226 07:40:48.220655 89771 net.cpp:228] Setting up conv4
I1226 07:40:48.220762 89771 net.cpp:235] Top shape: 64 384 13 13 (4153344)
I1226 07:40:48.220791 89771 net.cpp:243] Memory required for data: 484761088
I1226 07:40:48.220854 89771 layer_factory.hpp:114] Creating layer relu4
I1226 07:40:48.220921 89771 net.cpp:178] Creating Layer relu4
I1226 07:40:48.220955 89771 net.cpp:612] relu4 <- conv4
I1226 07:40:48.221628 89771 net.cpp:573] relu4 -> conv4 (in-place)
I1226 07:40:48.221771 89771 net.cpp:228] Setting up relu4
I1226 07:40:48.221842 89771 net.cpp:235] Top shape: 64 384 13 13 (4153344)
I1226 07:40:48.221871 89771 net.cpp:243] Memory required for data: 501374464
I1226 07:40:48.221907 89771 layer_factory.hpp:114] Creating layer conv5
I1226 07:40:48.222000 89771 net.cpp:178] Creating Layer conv5
I1226 07:40:48.222034 89771 net.cpp:612] conv5 <- conv4
I1226 07:40:48.222092 89771 net.cpp:586] conv5 -> conv5
I1226 07:40:48.257977 89800 net.cpp:228] Setting up conv4
I1226 07:40:48.258111 89800 net.cpp:235] Top shape: 64 384 13 13 (4153344)
I1226 07:40:48.258141 89800 net.cpp:243] Memory required for data: 484761088
I1226 07:40:48.258200 89800 layer_factory.hpp:114] Creating layer relu4
I1226 07:40:48.258268 89800 net.cpp:178] Creating Layer relu4
I1226 07:40:48.258301 89800 net.cpp:612] relu4 <- conv4
I1226 07:40:48.258352 89800 net.cpp:573] relu4 -> conv4 (in-place)
I1226 07:40:48.258479 89800 net.cpp:228] Setting up relu4
I1226 07:40:48.258528 89800 net.cpp:235] Top shape: 64 384 13 13 (4153344)
I1226 07:40:48.258554 89800 net.cpp:243] Memory required for data: 501374464
I1226 07:40:48.258584 89800 layer_factory.hpp:114] Creating layer conv5
I1226 07:40:48.258671 89800 net.cpp:178] Creating Layer conv5
I1226 07:40:48.258739 89800 net.cpp:612] conv5 <- conv4
I1226 07:40:48.258780 89800 net.cpp:586] conv5 -> conv5
I1226 07:40:48.257457 86513 net.cpp:228] Setting up conv4
I1226 07:40:48.257738 86513 net.cpp:235] Top shape: 64 384 13 13 (4153344)
I1226 07:40:48.257786 86513 net.cpp:243] Memory required for data: 484761088
I1226 07:40:48.257885 86513 layer_factory.hpp:114] Creating layer relu4
I1226 07:40:48.258473 86513 net.cpp:178] Creating Layer relu4
I1226 07:40:48.258589 86513 net.cpp:612] relu4 <- conv4
I1226 07:40:48.258683 86513 net.cpp:573] relu4 -> conv4 (in-place)
I1226 07:40:48.258996 86513 net.cpp:228] Setting up relu4
I1226 07:40:48.259091 86513 net.cpp:235] Top shape: 64 384 13 13 (4153344)
I1226 07:40:48.259151 86513 net.cpp:243] Memory required for data: 501374464
I1226 07:40:48.259383 86513 layer_factory.hpp:114] Creating layer conv5
I1226 07:40:48.259521 86513 net.cpp:178] Creating Layer conv5
I1226 07:40:48.259570 86513 net.cpp:612] conv5 <- conv4
I1226 07:40:48.259660 86513 net.cpp:586] conv5 -> conv5
I1226 07:40:48.277351 90196 net.cpp:228] Setting up conv4
I1226 07:40:48.277523 90196 net.cpp:235] Top shape: 64 384 13 13 (4153344)
I1226 07:40:48.277577 90196 net.cpp:243] Memory required for data: 484761088
I1226 07:40:48.277642 90196 layer_factory.hpp:114] Creating layer relu4
I1226 07:40:48.277745 90196 net.cpp:178] Creating Layer relu4
I1226 07:40:48.277781 90196 net.cpp:612] relu4 <- conv4
I1226 07:40:48.277827 90196 net.cpp:573] relu4 -> conv4 (in-place)
I1226 07:40:48.278255 90196 net.cpp:228] Setting up relu4
I1226 07:40:48.278376 90196 net.cpp:235] Top shape: 64 384 13 13 (4153344)
I1226 07:40:48.278479 90196 net.cpp:243] Memory required for data: 501374464
I1226 07:40:48.278532 90196 layer_factory.hpp:114] Creating layer conv5
I1226 07:40:48.278657 90196 net.cpp:178] Creating Layer conv5
I1226 07:40:48.278702 90196 net.cpp:612] conv5 <- conv4
I1226 07:40:48.278748 90196 net.cpp:586] conv5 -> conv5
I1226 07:40:48.346879 89665 net.cpp:228] Setting up conv4
I1226 07:40:48.347000 89665 net.cpp:235] Top shape: 64 384 13 13 (4153344)
I1226 07:40:48.347031 89665 net.cpp:243] Memory required for data: 484761088
I1226 07:40:48.347090 89665 layer_factory.hpp:114] Creating layer relu4
I1226 07:40:48.347153 89665 net.cpp:178] Creating Layer relu4
I1226 07:40:48.347183 89665 net.cpp:612] relu4 <- conv4
I1226 07:40:48.347231 89665 net.cpp:573] relu4 -> conv4 (in-place)
I1226 07:40:48.347324 89665 net.cpp:228] Setting up relu4
I1226 07:40:48.347368 89665 net.cpp:235] Top shape: 64 384 13 13 (4153344)
I1226 07:40:48.347391 89665 net.cpp:243] Memory required for data: 501374464
I1226 07:40:48.347419 89665 layer_factory.hpp:114] Creating layer conv5
I1226 07:40:48.347494 89665 net.cpp:178] Creating Layer conv5
I1226 07:40:48.347535 89665 net.cpp:612] conv5 <- conv4
I1226 07:40:48.347574 89665 net.cpp:586] conv5 -> conv5
I1226 07:40:48.359325 88308 net.cpp:228] Setting up conv4
I1226 07:40:48.359484 88308 net.cpp:235] Top shape: 64 384 13 13 (4153344)
I1226 07:40:48.359535 88308 net.cpp:243] Memory required for data: 484761088
I1226 07:40:48.359652 88308 layer_factory.hpp:114] Creating layer relu4
I1226 07:40:48.359794 88308 net.cpp:178] Creating Layer relu4
I1226 07:40:48.359828 88308 net.cpp:612] relu4 <- conv4
I1226 07:40:48.359879 88308 net.cpp:573] relu4 -> conv4 (in-place)
I1226 07:40:48.359977 88308 net.cpp:228] Setting up relu4
I1226 07:40:48.360045 88308 net.cpp:235] Top shape: 64 384 13 13 (4153344)
I1226 07:40:48.360071 88308 net.cpp:243] Memory required for data: 501374464
I1226 07:40:48.360100 88308 layer_factory.hpp:114] Creating layer conv5
I1226 07:40:48.360352 88308 net.cpp:178] Creating Layer conv5
I1226 07:40:48.360394 88308 net.cpp:612] conv5 <- conv4
I1226 07:40:48.360478 88308 net.cpp:586] conv5 -> conv5
I1226 07:40:48.444412 89771 net.cpp:228] Setting up conv5
I1226 07:40:48.444892 89771 net.cpp:235] Top shape: 64 256 13 13 (2768896)
I1226 07:40:48.445006 89771 net.cpp:243] Memory required for data: 512450048
I1226 07:40:48.445125 89771 layer_factory.hpp:114] Creating layer relu5
I1226 07:40:48.445227 89771 net.cpp:178] Creating Layer relu5
I1226 07:40:48.445288 89771 net.cpp:612] relu5 <- conv5
I1226 07:40:48.445365 89771 net.cpp:573] relu5 -> conv5 (in-place)
I1226 07:40:48.445510 89771 net.cpp:228] Setting up relu5
I1226 07:40:48.445585 89771 net.cpp:235] Top shape: 64 256 13 13 (2768896)
I1226 07:40:48.445618 89771 net.cpp:243] Memory required for data: 523525632
I1226 07:40:48.445669 89771 layer_factory.hpp:114] Creating layer pool5
I1226 07:40:48.445744 89771 net.cpp:178] Creating Layer pool5
I1226 07:40:48.445773 89771 net.cpp:612] pool5 <- conv5
I1226 07:40:48.445838 89771 net.cpp:586] pool5 -> pool5
I1226 07:40:48.446517 89771 net.cpp:228] Setting up pool5
I1226 07:40:48.446714 89771 net.cpp:235] Top shape: 64 256 6 6 (589824)
I1226 07:40:48.446760 89771 net.cpp:243] Memory required for data: 525884928
I1226 07:40:48.446799 89771 layer_factory.hpp:114] Creating layer fc6
I1226 07:40:48.446876 89771 net.cpp:178] Creating Layer fc6
I1226 07:40:48.446910 89771 net.cpp:612] fc6 <- pool5
I1226 07:40:48.447013 89771 net.cpp:586] fc6 -> fc6
I1226 07:40:48.512205 86513 net.cpp:228] Setting up conv5
I1226 07:40:48.512372 86513 net.cpp:235] Top shape: 64 256 13 13 (2768896)
I1226 07:40:48.512413 86513 net.cpp:243] Memory required for data: 512450048
I1226 07:40:48.512509 86513 layer_factory.hpp:114] Creating layer relu5
I1226 07:40:48.512604 86513 net.cpp:178] Creating Layer relu5
I1226 07:40:48.512681 86513 net.cpp:612] relu5 <- conv5
I1226 07:40:48.512764 86513 net.cpp:573] relu5 -> conv5 (in-place)
I1226 07:40:48.513010 86513 net.cpp:228] Setting up relu5
I1226 07:40:48.513178 86513 net.cpp:235] Top shape: 64 256 13 13 (2768896)
I1226 07:40:48.513211 86513 net.cpp:243] Memory required for data: 523525632
I1226 07:40:45.247546 89666 net.cpp:228] Setting up conv4
I1226 07:40:45.247700 89666 net.cpp:235] Top shape: 64 384 13 13 (4153344)
I1226 07:40:45.247730 89666 net.cpp:243] Memory required for data: 484761088
I1226 07:40:48.513958 86513 layer_factory.hpp:114] Creating layer pool5
I1226 07:40:45.247791 89666 layer_factory.hpp:114] Creating layer relu4
I1226 07:40:45.247879 89666 net.cpp:178] Creating Layer relu4
I1226 07:40:45.247920 89666 net.cpp:612] relu4 <- conv4
I1226 07:40:48.514147 86513 net.cpp:178] Creating Layer pool5
I1226 07:40:45.247969 89666 net.cpp:573] relu4 -> conv4 (in-place)
I1226 07:40:48.514323 86513 net.cpp:612] pool5 <- conv5
I1226 07:40:45.248065 89666 net.cpp:228] Setting up relu4
I1226 07:40:48.514410 86513 net.cpp:586] pool5 -> pool5
I1226 07:40:45.248108 89666 net.cpp:235] Top shape: 64 384 13 13 (4153344)
I1226 07:40:45.248132 89666 net.cpp:243] Memory required for data: 501374464
I1226 07:40:48.514624 86513 net.cpp:228] Setting up pool5
I1226 07:40:45.248162 89666 layer_factory.hpp:114] Creating layer conv5
I1226 07:40:48.514789 86513 net.cpp:235] Top shape: 64 256 6 6 (589824)
I1226 07:40:45.248229 89666 net.cpp:178] Creating Layer conv5
I1226 07:40:48.514824 86513 net.cpp:243] Memory required for data: 525884928
I1226 07:40:48.514909 86513 layer_factory.hpp:114] Creating layer fc6
I1226 07:40:45.248261 89666 net.cpp:612] conv5 <- conv4
I1226 07:40:45.249006 89666 net.cpp:586] conv5 -> conv5
I1226 07:40:48.515019 86513 net.cpp:178] Creating Layer fc6
I1226 07:40:48.515481 86513 net.cpp:612] fc6 <- pool5
I1226 07:40:48.515573 86513 net.cpp:586] fc6 -> fc6
I1226 07:40:48.533840 89800 net.cpp:228] Setting up conv5
I1226 07:40:48.533956 89800 net.cpp:235] Top shape: 64 256 13 13 (2768896)
I1226 07:40:48.533984 89800 net.cpp:243] Memory required for data: 512450048
I1226 07:40:48.534061 89800 layer_factory.hpp:114] Creating layer relu5
I1226 07:40:48.534152 89800 net.cpp:178] Creating Layer relu5
I1226 07:40:48.534240 89800 net.cpp:612] relu5 <- conv5
I1226 07:40:48.534368 89800 net.cpp:573] relu5 -> conv5 (in-place)
I1226 07:40:48.534497 89800 net.cpp:228] Setting up relu5
I1226 07:40:48.534606 89800 net.cpp:235] Top shape: 64 256 13 13 (2768896)
I1226 07:40:48.534632 89800 net.cpp:243] Memory required for data: 523525632
I1226 07:40:48.534663 89800 layer_factory.hpp:114] Creating layer pool5
I1226 07:40:48.534806 89800 net.cpp:178] Creating Layer pool5
I1226 07:40:48.534907 89800 net.cpp:612] pool5 <- conv5
I1226 07:40:48.534960 89800 net.cpp:586] pool5 -> pool5
I1226 07:40:48.535051 89800 net.cpp:228] Setting up pool5
I1226 07:40:48.535095 89800 net.cpp:235] Top shape: 64 256 6 6 (589824)
I1226 07:40:48.535130 89800 net.cpp:243] Memory required for data: 525884928
I1226 07:40:48.535166 89800 layer_factory.hpp:114] Creating layer fc6
I1226 07:40:48.535230 89800 net.cpp:178] Creating Layer fc6
I1226 07:40:48.535264 89800 net.cpp:612] fc6 <- pool5
I1226 07:40:48.535313 89800 net.cpp:586] fc6 -> fc6
I1226 07:40:48.534034 90196 net.cpp:228] Setting up conv5
I1226 07:40:48.534152 90196 net.cpp:235] Top shape: 64 256 13 13 (2768896)
I1226 07:40:48.534181 90196 net.cpp:243] Memory required for data: 512450048
I1226 07:40:48.534257 90196 layer_factory.hpp:114] Creating layer relu5
I1226 07:40:48.534378 90196 net.cpp:178] Creating Layer relu5
I1226 07:40:48.534438 90196 net.cpp:612] relu5 <- conv5
I1226 07:40:48.534520 90196 net.cpp:573] relu5 -> conv5 (in-place)
I1226 07:40:48.534656 90196 net.cpp:228] Setting up relu5
I1226 07:40:48.534754 90196 net.cpp:235] Top shape: 64 256 13 13 (2768896)
I1226 07:40:48.534780 90196 net.cpp:243] Memory required for data: 523525632
I1226 07:40:48.534809 90196 layer_factory.hpp:114] Creating layer pool5
I1226 07:40:48.534864 90196 net.cpp:178] Creating Layer pool5
I1226 07:40:48.534903 90196 net.cpp:612] pool5 <- conv5
I1226 07:40:48.534963 90196 net.cpp:586] pool5 -> pool5
I1226 07:40:48.535076 90196 net.cpp:228] Setting up pool5
I1226 07:40:48.535190 90196 net.cpp:235] Top shape: 64 256 6 6 (589824)
I1226 07:40:48.535223 90196 net.cpp:243] Memory required for data: 525884928
I1226 07:40:48.535251 90196 layer_factory.hpp:114] Creating layer fc6
I1226 07:40:48.535322 90196 net.cpp:178] Creating Layer fc6
I1226 07:40:48.535368 90196 net.cpp:612] fc6 <- pool5
I1226 07:40:48.535408 90196 net.cpp:586] fc6 -> fc6
I1226 07:40:48.611259 88308 net.cpp:228] Setting up conv5
I1226 07:40:48.611373 88308 net.cpp:235] Top shape: 64 256 13 13 (2768896)
I1226 07:40:48.611402 88308 net.cpp:243] Memory required for data: 512450048
I1226 07:40:48.611476 88308 layer_factory.hpp:114] Creating layer relu5
I1226 07:40:48.611601 88308 net.cpp:178] Creating Layer relu5
I1226 07:40:48.611794 88308 net.cpp:612] relu5 <- conv5
I1226 07:40:48.611850 88308 net.cpp:573] relu5 -> conv5 (in-place)
I1226 07:40:48.611969 88308 net.cpp:228] Setting up relu5
I1226 07:40:48.612032 88308 net.cpp:235] Top shape: 64 256 13 13 (2768896)
I1226 07:40:48.612057 88308 net.cpp:243] Memory required for data: 523525632
I1226 07:40:48.612092 88308 layer_factory.hpp:114] Creating layer pool5
I1226 07:40:48.612150 88308 net.cpp:178] Creating Layer pool5
I1226 07:40:48.612193 88308 net.cpp:612] pool5 <- conv5
I1226 07:40:48.612257 88308 net.cpp:586] pool5 -> pool5
I1226 07:40:48.612359 88308 net.cpp:228] Setting up pool5
I1226 07:40:48.612408 88308 net.cpp:235] Top shape: 64 256 6 6 (589824)
I1226 07:40:48.612437 88308 net.cpp:243] Memory required for data: 525884928
I1226 07:40:48.612468 88308 layer_factory.hpp:114] Creating layer fc6
I1226 07:40:48.612542 88308 net.cpp:178] Creating Layer fc6
I1226 07:40:48.612601 88308 net.cpp:612] fc6 <- pool5
I1226 07:40:48.612673 88308 net.cpp:586] fc6 -> fc6
I1226 07:40:48.624017 89665 net.cpp:228] Setting up conv5
I1226 07:40:48.624125 89665 net.cpp:235] Top shape: 64 256 13 13 (2768896)
I1226 07:40:48.624153 89665 net.cpp:243] Memory required for data: 512450048
I1226 07:40:48.624258 89665 layer_factory.hpp:114] Creating layer relu5
I1226 07:40:48.624341 89665 net.cpp:178] Creating Layer relu5
I1226 07:40:48.624382 89665 net.cpp:612] relu5 <- conv5
I1226 07:40:48.624423 89665 net.cpp:573] relu5 -> conv5 (in-place)
I1226 07:40:48.624517 89665 net.cpp:228] Setting up relu5
I1226 07:40:48.624560 89665 net.cpp:235] Top shape: 64 256 13 13 (2768896)
I1226 07:40:48.624583 89665 net.cpp:243] Memory required for data: 523525632
I1226 07:40:48.624611 89665 layer_factory.hpp:114] Creating layer pool5
I1226 07:40:48.624660 89665 net.cpp:178] Creating Layer pool5
I1226 07:40:48.624693 89665 net.cpp:612] pool5 <- conv5
I1226 07:40:48.624729 89665 net.cpp:586] pool5 -> pool5
I1226 07:40:48.624864 89665 net.cpp:228] Setting up pool5
I1226 07:40:48.624917 89665 net.cpp:235] Top shape: 64 256 6 6 (589824)
I1226 07:40:48.624939 89665 net.cpp:243] Memory required for data: 525884928
I1226 07:40:48.624968 89665 layer_factory.hpp:114] Creating layer fc6
I1226 07:40:48.625038 89665 net.cpp:178] Creating Layer fc6
I1226 07:40:48.625071 89665 net.cpp:612] fc6 <- pool5
I1226 07:40:48.625118 89665 net.cpp:586] fc6 -> fc6
I1226 07:40:45.551832 89666 net.cpp:228] Setting up conv5
I1226 07:40:45.551944 89666 net.cpp:235] Top shape: 64 256 13 13 (2768896)
I1226 07:40:45.551970 89666 net.cpp:243] Memory required for data: 512450048
I1226 07:40:45.552074 89666 layer_factory.hpp:114] Creating layer relu5
I1226 07:40:45.552150 89666 net.cpp:178] Creating Layer relu5
I1226 07:40:45.552319 89666 net.cpp:612] relu5 <- conv5
I1226 07:40:45.552379 89666 net.cpp:573] relu5 -> conv5 (in-place)
I1226 07:40:45.552496 89666 net.cpp:228] Setting up relu5
I1226 07:40:45.552546 89666 net.cpp:235] Top shape: 64 256 13 13 (2768896)
I1226 07:40:45.552570 89666 net.cpp:243] Memory required for data: 523525632
I1226 07:40:45.552628 89666 layer_factory.hpp:114] Creating layer pool5
I1226 07:40:45.552681 89666 net.cpp:178] Creating Layer pool5
I1226 07:40:45.552722 89666 net.cpp:612] pool5 <- conv5
I1226 07:40:45.552767 89666 net.cpp:586] pool5 -> pool5
I1226 07:40:45.552868 89666 net.cpp:228] Setting up pool5
I1226 07:40:45.552918 89666 net.cpp:235] Top shape: 64 256 6 6 (589824)
I1226 07:40:45.552944 89666 net.cpp:243] Memory required for data: 525884928
I1226 07:40:45.552973 89666 layer_factory.hpp:114] Creating layer fc6
I1226 07:40:45.553052 89666 net.cpp:178] Creating Layer fc6
I1226 07:40:45.553081 89666 net.cpp:612] fc6 <- pool5
I1226 07:40:45.553128 89666 net.cpp:586] fc6 -> fc6
I1226 07:40:49.065333 92408 net.cpp:228] Setting up conv3
I1226 07:40:49.065455 92408 net.cpp:235] Top shape: 64 384 13 13 (4153344)
I1226 07:40:49.065491 92408 net.cpp:243] Memory required for data: 451534336
I1226 07:40:49.065573 92408 layer_factory.hpp:114] Creating layer relu3
I1226 07:40:49.065681 92408 net.cpp:178] Creating Layer relu3
I1226 07:40:49.065739 92408 net.cpp:612] relu3 <- conv3
I1226 07:40:49.065796 92408 net.cpp:573] relu3 -> conv3 (in-place)
I1226 07:40:49.065935 92408 net.cpp:228] Setting up relu3
I1226 07:40:49.066007 92408 net.cpp:235] Top shape: 64 384 13 13 (4153344)
I1226 07:40:49.066052 92408 net.cpp:243] Memory required for data: 468147712
I1226 07:40:49.066088 92408 layer_factory.hpp:114] Creating layer conv4
I1226 07:40:49.066200 92408 net.cpp:178] Creating Layer conv4
I1226 07:40:49.066248 92408 net.cpp:612] conv4 <- conv3
I1226 07:40:49.066299 92408 net.cpp:586] conv4 -> conv4
I1226 07:40:49.239517 93793 net.cpp:228] Setting up conv2
I1226 07:40:49.239635 93793 net.cpp:235] Top shape: 64 256 27 27 (11943936)
I1226 07:40:49.239667 93793 net.cpp:243] Memory required for data: 328293888
I1226 07:40:49.239742 93793 layer_factory.hpp:114] Creating layer relu2
I1226 07:40:49.239917 93793 net.cpp:178] Creating Layer relu2
I1226 07:40:49.239959 93793 net.cpp:612] relu2 <- conv2
I1226 07:40:49.240020 93793 net.cpp:573] relu2 -> conv2 (in-place)
I1226 07:40:49.240120 93793 net.cpp:228] Setting up relu2
I1226 07:40:49.240165 93793 net.cpp:235] Top shape: 64 256 27 27 (11943936)
I1226 07:40:49.240190 93793 net.cpp:243] Memory required for data: 376069632
I1226 07:40:49.240219 93793 layer_factory.hpp:114] Creating layer norm2
I1226 07:40:49.240278 93793 net.cpp:178] Creating Layer norm2
I1226 07:40:49.240311 93793 net.cpp:612] norm2 <- conv2
I1226 07:40:49.240360 93793 net.cpp:586] norm2 -> norm2
I1226 07:40:49.240466 93793 net.cpp:228] Setting up norm2
I1226 07:40:49.240514 93793 net.cpp:235] Top shape: 64 256 27 27 (11943936)
I1226 07:40:49.240538 93793 net.cpp:243] Memory required for data: 423845376
I1226 07:40:49.240566 93793 layer_factory.hpp:114] Creating layer pool2
I1226 07:40:49.240627 93793 net.cpp:178] Creating Layer pool2
I1226 07:40:49.240663 93793 net.cpp:612] pool2 <- norm2
I1226 07:40:49.240717 93793 net.cpp:586] pool2 -> pool2
I1226 07:40:49.240826 93793 net.cpp:228] Setting up pool2
I1226 07:40:49.240876 93793 net.cpp:235] Top shape: 64 256 13 13 (2768896)
I1226 07:40:49.241000 93793 net.cpp:243] Memory required for data: 434920960
I1226 07:40:49.241039 93793 layer_factory.hpp:114] Creating layer conv3
I1226 07:40:49.241124 93793 net.cpp:178] Creating Layer conv3
I1226 07:40:49.241160 93793 net.cpp:612] conv3 <- pool2
I1226 07:40:49.241224 93793 net.cpp:586] conv3 -> conv3
I1226 07:40:49.943486 92408 net.cpp:228] Setting up conv4
I1226 07:40:49.943610 92408 net.cpp:235] Top shape: 64 384 13 13 (4153344)
I1226 07:40:49.943650 92408 net.cpp:243] Memory required for data: 484761088
I1226 07:40:49.943720 92408 layer_factory.hpp:114] Creating layer relu4
I1226 07:40:49.943861 92408 net.cpp:178] Creating Layer relu4
I1226 07:40:49.943927 92408 net.cpp:612] relu4 <- conv4
I1226 07:40:49.943980 92408 net.cpp:573] relu4 -> conv4 (in-place)
I1226 07:40:49.944090 92408 net.cpp:228] Setting up relu4
I1226 07:40:49.944154 92408 net.cpp:235] Top shape: 64 384 13 13 (4153344)
I1226 07:40:49.944182 92408 net.cpp:243] Memory required for data: 501374464
I1226 07:40:49.944217 92408 layer_factory.hpp:114] Creating layer conv5
I1226 07:40:49.944324 92408 net.cpp:178] Creating Layer conv5
I1226 07:40:49.944381 92408 net.cpp:612] conv5 <- conv4
I1226 07:40:49.944433 92408 net.cpp:586] conv5 -> conv5
I1226 07:40:50.424149 93793 net.cpp:228] Setting up conv3
I1226 07:40:50.424269 93793 net.cpp:235] Top shape: 64 384 13 13 (4153344)
I1226 07:40:50.424299 93793 net.cpp:243] Memory required for data: 451534336
I1226 07:40:50.424379 93793 layer_factory.hpp:114] Creating layer relu3
I1226 07:40:50.424556 93793 net.cpp:178] Creating Layer relu3
I1226 07:40:50.424607 93793 net.cpp:612] relu3 <- conv3
I1226 07:40:50.424654 93793 net.cpp:573] relu3 -> conv3 (in-place)
I1226 07:40:50.424768 93793 net.cpp:228] Setting up relu3
I1226 07:40:50.424846 93793 net.cpp:235] Top shape: 64 384 13 13 (4153344)
I1226 07:40:50.424873 93793 net.cpp:243] Memory required for data: 468147712
I1226 07:40:50.424903 93793 layer_factory.hpp:114] Creating layer conv4
I1226 07:40:50.425007 93793 net.cpp:178] Creating Layer conv4
I1226 07:40:50.425048 93793 net.cpp:612] conv4 <- conv3
I1226 07:40:50.425102 93793 net.cpp:586] conv4 -> conv4
I1226 07:40:50.573132 92408 net.cpp:228] Setting up conv5
I1226 07:40:50.573251 92408 net.cpp:235] Top shape: 64 256 13 13 (2768896)
I1226 07:40:50.573290 92408 net.cpp:243] Memory required for data: 512450048
I1226 07:40:50.573376 92408 layer_factory.hpp:114] Creating layer relu5
I1226 07:40:50.573483 92408 net.cpp:178] Creating Layer relu5
I1226 07:40:50.573539 92408 net.cpp:612] relu5 <- conv5
I1226 07:40:50.573602 92408 net.cpp:573] relu5 -> conv5 (in-place)
I1226 07:40:50.573704 92408 net.cpp:228] Setting up relu5
I1226 07:40:50.573762 92408 net.cpp:235] Top shape: 64 256 13 13 (2768896)
I1226 07:40:50.573791 92408 net.cpp:243] Memory required for data: 523525632
I1226 07:40:50.573856 92408 layer_factory.hpp:114] Creating layer pool5
I1226 07:40:50.573925 92408 net.cpp:178] Creating Layer pool5
I1226 07:40:50.573978 92408 net.cpp:612] pool5 <- conv5
I1226 07:40:50.574034 92408 net.cpp:586] pool5 -> pool5
I1226 07:40:50.574139 92408 net.cpp:228] Setting up pool5
I1226 07:40:50.574200 92408 net.cpp:235] Top shape: 64 256 6 6 (589824)
I1226 07:40:50.574226 92408 net.cpp:243] Memory required for data: 525884928
I1226 07:40:50.574259 92408 layer_factory.hpp:114] Creating layer fc6
I1226 07:40:50.574345 92408 net.cpp:178] Creating Layer fc6
I1226 07:40:50.574386 92408 net.cpp:612] fc6 <- pool5
I1226 07:40:50.574434 92408 net.cpp:586] fc6 -> fc6
I1226 07:40:51.301380 93793 net.cpp:228] Setting up conv4
I1226 07:40:51.301520 93793 net.cpp:235] Top shape: 64 384 13 13 (4153344)
I1226 07:40:51.301559 93793 net.cpp:243] Memory required for data: 484761088
I1226 07:40:51.301625 93793 layer_factory.hpp:114] Creating layer relu4
I1226 07:40:51.301698 93793 net.cpp:178] Creating Layer relu4
I1226 07:40:51.301745 93793 net.cpp:612] relu4 <- conv4
I1226 07:40:51.301841 93793 net.cpp:573] relu4 -> conv4 (in-place)
I1226 07:40:51.301955 93793 net.cpp:228] Setting up relu4
I1226 07:40:51.302017 93793 net.cpp:235] Top shape: 64 384 13 13 (4153344)
I1226 07:40:51.302045 93793 net.cpp:243] Memory required for data: 501374464
I1226 07:40:51.302076 93793 layer_factory.hpp:114] Creating layer conv5
I1226 07:40:51.302183 93793 net.cpp:178] Creating Layer conv5
I1226 07:40:51.302224 93793 net.cpp:612] conv5 <- conv4
I1226 07:40:51.302284 93793 net.cpp:586] conv5 -> conv5
I1226 07:40:51.929278 93793 net.cpp:228] Setting up conv5
I1226 07:40:51.929395 93793 net.cpp:235] Top shape: 64 256 13 13 (2768896)
I1226 07:40:51.929426 93793 net.cpp:243] Memory required for data: 512450048
I1226 07:40:51.929507 93793 layer_factory.hpp:114] Creating layer relu5
I1226 07:40:51.929605 93793 net.cpp:178] Creating Layer relu5
I1226 07:40:51.929653 93793 net.cpp:612] relu5 <- conv5
I1226 07:40:51.929697 93793 net.cpp:573] relu5 -> conv5 (in-place)
I1226 07:40:51.929841 93793 net.cpp:228] Setting up relu5
I1226 07:40:51.929900 93793 net.cpp:235] Top shape: 64 256 13 13 (2768896)
I1226 07:40:51.929927 93793 net.cpp:243] Memory required for data: 523525632
I1226 07:40:51.929958 93793 layer_factory.hpp:114] Creating layer pool5
I1226 07:40:51.930038 93793 net.cpp:178] Creating Layer pool5
I1226 07:40:51.930076 93793 net.cpp:612] pool5 <- conv5
I1226 07:40:51.930119 93793 net.cpp:586] pool5 -> pool5
I1226 07:40:51.930225 93793 net.cpp:228] Setting up pool5
I1226 07:40:51.930279 93793 net.cpp:235] Top shape: 64 256 6 6 (589824)
I1226 07:40:51.930302 93793 net.cpp:243] Memory required for data: 525884928
I1226 07:40:51.930332 93793 layer_factory.hpp:114] Creating layer fc6
I1226 07:40:51.930399 93793 net.cpp:178] Creating Layer fc6
I1226 07:40:51.930433 93793 net.cpp:612] fc6 <- pool5
I1226 07:40:51.930485 93793 net.cpp:586] fc6 -> fc6
I1226 07:40:54.043457 89800 net.cpp:228] Setting up fc6
I1226 07:40:54.043572 89800 net.cpp:235] Top shape: 64 4096 (262144)
I1226 07:40:54.043604 89800 net.cpp:243] Memory required for data: 526933504
I1226 07:40:54.043663 89800 layer_factory.hpp:114] Creating layer relu6
I1226 07:40:54.043766 89800 net.cpp:178] Creating Layer relu6
I1226 07:40:54.043879 89800 net.cpp:612] relu6 <- fc6
I1226 07:40:54.043922 89800 net.cpp:573] relu6 -> fc6 (in-place)
I1226 07:40:54.044008 89800 net.cpp:228] Setting up relu6
I1226 07:40:54.044160 89800 net.cpp:235] Top shape: 64 4096 (262144)
I1226 07:40:54.044193 89800 net.cpp:243] Memory required for data: 527982080
I1226 07:40:54.044225 89800 layer_factory.hpp:114] Creating layer drop6
I1226 07:40:54.044287 89800 net.cpp:178] Creating Layer drop6
I1226 07:40:54.044319 89800 net.cpp:612] drop6 <- fc6
I1226 07:40:54.044358 89800 net.cpp:573] drop6 -> fc6 (in-place)
I1226 07:40:54.044443 89800 net.cpp:228] Setting up drop6
I1226 07:40:54.044486 89800 net.cpp:235] Top shape: 64 4096 (262144)
I1226 07:40:54.044519 89800 net.cpp:243] Memory required for data: 529030656
I1226 07:40:54.044558 89800 layer_factory.hpp:114] Creating layer fc7
I1226 07:40:54.044646 89800 net.cpp:178] Creating Layer fc7
I1226 07:40:54.044683 89800 net.cpp:612] fc7 <- fc6
I1226 07:40:54.044729 89800 net.cpp:586] fc7 -> fc7
I1226 07:40:54.125360 86513 net.cpp:228] Setting up fc6
I1226 07:40:54.125509 86513 net.cpp:235] Top shape: 64 4096 (262144)
I1226 07:40:54.125558 86513 net.cpp:243] Memory required for data: 526933504
I1226 07:40:54.125638 86513 layer_factory.hpp:114] Creating layer relu6
I1226 07:40:54.125905 86513 net.cpp:178] Creating Layer relu6
I1226 07:40:54.126070 86513 net.cpp:612] relu6 <- fc6
I1226 07:40:54.126118 86513 net.cpp:573] relu6 -> fc6 (in-place)
I1226 07:40:54.126219 86513 net.cpp:228] Setting up relu6
I1226 07:40:54.126588 86513 net.cpp:235] Top shape: 64 4096 (262144)
I1226 07:40:54.126638 86513 net.cpp:243] Memory required for data: 527982080
I1226 07:40:54.126727 86513 layer_factory.hpp:114] Creating layer drop6
I1226 07:40:54.126806 86513 net.cpp:178] Creating Layer drop6
I1226 07:40:54.126968 86513 net.cpp:612] drop6 <- fc6
I1226 07:40:54.127059 86513 net.cpp:573] drop6 -> fc6 (in-place)
I1226 07:40:54.127167 86513 net.cpp:228] Setting up drop6
I1226 07:40:54.127226 86513 net.cpp:235] Top shape: 64 4096 (262144)
I1226 07:40:54.127271 86513 net.cpp:243] Memory required for data: 529030656
I1226 07:40:54.127367 86513 layer_factory.hpp:114] Creating layer fc7
I1226 07:40:54.127459 86513 net.cpp:178] Creating Layer fc7
I1226 07:40:54.127497 86513 net.cpp:612] fc7 <- fc6
I1226 07:40:54.127554 86513 net.cpp:586] fc7 -> fc7
I1226 07:40:54.115916 90196 net.cpp:228] Setting up fc6
I1226 07:40:54.116030 90196 net.cpp:235] Top shape: 64 4096 (262144)
I1226 07:40:54.116060 90196 net.cpp:243] Memory required for data: 526933504
I1226 07:40:54.116117 90196 layer_factory.hpp:114] Creating layer relu6
I1226 07:40:54.116215 90196 net.cpp:178] Creating Layer relu6
I1226 07:40:54.116315 90196 net.cpp:612] relu6 <- fc6
I1226 07:40:54.116358 90196 net.cpp:573] relu6 -> fc6 (in-place)
I1226 07:40:54.116470 90196 net.cpp:228] Setting up relu6
I1226 07:40:54.116617 90196 net.cpp:235] Top shape: 64 4096 (262144)
I1226 07:40:54.116648 90196 net.cpp:243] Memory required for data: 527982080
I1226 07:40:54.116680 90196 layer_factory.hpp:114] Creating layer drop6
I1226 07:40:54.116737 90196 net.cpp:178] Creating Layer drop6
I1226 07:40:54.116771 90196 net.cpp:612] drop6 <- fc6
I1226 07:40:54.116807 90196 net.cpp:573] drop6 -> fc6 (in-place)
I1226 07:40:54.116866 90196 net.cpp:228] Setting up drop6
I1226 07:40:54.116904 90196 net.cpp:235] Top shape: 64 4096 (262144)
I1226 07:40:54.116928 90196 net.cpp:243] Memory required for data: 529030656
I1226 07:40:54.116956 90196 layer_factory.hpp:114] Creating layer fc7
I1226 07:40:54.117024 90196 net.cpp:178] Creating Layer fc7
I1226 07:40:54.117054 90196 net.cpp:612] fc7 <- fc6
I1226 07:40:54.117092 90196 net.cpp:586] fc7 -> fc7
I1226 07:40:54.148154 88308 net.cpp:228] Setting up fc6
I1226 07:40:54.148275 88308 net.cpp:235] Top shape: 64 4096 (262144)
I1226 07:40:54.148308 88308 net.cpp:243] Memory required for data: 526933504
I1226 07:40:54.148370 88308 layer_factory.hpp:114] Creating layer relu6
I1226 07:40:54.148475 88308 net.cpp:178] Creating Layer relu6
I1226 07:40:54.148586 88308 net.cpp:612] relu6 <- fc6
I1226 07:40:54.148633 88308 net.cpp:573] relu6 -> fc6 (in-place)
I1226 07:40:54.148735 88308 net.cpp:228] Setting up relu6
I1226 07:40:54.148887 88308 net.cpp:235] Top shape: 64 4096 (262144)
I1226 07:40:54.148922 88308 net.cpp:243] Memory required for data: 527982080
I1226 07:40:54.148957 88308 layer_factory.hpp:114] Creating layer drop6
I1226 07:40:54.149024 88308 net.cpp:178] Creating Layer drop6
I1226 07:40:54.149057 88308 net.cpp:612] drop6 <- fc6
I1226 07:40:54.149108 88308 net.cpp:573] drop6 -> fc6 (in-place)
I1226 07:40:54.149170 88308 net.cpp:228] Setting up drop6
I1226 07:40:54.149211 88308 net.cpp:235] Top shape: 64 4096 (262144)
I1226 07:40:54.149245 88308 net.cpp:243] Memory required for data: 529030656
I1226 07:40:54.149274 88308 layer_factory.hpp:114] Creating layer fc7
I1226 07:40:54.149351 88308 net.cpp:178] Creating Layer fc7
I1226 07:40:54.149394 88308 net.cpp:612] fc7 <- fc6
I1226 07:40:54.149438 88308 net.cpp:586] fc7 -> fc7
I1226 07:40:54.182152 89665 net.cpp:228] Setting up fc6
I1226 07:40:54.182265 89665 net.cpp:235] Top shape: 64 4096 (262144)
I1226 07:40:54.182292 89665 net.cpp:243] Memory required for data: 526933504
I1226 07:40:54.182346 89665 layer_factory.hpp:114] Creating layer relu6
I1226 07:40:54.182428 89665 net.cpp:178] Creating Layer relu6
I1226 07:40:54.182461 89665 net.cpp:612] relu6 <- fc6
I1226 07:40:54.182533 89665 net.cpp:573] relu6 -> fc6 (in-place)
I1226 07:40:54.182626 89665 net.cpp:228] Setting up relu6
I1226 07:40:54.182770 89665 net.cpp:235] Top shape: 64 4096 (262144)
I1226 07:40:54.182801 89665 net.cpp:243] Memory required for data: 527982080
I1226 07:40:54.182862 89665 layer_factory.hpp:114] Creating layer drop6
I1226 07:40:54.182919 89665 net.cpp:178] Creating Layer drop6
I1226 07:40:54.182957 89665 net.cpp:612] drop6 <- fc6
I1226 07:40:54.182993 89665 net.cpp:573] drop6 -> fc6 (in-place)
I1226 07:40:54.183050 89665 net.cpp:228] Setting up drop6
I1226 07:40:54.183096 89665 net.cpp:235] Top shape: 64 4096 (262144)
I1226 07:40:54.183128 89665 net.cpp:243] Memory required for data: 529030656
I1226 07:40:54.183156 89665 layer_factory.hpp:114] Creating layer fc7
I1226 07:40:54.183229 89665 net.cpp:178] Creating Layer fc7
I1226 07:40:54.183265 89665 net.cpp:612] fc7 <- fc6
I1226 07:40:54.183305 89665 net.cpp:586] fc7 -> fc7
I1226 07:40:54.225277 89771 net.cpp:228] Setting up fc6
I1226 07:40:54.225390 89771 net.cpp:235] Top shape: 64 4096 (262144)
I1226 07:40:54.225422 89771 net.cpp:243] Memory required for data: 526933504
I1226 07:40:54.225509 89771 layer_factory.hpp:114] Creating layer relu6
I1226 07:40:54.225621 89771 net.cpp:178] Creating Layer relu6
I1226 07:40:54.225661 89771 net.cpp:612] relu6 <- fc6
I1226 07:40:54.225697 89771 net.cpp:573] relu6 -> fc6 (in-place)
I1226 07:40:54.225796 89771 net.cpp:228] Setting up relu6
I1226 07:40:54.225965 89771 net.cpp:235] Top shape: 64 4096 (262144)
I1226 07:40:54.226002 89771 net.cpp:243] Memory required for data: 527982080
I1226 07:40:54.226040 89771 layer_factory.hpp:114] Creating layer drop6
I1226 07:40:54.226150 89771 net.cpp:178] Creating Layer drop6
I1226 07:40:54.226188 89771 net.cpp:612] drop6 <- fc6
I1226 07:40:54.226230 89771 net.cpp:573] drop6 -> fc6 (in-place)
I1226 07:40:54.226289 89771 net.cpp:228] Setting up drop6
I1226 07:40:54.226326 89771 net.cpp:235] Top shape: 64 4096 (262144)
I1226 07:40:54.226351 89771 net.cpp:243] Memory required for data: 529030656
I1226 07:40:54.226383 89771 layer_factory.hpp:114] Creating layer fc7
I1226 07:40:54.226446 89771 net.cpp:178] Creating Layer fc7
I1226 07:40:54.226541 89771 net.cpp:612] fc7 <- fc6
I1226 07:40:54.226593 89771 net.cpp:586] fc7 -> fc7
I1226 07:40:56.306493 89800 net.cpp:228] Setting up fc7
I1226 07:40:56.306599 89800 net.cpp:235] Top shape: 64 4096 (262144)
I1226 07:40:56.306632 89800 net.cpp:243] Memory required for data: 530079232
I1226 07:40:56.306690 89800 layer_factory.hpp:114] Creating layer relu7
I1226 07:40:56.306797 89800 net.cpp:178] Creating Layer relu7
I1226 07:40:56.306947 89800 net.cpp:612] relu7 <- fc7
I1226 07:40:56.307013 89800 net.cpp:573] relu7 -> fc7 (in-place)
I1226 07:40:56.307103 89800 net.cpp:228] Setting up relu7
I1226 07:40:56.307145 89800 net.cpp:235] Top shape: 64 4096 (262144)
I1226 07:40:56.307173 89800 net.cpp:243] Memory required for data: 531127808
I1226 07:40:56.307227 89800 layer_factory.hpp:114] Creating layer drop7
I1226 07:40:56.307265 89800 net.cpp:178] Creating Layer drop7
I1226 07:40:56.307303 89800 net.cpp:612] drop7 <- fc7
I1226 07:40:56.307341 89800 net.cpp:573] drop7 -> fc7 (in-place)
I1226 07:40:56.307417 89800 net.cpp:228] Setting up drop7
I1226 07:40:56.307483 89800 net.cpp:235] Top shape: 64 4096 (262144)
I1226 07:40:56.307512 89800 net.cpp:243] Memory required for data: 532176384
I1226 07:40:56.307564 89800 layer_factory.hpp:114] Creating layer fc8
I1226 07:40:56.307651 89800 net.cpp:178] Creating Layer fc8
I1226 07:40:56.307684 89800 net.cpp:612] fc8 <- fc7
I1226 07:40:56.307736 89800 net.cpp:586] fc8 -> fc8
I1226 07:40:56.396721 86513 net.cpp:228] Setting up fc7
I1226 07:40:56.396841 86513 net.cpp:235] Top shape: 64 4096 (262144)
I1226 07:40:56.396877 86513 net.cpp:243] Memory required for data: 530079232
I1226 07:40:56.380563 90196 net.cpp:228] Setting up fc7
I1226 07:40:56.396957 86513 layer_factory.hpp:114] Creating layer relu7
I1226 07:40:56.380671 90196 net.cpp:235] Top shape: 64 4096 (262144)
I1226 07:40:56.380700 90196 net.cpp:243] Memory required for data: 530079232
I1226 07:40:56.397068 86513 net.cpp:178] Creating Layer relu7
I1226 07:40:56.380758 90196 layer_factory.hpp:114] Creating layer relu7
I1226 07:40:56.397255 86513 net.cpp:612] relu7 <- fc7
I1226 07:40:56.397333 86513 net.cpp:573] relu7 -> fc7 (in-place)
I1226 07:40:56.380849 90196 net.cpp:178] Creating Layer relu7
I1226 07:40:56.397454 86513 net.cpp:228] Setting up relu7
I1226 07:40:56.380892 90196 net.cpp:612] relu7 <- fc7
I1226 07:40:56.397531 86513 net.cpp:235] Top shape: 64 4096 (262144)
I1226 07:40:56.397562 86513 net.cpp:243] Memory required for data: 531127808
I1226 07:40:56.397601 86513 layer_factory.hpp:114] Creating layer drop7
I1226 07:40:56.380937 90196 net.cpp:573] relu7 -> fc7 (in-place)
I1226 07:40:56.397652 86513 net.cpp:178] Creating Layer drop7
I1226 07:40:56.381021 90196 net.cpp:228] Setting up relu7
I1226 07:40:56.381060 90196 net.cpp:235] Top shape: 64 4096 (262144)
I1226 07:40:56.381084 90196 net.cpp:243] Memory required for data: 531127808
I1226 07:40:56.397698 86513 net.cpp:612] drop7 <- fc7
I1226 07:40:56.397747 86513 net.cpp:573] drop7 -> fc7 (in-place)
I1226 07:40:56.381114 90196 layer_factory.hpp:114] Creating layer drop7
I1226 07:40:56.397814 86513 net.cpp:228] Setting up drop7
I1226 07:40:56.381150 90196 net.cpp:178] Creating Layer drop7
I1226 07:40:56.397876 86513 net.cpp:235] Top shape: 64 4096 (262144)
I1226 07:40:56.397907 86513 net.cpp:243] Memory required for data: 532176384
I1226 07:40:56.381193 90196 net.cpp:612] drop7 <- fc7
I1226 07:40:56.397948 86513 layer_factory.hpp:114] Creating layer fc8
I1226 07:40:56.381233 90196 net.cpp:573] drop7 -> fc7 (in-place)
I1226 07:40:56.381276 90196 net.cpp:228] Setting up drop7
I1226 07:40:56.381310 90196 net.cpp:235] Top shape: 64 4096 (262144)
I1226 07:40:56.398041 86513 net.cpp:178] Creating Layer fc8
I1226 07:40:56.398077 86513 net.cpp:612] fc8 <- fc7
I1226 07:40:56.381333 90196 net.cpp:243] Memory required for data: 532176384
I1226 07:40:56.398131 86513 net.cpp:586] fc8 -> fc8
I1226 07:40:56.381361 90196 layer_factory.hpp:114] Creating layer fc8
I1226 07:40:56.381425 90196 net.cpp:178] Creating Layer fc8
I1226 07:40:56.381484 90196 net.cpp:612] fc8 <- fc7
I1226 07:40:56.381528 90196 net.cpp:586] fc8 -> fc8
I1226 07:40:56.421308 88308 net.cpp:228] Setting up fc7
I1226 07:40:56.421422 88308 net.cpp:235] Top shape: 64 4096 (262144)
I1226 07:40:56.421450 88308 net.cpp:243] Memory required for data: 530079232
I1226 07:40:56.421509 88308 layer_factory.hpp:114] Creating layer relu7
I1226 07:40:56.421612 88308 net.cpp:178] Creating Layer relu7
I1226 07:40:56.421658 88308 net.cpp:612] relu7 <- fc7
I1226 07:40:56.421717 88308 net.cpp:573] relu7 -> fc7 (in-place)
I1226 07:40:56.421917 88308 net.cpp:228] Setting up relu7
I1226 07:40:56.421967 88308 net.cpp:235] Top shape: 64 4096 (262144)
I1226 07:40:56.421991 88308 net.cpp:243] Memory required for data: 531127808
I1226 07:40:56.422020 88308 layer_factory.hpp:114] Creating layer drop7
I1226 07:40:56.422058 88308 net.cpp:178] Creating Layer drop7
I1226 07:40:56.422086 88308 net.cpp:612] drop7 <- fc7
I1226 07:40:56.422122 88308 net.cpp:573] drop7 -> fc7 (in-place)
I1226 07:40:56.422168 88308 net.cpp:228] Setting up drop7
I1226 07:40:56.422216 88308 net.cpp:235] Top shape: 64 4096 (262144)
I1226 07:40:56.422241 88308 net.cpp:243] Memory required for data: 532176384
I1226 07:40:56.422269 88308 layer_factory.hpp:114] Creating layer fc8
I1226 07:40:56.422344 88308 net.cpp:178] Creating Layer fc8
I1226 07:40:56.422374 88308 net.cpp:612] fc8 <- fc7
I1226 07:40:56.422415 88308 net.cpp:586] fc8 -> fc8
I1226 07:40:53.190340 89666 net.cpp:228] Setting up fc6
I1226 07:40:53.190460 89666 net.cpp:235] Top shape: 64 4096 (262144)
I1226 07:40:53.190492 89666 net.cpp:243] Memory required for data: 526933504
I1226 07:40:53.190556 89666 layer_factory.hpp:114] Creating layer relu6
I1226 07:40:53.190665 89666 net.cpp:178] Creating Layer relu6
I1226 07:40:53.190803 89666 net.cpp:612] relu6 <- fc6
I1226 07:40:53.190850 89666 net.cpp:573] relu6 -> fc6 (in-place)
I1226 07:40:53.190951 89666 net.cpp:228] Setting up relu6
I1226 07:40:53.191102 89666 net.cpp:235] Top shape: 64 4096 (262144)
I1226 07:40:53.191134 89666 net.cpp:243] Memory required for data: 527982080
I1226 07:40:53.191169 89666 layer_factory.hpp:114] Creating layer drop6
I1226 07:40:53.191233 89666 net.cpp:178] Creating Layer drop6
I1226 07:40:53.191267 89666 net.cpp:612] drop6 <- fc6
I1226 07:40:53.191308 89666 net.cpp:573] drop6 -> fc6 (in-place)
I1226 07:40:53.191375 89666 net.cpp:228] Setting up drop6
I1226 07:40:53.191412 89666 net.cpp:235] Top shape: 64 4096 (262144)
I1226 07:40:53.191438 89666 net.cpp:243] Memory required for data: 529030656
I1226 07:40:53.191469 89666 layer_factory.hpp:114] Creating layer fc7
I1226 07:40:53.191540 89666 net.cpp:178] Creating Layer fc7
I1226 07:40:53.191601 89666 net.cpp:612] fc7 <- fc6
I1226 07:40:53.191666 89666 net.cpp:586] fc7 -> fc7
I1226 07:40:56.471531 89665 net.cpp:228] Setting up fc7
I1226 07:40:56.471638 89665 net.cpp:235] Top shape: 64 4096 (262144)
I1226 07:40:56.471665 89665 net.cpp:243] Memory required for data: 530079232
I1226 07:40:56.471721 89665 layer_factory.hpp:114] Creating layer relu7
I1226 07:40:56.471794 89665 net.cpp:178] Creating Layer relu7
I1226 07:40:56.471855 89665 net.cpp:612] relu7 <- fc7
I1226 07:40:56.471917 89665 net.cpp:573] relu7 -> fc7 (in-place)
I1226 07:40:56.472008 89665 net.cpp:228] Setting up relu7
I1226 07:40:56.472049 89665 net.cpp:235] Top shape: 64 4096 (262144)
I1226 07:40:56.472074 89665 net.cpp:243] Memory required for data: 531127808
I1226 07:40:56.472206 89665 layer_factory.hpp:114] Creating layer drop7
I1226 07:40:56.472246 89665 net.cpp:178] Creating Layer drop7
I1226 07:40:56.472273 89665 net.cpp:612] drop7 <- fc7
I1226 07:40:56.472321 89665 net.cpp:573] drop7 -> fc7 (in-place)
I1226 07:40:56.472373 89665 net.cpp:228] Setting up drop7
I1226 07:40:56.472407 89665 net.cpp:235] Top shape: 64 4096 (262144)
I1226 07:40:56.472429 89665 net.cpp:243] Memory required for data: 532176384
I1226 07:40:56.472466 89665 layer_factory.hpp:114] Creating layer fc8
I1226 07:40:56.472537 89665 net.cpp:178] Creating Layer fc8
I1226 07:40:56.472576 89665 net.cpp:612] fc8 <- fc7
I1226 07:40:56.472616 89665 net.cpp:586] fc8 -> fc8
I1226 07:40:56.537340 89771 net.cpp:228] Setting up fc7
I1226 07:40:56.537456 89771 net.cpp:235] Top shape: 64 4096 (262144)
I1226 07:40:56.537511 89771 net.cpp:243] Memory required for data: 530079232
I1226 07:40:56.537575 89771 layer_factory.hpp:114] Creating layer relu7
I1226 07:40:56.537672 89771 net.cpp:178] Creating Layer relu7
I1226 07:40:56.537720 89771 net.cpp:612] relu7 <- fc7
I1226 07:40:56.537768 89771 net.cpp:573] relu7 -> fc7 (in-place)
I1226 07:40:56.537864 89771 net.cpp:228] Setting up relu7
I1226 07:40:56.537910 89771 net.cpp:235] Top shape: 64 4096 (262144)
I1226 07:40:56.537935 89771 net.cpp:243] Memory required for data: 531127808
I1226 07:40:56.537966 89771 layer_factory.hpp:114] Creating layer drop7
I1226 07:40:56.538007 89771 net.cpp:178] Creating Layer drop7
I1226 07:40:56.538035 89771 net.cpp:612] drop7 <- fc7
I1226 07:40:56.538084 89771 net.cpp:573] drop7 -> fc7 (in-place)
I1226 07:40:56.538141 89771 net.cpp:228] Setting up drop7
I1226 07:40:56.538178 89771 net.cpp:235] Top shape: 64 4096 (262144)
I1226 07:40:56.538202 89771 net.cpp:243] Memory required for data: 532176384
I1226 07:40:56.538229 89771 layer_factory.hpp:114] Creating layer fc8
I1226 07:40:56.538287 89771 net.cpp:178] Creating Layer fc8
I1226 07:40:56.538316 89771 net.cpp:612] fc8 <- fc7
I1226 07:40:56.538355 89771 net.cpp:586] fc8 -> fc8
I1226 07:40:56.855495 89800 net.cpp:228] Setting up fc8
I1226 07:40:56.855620 89800 net.cpp:235] Top shape: 64 1000 (64000)
I1226 07:40:56.855653 89800 net.cpp:243] Memory required for data: 532432384
I1226 07:40:56.855736 89800 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 07:40:56.855808 89800 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 07:40:56.855840 89800 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 07:40:56.855901 89800 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 07:40:56.855957 89800 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 07:40:56.856043 89800 net.cpp:228] Setting up fc8_fc8_0_split
I1226 07:40:56.856096 89800 net.cpp:235] Top shape: 64 1000 (64000)
I1226 07:40:56.856127 89800 net.cpp:235] Top shape: 64 1000 (64000)
I1226 07:40:56.856150 89800 net.cpp:243] Memory required for data: 532944384
I1226 07:40:56.856179 89800 layer_factory.hpp:114] Creating layer accuracy
I1226 07:40:56.856261 89800 net.cpp:178] Creating Layer accuracy
I1226 07:40:56.856297 89800 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 07:40:56.856328 89800 net.cpp:612] accuracy <- label_data_1_split_0
I1226 07:40:56.856364 89800 net.cpp:586] accuracy -> accuracy
I1226 07:40:56.856442 89800 net.cpp:228] Setting up accuracy
I1226 07:40:56.856480 89800 net.cpp:235] Top shape: (1)
I1226 07:40:56.856504 89800 net.cpp:243] Memory required for data: 532944388
I1226 07:40:56.856531 89800 layer_factory.hpp:114] Creating layer loss
I1226 07:40:56.856580 89800 net.cpp:178] Creating Layer loss
I1226 07:40:56.856649 89800 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 07:40:56.856681 89800 net.cpp:612] loss <- label_data_1_split_1
I1226 07:40:56.856719 89800 net.cpp:586] loss -> loss
I1226 07:40:56.856781 89800 layer_factory.hpp:114] Creating layer loss
I1226 07:40:56.884443 89800 net.cpp:228] Setting up loss
I1226 07:40:56.884562 89800 net.cpp:235] Top shape: (1)
I1226 07:40:56.884734 89800 net.cpp:238]     with loss weight 1
I1226 07:40:56.884894 89800 net.cpp:243] Memory required for data: 532944392
I1226 07:40:56.884951 89800 net.cpp:305] loss needs backward computation.
I1226 07:40:56.885010 89800 net.cpp:307] accuracy does not need backward computation.
I1226 07:40:56.885052 89800 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 07:40:56.885087 89800 net.cpp:305] fc8 needs backward computation.
I1226 07:40:56.885118 89800 net.cpp:305] drop7 needs backward computation.
I1226 07:40:56.885151 89800 net.cpp:305] relu7 needs backward computation.
I1226 07:40:56.885184 89800 net.cpp:305] fc7 needs backward computation.
I1226 07:40:56.885226 89800 net.cpp:305] drop6 needs backward computation.
I1226 07:40:56.885258 89800 net.cpp:305] relu6 needs backward computation.
I1226 07:40:56.885300 89800 net.cpp:305] fc6 needs backward computation.
I1226 07:40:56.885344 89800 net.cpp:305] pool5 needs backward computation.
I1226 07:40:56.885408 89800 net.cpp:305] relu5 needs backward computation.
I1226 07:40:56.885445 89800 net.cpp:305] conv5 needs backward computation.
I1226 07:40:56.885478 89800 net.cpp:305] relu4 needs backward computation.
I1226 07:40:56.885517 89800 net.cpp:305] conv4 needs backward computation.
I1226 07:40:56.885550 89800 net.cpp:305] relu3 needs backward computation.
I1226 07:40:56.885589 89800 net.cpp:305] conv3 needs backward computation.
I1226 07:40:56.885632 89800 net.cpp:305] pool2 needs backward computation.
I1226 07:40:56.885663 89800 net.cpp:305] norm2 needs backward computation.
I1226 07:40:56.885694 89800 net.cpp:305] relu2 needs backward computation.
I1226 07:40:56.885725 89800 net.cpp:305] conv2 needs backward computation.
I1226 07:40:56.885757 89800 net.cpp:305] pool1 needs backward computation.
I1226 07:40:56.885787 89800 net.cpp:305] norm1 needs backward computation.
I1226 07:40:56.885818 89800 net.cpp:305] relu1 needs backward computation.
I1226 07:40:56.885848 89800 net.cpp:305] conv1 needs backward computation.
I1226 07:40:56.885884 89800 net.cpp:307] label_data_1_split does not need backward computation.
I1226 07:40:56.885924 89800 net.cpp:307] data does not need backward computation.
I1226 07:40:56.885958 89800 net.cpp:349] This network produces output accuracy
I1226 07:40:56.885995 89800 net.cpp:349] This network produces output loss
I1226 07:40:56.886107 89800 net.cpp:363] Network initialization done.
I1226 07:40:56.886597 89800 solver.cpp:119] Solver scaffolding done.
I1226 07:40:56.886836 89800 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 07:40:56.942694 90196 net.cpp:228] Setting up fc8
I1226 07:40:56.942806 90196 net.cpp:235] Top shape: 64 1000 (64000)
I1226 07:40:56.942838 90196 net.cpp:243] Memory required for data: 532432384
I1226 07:40:56.942895 90196 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 07:40:56.942987 90196 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 07:40:56.943032 90196 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 07:40:56.943089 90196 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 07:40:56.943146 90196 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 07:40:56.943238 90196 net.cpp:228] Setting up fc8_fc8_0_split
I1226 07:40:56.943289 90196 net.cpp:235] Top shape: 64 1000 (64000)
I1226 07:40:56.943320 90196 net.cpp:235] Top shape: 64 1000 (64000)
I1226 07:40:56.943343 90196 net.cpp:243] Memory required for data: 532944384
I1226 07:40:56.943372 90196 layer_factory.hpp:114] Creating layer accuracy
I1226 07:40:56.943431 90196 net.cpp:178] Creating Layer accuracy
I1226 07:40:56.943497 90196 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 07:40:56.943531 90196 net.cpp:612] accuracy <- label_data_1_split_0
I1226 07:40:56.943565 90196 net.cpp:586] accuracy -> accuracy
I1226 07:40:56.943610 90196 net.cpp:228] Setting up accuracy
I1226 07:40:56.943646 90196 net.cpp:235] Top shape: (1)
I1226 07:40:56.943681 90196 net.cpp:243] Memory required for data: 532944388
I1226 07:40:56.943709 90196 layer_factory.hpp:114] Creating layer loss
I1226 07:40:56.943754 90196 net.cpp:178] Creating Layer loss
I1226 07:40:56.943786 90196 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 07:40:56.943816 90196 net.cpp:612] loss <- label_data_1_split_1
I1226 07:40:56.943850 90196 net.cpp:586] loss -> loss
I1226 07:40:56.943912 90196 layer_factory.hpp:114] Creating layer loss
I1226 07:40:56.971472 86513 net.cpp:228] Setting up fc8
I1226 07:40:56.971602 86513 net.cpp:235] Top shape: 64 1000 (64000)
I1226 07:40:56.971639 86513 net.cpp:243] Memory required for data: 532432384
I1226 07:40:56.971717 86513 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 07:40:56.971798 86513 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 07:40:56.971840 86513 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 07:40:56.971899 86513 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 07:40:56.971954 86513 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 07:40:56.972053 86513 net.cpp:228] Setting up fc8_fc8_0_split
I1226 07:40:56.972115 86513 net.cpp:235] Top shape: 64 1000 (64000)
I1226 07:40:56.972152 86513 net.cpp:235] Top shape: 64 1000 (64000)
I1226 07:40:56.972182 86513 net.cpp:243] Memory required for data: 532944384
I1226 07:40:56.972216 86513 layer_factory.hpp:114] Creating layer accuracy
I1226 07:40:56.972295 86513 net.cpp:178] Creating Layer accuracy
I1226 07:40:56.972367 86513 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 07:40:56.972409 86513 net.cpp:612] accuracy <- label_data_1_split_0
I1226 07:40:56.972456 86513 net.cpp:586] accuracy -> accuracy
I1226 07:40:56.972514 86513 net.cpp:228] Setting up accuracy
I1226 07:40:56.972576 86513 net.cpp:235] Top shape: (1)
I1226 07:40:56.972605 86513 net.cpp:243] Memory required for data: 532944388
I1226 07:40:56.972642 86513 layer_factory.hpp:114] Creating layer loss
I1226 07:40:56.973682 88308 net.cpp:228] Setting up fc8
I1226 07:40:56.972707 86513 net.cpp:178] Creating Layer loss
I1226 07:40:56.973801 88308 net.cpp:235] Top shape: 64 1000 (64000)
I1226 07:40:56.973834 88308 net.cpp:243] Memory required for data: 532432384
I1226 07:40:56.972748 86513 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 07:40:56.972782 86513 net.cpp:612] loss <- label_data_1_split_1
I1226 07:40:56.973889 88308 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 07:40:56.972826 86513 net.cpp:586] loss -> loss
I1226 07:40:56.973951 88308 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 07:40:56.972908 86513 layer_factory.hpp:114] Creating layer loss
I1226 07:40:56.973994 88308 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 07:40:56.974048 88308 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 07:40:56.974105 88308 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 07:40:56.974189 88308 net.cpp:228] Setting up fc8_fc8_0_split
I1226 07:40:56.974241 88308 net.cpp:235] Top shape: 64 1000 (64000)
I1226 07:40:56.974278 88308 net.cpp:235] Top shape: 64 1000 (64000)
I1226 07:40:56.974301 88308 net.cpp:243] Memory required for data: 532944384
I1226 07:40:56.974329 88308 layer_factory.hpp:114] Creating layer accuracy
I1226 07:40:56.974390 88308 net.cpp:178] Creating Layer accuracy
I1226 07:40:56.974429 88308 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 07:40:56.974462 88308 net.cpp:612] accuracy <- label_data_1_split_0
I1226 07:40:56.974496 88308 net.cpp:586] accuracy -> accuracy
I1226 07:40:56.974548 88308 net.cpp:228] Setting up accuracy
I1226 07:40:56.974622 88308 net.cpp:235] Top shape: (1)
I1226 07:40:56.974645 88308 net.cpp:243] Memory required for data: 532944388
I1226 07:40:56.974673 88308 layer_factory.hpp:114] Creating layer loss
I1226 07:40:56.974730 88308 net.cpp:178] Creating Layer loss
I1226 07:40:56.974761 88308 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 07:40:56.974797 88308 net.cpp:612] loss <- label_data_1_split_1
I1226 07:40:56.974834 88308 net.cpp:586] loss -> loss
I1226 07:40:56.974897 88308 layer_factory.hpp:114] Creating layer loss
I1226 07:40:56.972277 90196 net.cpp:228] Setting up loss
I1226 07:40:56.972390 90196 net.cpp:235] Top shape: (1)
I1226 07:40:56.972581 90196 net.cpp:238]     with loss weight 1
I1226 07:40:56.972715 90196 net.cpp:243] Memory required for data: 532944392
I1226 07:40:56.972767 90196 net.cpp:305] loss needs backward computation.
I1226 07:40:56.972812 90196 net.cpp:307] accuracy does not need backward computation.
I1226 07:40:56.972848 90196 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 07:40:56.972882 90196 net.cpp:305] fc8 needs backward computation.
I1226 07:40:56.972935 90196 net.cpp:305] drop7 needs backward computation.
I1226 07:40:56.972968 90196 net.cpp:305] relu7 needs backward computation.
I1226 07:40:56.973001 90196 net.cpp:305] fc7 needs backward computation.
I1226 07:40:56.973032 90196 net.cpp:305] drop6 needs backward computation.
I1226 07:40:56.973063 90196 net.cpp:305] relu6 needs backward computation.
I1226 07:40:56.973099 90196 net.cpp:305] fc6 needs backward computation.
I1226 07:40:56.973132 90196 net.cpp:305] pool5 needs backward computation.
I1226 07:40:56.973176 90196 net.cpp:305] relu5 needs backward computation.
I1226 07:40:56.973213 90196 net.cpp:305] conv5 needs backward computation.
I1226 07:40:56.973253 90196 net.cpp:305] relu4 needs backward computation.
I1226 07:40:56.973289 90196 net.cpp:305] conv4 needs backward computation.
I1226 07:40:56.973321 90196 net.cpp:305] relu3 needs backward computation.
I1226 07:40:56.973351 90196 net.cpp:305] conv3 needs backward computation.
I1226 07:40:56.973384 90196 net.cpp:305] pool2 needs backward computation.
I1226 07:40:56.973415 90196 net.cpp:305] norm2 needs backward computation.
I1226 07:40:56.973477 90196 net.cpp:305] relu2 needs backward computation.
I1226 07:40:56.973518 90196 net.cpp:305] conv2 needs backward computation.
I1226 07:40:56.973559 90196 net.cpp:305] pool1 needs backward computation.
I1226 07:40:56.973590 90196 net.cpp:305] norm1 needs backward computation.
I1226 07:40:56.973623 90196 net.cpp:305] relu1 needs backward computation.
I1226 07:40:56.973654 90196 net.cpp:305] conv1 needs backward computation.
I1226 07:40:56.973687 90196 net.cpp:307] label_data_1_split does not need backward computation.
I1226 07:40:56.973729 90196 net.cpp:307] data does not need backward computation.
I1226 07:40:56.973764 90196 net.cpp:349] This network produces output accuracy
I1226 07:40:56.973799 90196 net.cpp:349] This network produces output loss
I1226 07:40:56.973904 90196 net.cpp:363] Network initialization done.
I1226 07:40:56.974360 90196 solver.cpp:119] Solver scaffolding done.
I1226 07:40:56.974611 90196 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 07:40:57.000201 88308 net.cpp:228] Setting up loss
I1226 07:40:57.000301 88308 net.cpp:235] Top shape: (1)
I1226 07:40:57.000437 88308 net.cpp:238]     with loss weight 1
I1226 07:40:57.000592 88308 net.cpp:243] Memory required for data: 532944392
I1226 07:40:57.000634 88308 net.cpp:305] loss needs backward computation.
I1226 07:40:57.000668 88308 net.cpp:307] accuracy does not need backward computation.
I1226 07:40:57.000695 88308 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 07:40:57.000720 88308 net.cpp:305] fc8 needs backward computation.
I1226 07:40:57.000746 88308 net.cpp:305] drop7 needs backward computation.
I1226 07:40:57.000768 88308 net.cpp:305] relu7 needs backward computation.
I1226 07:40:57.000792 88308 net.cpp:305] fc7 needs backward computation.
I1226 07:40:57.000816 88308 net.cpp:305] drop6 needs backward computation.
I1226 07:40:57.000839 88308 net.cpp:305] relu6 needs backward computation.
I1226 07:40:57.000861 88308 net.cpp:305] fc6 needs backward computation.
I1226 07:40:57.000885 88308 net.cpp:305] pool5 needs backward computation.
I1226 07:40:57.000907 88308 net.cpp:305] relu5 needs backward computation.
I1226 07:40:57.000931 88308 net.cpp:305] conv5 needs backward computation.
I1226 07:40:57.000955 88308 net.cpp:305] relu4 needs backward computation.
I1226 07:40:57.000978 88308 net.cpp:305] conv4 needs backward computation.
I1226 07:40:57.001011 88308 net.cpp:305] relu3 needs backward computation.
I1226 07:40:57.001037 88308 net.cpp:305] conv3 needs backward computation.
I1226 07:40:57.001067 88308 net.cpp:305] pool2 needs backward computation.
I1226 07:40:57.001092 88308 net.cpp:305] norm2 needs backward computation.
I1226 07:40:57.001116 88308 net.cpp:305] relu2 needs backward computation.
I1226 07:40:57.001138 88308 net.cpp:305] conv2 needs backward computation.
I1226 07:40:57.001163 88308 net.cpp:305] pool1 needs backward computation.
I1226 07:40:57.001185 88308 net.cpp:305] norm1 needs backward computation.
I1226 07:40:57.001215 88308 net.cpp:305] relu1 needs backward computation.
I1226 07:40:57.001238 88308 net.cpp:305] conv1 needs backward computation.
I1226 07:40:57.001263 88308 net.cpp:307] label_data_1_split does not need backward computation.
I1226 07:40:57.001287 88308 net.cpp:307] data does not need backward computation.
I1226 07:40:57.001309 88308 net.cpp:349] This network produces output accuracy
I1226 07:40:57.001338 88308 net.cpp:349] This network produces output loss
I1226 07:40:57.001425 88308 net.cpp:363] Network initialization done.
I1226 07:40:57.001885 88308 solver.cpp:119] Solver scaffolding done.
I1226 07:40:57.002073 88308 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 07:40:57.003640 86513 net.cpp:228] Setting up loss
I1226 07:40:57.003741 86513 net.cpp:235] Top shape: (1)
I1226 07:40:57.003862 86513 net.cpp:238]     with loss weight 1
I1226 07:40:57.003975 86513 net.cpp:243] Memory required for data: 532944392
I1226 07:40:57.004011 86513 net.cpp:305] loss needs backward computation.
I1226 07:40:57.004046 86513 net.cpp:307] accuracy does not need backward computation.
I1226 07:40:57.004078 86513 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 07:40:57.004109 86513 net.cpp:305] fc8 needs backward computation.
I1226 07:40:57.004140 86513 net.cpp:305] drop7 needs backward computation.
I1226 07:40:57.004176 86513 net.cpp:305] relu7 needs backward computation.
I1226 07:40:57.004216 86513 net.cpp:305] fc7 needs backward computation.
I1226 07:40:57.004254 86513 net.cpp:305] drop6 needs backward computation.
I1226 07:40:57.004283 86513 net.cpp:305] relu6 needs backward computation.
I1226 07:40:57.004336 86513 net.cpp:305] fc6 needs backward computation.
I1226 07:40:57.004375 86513 net.cpp:305] pool5 needs backward computation.
I1226 07:40:57.004411 86513 net.cpp:305] relu5 needs backward computation.
I1226 07:40:57.004448 86513 net.cpp:305] conv5 needs backward computation.
I1226 07:40:57.004487 86513 net.cpp:305] relu4 needs backward computation.
I1226 07:40:57.004523 86513 net.cpp:305] conv4 needs backward computation.
I1226 07:40:57.004559 86513 net.cpp:305] relu3 needs backward computation.
I1226 07:40:57.004595 86513 net.cpp:305] conv3 needs backward computation.
I1226 07:40:57.004636 86513 net.cpp:305] pool2 needs backward computation.
I1226 07:40:57.004670 86513 net.cpp:305] norm2 needs backward computation.
I1226 07:40:57.004709 86513 net.cpp:305] relu2 needs backward computation.
I1226 07:40:57.004745 86513 net.cpp:305] conv2 needs backward computation.
I1226 07:40:57.004781 86513 net.cpp:305] pool1 needs backward computation.
I1226 07:40:57.004812 86513 net.cpp:305] norm1 needs backward computation.
I1226 07:40:57.004842 86513 net.cpp:305] relu1 needs backward computation.
I1226 07:40:57.004878 86513 net.cpp:305] conv1 needs backward computation.
I1226 07:40:57.004920 86513 net.cpp:307] label_data_1_split does not need backward computation.
I1226 07:40:57.004957 86513 net.cpp:307] data does not need backward computation.
I1226 07:40:57.004986 86513 net.cpp:349] This network produces output accuracy
I1226 07:40:57.005018 86513 net.cpp:349] This network produces output loss
I1226 07:40:57.005113 86513 net.cpp:363] Network initialization done.
I1226 07:40:57.005527 86513 solver.cpp:119] Solver scaffolding done.
I1226 07:40:57.005692 86513 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 07:40:57.036511 89665 net.cpp:228] Setting up fc8
I1226 07:40:57.036631 89665 net.cpp:235] Top shape: 64 1000 (64000)
I1226 07:40:57.036662 89665 net.cpp:243] Memory required for data: 532432384
I1226 07:40:57.036718 89665 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 07:40:57.036790 89665 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 07:40:57.036859 89665 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 07:40:57.036918 89665 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 07:40:57.036983 89665 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 07:40:57.037082 89665 net.cpp:228] Setting up fc8_fc8_0_split
I1226 07:40:57.037129 89665 net.cpp:235] Top shape: 64 1000 (64000)
I1226 07:40:57.037161 89665 net.cpp:235] Top shape: 64 1000 (64000)
I1226 07:40:57.037185 89665 net.cpp:243] Memory required for data: 532944384
I1226 07:40:57.037223 89665 layer_factory.hpp:114] Creating layer accuracy
I1226 07:40:57.037291 89665 net.cpp:178] Creating Layer accuracy
I1226 07:40:57.037323 89665 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 07:40:57.037354 89665 net.cpp:612] accuracy <- label_data_1_split_0
I1226 07:40:57.037389 89665 net.cpp:586] accuracy -> accuracy
I1226 07:40:57.037441 89665 net.cpp:228] Setting up accuracy
I1226 07:40:57.037475 89665 net.cpp:235] Top shape: (1)
I1226 07:40:57.037498 89665 net.cpp:243] Memory required for data: 532944388
I1226 07:40:57.037524 89665 layer_factory.hpp:114] Creating layer loss
I1226 07:40:57.037576 89665 net.cpp:178] Creating Layer loss
I1226 07:40:57.037601 89665 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 07:40:57.037631 89665 net.cpp:612] loss <- label_data_1_split_1
I1226 07:40:57.037662 89665 net.cpp:586] loss -> loss
I1226 07:40:57.037720 89665 layer_factory.hpp:114] Creating layer loss
I1226 07:40:57.061532 89665 net.cpp:228] Setting up loss
I1226 07:40:57.061648 89665 net.cpp:235] Top shape: (1)
I1226 07:40:57.061848 89665 net.cpp:238]     with loss weight 1
I1226 07:40:57.062013 89665 net.cpp:243] Memory required for data: 532944392
I1226 07:40:57.062072 89665 net.cpp:305] loss needs backward computation.
I1226 07:40:57.062119 89665 net.cpp:307] accuracy does not need backward computation.
I1226 07:40:57.062165 89665 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 07:40:57.062206 89665 net.cpp:305] fc8 needs backward computation.
I1226 07:40:57.062242 89665 net.cpp:305] drop7 needs backward computation.
I1226 07:40:57.062273 89665 net.cpp:305] relu7 needs backward computation.
I1226 07:40:57.062302 89665 net.cpp:305] fc7 needs backward computation.
I1226 07:40:57.062335 89665 net.cpp:305] drop6 needs backward computation.
I1226 07:40:57.062366 89665 net.cpp:305] relu6 needs backward computation.
I1226 07:40:57.062402 89665 net.cpp:305] fc6 needs backward computation.
I1226 07:40:57.062434 89665 net.cpp:305] pool5 needs backward computation.
I1226 07:40:57.062470 89665 net.cpp:305] relu5 needs backward computation.
I1226 07:40:57.062506 89665 net.cpp:305] conv5 needs backward computation.
I1226 07:40:57.062538 89665 net.cpp:305] relu4 needs backward computation.
I1226 07:40:57.062574 89665 net.cpp:305] conv4 needs backward computation.
I1226 07:40:57.062607 89665 net.cpp:305] relu3 needs backward computation.
I1226 07:40:57.062643 89665 net.cpp:305] conv3 needs backward computation.
I1226 07:40:57.062675 89665 net.cpp:305] pool2 needs backward computation.
I1226 07:40:57.062712 89665 net.cpp:305] norm2 needs backward computation.
I1226 07:40:57.062752 89665 net.cpp:305] relu2 needs backward computation.
I1226 07:40:57.062786 89665 net.cpp:305] conv2 needs backward computation.
I1226 07:40:57.062837 89665 net.cpp:305] pool1 needs backward computation.
I1226 07:40:57.062872 89665 net.cpp:305] norm1 needs backward computation.
I1226 07:40:57.062903 89665 net.cpp:305] relu1 needs backward computation.
I1226 07:40:57.062934 89665 net.cpp:305] conv1 needs backward computation.
I1226 07:40:57.062968 89665 net.cpp:307] label_data_1_split does not need backward computation.
I1226 07:40:57.062999 89665 net.cpp:307] data does not need backward computation.
I1226 07:40:57.063038 89665 net.cpp:349] This network produces output accuracy
I1226 07:40:57.063073 89665 net.cpp:349] This network produces output loss
I1226 07:40:57.063156 89665 net.cpp:363] Network initialization done.
I1226 07:40:57.063612 89665 solver.cpp:119] Solver scaffolding done.
I1226 07:40:57.063868 89665 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 07:40:57.103370 89771 net.cpp:228] Setting up fc8
I1226 07:40:57.103516 89771 net.cpp:235] Top shape: 64 1000 (64000)
I1226 07:40:57.103548 89771 net.cpp:243] Memory required for data: 532432384
I1226 07:40:57.103602 89771 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 07:40:57.103718 89771 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 07:40:57.103909 89771 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 07:40:57.103979 89771 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 07:40:57.104068 89771 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 07:40:57.104190 89771 net.cpp:228] Setting up fc8_fc8_0_split
I1226 07:40:57.104408 89771 net.cpp:235] Top shape: 64 1000 (64000)
I1226 07:40:57.104446 89771 net.cpp:235] Top shape: 64 1000 (64000)
I1226 07:40:57.104471 89771 net.cpp:243] Memory required for data: 532944384
I1226 07:40:57.104537 89771 layer_factory.hpp:114] Creating layer accuracy
I1226 07:40:57.104637 89771 net.cpp:178] Creating Layer accuracy
I1226 07:40:57.104679 89771 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 07:40:57.104714 89771 net.cpp:612] accuracy <- label_data_1_split_0
I1226 07:40:57.104753 89771 net.cpp:586] accuracy -> accuracy
I1226 07:40:57.104804 89771 net.cpp:228] Setting up accuracy
I1226 07:40:57.104869 89771 net.cpp:235] Top shape: (1)
I1226 07:40:57.104893 89771 net.cpp:243] Memory required for data: 532944388
I1226 07:40:57.104923 89771 layer_factory.hpp:114] Creating layer loss
I1226 07:40:57.104996 89771 net.cpp:178] Creating Layer loss
I1226 07:40:57.105023 89771 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 07:40:57.105052 89771 net.cpp:612] loss <- label_data_1_split_1
I1226 07:40:57.105088 89771 net.cpp:586] loss -> loss
I1226 07:40:57.105149 89771 layer_factory.hpp:114] Creating layer loss
I1226 07:40:57.136881 89771 net.cpp:228] Setting up loss
I1226 07:40:57.136986 89771 net.cpp:235] Top shape: (1)
I1226 07:40:57.137110 89771 net.cpp:238]     with loss weight 1
I1226 07:40:57.137248 89771 net.cpp:243] Memory required for data: 532944392
I1226 07:40:57.137295 89771 net.cpp:305] loss needs backward computation.
I1226 07:40:57.137336 89771 net.cpp:307] accuracy does not need backward computation.
I1226 07:40:57.137375 89771 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 07:40:57.137411 89771 net.cpp:305] fc8 needs backward computation.
I1226 07:40:57.137446 89771 net.cpp:305] drop7 needs backward computation.
I1226 07:40:57.137503 89771 net.cpp:305] relu7 needs backward computation.
I1226 07:40:57.137539 89771 net.cpp:305] fc7 needs backward computation.
I1226 07:40:57.137573 89771 net.cpp:305] drop6 needs backward computation.
I1226 07:40:57.137604 89771 net.cpp:305] relu6 needs backward computation.
I1226 07:40:57.137635 89771 net.cpp:305] fc6 needs backward computation.
I1226 07:40:57.137668 89771 net.cpp:305] pool5 needs backward computation.
I1226 07:40:57.137701 89771 net.cpp:305] relu5 needs backward computation.
I1226 07:40:57.137733 89771 net.cpp:305] conv5 needs backward computation.
I1226 07:40:57.137768 89771 net.cpp:305] relu4 needs backward computation.
I1226 07:40:57.137799 89771 net.cpp:305] conv4 needs backward computation.
I1226 07:40:57.137831 89771 net.cpp:305] relu3 needs backward computation.
I1226 07:40:57.137863 89771 net.cpp:305] conv3 needs backward computation.
I1226 07:40:57.137897 89771 net.cpp:305] pool2 needs backward computation.
I1226 07:40:57.137930 89771 net.cpp:305] norm2 needs backward computation.
I1226 07:40:57.137962 89771 net.cpp:305] relu2 needs backward computation.
I1226 07:40:57.137994 89771 net.cpp:305] conv2 needs backward computation.
I1226 07:40:57.138026 89771 net.cpp:305] pool1 needs backward computation.
I1226 07:40:57.138061 89771 net.cpp:305] norm1 needs backward computation.
I1226 07:40:57.138093 89771 net.cpp:305] relu1 needs backward computation.
I1226 07:40:57.138124 89771 net.cpp:305] conv1 needs backward computation.
I1226 07:40:57.138159 89771 net.cpp:307] label_data_1_split does not need backward computation.
I1226 07:40:57.138191 89771 net.cpp:307] data does not need backward computation.
I1226 07:40:57.138221 89771 net.cpp:349] This network produces output accuracy
I1226 07:40:57.138255 89771 net.cpp:349] This network produces output loss
I1226 07:40:57.138361 89771 net.cpp:363] Network initialization done.
I1226 07:40:57.138825 89771 solver.cpp:119] Solver scaffolding done.
I1226 07:40:57.139029 89771 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 07:40:56.431377 89666 net.cpp:228] Setting up fc7
I1226 07:40:56.431488 89666 net.cpp:235] Top shape: 64 4096 (262144)
I1226 07:40:56.431519 89666 net.cpp:243] Memory required for data: 530079232
I1226 07:40:56.431605 89666 layer_factory.hpp:114] Creating layer relu7
I1226 07:40:56.431685 89666 net.cpp:178] Creating Layer relu7
I1226 07:40:56.431730 89666 net.cpp:612] relu7 <- fc7
I1226 07:40:56.431793 89666 net.cpp:573] relu7 -> fc7 (in-place)
I1226 07:40:56.431886 89666 net.cpp:228] Setting up relu7
I1226 07:40:56.431947 89666 net.cpp:235] Top shape: 64 4096 (262144)
I1226 07:40:56.431977 89666 net.cpp:243] Memory required for data: 531127808
I1226 07:40:56.432008 89666 layer_factory.hpp:114] Creating layer drop7
I1226 07:40:56.432049 89666 net.cpp:178] Creating Layer drop7
I1226 07:40:56.432085 89666 net.cpp:612] drop7 <- fc7
I1226 07:40:56.432121 89666 net.cpp:573] drop7 -> fc7 (in-place)
I1226 07:40:56.432168 89666 net.cpp:228] Setting up drop7
I1226 07:40:56.432200 89666 net.cpp:235] Top shape: 64 4096 (262144)
I1226 07:40:56.432224 89666 net.cpp:243] Memory required for data: 532176384
I1226 07:40:56.432252 89666 layer_factory.hpp:114] Creating layer fc8
I1226 07:40:56.432333 89666 net.cpp:178] Creating Layer fc8
I1226 07:40:56.432363 89666 net.cpp:612] fc8 <- fc7
I1226 07:40:56.432404 89666 net.cpp:586] fc8 -> fc8
I1226 07:41:00.322465 89665 caffe.cpp:376] Configuring multinode setup
I1226 07:41:00.323910 89665 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 07:41:00.481856 89771 caffe.cpp:376] Configuring multinode setup
I1226 07:41:00.483440 89771 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 07:40:57.243010 89666 net.cpp:228] Setting up fc8
I1226 07:40:57.243122 89666 net.cpp:235] Top shape: 64 1000 (64000)
I1226 07:40:57.243150 89666 net.cpp:243] Memory required for data: 532432384
I1226 07:40:57.243232 89666 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 07:40:57.243326 89666 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 07:40:57.243368 89666 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 07:40:57.243422 89666 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 07:40:57.243571 89666 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 07:40:57.243690 89666 net.cpp:228] Setting up fc8_fc8_0_split
I1226 07:40:57.243743 89666 net.cpp:235] Top shape: 64 1000 (64000)
I1226 07:40:57.243775 89666 net.cpp:235] Top shape: 64 1000 (64000)
I1226 07:40:57.243798 89666 net.cpp:243] Memory required for data: 532944384
I1226 07:40:57.243827 89666 layer_factory.hpp:114] Creating layer accuracy
I1226 07:40:57.243897 89666 net.cpp:178] Creating Layer accuracy
I1226 07:40:57.243933 89666 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 07:40:57.243965 89666 net.cpp:612] accuracy <- label_data_1_split_0
I1226 07:40:57.244004 89666 net.cpp:586] accuracy -> accuracy
I1226 07:40:57.244051 89666 net.cpp:228] Setting up accuracy
I1226 07:40:57.244093 89666 net.cpp:235] Top shape: (1)
I1226 07:40:57.244123 89666 net.cpp:243] Memory required for data: 532944388
I1226 07:40:57.244151 89666 layer_factory.hpp:114] Creating layer loss
I1226 07:40:57.244199 89666 net.cpp:178] Creating Layer loss
I1226 07:40:57.244225 89666 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 07:40:57.244256 89666 net.cpp:612] loss <- label_data_1_split_1
I1226 07:40:57.244302 89666 net.cpp:586] loss -> loss
I1226 07:40:57.244361 89666 layer_factory.hpp:114] Creating layer loss
I1226 07:40:57.281107 89666 net.cpp:228] Setting up loss
I1226 07:40:57.281224 89666 net.cpp:235] Top shape: (1)
I1226 07:40:57.281390 89666 net.cpp:238]     with loss weight 1
I1226 07:40:57.281546 89666 net.cpp:243] Memory required for data: 532944392
I1226 07:40:57.281626 89666 net.cpp:305] loss needs backward computation.
I1226 07:40:57.281671 89666 net.cpp:307] accuracy does not need backward computation.
I1226 07:40:57.281704 89666 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 07:40:57.281736 89666 net.cpp:305] fc8 needs backward computation.
I1226 07:40:57.281766 89666 net.cpp:305] drop7 needs backward computation.
I1226 07:40:57.281795 89666 net.cpp:305] relu7 needs backward computation.
I1226 07:40:57.281824 89666 net.cpp:305] fc7 needs backward computation.
I1226 07:40:57.281855 89666 net.cpp:305] drop6 needs backward computation.
I1226 07:40:57.281886 89666 net.cpp:305] relu6 needs backward computation.
I1226 07:40:57.281919 89666 net.cpp:305] fc6 needs backward computation.
I1226 07:40:57.281954 89666 net.cpp:305] pool5 needs backward computation.
I1226 07:40:57.281987 89666 net.cpp:305] relu5 needs backward computation.
I1226 07:40:57.282018 89666 net.cpp:305] conv5 needs backward computation.
I1226 07:40:57.282052 89666 net.cpp:305] relu4 needs backward computation.
I1226 07:40:57.282091 89666 net.cpp:305] conv4 needs backward computation.
I1226 07:40:57.282124 89666 net.cpp:305] relu3 needs backward computation.
I1226 07:40:57.282156 89666 net.cpp:305] conv3 needs backward computation.
I1226 07:40:57.282201 89666 net.cpp:305] pool2 needs backward computation.
I1226 07:40:57.282241 89666 net.cpp:305] norm2 needs backward computation.
I1226 07:40:57.282275 89666 net.cpp:305] relu2 needs backward computation.
I1226 07:40:57.282313 89666 net.cpp:305] conv2 needs backward computation.
I1226 07:40:57.282353 89666 net.cpp:305] pool1 needs backward computation.
I1226 07:40:57.282393 89666 net.cpp:305] norm1 needs backward computation.
I1226 07:40:57.282430 89666 net.cpp:305] relu1 needs backward computation.
I1226 07:40:57.282461 89666 net.cpp:305] conv1 needs backward computation.
I1226 07:40:57.282496 89666 net.cpp:307] label_data_1_split does not need backward computation.
I1226 07:40:57.282531 89666 net.cpp:307] data does not need backward computation.
I1226 07:40:57.282559 89666 net.cpp:349] This network produces output accuracy
I1226 07:40:57.282630 89666 net.cpp:349] This network produces output loss
I1226 07:40:57.282749 89666 net.cpp:363] Network initialization done.
I1226 07:40:57.283241 89666 solver.cpp:119] Solver scaffolding done.
I1226 07:40:57.283476 89666 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 07:41:00.917901 89800 caffe.cpp:376] Configuring multinode setup
I1226 07:41:00.919360 89800 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 07:41:01.005738 90196 caffe.cpp:376] Configuring multinode setup
I1226 07:41:01.007349 90196 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 07:41:01.033462 86513 caffe.cpp:376] Configuring multinode setup
I1226 07:41:01.035017 86513 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 07:41:01.044273 88308 caffe.cpp:376] Configuring multinode setup
I1226 07:41:01.045737 88308 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 07:41:01.339850 89666 caffe.cpp:376] Configuring multinode setup
I1226 07:41:01.341349 89666 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 07:41:19.957460 92408 net.cpp:228] Setting up fc6
I1226 07:41:19.957731 92408 net.cpp:235] Top shape: 64 4096 (262144)
I1226 07:41:19.957777 92408 net.cpp:243] Memory required for data: 526933504
I1226 07:41:19.957878 92408 layer_factory.hpp:114] Creating layer relu6
I1226 07:41:19.957959 92408 net.cpp:178] Creating Layer relu6
I1226 07:41:19.958065 92408 net.cpp:612] relu6 <- fc6
I1226 07:41:19.958112 92408 net.cpp:573] relu6 -> fc6 (in-place)
I1226 07:41:19.958221 92408 net.cpp:228] Setting up relu6
I1226 07:41:19.958283 92408 net.cpp:235] Top shape: 64 4096 (262144)
I1226 07:41:19.958310 92408 net.cpp:243] Memory required for data: 527982080
I1226 07:41:19.958344 92408 layer_factory.hpp:114] Creating layer drop6
I1226 07:41:19.958412 92408 net.cpp:178] Creating Layer drop6
I1226 07:41:19.958451 92408 net.cpp:612] drop6 <- fc6
I1226 07:41:19.958492 92408 net.cpp:573] drop6 -> fc6 (in-place)
I1226 07:41:19.958557 92408 net.cpp:228] Setting up drop6
I1226 07:41:19.958605 92408 net.cpp:235] Top shape: 64 4096 (262144)
I1226 07:41:19.958629 92408 net.cpp:243] Memory required for data: 529030656
I1226 07:41:19.958657 92408 layer_factory.hpp:114] Creating layer fc7
I1226 07:41:19.958739 92408 net.cpp:178] Creating Layer fc7
I1226 07:41:19.958778 92408 net.cpp:612] fc7 <- fc6
I1226 07:41:19.958843 92408 net.cpp:586] fc7 -> fc7
I1226 07:41:21.339978 93793 net.cpp:228] Setting up fc6
I1226 07:41:21.340245 93793 net.cpp:235] Top shape: 64 4096 (262144)
I1226 07:41:21.340291 93793 net.cpp:243] Memory required for data: 526933504
I1226 07:41:21.340358 93793 layer_factory.hpp:114] Creating layer relu6
I1226 07:41:21.340464 93793 net.cpp:178] Creating Layer relu6
I1226 07:41:21.340522 93793 net.cpp:612] relu6 <- fc6
I1226 07:41:21.340569 93793 net.cpp:573] relu6 -> fc6 (in-place)
I1226 07:41:21.340672 93793 net.cpp:228] Setting up relu6
I1226 07:41:21.340728 93793 net.cpp:235] Top shape: 64 4096 (262144)
I1226 07:41:21.340755 93793 net.cpp:243] Memory required for data: 527982080
I1226 07:41:21.340809 93793 layer_factory.hpp:114] Creating layer drop6
I1226 07:41:21.340881 93793 net.cpp:178] Creating Layer drop6
I1226 07:41:21.340914 93793 net.cpp:612] drop6 <- fc6
I1226 07:41:21.340970 93793 net.cpp:573] drop6 -> fc6 (in-place)
I1226 07:41:21.341043 93793 net.cpp:228] Setting up drop6
I1226 07:41:21.341081 93793 net.cpp:235] Top shape: 64 4096 (262144)
I1226 07:41:21.341115 93793 net.cpp:243] Memory required for data: 529030656
I1226 07:41:21.341157 93793 layer_factory.hpp:114] Creating layer fc7
I1226 07:41:21.341223 93793 net.cpp:178] Creating Layer fc7
I1226 07:41:21.341259 93793 net.cpp:612] fc7 <- fc6
I1226 07:41:21.341302 93793 net.cpp:586] fc7 -> fc7
I1226 07:41:33.019529 92408 net.cpp:228] Setting up fc7
I1226 07:41:33.019645 92408 net.cpp:235] Top shape: 64 4096 (262144)
I1226 07:41:33.019683 92408 net.cpp:243] Memory required for data: 530079232
I1226 07:41:33.019748 92408 layer_factory.hpp:114] Creating layer relu7
I1226 07:41:33.019959 92408 net.cpp:178] Creating Layer relu7
I1226 07:41:33.020017 92408 net.cpp:612] relu7 <- fc7
I1226 07:41:33.020063 92408 net.cpp:573] relu7 -> fc7 (in-place)
I1226 07:41:33.020169 92408 net.cpp:228] Setting up relu7
I1226 07:41:33.020227 92408 net.cpp:235] Top shape: 64 4096 (262144)
I1226 07:41:33.020256 92408 net.cpp:243] Memory required for data: 531127808
I1226 07:41:33.020289 92408 layer_factory.hpp:114] Creating layer drop7
I1226 07:41:33.020333 92408 net.cpp:178] Creating Layer drop7
I1226 07:41:33.020370 92408 net.cpp:612] drop7 <- fc7
I1226 07:41:33.020434 92408 net.cpp:573] drop7 -> fc7 (in-place)
I1226 07:41:33.020496 92408 net.cpp:228] Setting up drop7
I1226 07:41:33.020539 92408 net.cpp:235] Top shape: 64 4096 (262144)
I1226 07:41:33.020570 92408 net.cpp:243] Memory required for data: 532176384
I1226 07:41:33.020611 92408 layer_factory.hpp:114] Creating layer fc8
I1226 07:41:33.020676 92408 net.cpp:178] Creating Layer fc8
I1226 07:41:33.020714 92408 net.cpp:612] fc8 <- fc7
I1226 07:41:33.020758 92408 net.cpp:586] fc8 -> fc8
I1226 07:41:34.409385 93793 net.cpp:228] Setting up fc7
I1226 07:41:34.409502 93793 net.cpp:235] Top shape: 64 4096 (262144)
I1226 07:41:34.409538 93793 net.cpp:243] Memory required for data: 530079232
I1226 07:41:34.409602 93793 layer_factory.hpp:114] Creating layer relu7
I1226 07:41:34.409770 93793 net.cpp:178] Creating Layer relu7
I1226 07:41:34.409837 93793 net.cpp:612] relu7 <- fc7
I1226 07:41:34.409883 93793 net.cpp:573] relu7 -> fc7 (in-place)
I1226 07:41:34.409982 93793 net.cpp:228] Setting up relu7
I1226 07:41:34.410063 93793 net.cpp:235] Top shape: 64 4096 (262144)
I1226 07:41:34.410107 93793 net.cpp:243] Memory required for data: 531127808
I1226 07:41:34.410157 93793 layer_factory.hpp:114] Creating layer drop7
I1226 07:41:34.410228 93793 net.cpp:178] Creating Layer drop7
I1226 07:41:34.410279 93793 net.cpp:612] drop7 <- fc7
I1226 07:41:34.410342 93793 net.cpp:573] drop7 -> fc7 (in-place)
I1226 07:41:34.410414 93793 net.cpp:228] Setting up drop7
I1226 07:41:34.410466 93793 net.cpp:235] Top shape: 64 4096 (262144)
I1226 07:41:34.410501 93793 net.cpp:243] Memory required for data: 532176384
I1226 07:41:34.410542 93793 layer_factory.hpp:114] Creating layer fc8
I1226 07:41:34.410620 93793 net.cpp:178] Creating Layer fc8
I1226 07:41:34.410665 93793 net.cpp:612] fc8 <- fc7
I1226 07:41:34.410742 93793 net.cpp:586] fc8 -> fc8
I1226 07:41:36.209951 92408 net.cpp:228] Setting up fc8
I1226 07:41:36.210072 92408 net.cpp:235] Top shape: 64 1000 (64000)
I1226 07:41:36.210111 92408 net.cpp:243] Memory required for data: 532432384
I1226 07:41:36.210176 92408 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 07:41:36.210367 92408 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 07:41:36.210420 92408 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 07:41:36.210474 92408 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 07:41:36.210530 92408 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 07:41:36.210640 92408 net.cpp:228] Setting up fc8_fc8_0_split
I1226 07:41:36.210701 92408 net.cpp:235] Top shape: 64 1000 (64000)
I1226 07:41:36.210736 92408 net.cpp:235] Top shape: 64 1000 (64000)
I1226 07:41:36.210760 92408 net.cpp:243] Memory required for data: 532944384
I1226 07:41:36.210798 92408 layer_factory.hpp:114] Creating layer accuracy
I1226 07:41:36.210887 92408 net.cpp:178] Creating Layer accuracy
I1226 07:41:36.210928 92408 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 07:41:36.210974 92408 net.cpp:612] accuracy <- label_data_1_split_0
I1226 07:41:36.211014 92408 net.cpp:586] accuracy -> accuracy
I1226 07:41:36.211064 92408 net.cpp:228] Setting up accuracy
I1226 07:41:36.211110 92408 net.cpp:235] Top shape: (1)
I1226 07:41:36.211141 92408 net.cpp:243] Memory required for data: 532944388
I1226 07:41:36.211182 92408 layer_factory.hpp:114] Creating layer loss
I1226 07:41:36.211344 92408 net.cpp:178] Creating Layer loss
I1226 07:41:36.211386 92408 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 07:41:36.211429 92408 net.cpp:612] loss <- label_data_1_split_1
I1226 07:41:36.211490 92408 net.cpp:586] loss -> loss
I1226 07:41:36.211568 92408 layer_factory.hpp:114] Creating layer loss
I1226 07:41:36.243854 92408 net.cpp:228] Setting up loss
I1226 07:41:36.243978 92408 net.cpp:235] Top shape: (1)
I1226 07:41:36.244022 92408 net.cpp:238]     with loss weight 1
I1226 07:41:36.244182 92408 net.cpp:243] Memory required for data: 532944392
I1226 07:41:36.244236 92408 net.cpp:305] loss needs backward computation.
I1226 07:41:36.244292 92408 net.cpp:307] accuracy does not need backward computation.
I1226 07:41:36.244386 92408 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 07:41:36.244447 92408 net.cpp:305] fc8 needs backward computation.
I1226 07:41:36.244485 92408 net.cpp:305] drop7 needs backward computation.
I1226 07:41:36.244524 92408 net.cpp:305] relu7 needs backward computation.
I1226 07:41:36.244559 92408 net.cpp:305] fc7 needs backward computation.
I1226 07:41:36.244606 92408 net.cpp:305] drop6 needs backward computation.
I1226 07:41:36.244642 92408 net.cpp:305] relu6 needs backward computation.
I1226 07:41:36.244674 92408 net.cpp:305] fc6 needs backward computation.
I1226 07:41:36.244710 92408 net.cpp:305] pool5 needs backward computation.
I1226 07:41:36.244755 92408 net.cpp:305] relu5 needs backward computation.
I1226 07:41:36.244799 92408 net.cpp:305] conv5 needs backward computation.
I1226 07:41:36.244861 92408 net.cpp:305] relu4 needs backward computation.
I1226 07:41:36.244896 92408 net.cpp:305] conv4 needs backward computation.
I1226 07:41:36.244949 92408 net.cpp:305] relu3 needs backward computation.
I1226 07:41:36.244992 92408 net.cpp:305] conv3 needs backward computation.
I1226 07:41:36.245028 92408 net.cpp:305] pool2 needs backward computation.
I1226 07:41:36.245071 92408 net.cpp:305] norm2 needs backward computation.
I1226 07:41:36.245116 92408 net.cpp:305] relu2 needs backward computation.
I1226 07:41:36.245151 92408 net.cpp:305] conv2 needs backward computation.
I1226 07:41:36.245192 92408 net.cpp:305] pool1 needs backward computation.
I1226 07:41:36.245234 92408 net.cpp:305] norm1 needs backward computation.
I1226 07:41:36.245272 92408 net.cpp:305] relu1 needs backward computation.
I1226 07:41:36.245313 92408 net.cpp:305] conv1 needs backward computation.
I1226 07:41:36.245357 92408 net.cpp:307] label_data_1_split does not need backward computation.
I1226 07:41:36.245404 92408 net.cpp:307] data does not need backward computation.
I1226 07:41:36.245434 92408 net.cpp:349] This network produces output accuracy
I1226 07:41:36.245471 92408 net.cpp:349] This network produces output loss
I1226 07:41:36.245573 92408 net.cpp:363] Network initialization done.
I1226 07:41:36.246064 92408 solver.cpp:119] Solver scaffolding done.
I1226 07:41:36.246301 92408 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 07:41:37.605123 93793 net.cpp:228] Setting up fc8
I1226 07:41:37.605259 93793 net.cpp:235] Top shape: 64 1000 (64000)
I1226 07:41:37.605307 93793 net.cpp:243] Memory required for data: 532432384
I1226 07:41:37.605394 93793 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I1226 07:41:37.606482 93793 net.cpp:178] Creating Layer fc8_fc8_0_split
I1226 07:41:37.606550 93793 net.cpp:612] fc8_fc8_0_split <- fc8
I1226 07:41:37.606611 93793 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1226 07:41:37.606681 93793 net.cpp:586] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1226 07:41:37.606870 93793 net.cpp:228] Setting up fc8_fc8_0_split
I1226 07:41:37.606955 93793 net.cpp:235] Top shape: 64 1000 (64000)
I1226 07:41:37.607004 93793 net.cpp:235] Top shape: 64 1000 (64000)
I1226 07:41:37.607035 93793 net.cpp:243] Memory required for data: 532944384
I1226 07:41:37.607076 93793 layer_factory.hpp:114] Creating layer accuracy
I1226 07:41:37.607142 93793 net.cpp:178] Creating Layer accuracy
I1226 07:41:37.607182 93793 net.cpp:612] accuracy <- fc8_fc8_0_split_0
I1226 07:41:37.607239 93793 net.cpp:612] accuracy <- label_data_1_split_0
I1226 07:41:37.607309 93793 net.cpp:586] accuracy -> accuracy
I1226 07:41:37.607386 93793 net.cpp:228] Setting up accuracy
I1226 07:41:37.607445 93793 net.cpp:235] Top shape: (1)
I1226 07:41:37.607486 93793 net.cpp:243] Memory required for data: 532944388
I1226 07:41:37.607524 93793 layer_factory.hpp:114] Creating layer loss
I1226 07:41:37.607704 93793 net.cpp:178] Creating Layer loss
I1226 07:41:37.607758 93793 net.cpp:612] loss <- fc8_fc8_0_split_1
I1226 07:41:37.607830 93793 net.cpp:612] loss <- label_data_1_split_1
I1226 07:41:37.607895 93793 net.cpp:586] loss -> loss
I1226 07:41:37.608019 93793 layer_factory.hpp:114] Creating layer loss
I1226 07:41:37.636827 93793 net.cpp:228] Setting up loss
I1226 07:41:37.636983 93793 net.cpp:235] Top shape: (1)
I1226 07:41:37.637035 93793 net.cpp:238]     with loss weight 1
I1226 07:41:37.637182 93793 net.cpp:243] Memory required for data: 532944392
I1226 07:41:37.637269 93793 net.cpp:305] loss needs backward computation.
I1226 07:41:37.637500 93793 net.cpp:307] accuracy does not need backward computation.
I1226 07:41:37.637542 93793 net.cpp:305] fc8_fc8_0_split needs backward computation.
I1226 07:41:37.637575 93793 net.cpp:305] fc8 needs backward computation.
I1226 07:41:37.637606 93793 net.cpp:305] drop7 needs backward computation.
I1226 07:41:37.637636 93793 net.cpp:305] relu7 needs backward computation.
I1226 07:41:37.637666 93793 net.cpp:305] fc7 needs backward computation.
I1226 07:41:37.637697 93793 net.cpp:305] drop6 needs backward computation.
I1226 07:41:37.637727 93793 net.cpp:305] relu6 needs backward computation.
I1226 07:41:37.637756 93793 net.cpp:305] fc6 needs backward computation.
I1226 07:41:37.638046 93793 net.cpp:305] pool5 needs backward computation.
I1226 07:41:37.638084 93793 net.cpp:305] relu5 needs backward computation.
I1226 07:41:37.638115 93793 net.cpp:305] conv5 needs backward computation.
I1226 07:41:37.638147 93793 net.cpp:305] relu4 needs backward computation.
I1226 07:41:37.638178 93793 net.cpp:305] conv4 needs backward computation.
I1226 07:41:37.638211 93793 net.cpp:305] relu3 needs backward computation.
I1226 07:41:37.638254 93793 net.cpp:305] conv3 needs backward computation.
I1226 07:41:37.638290 93793 net.cpp:305] pool2 needs backward computation.
I1226 07:41:37.638332 93793 net.cpp:305] norm2 needs backward computation.
I1226 07:41:37.638375 93793 net.cpp:305] relu2 needs backward computation.
I1226 07:41:37.638416 93793 net.cpp:305] conv2 needs backward computation.
I1226 07:41:37.638447 93793 net.cpp:305] pool1 needs backward computation.
I1226 07:41:37.638512 93793 net.cpp:305] norm1 needs backward computation.
I1226 07:41:37.638550 93793 net.cpp:305] relu1 needs backward computation.
I1226 07:41:37.638578 93793 net.cpp:305] conv1 needs backward computation.
I1226 07:41:37.638613 93793 net.cpp:307] label_data_1_split does not need backward computation.
I1226 07:41:37.638645 93793 net.cpp:307] data does not need backward computation.
I1226 07:41:37.638682 93793 net.cpp:349] This network produces output accuracy
I1226 07:41:37.638718 93793 net.cpp:349] This network produces output loss
I1226 07:41:37.638870 93793 net.cpp:363] Network initialization done.
I1226 07:41:37.639395 93793 solver.cpp:119] Solver scaffolding done.
I1226 07:41:37.639653 93793 caffe.cpp:212] Finetuning from /export/data1/stanford/hazy/gordon_bell/caffenet_knl/snapshot/caffenet_MN8_B1024/_iter_25000.caffemodel
I1226 07:41:41.669980 92408 caffe.cpp:376] Configuring multinode setup
I1226 07:41:41.674955 92408 caffe.cpp:386] Starting parameter server in mpi environment
I1226 07:41:42.213856 93793 caffe.cpp:376] Configuring multinode setup
I1226 07:41:42.215684 93793 caffe.cpp:392] Starting Multi-node Optimization in mpi environment
I1226 07:41:42.215929 93793 SynchronousNode.cpp:662] [1] [proc 1] solving
I1226 07:41:42.216045 93793 solver.cpp:366] Solving AlexNet
I1226 07:41:42.219766 89771 SynchronousNode.cpp:662] [0] [proc 0] solving
I1226 07:41:42.216084 93793 solver.cpp:367] Learning Rate Policy: step
I1226 07:41:42.220049 89771 solver.cpp:366] Solving AlexNet
I1226 07:41:42.220093 89771 solver.cpp:367] Learning Rate Policy: step
I1226 07:41:42.201098 90196 SynchronousNode.cpp:662] [4] [proc 4] solving
I1226 07:41:42.216344 93793 MultiSolver.cpp:76] [1] PROFILING BEGIN[Forward]
I1226 07:41:42.214457 89665 SynchronousNode.cpp:662] [3] [proc 3] solving
I1226 07:41:38.951160 89666 SynchronousNode.cpp:662] [5] [proc 5] solving
I1226 07:41:42.211335 92408 async_param_server.cpp:187] PS: Comm loop
I1226 07:41:42.217440 86513 SynchronousNode.cpp:662] [7] [proc 7] solving
I1226 07:41:42.201391 90196 solver.cpp:366] Solving AlexNet
I1226 07:41:42.201441 90196 solver.cpp:367] Learning Rate Policy: step
I1226 07:41:42.218514 88308 SynchronousNode.cpp:662] [2] [proc 2] solving
I1226 07:41:42.220445 89800 SynchronousNode.cpp:662] [6] [proc 6] solving
I1226 07:41:42.214743 89665 solver.cpp:366] Solving AlexNet
I1226 07:41:42.214792 89665 solver.cpp:367] Learning Rate Policy: step
I1226 07:41:42.220330 89771 MultiSolver.cpp:76] [0] PROFILING BEGIN[Forward]
I1226 07:41:38.951439 89666 solver.cpp:366] Solving AlexNet
I1226 07:41:38.951483 89666 solver.cpp:367] Learning Rate Policy: step
I1226 07:41:42.217725 86513 solver.cpp:366] Solving AlexNet
I1226 07:41:42.217772 86513 solver.cpp:367] Learning Rate Policy: step
I1226 07:41:42.201720 90196 MultiSolver.cpp:76] [4] PROFILING BEGIN[Forward]
I1226 07:41:42.218822 88308 solver.cpp:366] Solving AlexNet
I1226 07:41:42.218863 88308 solver.cpp:367] Learning Rate Policy: step
I1226 07:41:42.220726 89800 solver.cpp:366] Solving AlexNet
I1226 07:41:42.220774 89800 solver.cpp:367] Learning Rate Policy: step
I1226 07:41:42.215070 89665 MultiSolver.cpp:76] [3] PROFILING BEGIN[Forward]
I1226 07:41:38.951767 89666 MultiSolver.cpp:76] [5] PROFILING BEGIN[Forward]
I1226 07:41:42.218005 86513 MultiSolver.cpp:76] [7] PROFILING BEGIN[Forward]
I1226 07:41:42.219080 88308 MultiSolver.cpp:76] [2] PROFILING BEGIN[Forward]
I1226 07:41:42.221035 89800 MultiSolver.cpp:76] [6] PROFILING BEGIN[Forward]
I1226 07:41:42.219274 93865 SynchronousNode.cpp:280] [1] Comm thread started 0 0
I1226 07:41:42.226646 88380 SynchronousNode.cpp:280] [2] Comm thread started 1 0
I1226 07:41:42.223770 92479 async_param_server.cpp:175] PS: Compute loop
I1226 07:41:42.231251 86581 SynchronousNode.cpp:280] [7] Comm thread started 1 0
I1226 07:41:42.216253 90267 SynchronousNode.cpp:280] [4] Comm thread started 0 1
I1226 07:41:42.235397 89845 SynchronousNode.cpp:280] [0] Comm thread started 0 1
I1226 07:41:42.249167 89738 SynchronousNode.cpp:280] [3] Comm thread started 1 0
I1226 07:41:38.987542 89736 SynchronousNode.cpp:280] [5] Comm thread started 0 0
I1226 07:41:42.257210 89872 SynchronousNode.cpp:280] [6] Comm thread started 1 0
I1226 07:41:42.243078 90267 SynchronousNode.cpp:466] [4] initialized root of cluster with nodes: 9 and the total iter size is: 4
I1226 07:41:42.262140 89845 SynchronousNode.cpp:466] [0] initialized root of cluster with nodes: 9 and the total iter size is: 4
I1226 07:41:42.555613 90196 MultiSolver.cpp:93] [4] PROFILING END[Forward]
I1226 07:41:42.555717 90196 MultiSolver.cpp:95] [4] PROFILING BEGIN[Backward]
I1226 07:41:42.591527 89771 MultiSolver.cpp:93] [0] PROFILING END[Forward]
I1226 07:41:42.591620 89771 MultiSolver.cpp:95] [0] PROFILING BEGIN[Backward]
I1226 07:41:43.028563 90196 MultiSolver.cpp:109] [4] PROFILING END[Backward]
I1226 07:41:43.028648 90196 solver.cpp:303] [4] Iteration 1, loss = 3.22814
I1226 07:41:43.028715 90196 solver.cpp:329]     Train net output #0: accuracy = 0.34375
I1226 07:41:43.028813 90196 solver.cpp:329]     Train net output #1: loss = 3.22814 (* 1 = 3.22814 loss)
I1226 07:41:43.028983 90196 MultiSolver.cpp:76] [4] PROFILING BEGIN[Forward]
I1226 07:41:43.066742 89771 MultiSolver.cpp:109] [0] PROFILING END[Backward]
I1226 07:41:43.066833 89771 solver.cpp:303] [0] Iteration 1, loss = 3.30418
I1226 07:41:43.066905 89771 solver.cpp:329]     Train net output #0: accuracy = 0.3125
I1226 07:41:43.066983 89771 solver.cpp:329]     Train net output #1: loss = 3.30418 (* 1 = 3.30418 loss)
I1226 07:41:43.067055 89771 MultiSolver.cpp:76] [0] PROFILING BEGIN[Forward]
I1226 07:41:43.643025 86513 MultiSolver.cpp:93] [7] PROFILING END[Forward]
I1226 07:41:43.643103 86513 MultiSolver.cpp:95] [7] PROFILING BEGIN[Backward]
I1226 07:41:43.661340 89800 MultiSolver.cpp:93] [6] PROFILING END[Forward]
I1226 07:41:43.661433 89800 MultiSolver.cpp:95] [6] PROFILING BEGIN[Backward]
I1226 07:41:40.446548 89666 MultiSolver.cpp:93] [5] PROFILING END[Forward]
I1226 07:41:40.446692 89666 MultiSolver.cpp:95] [5] PROFILING BEGIN[Backward]
I1226 07:41:43.846122 86513 MultiSolver.cpp:109] [7] PROFILING END[Backward]
I1226 07:41:43.846215 86513 solver.cpp:303] [7] Iteration 1, loss = 3.26565
I1226 07:41:43.846293 86513 solver.cpp:329]     Train net output #0: accuracy = 0.328125
I1226 07:41:43.846412 86513 solver.cpp:329]     Train net output #1: loss = 3.26565 (* 1 = 3.26565 loss)
I1226 07:41:43.846499 86513 MultiSolver.cpp:76] [7] PROFILING BEGIN[Forward]
I1226 07:41:43.887851 89800 MultiSolver.cpp:109] [6] PROFILING END[Backward]
I1226 07:41:43.887995 89800 solver.cpp:303] [6] Iteration 1, loss = 3.28448
I1226 07:41:43.888250 89800 solver.cpp:329]     Train net output #0: accuracy = 0.34375
I1226 07:41:43.888412 89800 solver.cpp:329]     Train net output #1: loss = 3.28448 (* 1 = 3.28448 loss)
I1226 07:41:43.888515 89800 MultiSolver.cpp:76] [6] PROFILING BEGIN[Forward]
I1226 07:41:40.715399 89666 MultiSolver.cpp:109] [5] PROFILING END[Backward]
I1226 07:41:40.715500 89666 solver.cpp:303] [5] Iteration 1, loss = 3.24646
I1226 07:41:40.715602 89666 solver.cpp:329]     Train net output #0: accuracy = 0.234375
I1226 07:41:40.715675 89666 solver.cpp:329]     Train net output #1: loss = 3.24646 (* 1 = 3.24646 loss)
I1226 07:41:40.715776 89666 MultiSolver.cpp:76] [5] PROFILING BEGIN[Forward]
I1226 07:41:45.055716 89665 MultiSolver.cpp:93] [3] PROFILING END[Forward]
I1226 07:41:45.055853 89665 MultiSolver.cpp:95] [3] PROFILING BEGIN[Backward]
I1226 07:41:45.173642 88308 MultiSolver.cpp:93] [2] PROFILING END[Forward]
I1226 07:41:45.173720 88308 MultiSolver.cpp:95] [2] PROFILING BEGIN[Backward]
I1226 07:41:45.270386 89665 MultiSolver.cpp:109] [3] PROFILING END[Backward]
I1226 07:41:45.270490 89665 solver.cpp:303] [3] Iteration 1, loss = 3.31336
I1226 07:41:45.270577 89665 solver.cpp:329]     Train net output #0: accuracy = 0.28125
I1226 07:41:45.270673 89665 solver.cpp:329]     Train net output #1: loss = 3.31336 (* 1 = 3.31336 loss)
I1226 07:41:45.270771 89665 MultiSolver.cpp:76] [3] PROFILING BEGIN[Forward]
I1226 07:41:45.406002 88308 MultiSolver.cpp:109] [2] PROFILING END[Backward]
I1226 07:41:45.406100 88308 solver.cpp:303] [2] Iteration 1, loss = 3.10557
I1226 07:41:45.406177 88308 solver.cpp:329]     Train net output #0: accuracy = 0.3125
I1226 07:41:45.406252 88308 solver.cpp:329]     Train net output #1: loss = 3.10557 (* 1 = 3.10557 loss)
I1226 07:41:45.406322 88308 MultiSolver.cpp:76] [2] PROFILING BEGIN[Forward]
I1226 07:41:45.572964 93793 MultiSolver.cpp:93] [1] PROFILING END[Forward]
I1226 07:41:45.573079 93793 MultiSolver.cpp:95] [1] PROFILING BEGIN[Backward]
I1226 07:41:45.939816 90196 MultiSolver.cpp:93] [4] PROFILING END[Forward]
I1226 07:41:45.939930 90196 MultiSolver.cpp:95] [4] PROFILING BEGIN[Backward]
I1226 07:41:46.048149 90196 MultiSolver.cpp:109] [4] PROFILING END[Backward]
I1226 07:41:46.048234 90196 solver.cpp:303] [4] Iteration 2, loss = 3.70727
I1226 07:41:46.048297 90196 solver.cpp:329]     Train net output #0: accuracy = 0.171875
I1226 07:41:46.048365 90196 solver.cpp:329]     Train net output #1: loss = 3.70727 (* 1 = 3.70727 loss)
I1226 07:41:46.048485 90196 MultiSolver.cpp:76] [4] PROFILING BEGIN[Forward]
I1226 07:41:46.235618 89800 MultiSolver.cpp:93] [6] PROFILING END[Forward]
I1226 07:41:46.235760 89800 MultiSolver.cpp:95] [6] PROFILING BEGIN[Backward]
I1226 07:41:42.966534 89666 MultiSolver.cpp:93] [5] PROFILING END[Forward]
I1226 07:41:42.966675 89666 MultiSolver.cpp:95] [5] PROFILING BEGIN[Backward]
I1226 07:41:46.236564 86513 MultiSolver.cpp:93] [7] PROFILING END[Forward]
I1226 07:41:46.236675 86513 MultiSolver.cpp:95] [7] PROFILING BEGIN[Backward]
I1226 07:41:43.067430 89666 MultiSolver.cpp:109] [5] PROFILING END[Backward]
I1226 07:41:43.067548 89666 solver.cpp:303] [5] Iteration 2, loss = 3.40549
I1226 07:41:43.067652 89666 solver.cpp:329]     Train net output #0: accuracy = 0.28125
I1226 07:41:43.067726 89666 solver.cpp:329]     Train net output #1: loss = 3.40549 (* 1 = 3.40549 loss)
I1226 07:41:43.067809 89666 MultiSolver.cpp:76] [5] PROFILING BEGIN[Forward]
I1226 07:41:46.379667 86513 MultiSolver.cpp:109] [7] PROFILING END[Backward]
I1226 07:41:46.379796 86513 solver.cpp:303] [7] Iteration 2, loss = 3.26189
I1226 07:41:46.379881 86513 solver.cpp:329]     Train net output #0: accuracy = 0.296875
I1226 07:41:46.380149 86513 solver.cpp:329]     Train net output #1: loss = 3.26189 (* 1 = 3.26189 loss)
I1226 07:41:46.380235 86513 MultiSolver.cpp:76] [7] PROFILING BEGIN[Forward]
I1226 07:41:46.389200 89800 MultiSolver.cpp:109] [6] PROFILING END[Backward]
I1226 07:41:46.389294 89800 solver.cpp:303] [6] Iteration 2, loss = 3.04565
I1226 07:41:46.389416 89800 solver.cpp:329]     Train net output #0: accuracy = 0.28125
I1226 07:41:46.389505 89800 solver.cpp:329]     Train net output #1: loss = 3.04565 (* 1 = 3.04565 loss)
I1226 07:41:46.389605 89800 MultiSolver.cpp:76] [6] PROFILING BEGIN[Forward]
I1226 07:41:46.549432 93793 MultiSolver.cpp:109] [1] PROFILING END[Backward]
I1226 07:41:46.549528 93793 solver.cpp:303] [1] Iteration 1, loss = 2.93718
I1226 07:41:46.549594 93793 solver.cpp:329]     Train net output #0: accuracy = 0.328125
I1226 07:41:46.549660 93793 solver.cpp:329]     Train net output #1: loss = 2.93718 (* 1 = 2.93718 loss)
I1226 07:41:46.549886 93793 MultiSolver.cpp:76] [1] PROFILING BEGIN[Forward]
I1226 07:41:47.845541 90196 MultiSolver.cpp:93] [4] PROFILING END[Forward]
I1226 07:41:47.845651 90196 MultiSolver.cpp:95] [4] PROFILING BEGIN[Backward]
I1226 07:41:47.939160 90196 MultiSolver.cpp:109] [4] PROFILING END[Backward]
I1226 07:41:47.939244 90196 solver.cpp:303] [4] Iteration 3, loss = 3.02393
I1226 07:41:47.939311 90196 solver.cpp:329]     Train net output #0: accuracy = 0.328125
I1226 07:41:47.939374 90196 solver.cpp:329]     Train net output #1: loss = 3.02393 (* 1 = 3.02393 loss)
I1226 07:41:47.939493 90196 MultiSolver.cpp:76] [4] PROFILING BEGIN[Forward]
I1226 07:41:48.239589 89800 MultiSolver.cpp:93] [6] PROFILING END[Forward]
I1226 07:41:48.239729 89800 MultiSolver.cpp:95] [6] PROFILING BEGIN[Backward]
I1226 07:41:44.970535 89666 MultiSolver.cpp:93] [5] PROFILING END[Forward]
I1226 07:41:44.970682 89666 MultiSolver.cpp:95] [5] PROFILING BEGIN[Backward]
I1226 07:41:48.243428 86513 MultiSolver.cpp:93] [7] PROFILING END[Forward]
I1226 07:41:48.243577 86513 MultiSolver.cpp:95] [7] PROFILING BEGIN[Backward]
I1226 07:41:45.059514 89666 MultiSolver.cpp:109] [5] PROFILING END[Backward]
I1226 07:41:45.059633 89666 solver.cpp:303] [5] Iteration 3, loss = 3.10131
I1226 07:41:45.059710 89666 solver.cpp:329]     Train net output #0: accuracy = 0.296875
I1226 07:41:45.059787 89666 solver.cpp:329]     Train net output #1: loss = 3.10131 (* 1 = 3.10131 loss)
I1226 07:41:45.059880 89666 MultiSolver.cpp:76] [5] PROFILING BEGIN[Forward]
I1226 07:41:48.382930 89800 MultiSolver.cpp:109] [6] PROFILING END[Backward]
I1226 07:41:48.383041 89800 solver.cpp:303] [6] Iteration 3, loss = 3.12415
I1226 07:41:48.383117 89800 solver.cpp:329]     Train net output #0: accuracy = 0.296875
I1226 07:41:48.383194 89800 solver.cpp:329]     Train net output #1: loss = 3.12415 (* 1 = 3.12415 loss)
I1226 07:41:48.383281 89800 MultiSolver.cpp:76] [6] PROFILING BEGIN[Forward]
I1226 07:41:48.392685 86513 MultiSolver.cpp:109] [7] PROFILING END[Backward]
I1226 07:41:48.392803 86513 solver.cpp:303] [7] Iteration 3, loss = 3.23281
I1226 07:41:48.392890 86513 solver.cpp:329]     Train net output #0: accuracy = 0.25
I1226 07:41:48.393012 86513 solver.cpp:329]     Train net output #1: loss = 3.23281 (* 1 = 3.23281 loss)
I1226 07:41:48.393268 86513 MultiSolver.cpp:76] [7] PROFILING BEGIN[Forward]
I1226 07:41:49.737272 89771 MultiSolver.cpp:93] [0] PROFILING END[Forward]
I1226 07:41:49.737381 89771 MultiSolver.cpp:95] [0] PROFILING BEGIN[Backward]
I1226 07:41:49.831461 89771 MultiSolver.cpp:109] [0] PROFILING END[Backward]
I1226 07:41:49.831599 89771 solver.cpp:303] [0] Iteration 2, loss = 3.41653
I1226 07:41:49.831660 89771 solver.cpp:329]     Train net output #0: accuracy = 0.390625
I1226 07:41:49.831738 89771 solver.cpp:329]     Train net output #1: loss = 3.41653 (* 1 = 3.41653 loss)
I1226 07:41:49.831815 89771 MultiSolver.cpp:76] [0] PROFILING BEGIN[Forward]
I1226 07:41:50.406280 90196 MultiSolver.cpp:93] [4] PROFILING END[Forward]
I1226 07:41:50.406390 90196 MultiSolver.cpp:95] [4] PROFILING BEGIN[Backward]
I1226 07:41:50.499330 90196 MultiSolver.cpp:109] [4] PROFILING END[Backward]
I1226 07:41:50.499488 90196 solver.cpp:303] [4] Iteration 4, loss = 3.56638
I1226 07:41:50.499555 90196 solver.cpp:329]     Train net output #0: accuracy = 0.21875
I1226 07:41:50.499624 90196 solver.cpp:329]     Train net output #1: loss = 3.56638 (* 1 = 3.56638 loss)
I1226 07:41:50.499728 90196 MultiSolver.cpp:76] [4] PROFILING BEGIN[Forward]
I1226 07:41:47.531811 89666 MultiSolver.cpp:93] [5] PROFILING END[Forward]
I1226 07:41:50.801141 89800 MultiSolver.cpp:93] [6] PROFILING END[Forward]
I1226 07:41:47.531942 89666 MultiSolver.cpp:95] [5] PROFILING BEGIN[Backward]
I1226 07:41:50.801254 89800 MultiSolver.cpp:95] [6] PROFILING BEGIN[Backward]
I1226 07:41:50.805490 86513 MultiSolver.cpp:93] [7] PROFILING END[Forward]
I1226 07:41:50.805629 86513 MultiSolver.cpp:95] [7] PROFILING BEGIN[Backward]
I1226 07:41:47.620635 89666 MultiSolver.cpp:109] [5] PROFILING END[Backward]
I1226 07:41:47.620729 89666 solver.cpp:303] [5] Iteration 4, loss = 3.18976
I1226 07:41:47.620807 89666 solver.cpp:329]     Train net output #0: accuracy = 0.390625
I1226 07:41:47.620884 89666 solver.cpp:329]     Train net output #1: loss = 3.18976 (* 1 = 3.18976 loss)
I1226 07:41:47.620961 89666 MultiSolver.cpp:76] [5] PROFILING BEGIN[Forward]
I1226 07:41:50.941773 89800 MultiSolver.cpp:109] [6] PROFILING END[Backward]
I1226 07:41:50.941869 89800 solver.cpp:303] [6] Iteration 4, loss = 3.39985
I1226 07:41:50.941946 89800 solver.cpp:329]     Train net output #0: accuracy = 0.25
I1226 07:41:50.942021 89800 solver.cpp:329]     Train net output #1: loss = 3.39985 (* 1 = 3.39985 loss)
I1226 07:41:50.942164 89800 MultiSolver.cpp:76] [6] PROFILING BEGIN[Forward]
I1226 07:41:50.946956 86513 MultiSolver.cpp:109] [7] PROFILING END[Backward]
I1226 07:41:50.947048 86513 solver.cpp:303] [7] Iteration 4, loss = 3.39557
I1226 07:41:50.947125 86513 solver.cpp:329]     Train net output #0: accuracy = 0.265625
I1226 07:41:50.947201 86513 solver.cpp:329]     Train net output #1: loss = 3.39557 (* 1 = 3.39557 loss)
I1226 07:41:50.947329 86513 MultiSolver.cpp:76] [7] PROFILING BEGIN[Forward]
I1226 07:41:51.255679 88308 MultiSolver.cpp:93] [2] PROFILING END[Forward]
I1226 07:41:51.255789 88308 MultiSolver.cpp:95] [2] PROFILING BEGIN[Backward]
I1226 07:41:51.265136 89665 MultiSolver.cpp:93] [3] PROFILING END[Forward]
I1226 07:41:51.265292 89665 MultiSolver.cpp:95] [3] PROFILING BEGIN[Backward]
I1226 07:41:51.317036 93793 MultiSolver.cpp:93] [1] PROFILING END[Forward]
I1226 07:41:51.317162 93793 MultiSolver.cpp:95] [1] PROFILING BEGIN[Backward]
I1226 07:41:51.394759 88308 MultiSolver.cpp:109] [2] PROFILING END[Backward]
I1226 07:41:51.394857 88308 solver.cpp:303] [2] Iteration 2, loss = 3.33208
I1226 07:41:51.394934 88308 solver.cpp:329]     Train net output #0: accuracy = 0.328125
I1226 07:41:51.395043 88308 solver.cpp:329]     Train net output #1: loss = 3.33208 (* 1 = 3.33208 loss)
I1226 07:41:51.395303 88308 MultiSolver.cpp:76] [2] PROFILING BEGIN[Forward]
I1226 07:41:51.422204 89665 MultiSolver.cpp:109] [3] PROFILING END[Backward]
I1226 07:41:51.422307 89665 solver.cpp:303] [3] Iteration 2, loss = 2.77244
I1226 07:41:51.422406 89665 solver.cpp:329]     Train net output #0: accuracy = 0.484375
I1226 07:41:51.422505 89665 solver.cpp:329]     Train net output #1: loss = 2.77244 (* 1 = 2.77244 loss)
I1226 07:41:51.422597 89665 MultiSolver.cpp:76] [3] PROFILING BEGIN[Forward]
I1226 07:41:52.217239 93793 MultiSolver.cpp:109] [1] PROFILING END[Backward]
I1226 07:41:52.218392 93793 solver.cpp:303] [1] Iteration 2, loss = 3.63924
I1226 07:41:52.218484 93793 solver.cpp:329]     Train net output #0: accuracy = 0.296875
I1226 07:41:52.218561 93793 solver.cpp:329]     Train net output #1: loss = 3.63924 (* 1 = 3.63924 loss)
I1226 07:41:52.218628 93793 MultiSolver.cpp:76] [1] PROFILING BEGIN[Forward]
I1226 07:41:52.429118 90196 MultiSolver.cpp:93] [4] PROFILING END[Forward]
I1226 07:41:52.429237 90196 MultiSolver.cpp:95] [4] PROFILING BEGIN[Backward]
I1226 07:41:52.521436 90196 MultiSolver.cpp:109] [4] PROFILING END[Backward]
I1226 07:41:52.521553 90196 solver.cpp:303] [4] Iteration 5, loss = 2.6161
I1226 07:41:52.521621 90196 solver.cpp:329]     Train net output #0: accuracy = 0.40625
I1226 07:41:52.521690 90196 solver.cpp:329]     Train net output #1: loss = 2.6161 (* 1 = 2.6161 loss)
I1226 07:41:52.521785 90196 MultiSolver.cpp:76] [4] PROFILING BEGIN[Forward]
I1226 07:41:52.827311 89800 MultiSolver.cpp:93] [6] PROFILING END[Forward]
I1226 07:41:52.827446 89800 MultiSolver.cpp:95] [6] PROFILING BEGIN[Backward]
I1226 07:41:49.559300 89666 MultiSolver.cpp:93] [5] PROFILING END[Forward]
I1226 07:41:49.559414 89666 MultiSolver.cpp:95] [5] PROFILING BEGIN[Backward]
I1226 07:41:52.833657 86513 MultiSolver.cpp:93] [7] PROFILING END[Forward]
I1226 07:41:52.833780 86513 MultiSolver.cpp:95] [7] PROFILING BEGIN[Backward]
I1226 07:41:49.646371 89666 MultiSolver.cpp:109] [5] PROFILING END[Backward]
I1226 07:41:49.646466 89666 solver.cpp:303] [5] Iteration 5, loss = 3.09253
I1226 07:41:49.646544 89666 solver.cpp:329]     Train net output #0: accuracy = 0.390625
I1226 07:41:49.646646 89666 solver.cpp:329]     Train net output #1: loss = 3.09253 (* 1 = 3.09253 loss)
I1226 07:41:49.646715 89666 MultiSolver.cpp:76] [5] PROFILING BEGIN[Forward]
I1226 07:41:52.974202 89800 MultiSolver.cpp:109] [6] PROFILING END[Backward]
I1226 07:41:52.974303 89800 solver.cpp:303] [6] Iteration 5, loss = 3.53225
I1226 07:41:52.974412 89800 solver.cpp:329]     Train net output #0: accuracy = 0.1875
I1226 07:41:52.974493 89800 solver.cpp:329]     Train net output #1: loss = 3.53225 (* 1 = 3.53225 loss)
I1226 07:41:52.974572 89800 MultiSolver.cpp:76] [6] PROFILING BEGIN[Forward]
I1226 07:41:52.982574 86513 MultiSolver.cpp:109] [7] PROFILING END[Backward]
I1226 07:41:52.982667 86513 solver.cpp:303] [7] Iteration 5, loss = 2.96296
I1226 07:41:52.982744 86513 solver.cpp:329]     Train net output #0: accuracy = 0.34375
I1226 07:41:52.982820 86513 solver.cpp:329]     Train net output #1: loss = 2.96296 (* 1 = 2.96296 loss)
I1226 07:41:52.982909 86513 MultiSolver.cpp:76] [7] PROFILING BEGIN[Forward]
I1226 07:41:54.428344 90196 MultiSolver.cpp:93] [4] PROFILING END[Forward]
I1226 07:41:54.428493 90196 MultiSolver.cpp:95] [4] PROFILING BEGIN[Backward]
I1226 07:41:54.519913 90196 MultiSolver.cpp:109] [4] PROFILING END[Backward]
I1226 07:41:54.519999 90196 solver.cpp:303] [4] Iteration 6, loss = 3.36683
I1226 07:41:54.520066 90196 solver.cpp:329]     Train net output #0: accuracy = 0.328125
I1226 07:41:54.520133 90196 solver.cpp:329]     Train net output #1: loss = 3.36683 (* 1 = 3.36683 loss)
I1226 07:41:54.520200 90196 MultiSolver.cpp:76] [4] PROFILING BEGIN[Forward]
I1226 07:41:51.592010 89666 MultiSolver.cpp:93] [5] PROFILING END[Forward]
I1226 07:41:54.861274 89800 MultiSolver.cpp:93] [6] PROFILING END[Forward]
I1226 07:41:51.592133 89666 MultiSolver.cpp:95] [5] PROFILING BEGIN[Backward]
I1226 07:41:54.861419 89800 MultiSolver.cpp:95] [6] PROFILING BEGIN[Backward]
I1226 07:41:54.862862 86513 MultiSolver.cpp:93] [7] PROFILING END[Forward]
I1226 07:41:54.862982 86513 MultiSolver.cpp:95] [7] PROFILING BEGIN[Backward]
I1226 07:41:51.686657 89666 MultiSolver.cpp:109] [5] PROFILING END[Backward]
I1226 07:41:51.686750 89666 solver.cpp:303] [5] Iteration 6, loss = 3.30048
I1226 07:41:51.686830 89666 solver.cpp:329]     Train net output #0: accuracy = 0.3125
I1226 07:41:51.686906 89666 solver.cpp:329]     Train net output #1: loss = 3.30048 (* 1 = 3.30048 loss)
I1226 07:41:51.687029 89666 MultiSolver.cpp:76] [5] PROFILING BEGIN[Forward]
I1226 07:41:55.003998 89800 MultiSolver.cpp:109] [6] PROFILING END[Backward]
I1226 07:41:55.004088 89800 solver.cpp:303] [6] Iteration 6, loss = 3.05734
I1226 07:41:55.004168 89800 solver.cpp:329]     Train net output #0: accuracy = 0.328125
I1226 07:41:55.004245 89800 solver.cpp:329]     Train net output #1: loss = 3.05734 (* 1 = 3.05734 loss)
I1226 07:41:55.004412 89800 MultiSolver.cpp:76] [6] PROFILING BEGIN[Forward]
I1226 07:41:55.008620 86513 MultiSolver.cpp:109] [7] PROFILING END[Backward]
I1226 07:41:55.008711 86513 solver.cpp:303] [7] Iteration 6, loss = 2.9065
I1226 07:41:55.008788 86513 solver.cpp:329]     Train net output #0: accuracy = 0.375
I1226 07:41:55.008867 86513 solver.cpp:329]     Train net output #1: loss = 2.9065 (* 1 = 2.9065 loss)
I1226 07:41:55.008942 86513 MultiSolver.cpp:76] [7] PROFILING BEGIN[Forward]
I1226 07:41:55.798189 89771 MultiSolver.cpp:93] [0] PROFILING END[Forward]
I1226 07:41:55.798302 89771 MultiSolver.cpp:95] [0] PROFILING BEGIN[Backward]
I1226 07:41:55.894330 89771 MultiSolver.cpp:109] [0] PROFILING END[Backward]
I1226 07:41:55.894421 89771 solver.cpp:303] [0] Iteration 3, loss = 3.32935
I1226 07:41:55.894515 89771 solver.cpp:329]     Train net output #0: accuracy = 0.359375
I1226 07:41:55.894587 89771 solver.cpp:329]     Train net output #1: loss = 3.32935 (* 1 = 3.32935 loss)
I1226 07:41:55.894712 89771 MultiSolver.cpp:76] [0] PROFILING BEGIN[Forward]
I1226 07:41:56.522636 90196 MultiSolver.cpp:93] [4] PROFILING END[Forward]
I1226 07:41:56.522768 90196 MultiSolver.cpp:95] [4] PROFILING BEGIN[Backward]
I1226 07:41:56.619657 90196 MultiSolver.cpp:109] [4] PROFILING END[Backward]
I1226 07:41:56.619819 90196 solver.cpp:303] [4] Iteration 7, loss = 3.42264
I1226 07:41:56.619886 90196 solver.cpp:329]     Train net output #0: accuracy = 0.375
I1226 07:41:56.619954 90196 solver.cpp:329]     Train net output #1: loss = 3.42264 (* 1 = 3.42264 loss)
I1226 07:41:56.620219 90196 MultiSolver.cpp:76] [4] PROFILING BEGIN[Forward]
I1226 07:41:53.631433 89666 MultiSolver.cpp:93] [5] PROFILING END[Forward]
I1226 07:41:56.900672 89800 MultiSolver.cpp:93] [6] PROFILING END[Forward]
I1226 07:41:53.631544 89666 MultiSolver.cpp:95] [5] PROFILING BEGIN[Backward]
I1226 07:41:56.900786 89800 MultiSolver.cpp:95] [6] PROFILING BEGIN[Backward]
I1226 07:41:56.903364 86513 MultiSolver.cpp:93] [7] PROFILING END[Forward]
I1226 07:41:56.903477 86513 MultiSolver.cpp:95] [7] PROFILING BEGIN[Backward]
I1226 07:41:56.959699 88308 MultiSolver.cpp:93] [2] PROFILING END[Forward]
I1226 07:41:56.959806 88308 MultiSolver.cpp:95] [2] PROFILING BEGIN[Backward]
I1226 07:41:56.977466 89665 MultiSolver.cpp:93] [3] PROFILING END[Forward]
I1226 07:41:56.977612 89665 MultiSolver.cpp:95] [3] PROFILING BEGIN[Backward]
I1226 07:41:53.722995 89666 MultiSolver.cpp:109] [5] PROFILING END[Backward]
I1226 07:41:53.723131 89666 solver.cpp:303] [5] Iteration 7, loss = 3.02592
I1226 07:41:53.723215 89666 solver.cpp:329]     Train net output #0: accuracy = 0.375
I1226 07:41:53.723384 89666 solver.cpp:329]     Train net output #1: loss = 3.02592 (* 1 = 3.02592 loss)
I1226 07:41:53.723454 89666 MultiSolver.cpp:76] [5] PROFILING BEGIN[Forward]
I1226 07:41:57.018472 93793 MultiSolver.cpp:93] [1] PROFILING END[Forward]
I1226 07:41:57.018645 93793 MultiSolver.cpp:95] [1] PROFILING BEGIN[Backward]
I1226 07:41:57.041729 89800 MultiSolver.cpp:109] [6] PROFILING END[Backward]
I1226 07:41:57.041823 89800 solver.cpp:303] [6] Iteration 7, loss = 3.22138
I1226 07:41:57.041900 89800 solver.cpp:329]     Train net output #0: accuracy = 0.296875
I1226 07:41:57.042075 89800 solver.cpp:329]     Train net output #1: loss = 3.22138 (* 1 = 3.22138 loss)
I1226 07:41:57.042189 89800 MultiSolver.cpp:76] [6] PROFILING BEGIN[Forward]
I1226 07:41:57.049695 86513 MultiSolver.cpp:109] [7] PROFILING END[Backward]
I1226 07:41:57.049788 86513 solver.cpp:303] [7] Iteration 7, loss = 3.33595
I1226 07:41:57.049866 86513 solver.cpp:329]     Train net output #0: accuracy = 0.375
I1226 07:41:57.050036 86513 solver.cpp:329]     Train net output #1: loss = 3.33595 (* 1 = 3.33595 loss)
I1226 07:41:57.050112 86513 MultiSolver.cpp:76] [7] PROFILING BEGIN[Forward]
I1226 07:41:57.098420 88308 MultiSolver.cpp:109] [2] PROFILING END[Backward]
I1226 07:41:57.098510 88308 solver.cpp:303] [2] Iteration 3, loss = 2.8434
I1226 07:41:57.098616 88308 solver.cpp:329]     Train net output #0: accuracy = 0.40625
I1226 07:41:57.098698 88308 solver.cpp:329]     Train net output #1: loss = 2.8434 (* 1 = 2.8434 loss)
I1226 07:41:57.098839 88308 MultiSolver.cpp:76] [2] PROFILING BEGIN[Forward]
I1226 07:41:57.115193 89665 MultiSolver.cpp:109] [3] PROFILING END[Backward]
I1226 07:41:57.115275 89665 solver.cpp:303] [3] Iteration 3, loss = 2.50089
I1226 07:41:57.115344 89665 solver.cpp:329]     Train net output #0: accuracy = 0.375
I1226 07:41:57.115411 89665 solver.cpp:329]     Train net output #1: loss = 2.50089 (* 1 = 2.50089 loss)
I1226 07:41:57.115485 89665 MultiSolver.cpp:76] [3] PROFILING BEGIN[Forward]
I1226 07:41:57.914230 93793 MultiSolver.cpp:109] [1] PROFILING END[Backward]
I1226 07:41:57.914324 93793 solver.cpp:303] [1] Iteration 3, loss = 3.55717
I1226 07:41:57.914432 93793 solver.cpp:329]     Train net output #0: accuracy = 0.171875
I1226 07:41:57.914676 93793 solver.cpp:329]     Train net output #1: loss = 3.55717 (* 1 = 3.55717 loss)
I1226 07:41:57.914902 93793 MultiSolver.cpp:76] [1] PROFILING BEGIN[Forward]
I1226 07:41:58.500771 90196 MultiSolver.cpp:93] [4] PROFILING END[Forward]
I1226 07:41:58.500890 90196 MultiSolver.cpp:95] [4] PROFILING BEGIN[Backward]
I1226 07:41:58.596148 90196 MultiSolver.cpp:109] [4] PROFILING END[Backward]
I1226 07:41:58.596231 90196 solver.cpp:303] [4] Iteration 8, loss = 2.89096
I1226 07:41:58.596297 90196 solver.cpp:329]     Train net output #0: accuracy = 0.359375
I1226 07:41:58.596364 90196 solver.cpp:329]     Train net output #1: loss = 2.89096 (* 1 = 2.89096 loss)
I1226 07:41:58.596479 90196 MultiSolver.cpp:76] [4] PROFILING BEGIN[Forward]
I1226 07:41:55.641242 89666 MultiSolver.cpp:93] [5] PROFILING END[Forward]
I1226 07:41:58.910518 89800 MultiSolver.cpp:93] [6] PROFILING END[Forward]
I1226 07:41:55.641355 89666 MultiSolver.cpp:95] [5] PROFILING BEGIN[Backward]
I1226 07:41:58.910636 89800 MultiSolver.cpp:95] [6] PROFILING BEGIN[Backward]
I1226 07:41:58.911703 86513 MultiSolver.cpp:93] [7] PROFILING END[Forward]
I1226 07:41:58.911829 86513 MultiSolver.cpp:95] [7] PROFILING BEGIN[Backward]
I1226 07:41:55.732450 89666 MultiSolver.cpp:109] [5] PROFILING END[Backward]
I1226 07:41:55.732543 89666 solver.cpp:303] [5] Iteration 8, loss = 2.70091
I1226 07:41:55.732650 89666 solver.cpp:329]     Train net output #0: accuracy = 0.453125
I1226 07:41:55.732728 89666 solver.cpp:329]     Train net output #1: loss = 2.70091 (* 1 = 2.70091 loss)
I1226 07:41:55.732843 89666 MultiSolver.cpp:76] [5] PROFILING BEGIN[Forward]
I1226 07:41:59.050285 89800 MultiSolver.cpp:109] [6] PROFILING END[Backward]
I1226 07:41:59.050405 89800 solver.cpp:303] [6] Iteration 8, loss = 3.49775
I1226 07:41:59.050483 89800 solver.cpp:329]     Train net output #0: accuracy = 0.28125
I1226 07:41:59.050561 89800 solver.cpp:329]     Train net output #1: loss = 3.49775 (* 1 = 3.49775 loss)
I1226 07:41:59.050674 89800 MultiSolver.cpp:76] [6] PROFILING BEGIN[Forward]
I1226 07:41:59.058174 86513 MultiSolver.cpp:109] [7] PROFILING END[Backward]
I1226 07:41:59.058267 86513 solver.cpp:303] [7] Iteration 8, loss = 3.28432
I1226 07:41:59.058368 86513 solver.cpp:329]     Train net output #0: accuracy = 0.234375
I1226 07:41:59.058449 86513 solver.cpp:329]     Train net output #1: loss = 3.28432 (* 1 = 3.28432 loss)
I1226 07:41:59.058517 86513 MultiSolver.cpp:76] [7] PROFILING BEGIN[Forward]
I1226 07:42:00.536803 90196 MultiSolver.cpp:93] [4] PROFILING END[Forward]
I1226 07:42:00.536957 90196 MultiSolver.cpp:95] [4] PROFILING BEGIN[Backward]
I1226 07:42:00.628124 90196 MultiSolver.cpp:109] [4] PROFILING END[Backward]
I1226 07:42:00.628216 90196 solver.cpp:303] [4] Iteration 9, loss = 2.87579
I1226 07:42:00.628283 90196 solver.cpp:329]     Train net output #0: accuracy = 0.421875
I1226 07:42:00.628350 90196 solver.cpp:329]     Train net output #1: loss = 2.87579 (* 1 = 2.87579 loss)
I1226 07:42:00.628408 90196 MultiSolver.cpp:76] [4] PROFILING BEGIN[Forward]
I1226 07:41:57.673231 89666 MultiSolver.cpp:93] [5] PROFILING END[Forward]
I1226 07:41:57.673360 89666 MultiSolver.cpp:95] [5] PROFILING BEGIN[Backward]
I1226 07:42:00.942675 89800 MultiSolver.cpp:93] [6] PROFILING END[Forward]
I1226 07:42:00.942796 89800 MultiSolver.cpp:95] [6] PROFILING BEGIN[Backward]
I1226 07:42:00.944095 86513 MultiSolver.cpp:93] [7] PROFILING END[Forward]
I1226 07:42:00.944216 86513 MultiSolver.cpp:95] [7] PROFILING BEGIN[Backward]
I1226 07:41:57.764778 89666 MultiSolver.cpp:109] [5] PROFILING END[Backward]
I1226 07:41:57.764907 89666 solver.cpp:303] [5] Iteration 9, loss = 3.2005
I1226 07:41:57.764991 89666 solver.cpp:329]     Train net output #0: accuracy = 0.34375
I1226 07:41:57.765076 89666 solver.cpp:329]     Train net output #1: loss = 3.2005 (* 1 = 3.2005 loss)
I1226 07:41:57.765153 89666 MultiSolver.cpp:76] [5] PROFILING BEGIN[Forward]
I1226 07:42:01.086870 89800 MultiSolver.cpp:109] [6] PROFILING END[Backward]
I1226 07:42:01.086964 89800 solver.cpp:303] [6] Iteration 9, loss = 3.38477
I1226 07:42:01.087043 89800 solver.cpp:329]     Train net output #0: accuracy = 0.25
I1226 07:42:01.087118 89800 solver.cpp:329]     Train net output #1: loss = 3.38477 (* 1 = 3.38477 loss)
I1226 07:42:01.087216 89800 MultiSolver.cpp:76] [6] PROFILING BEGIN[Forward]
I1226 07:42:01.088460 86513 MultiSolver.cpp:109] [7] PROFILING END[Backward]
I1226 07:42:01.088551 86513 solver.cpp:303] [7] Iteration 9, loss = 3.31712
I1226 07:42:01.088629 86513 solver.cpp:329]     Train net output #0: accuracy = 0.3125
I1226 07:42:01.088704 86513 solver.cpp:329]     Train net output #1: loss = 3.31712 (* 1 = 3.31712 loss)
I1226 07:42:01.088783 86513 MultiSolver.cpp:76] [7] PROFILING BEGIN[Forward]
I1226 07:42:01.369451 89771 MultiSolver.cpp:93] [0] PROFILING END[Forward]
I1226 07:42:01.369586 89771 MultiSolver.cpp:95] [0] PROFILING BEGIN[Backward]
I1226 07:42:01.458071 89771 MultiSolver.cpp:109] [0] PROFILING END[Backward]
I1226 07:42:01.458153 89771 solver.cpp:303] [0] Iteration 4, loss = 3.687
I1226 07:42:01.458214 89771 solver.cpp:329]     Train net output #0: accuracy = 0.234375
I1226 07:42:01.458292 89771 solver.cpp:329]     Train net output #1: loss = 3.687 (* 1 = 3.687 loss)
I1226 07:42:01.458369 89771 MultiSolver.cpp:76] [0] PROFILING BEGIN[Forward]
I1226 07:42:02.568290 90196 MultiSolver.cpp:93] [4] PROFILING END[Forward]
I1226 07:42:02.568404 90196 MultiSolver.cpp:95] [4] PROFILING BEGIN[Backward]
I1226 07:42:02.664582 90196 MultiSolver.cpp:109] [4] PROFILING END[Backward]
I1226 07:42:02.664667 90196 solver.cpp:303] [4] Iteration 10, loss = 2.74311
I1226 07:42:02.664736 90196 solver.cpp:329]     Train net output #0: accuracy = 0.453125
I1226 07:42:02.664803 90196 solver.cpp:329]     Train net output #1: loss = 2.74311 (* 1 = 2.74311 loss)
I1226 07:42:02.664880 90196 MultiSolver.cpp:76] [4] PROFILING BEGIN[Forward]
I1226 07:42:02.984001 89800 MultiSolver.cpp:93] [6] PROFILING END[Forward]
I1226 07:42:02.984113 89800 MultiSolver.cpp:95] [6] PROFILING BEGIN[Backward]
I1226 07:41:59.715713 89666 MultiSolver.cpp:93] [5] PROFILING END[Forward]
I1226 07:41:59.715826 89666 MultiSolver.cpp:95] [5] PROFILING BEGIN[Backward]
I1226 07:42:02.988128 86513 MultiSolver.cpp:93] [7] PROFILING END[Forward]
I1226 07:42:02.988255 86513 MultiSolver.cpp:95] [7] PROFILING BEGIN[Backward]
I1226 07:42:02.998636 88308 MultiSolver.cpp:93] [2] PROFILING END[Forward]
I1226 07:42:02.998744 88308 MultiSolver.cpp:95] [2] PROFILING BEGIN[Backward]
I1226 07:42:02.998147 89665 MultiSolver.cpp:93] [3] PROFILING END[Forward]
I1226 07:42:02.998261 89665 MultiSolver.cpp:95] [3] PROFILING BEGIN[Backward]
I1226 07:42:03.058509 93793 MultiSolver.cpp:93] [1] PROFILING END[Forward]
I1226 07:42:03.058631 93793 MultiSolver.cpp:95] [1] PROFILING BEGIN[Backward]
I1226 07:41:59.804471 89666 MultiSolver.cpp:109] [5] PROFILING END[Backward]
I1226 07:41:59.804566 89666 solver.cpp:303] [5] Iteration 10, loss = 3.3598
I1226 07:41:59.804672 89666 solver.cpp:329]     Train net output #0: accuracy = 0.359375
I1226 07:41:59.804750 89666 solver.cpp:329]     Train net output #1: loss = 3.3598 (* 1 = 3.3598 loss)
I1226 07:41:59.804821 89666 MultiSolver.cpp:76] [5] PROFILING BEGIN[Forward]
I1226 07:42:03.128294 89800 MultiSolver.cpp:109] [6] PROFILING END[Backward]
I1226 07:42:03.128414 89800 solver.cpp:303] [6] Iteration 10, loss = 3.32981
I1226 07:42:03.128494 89800 solver.cpp:329]     Train net output #0: accuracy = 0.21875
I1226 07:42:03.128571 89800 solver.cpp:329]     Train net output #1: loss = 3.32981 (* 1 = 3.32981 loss)
I1226 07:42:03.128653 89800 MultiSolver.cpp:76] [6] PROFILING BEGIN[Forward]
I1226 07:42:03.134893 86513 MultiSolver.cpp:109] [7] PROFILING END[Backward]
I1226 07:42:03.134984 86513 solver.cpp:303] [7] Iteration 10, loss = 3.09077
I1226 07:42:03.135059 86513 solver.cpp:329]     Train net output #0: accuracy = 0.34375
I1226 07:42:03.135138 86513 solver.cpp:329]     Train net output #1: loss = 3.09077 (* 1 = 3.09077 loss)
I1226 07:42:03.135220 86513 MultiSolver.cpp:76] [7] PROFILING BEGIN[Forward]
I1226 07:42:03.139082 88308 MultiSolver.cpp:109] [2] PROFILING END[Backward]
I1226 07:42:03.139173 88308 solver.cpp:303] [2] Iteration 4, loss = 3.63634
I1226 07:42:03.139247 88308 solver.cpp:329]     Train net output #0: accuracy = 0.28125
I1226 07:42:03.139524 88308 solver.cpp:329]     Train net output #1: loss = 3.63634 (* 1 = 3.63634 loss)
I1226 07:42:03.139658 88308 MultiSolver.cpp:76] [2] PROFILING BEGIN[Forward]
I1226 07:42:03.154008 89665 MultiSolver.cpp:109] [3] PROFILING END[Backward]
I1226 07:42:03.154099 89665 solver.cpp:303] [3] Iteration 4, loss = 3.5816
I1226 07:42:03.154177 89665 solver.cpp:329]     Train net output #0: accuracy = 0.3125
I1226 07:42:03.154254 89665 solver.cpp:329]     Train net output #1: loss = 3.5816 (* 1 = 3.5816 loss)
I1226 07:42:03.154347 89665 MultiSolver.cpp:76] [3] PROFILING BEGIN[Forward]
I1226 07:42:03.957187 93793 MultiSolver.cpp:109] [1] PROFILING END[Backward]
I1226 07:42:03.957279 93793 solver.cpp:303] [1] Iteration 4, loss = 3.47033
I1226 07:42:03.957342 93793 solver.cpp:329]     Train net output #0: accuracy = 0.328125
I1226 07:42:03.957412 93793 solver.cpp:329]     Train net output #1: loss = 3.47033 (* 1 = 3.47033 loss)
I1226 07:42:03.957478 93793 MultiSolver.cpp:76] [1] PROFILING BEGIN[Forward]
I1226 07:42:04.590729 90196 MultiSolver.cpp:93] [4] PROFILING END[Forward]
I1226 07:42:04.590837 90196 MultiSolver.cpp:95] [4] PROFILING BEGIN[Backward]
I1226 07:42:04.686916 90196 MultiSolver.cpp:109] [4] PROFILING END[Backward]
I1226 07:42:04.687003 90196 solver.cpp:303] [4] Iteration 11, loss = 2.90554
I1226 07:42:04.687068 90196 solver.cpp:329]     Train net output #0: accuracy = 0.3125
I1226 07:42:04.687134 90196 solver.cpp:329]     Train net output #1: loss = 2.90554 (* 1 = 2.90554 loss)
I1226 07:42:04.687239 90196 MultiSolver.cpp:76] [4] PROFILING BEGIN[Forward]
I1226 07:42:05.003638 89800 MultiSolver.cpp:93] [6] PROFILING END[Forward]
I1226 07:42:05.003756 89800 MultiSolver.cpp:95] [6] PROFILING BEGIN[Backward]
I1226 07:42:01.735215 89666 MultiSolver.cpp:93] [5] PROFILING END[Forward]
I1226 07:42:01.735340 89666 MultiSolver.cpp:95] [5] PROFILING BEGIN[Backward]
I1226 07:42:05.007711 86513 MultiSolver.cpp:93] [7] PROFILING END[Forward]
I1226 07:42:05.007913 86513 MultiSolver.cpp:95] [7] PROFILING BEGIN[Backward]
I1226 07:42:01.824342 89666 MultiSolver.cpp:109] [5] PROFILING END[Backward]
I1226 07:42:01.824441 89666 solver.cpp:303] [5] Iteration 11, loss = 3.27148
I1226 07:42:01.824518 89666 solver.cpp:329]     Train net output #0: accuracy = 0.359375
I1226 07:42:01.824625 89666 solver.cpp:329]     Train net output #1: loss = 3.27148 (* 1 = 3.27148 loss)
I1226 07:42:01.824717 89666 MultiSolver.cpp:76] [5] PROFILING BEGIN[Forward]
I1226 07:42:05.148716 89800 MultiSolver.cpp:109] [6] PROFILING END[Backward]
I1226 07:42:05.148808 89800 solver.cpp:303] [6] Iteration 11, loss = 3.13454
I1226 07:42:05.148885 89800 solver.cpp:329]     Train net output #0: accuracy = 0.34375
I1226 07:42:05.148962 89800 solver.cpp:329]     Train net output #1: loss = 3.13454 (* 1 = 3.13454 loss)
I1226 07:42:05.149222 89800 MultiSolver.cpp:76] [6] PROFILING BEGIN[Forward]
I1226 07:42:05.152060 86513 MultiSolver.cpp:109] [7] PROFILING END[Backward]
I1226 07:42:05.152153 86513 solver.cpp:303] [7] Iteration 11, loss = 3.25158
I1226 07:42:05.152230 86513 solver.cpp:329]     Train net output #0: accuracy = 0.28125
I1226 07:42:05.152338 86513 solver.cpp:329]     Train net output #1: loss = 3.25158 (* 1 = 3.25158 loss)
I1226 07:42:05.152489 86513 MultiSolver.cpp:76] [7] PROFILING BEGIN[Forward]
I1226 07:42:06.606644 90196 MultiSolver.cpp:93] [4] PROFILING END[Forward]
I1226 07:42:06.606771 90196 MultiSolver.cpp:95] [4] PROFILING BEGIN[Backward]
I1226 07:42:06.701032 90196 MultiSolver.cpp:109] [4] PROFILING END[Backward]
I1226 07:42:06.701117 90196 solver.cpp:303] [4] Iteration 12, loss = 2.77366
I1226 07:42:06.701181 90196 solver.cpp:329]     Train net output #0: accuracy = 0.46875
I1226 07:42:06.701247 90196 solver.cpp:329]     Train net output #1: loss = 2.77366 (* 1 = 2.77366 loss)
I1226 07:42:06.701313 90196 MultiSolver.cpp:76] [4] PROFILING BEGIN[Forward]
I1226 07:42:03.757220 89666 MultiSolver.cpp:93] [5] PROFILING END[Forward]
I1226 07:42:07.026476 89800 MultiSolver.cpp:93] [6] PROFILING END[Forward]
I1226 07:42:03.757339 89666 MultiSolver.cpp:95] [5] PROFILING BEGIN[Backward]
I1226 07:42:07.026587 89800 MultiSolver.cpp:95] [6] PROFILING BEGIN[Backward]
I1226 07:42:07.029403 86513 MultiSolver.cpp:93] [7] PROFILING END[Forward]
I1226 07:42:07.029515 86513 MultiSolver.cpp:95] [7] PROFILING BEGIN[Backward]
I1226 07:42:03.848101 89666 MultiSolver.cpp:109] [5] PROFILING END[Backward]
I1226 07:42:03.848194 89666 solver.cpp:303] [5] Iteration 12, loss = 3.35211
I1226 07:42:03.848299 89666 solver.cpp:329]     Train net output #0: accuracy = 0.328125
I1226 07:42:03.848376 89666 solver.cpp:329]     Train net output #1: loss = 3.35211 (* 1 = 3.35211 loss)
I1226 07:42:03.848687 89666 MultiSolver.cpp:76] [5] PROFILING BEGIN[Forward]
I1226 07:42:07.172030 89800 MultiSolver.cpp:109] [6] PROFILING END[Backward]
I1226 07:42:07.172128 89800 solver.cpp:303] [6] Iteration 12, loss = 3.64199
I1226 07:42:07.172204 89800 solver.cpp:329]     Train net output #0: accuracy = 0.28125
I1226 07:42:07.172281 89800 solver.cpp:329]     Train net output #1: loss = 3.64199 (* 1 = 3.64199 loss)
I1226 07:42:07.172422 89800 MultiSolver.cpp:76] [6] PROFILING BEGIN[Forward]
I1226 07:42:07.175565 86513 MultiSolver.cpp:109] [7] PROFILING END[Backward]
I1226 07:42:07.175655 86513 solver.cpp:303] [7] Iteration 12, loss = 3.36687
I1226 07:42:07.175730 86513 solver.cpp:329]     Train net output #0: accuracy = 0.296875
I1226 07:42:07.175806 86513 solver.cpp:329]     Train net output #1: loss = 3.36687 (* 1 = 3.36687 loss)
I1226 07:42:07.175902 86513 MultiSolver.cpp:76] [7] PROFILING BEGIN[Forward]
I1226 07:42:07.368664 89771 MultiSolver.cpp:93] [0] PROFILING END[Forward]
I1226 07:42:07.368778 89771 MultiSolver.cpp:95] [0] PROFILING BEGIN[Backward]
I1226 07:42:07.463773 89771 MultiSolver.cpp:109] [0] PROFILING END[Backward]
I1226 07:42:07.463855 89771 solver.cpp:303] [0] Iteration 5, loss = 3.41805
I1226 07:42:07.463917 89771 solver.cpp:329]     Train net output #0: accuracy = 0.28125
I1226 07:42:07.463994 89771 solver.cpp:329]     Train net output #1: loss = 3.41805 (* 1 = 3.41805 loss)
I1226 07:42:07.464092 89771 MultiSolver.cpp:76] [0] PROFILING BEGIN[Forward]
I1226 07:42:08.746804 88308 MultiSolver.cpp:93] [2] PROFILING END[Forward]
I1226 07:42:08.746913 88308 MultiSolver.cpp:95] [2] PROFILING BEGIN[Backward]
I1226 07:42:08.759016 89665 MultiSolver.cpp:93] [3] PROFILING END[Forward]
I1226 07:42:08.759131 89665 MultiSolver.cpp:95] [3] PROFILING BEGIN[Backward]
I1226 07:42:08.769431 93793 MultiSolver.cpp:93] [1] PROFILING END[Forward]
I1226 07:42:08.769551 93793 MultiSolver.cpp:95] [1] PROFILING BEGIN[Backward]
I1226 07:42:08.863379 90196 MultiSolver.cpp:93] [4] PROFILING END[Forward]
I1226 07:42:08.863536 90196 MultiSolver.cpp:95] [4] PROFILING BEGIN[Backward]
I1226 07:42:08.886922 88308 MultiSolver.cpp:109] [2] PROFILING END[Backward]
I1226 07:42:08.887012 88308 solver.cpp:303] [2] Iteration 5, loss = 3.19589
I1226 07:42:08.887087 88308 solver.cpp:329]     Train net output #0: accuracy = 0.328125
I1226 07:42:08.887163 88308 solver.cpp:329]     Train net output #1: loss = 3.19589 (* 1 = 3.19589 loss)
I1226 07:42:08.887251 88308 MultiSolver.cpp:76] [2] PROFILING BEGIN[Forward]
I1226 07:42:08.898501 89665 MultiSolver.cpp:109] [3] PROFILING END[Backward]
I1226 07:42:08.898593 89665 solver.cpp:303] [3] Iteration 5, loss = 3.09205
I1226 07:42:08.898671 89665 solver.cpp:329]     Train net output #0: accuracy = 0.359375
I1226 07:42:08.898748 89665 solver.cpp:329]     Train net output #1: loss = 3.09205 (* 1 = 3.09205 loss)
I1226 07:42:08.898859 89665 MultiSolver.cpp:76] [3] PROFILING BEGIN[Forward]
I1226 07:42:08.954367 90196 MultiSolver.cpp:109] [4] PROFILING END[Backward]
I1226 07:42:08.954480 90196 solver.cpp:303] [4] Iteration 13, loss = 3.19231
I1226 07:42:08.954543 90196 solver.cpp:329]     Train net output #0: accuracy = 0.296875
I1226 07:42:08.954610 90196 solver.cpp:329]     Train net output #1: loss = 3.19231 (* 1 = 3.19231 loss)
I1226 07:42:08.954677 90196 MultiSolver.cpp:76] [4] PROFILING BEGIN[Forward]
I1226 07:42:06.006348 89666 MultiSolver.cpp:93] [5] PROFILING END[Forward]
I1226 07:42:09.275688 89800 MultiSolver.cpp:93] [6] PROFILING END[Forward]
I1226 07:42:06.006464 89666 MultiSolver.cpp:95] [5] PROFILING BEGIN[Backward]
I1226 07:42:09.275799 89800 MultiSolver.cpp:95] [6] PROFILING BEGIN[Backward]
I1226 07:42:09.279109 86513 MultiSolver.cpp:93] [7] PROFILING END[Forward]
I1226 07:42:09.279222 86513 MultiSolver.cpp:95] [7] PROFILING BEGIN[Backward]
I1226 07:42:06.105257 89666 MultiSolver.cpp:109] [5] PROFILING END[Backward]
I1226 07:42:06.105353 89666 solver.cpp:303] [5] Iteration 13, loss = 2.99013
I1226 07:42:06.105432 89666 solver.cpp:329]     Train net output #0: accuracy = 0.46875
I1226 07:42:06.105509 89666 solver.cpp:329]     Train net output #1: loss = 2.99013 (* 1 = 2.99013 loss)
I1226 07:42:06.105618 89666 MultiSolver.cpp:76] [5] PROFILING BEGIN[Forward]
I1226 07:42:09.417711 89800 MultiSolver.cpp:109] [6] PROFILING END[Backward]
I1226 07:42:09.417801 89800 solver.cpp:303] [6] Iteration 13, loss = 3.96941
I1226 07:42:09.417878 89800 solver.cpp:329]     Train net output #0: accuracy = 0.203125
I1226 07:42:09.417954 89800 solver.cpp:329]     Train net output #1: loss = 3.96941 (* 1 = 3.96941 loss)
I1226 07:42:09.418037 89800 MultiSolver.cpp:76] [6] PROFILING BEGIN[Forward]
I1226 07:42:09.427428 86513 MultiSolver.cpp:109] [7] PROFILING END[Backward]
I1226 07:42:09.427516 86513 solver.cpp:303] [7] Iteration 13, loss = 3.08828
I1226 07:42:09.427592 86513 solver.cpp:329]     Train net output #0: accuracy = 0.25
I1226 07:42:09.427669 86513 solver.cpp:329]     Train net output #1: loss = 3.08828 (* 1 = 3.08828 loss)
I1226 07:42:09.427778 86513 MultiSolver.cpp:76] [7] PROFILING BEGIN[Forward]
I1226 07:42:09.666761 93793 MultiSolver.cpp:109] [1] PROFILING END[Backward]
I1226 07:42:09.666903 93793 solver.cpp:303] [1] Iteration 5, loss = 3.33749
I1226 07:42:09.666965 93793 solver.cpp:329]     Train net output #0: accuracy = 0.328125
I1226 07:42:09.667032 93793 solver.cpp:329]     Train net output #1: loss = 3.33749 (* 1 = 3.33749 loss)
I1226 07:42:09.667098 93793 MultiSolver.cpp:76] [1] PROFILING BEGIN[Forward]
I1226 07:42:10.902515 90196 MultiSolver.cpp:93] [4] PROFILING END[Forward]
I1226 07:42:10.902628 90196 MultiSolver.cpp:95] [4] PROFILING BEGIN[Backward]
I1226 07:42:10.994814 90196 MultiSolver.cpp:109] [4] PROFILING END[Backward]
I1226 07:42:10.994901 90196 solver.cpp:303] [4] Iteration 14, loss = 3.14079
I1226 07:42:10.995059 90196 solver.cpp:329]     Train net output #0: accuracy = 0.421875
I1226 07:42:10.995128 90196 solver.cpp:329]     Train net output #1: loss = 3.14079 (* 1 = 3.14079 loss)
I1226 07:42:10.995203 90196 MultiSolver.cpp:76] [4] PROFILING BEGIN[Forward]
I1226 07:42:11.312064 89800 MultiSolver.cpp:93] [6] PROFILING END[Forward]
I1226 07:42:11.312180 89800 MultiSolver.cpp:95] [6] PROFILING BEGIN[Backward]
I1226 07:42:08.043066 89666 MultiSolver.cpp:93] [5] PROFILING END[Forward]
I1226 07:42:08.043193 89666 MultiSolver.cpp:95] [5] PROFILING BEGIN[Backward]
I1226 07:42:11.315276 86513 MultiSolver.cpp:93] [7] PROFILING END[Forward]
I1226 07:42:11.315418 86513 MultiSolver.cpp:95] [7] PROFILING BEGIN[Backward]
I1226 07:42:08.134986 89666 MultiSolver.cpp:109] [5] PROFILING END[Backward]
I1226 07:42:08.135084 89666 solver.cpp:303] [5] Iteration 14, loss = 3.01393
I1226 07:42:08.135164 89666 solver.cpp:329]     Train net output #0: accuracy = 0.3125
I1226 07:42:08.135243 89666 solver.cpp:329]     Train net output #1: loss = 3.01393 (* 1 = 3.01393 loss)
I1226 07:42:08.135403 89666 MultiSolver.cpp:76] [5] PROFILING BEGIN[Forward]
I1226 07:42:11.454422 89800 MultiSolver.cpp:109] [6] PROFILING END[Backward]
I1226 07:42:11.454515 89800 solver.cpp:303] [6] Iteration 14, loss = 3.1261
I1226 07:42:11.454592 89800 solver.cpp:329]     Train net output #0: accuracy = 0.265625
I1226 07:42:11.454668 89800 solver.cpp:329]     Train net output #1: loss = 3.1261 (* 1 = 3.1261 loss)
I1226 07:42:11.454838 89800 MultiSolver.cpp:76] [6] PROFILING BEGIN[Forward]
I1226 07:42:11.464583 86513 MultiSolver.cpp:109] [7] PROFILING END[Backward]
I1226 07:42:11.464704 86513 solver.cpp:303] [7] Iteration 14, loss = 3.19086
I1226 07:42:11.464779 86513 solver.cpp:329]     Train net output #0: accuracy = 0.296875
I1226 07:42:11.464869 86513 solver.cpp:329]     Train net output #1: loss = 3.19086 (* 1 = 3.19086 loss)
I1226 07:42:11.465082 86513 MultiSolver.cpp:76] [7] PROFILING BEGIN[Forward]
I1226 07:42:13.098831 90196 MultiSolver.cpp:93] [4] PROFILING END[Forward]
I1226 07:42:13.100240 90196 MultiSolver.cpp:95] [4] PROFILING BEGIN[Backward]
I1226 07:42:13.191083 90196 MultiSolver.cpp:109] [4] PROFILING END[Backward]
I1226 07:42:13.191164 90196 solver.cpp:303] [4] Iteration 15, loss = 2.97003
I1226 07:42:13.191231 90196 solver.cpp:329]     Train net output #0: accuracy = 0.359375
I1226 07:42:13.191319 90196 solver.cpp:329]     Train net output #1: loss = 2.97003 (* 1 = 2.97003 loss)
I1226 07:42:13.191516 90196 MultiSolver.cpp:76] [4] PROFILING BEGIN[Forward]
I1226 07:42:10.259755 89666 MultiSolver.cpp:93] [5] PROFILING END[Forward]
I1226 07:42:13.528908 89800 MultiSolver.cpp:93] [6] PROFILING END[Forward]
I1226 07:42:10.260700 89666 MultiSolver.cpp:95] [5] PROFILING BEGIN[Backward]
I1226 07:42:13.530136 89800 MultiSolver.cpp:95] [6] PROFILING BEGIN[Backward]
I1226 07:42:13.530943 86513 MultiSolver.cpp:93] [7] PROFILING END[Forward]
I1226 07:42:13.531759 86513 MultiSolver.cpp:95] [7] PROFILING BEGIN[Backward]
I1226 07:42:13.620913 89771 MultiSolver.cpp:93] [0] PROFILING END[Forward]
I1226 07:42:13.622087 89771 MultiSolver.cpp:95] [0] PROFILING BEGIN[Backward]
I1226 07:42:10.359724 89666 MultiSolver.cpp:109] [5] PROFILING END[Backward]
I1226 07:42:10.359818 89666 solver.cpp:303] [5] Iteration 15, loss = 3.59157
I1226 07:42:10.359895 89666 solver.cpp:329]     Train net output #0: accuracy = 0.25
I1226 07:42:10.359974 89666 solver.cpp:329]     Train net output #1: loss = 3.59157 (* 1 = 3.59157 loss)
I1226 07:42:10.360051 89666 MultiSolver.cpp:76] [5] PROFILING BEGIN[Forward]
I1226 07:42:13.685472 86513 MultiSolver.cpp:109] [7] PROFILING END[Backward]
I1226 07:42:13.685562 86513 solver.cpp:303] [7] Iteration 15, loss = 3.43753
I1226 07:42:13.685638 86513 solver.cpp:329]     Train net output #0: accuracy = 0.265625
I1226 07:42:13.685714 86513 solver.cpp:329]     Train net output #1: loss = 3.43753 (* 1 = 3.43753 loss)
I1226 07:42:13.685811 86513 MultiSolver.cpp:76] [7] PROFILING BEGIN[Forward]
I1226 07:42:13.694962 89800 MultiSolver.cpp:109] [6] PROFILING END[Backward]
I1226 07:42:13.695055 89800 solver.cpp:303] [6] Iteration 15, loss = 3.27012
I1226 07:42:13.695132 89800 solver.cpp:329]     Train net output #0: accuracy = 0.3125
I1226 07:42:13.695209 89800 solver.cpp:329]     Train net output #1: loss = 3.27012 (* 1 = 3.27012 loss)
I1226 07:42:13.695483 89800 MultiSolver.cpp:76] [6] PROFILING BEGIN[Forward]
I1226 07:42:13.717005 89771 MultiSolver.cpp:109] [0] PROFILING END[Backward]
I1226 07:42:13.717087 89771 solver.cpp:303] [0] Iteration 6, loss = 2.80221
I1226 07:42:13.717149 89771 solver.cpp:329]     Train net output #0: accuracy = 0.359375
I1226 07:42:13.717226 89771 solver.cpp:329]     Train net output #1: loss = 2.80221 (* 1 = 2.80221 loss)
I1226 07:42:13.717373 89771 MultiSolver.cpp:76] [0] PROFILING BEGIN[Forward]
I1226 07:42:14.757344 88308 MultiSolver.cpp:93] [2] PROFILING END[Forward]
I1226 07:42:14.758523 88308 MultiSolver.cpp:95] [2] PROFILING BEGIN[Backward]
I1226 07:42:14.792307 89665 MultiSolver.cpp:93] [3] PROFILING END[Forward]
I1226 07:42:14.793503 89665 MultiSolver.cpp:95] [3] PROFILING BEGIN[Backward]
I1226 07:42:14.864500 93793 MultiSolver.cpp:93] [1] PROFILING END[Forward]
I1226 07:42:14.864617 93793 MultiSolver.cpp:95] [1] PROFILING BEGIN[Backward]
I1226 07:42:14.900230 88308 MultiSolver.cpp:109] [2] PROFILING END[Backward]
I1226 07:42:14.900351 88308 solver.cpp:303] [2] Iteration 6, loss = 3.19349
I1226 07:42:14.900455 88308 solver.cpp:329]     Train net output #0: accuracy = 0.328125
I1226 07:42:14.900532 88308 solver.cpp:329]     Train net output #1: loss = 3.19349 (* 1 = 3.19349 loss)
I1226 07:42:14.900708 88308 MultiSolver.cpp:76] [2] PROFILING BEGIN[Forward]
I1226 07:42:15.033263 89665 MultiSolver.cpp:109] [3] PROFILING END[Backward]
I1226 07:42:15.033354 89665 solver.cpp:303] [3] Iteration 6, loss = 2.83795
I1226 07:42:15.033432 89665 solver.cpp:329]     Train net output #0: accuracy = 0.390625
I1226 07:42:15.033509 89665 solver.cpp:329]     Train net output #1: loss = 2.83795 (* 1 = 2.83795 loss)
I1226 07:42:15.033599 89665 MultiSolver.cpp:76] [3] PROFILING BEGIN[Forward]
I1226 07:42:15.164487 90196 MultiSolver.cpp:93] [4] PROFILING END[Forward]
I1226 07:42:15.164602 90196 MultiSolver.cpp:95] [4] PROFILING BEGIN[Backward]
I1226 07:42:15.253939 90196 MultiSolver.cpp:109] [4] PROFILING END[Backward]
I1226 07:42:15.254045 90196 solver.cpp:303] [4] Iteration 16, loss = 2.93076
I1226 07:42:15.254112 90196 solver.cpp:329]     Train net output #0: accuracy = 0.296875
I1226 07:42:15.254179 90196 solver.cpp:329]     Train net output #1: loss = 2.93076 (* 1 = 2.93076 loss)
I1226 07:42:15.254241 90196 MultiSolver.cpp:76] [4] PROFILING BEGIN[Forward]
I1226 07:42:15.616411 86513 MultiSolver.cpp:93] [7] PROFILING END[Forward]
I1226 07:42:15.616549 86513 MultiSolver.cpp:95] [7] PROFILING BEGIN[Backward]
I1226 07:42:12.353099 89666 MultiSolver.cpp:93] [5] PROFILING END[Forward]
I1226 07:42:12.353226 89666 MultiSolver.cpp:95] [5] PROFILING BEGIN[Backward]
I1226 07:42:15.624593 89800 MultiSolver.cpp:93] [6] PROFILING END[Forward]
I1226 07:42:15.624688 89800 MultiSolver.cpp:95] [6] PROFILING BEGIN[Backward]
I1226 07:42:12.530925 89666 MultiSolver.cpp:109] [5] PROFILING END[Backward]
I1226 07:42:12.531085 89666 solver.cpp:303] [5] Iteration 16, loss = 3.47957
I1226 07:42:12.531448 89666 solver.cpp:329]     Train net output #0: accuracy = 0.234375
I1226 07:42:12.531534 89666 solver.cpp:329]     Train net output #1: loss = 3.47957 (* 1 = 3.47957 loss)
I1226 07:42:12.531680 89666 MultiSolver.cpp:76] [5] PROFILING BEGIN[Forward]
I1226 07:42:15.869729 89800 MultiSolver.cpp:109] [6] PROFILING END[Backward]
I1226 07:42:15.869829 89800 solver.cpp:303] [6] Iteration 16, loss = 3.48173
I1226 07:42:15.869906 89800 solver.cpp:329]     Train net output #0: accuracy = 0.265625
I1226 07:42:15.869984 89800 solver.cpp:329]     Train net output #1: loss = 3.48173 (* 1 = 3.48173 loss)
I1226 07:42:15.870252 89800 MultiSolver.cpp:76] [6] PROFILING BEGIN[Forward]
I1226 07:42:15.892756 86513 MultiSolver.cpp:109] [7] PROFILING END[Backward]
I1226 07:42:15.892849 86513 solver.cpp:303] [7] Iteration 16, loss = 3.35128
I1226 07:42:15.892927 86513 solver.cpp:329]     Train net output #0: accuracy = 0.359375
I1226 07:42:15.893005 86513 solver.cpp:329]     Train net output #1: loss = 3.35128 (* 1 = 3.35128 loss)
I1226 07:42:15.893126 86513 MultiSolver.cpp:76] [7] PROFILING BEGIN[Forward]
I1226 07:42:16.193680 93793 MultiSolver.cpp:109] [1] PROFILING END[Backward]
I1226 07:42:16.193776 93793 solver.cpp:303] [1] Iteration 6, loss = 3.37147
I1226 07:42:16.193882 93793 solver.cpp:329]     Train net output #0: accuracy = 0.234375
I1226 07:42:16.193951 93793 solver.cpp:329]     Train net output #1: loss = 3.37147 (* 1 = 3.37147 loss)
I1226 07:42:16.194020 93793 MultiSolver.cpp:76] [1] PROFILING BEGIN[Forward]
I1226 07:42:17.256425 90196 MultiSolver.cpp:93] [4] PROFILING END[Forward]
I1226 07:42:17.256624 90196 MultiSolver.cpp:95] [4] PROFILING BEGIN[Backward]
I1226 07:42:17.349025 90196 MultiSolver.cpp:109] [4] PROFILING END[Backward]
I1226 07:42:17.349149 90196 solver.cpp:303] [4] Iteration 17, loss = 2.53936
I1226 07:42:17.349220 90196 solver.cpp:329]     Train net output #0: accuracy = 0.421875
I1226 07:42:17.349295 90196 solver.cpp:329]     Train net output #1: loss = 2.53936 (* 1 = 2.53936 loss)
I1226 07:42:17.349359 90196 MultiSolver.cpp:76] [4] PROFILING BEGIN[Forward]
I1226 07:42:14.410574 89666 MultiSolver.cpp:93] [5] PROFILING END[Forward]
I1226 07:42:14.410717 89666 MultiSolver.cpp:95] [5] PROFILING BEGIN[Backward]
I1226 07:42:17.680611 89800 MultiSolver.cpp:93] [6] PROFILING END[Forward]
I1226 07:42:17.680770 89800 MultiSolver.cpp:95] [6] PROFILING BEGIN[Backward]
I1226 07:42:17.682741 86513 MultiSolver.cpp:93] [7] PROFILING END[Forward]
I1226 07:42:17.682854 86513 MultiSolver.cpp:95] [7] PROFILING BEGIN[Backward]
I1226 07:42:14.499357 89666 MultiSolver.cpp:109] [5] PROFILING END[Backward]
I1226 07:42:14.499456 89666 solver.cpp:303] [5] Iteration 17, loss = 3.21433
I1226 07:42:14.499533 89666 solver.cpp:329]     Train net output #0: accuracy = 0.3125
I1226 07:42:14.499644 89666 solver.cpp:329]     Train net output #1: loss = 3.21433 (* 1 = 3.21433 loss)
I1226 07:42:14.499716 89666 MultiSolver.cpp:76] [5] PROFILING BEGIN[Forward]
I1226 07:42:17.824566 89800 MultiSolver.cpp:109] [6] PROFILING END[Backward]
I1226 07:42:17.824661 89800 solver.cpp:303] [6] Iteration 17, loss = 3.67832
I1226 07:42:17.824738 89800 solver.cpp:329]     Train net output #0: accuracy = 0.296875
I1226 07:42:17.824813 89800 solver.cpp:329]     Train net output #1: loss = 3.67832 (* 1 = 3.67832 loss)
I1226 07:42:17.824897 89800 MultiSolver.cpp:76] [6] PROFILING BEGIN[Forward]
I1226 07:42:17.825244 86513 MultiSolver.cpp:109] [7] PROFILING END[Backward]
I1226 07:42:17.825371 86513 solver.cpp:303] [7] Iteration 17, loss = 3.28141
I1226 07:42:17.825469 86513 solver.cpp:329]     Train net output #0: accuracy = 0.265625
I1226 07:42:17.825546 86513 solver.cpp:329]     Train net output #1: loss = 3.28141 (* 1 = 3.28141 loss)
I1226 07:42:17.825755 86513 MultiSolver.cpp:76] [7] PROFILING BEGIN[Forward]
I1226 07:42:19.347196 89771 MultiSolver.cpp:93] [0] PROFILING END[Forward]
I1226 07:42:19.347334 89771 MultiSolver.cpp:95] [0] PROFILING BEGIN[Backward]
I1226 07:42:19.444878 89771 MultiSolver.cpp:109] [0] PROFILING END[Backward]
I1226 07:42:19.444983 89771 solver.cpp:303] [0] Iteration 7, loss = 3.10536
I1226 07:42:19.445044 89771 solver.cpp:329]     Train net output #0: accuracy = 0.265625
I1226 07:42:19.445148 89771 solver.cpp:329]     Train net output #1: loss = 3.10536 (* 1 = 3.10536 loss)
I1226 07:42:19.445224 89771 MultiSolver.cpp:76] [0] PROFILING BEGIN[Forward]
I1226 07:42:19.976234 90196 MultiSolver.cpp:93] [4] PROFILING END[Forward]
I1226 07:42:19.976344 90196 MultiSolver.cpp:95] [4] PROFILING BEGIN[Backward]
I1226 07:42:20.067673 90196 MultiSolver.cpp:109] [4] PROFILING END[Backward]
I1226 07:42:20.067759 90196 solver.cpp:303] [4] Iteration 18, loss = 2.64881
I1226 07:42:20.067824 90196 solver.cpp:329]     Train net output #0: accuracy = 0.40625
I1226 07:42:20.067890 90196 solver.cpp:329]     Train net output #1: loss = 2.64881 (* 1 = 2.64881 loss)
I1226 07:42:20.067981 90196 MultiSolver.cpp:76] [4] PROFILING BEGIN[Forward]
I1226 07:42:17.129990 89666 MultiSolver.cpp:93] [5] PROFILING END[Forward]
I1226 07:42:20.399348 89800 MultiSolver.cpp:93] [6] PROFILING END[Forward]
I1226 07:42:17.130122 89666 MultiSolver.cpp:95] [5] PROFILING BEGIN[Backward]
I1226 07:42:20.399487 89800 MultiSolver.cpp:95] [6] PROFILING BEGIN[Backward]
I1226 07:42:20.401386 86513 MultiSolver.cpp:93] [7] PROFILING END[Forward]
I1226 07:42:20.401499 86513 MultiSolver.cpp:95] [7] PROFILING BEGIN[Backward]
I1226 07:42:17.220798 89666 MultiSolver.cpp:109] [5] PROFILING END[Backward]
I1226 07:42:17.220896 89666 solver.cpp:303] [5] Iteration 18, loss = 3.79545
I1226 07:42:17.220975 89666 solver.cpp:329]     Train net output #0: accuracy = 0.203125
I1226 07:42:17.221055 89666 solver.cpp:329]     Train net output #1: loss = 3.79545 (* 1 = 3.79545 loss)
I1226 07:42:17.221179 89666 MultiSolver.cpp:76] [5] PROFILING BEGIN[Forward]
I1226 07:42:20.544086 89800 MultiSolver.cpp:109] [6] PROFILING END[Backward]
I1226 07:42:20.544193 89800 solver.cpp:303] [6] Iteration 18, loss = 2.82312
I1226 07:42:20.544270 89800 solver.cpp:329]     Train net output #0: accuracy = 0.359375
I1226 07:42:20.544348 89800 solver.cpp:329]     Train net output #1: loss = 2.82312 (* 1 = 2.82312 loss)
I1226 07:42:20.544461 89800 MultiSolver.cpp:76] [6] PROFILING BEGIN[Forward]
I1226 07:42:20.558464 86513 MultiSolver.cpp:109] [7] PROFILING END[Backward]
I1226 07:42:20.558562 86513 solver.cpp:303] [7] Iteration 18, loss = 3.12956
I1226 07:42:20.558636 86513 solver.cpp:329]     Train net output #0: accuracy = 0.265625
I1226 07:42:20.558712 86513 solver.cpp:329]     Train net output #1: loss = 3.12956 (* 1 = 3.12956 loss)
I1226 07:42:20.558822 86513 MultiSolver.cpp:76] [7] PROFILING BEGIN[Forward]
I1226 07:42:20.877410 88308 MultiSolver.cpp:93] [2] PROFILING END[Forward]
I1226 07:42:20.877516 88308 MultiSolver.cpp:95] [2] PROFILING BEGIN[Backward]
I1226 07:42:20.888854 89665 MultiSolver.cpp:93] [3] PROFILING END[Forward]
I1226 07:42:20.888978 89665 MultiSolver.cpp:95] [3] PROFILING BEGIN[Backward]
I1226 07:42:20.938462 93793 MultiSolver.cpp:93] [1] PROFILING END[Forward]
I1226 07:42:20.938580 93793 MultiSolver.cpp:95] [1] PROFILING BEGIN[Backward]
I1226 07:42:21.014052 88308 MultiSolver.cpp:109] [2] PROFILING END[Backward]
I1226 07:42:21.014149 88308 solver.cpp:303] [2] Iteration 7, loss = 2.53787
I1226 07:42:21.014225 88308 solver.cpp:329]     Train net output #0: accuracy = 0.4375
I1226 07:42:21.014302 88308 solver.cpp:329]     Train net output #1: loss = 2.53787 (* 1 = 2.53787 loss)
I1226 07:42:21.014394 88308 MultiSolver.cpp:76] [2] PROFILING BEGIN[Forward]
I1226 07:42:21.043308 89665 MultiSolver.cpp:109] [3] PROFILING END[Backward]
I1226 07:42:21.043404 89665 solver.cpp:303] [3] Iteration 7, loss = 3.34144
I1226 07:42:21.043481 89665 solver.cpp:329]     Train net output #0: accuracy = 0.359375
I1226 07:42:21.043557 89665 solver.cpp:329]     Train net output #1: loss = 3.34144 (* 1 = 3.34144 loss)
I1226 07:42:21.043674 89665 MultiSolver.cpp:76] [3] PROFILING BEGIN[Forward]
I1226 07:42:21.838860 93793 MultiSolver.cpp:109] [1] PROFILING END[Backward]
I1226 07:42:21.838948 93793 solver.cpp:303] [1] Iteration 7, loss = 2.95768
I1226 07:42:21.839015 93793 solver.cpp:329]     Train net output #0: accuracy = 0.359375
I1226 07:42:21.839079 93793 solver.cpp:329]     Train net output #1: loss = 2.95768 (* 1 = 2.95768 loss)
I1226 07:42:21.839136 93793 MultiSolver.cpp:76] [1] PROFILING BEGIN[Forward]
I1226 07:42:22.015602 90196 MultiSolver.cpp:93] [4] PROFILING END[Forward]
I1226 07:42:22.015732 90196 MultiSolver.cpp:95] [4] PROFILING BEGIN[Backward]
I1226 07:42:22.109701 90196 MultiSolver.cpp:109] [4] PROFILING END[Backward]
I1226 07:42:22.109787 90196 solver.cpp:303] [4] Iteration 19, loss = 3.35448
I1226 07:42:22.109856 90196 solver.cpp:329]     Train net output #0: accuracy = 0.25
I1226 07:42:22.109922 90196 solver.cpp:329]     Train net output #1: loss = 3.35448 (* 1 = 3.35448 loss)
I1226 07:42:22.110002 90196 MultiSolver.cpp:76] [4] PROFILING BEGIN[Forward]
I1226 07:42:22.434254 89800 MultiSolver.cpp:93] [6] PROFILING END[Forward]
I1226 07:42:22.434365 89800 MultiSolver.cpp:95] [6] PROFILING BEGIN[Backward]
I1226 07:42:19.165319 89666 MultiSolver.cpp:93] [5] PROFILING END[Forward]
I1226 07:42:19.165447 89666 MultiSolver.cpp:95] [5] PROFILING BEGIN[Backward]
I1226 07:42:22.437768 86513 MultiSolver.cpp:93] [7] PROFILING END[Forward]
I1226 07:42:22.437880 86513 MultiSolver.cpp:95] [7] PROFILING BEGIN[Backward]
I1226 07:42:19.254693 89666 MultiSolver.cpp:109] [5] PROFILING END[Backward]
I1226 07:42:19.254784 89666 solver.cpp:303] [5] Iteration 19, loss = 3.31984
I1226 07:42:19.254863 89666 solver.cpp:329]     Train net output #0: accuracy = 0.359375
I1226 07:42:19.254941 89666 solver.cpp:329]     Train net output #1: loss = 3.31984 (* 1 = 3.31984 loss)
I1226 07:42:19.255018 89666 MultiSolver.cpp:76] [5] PROFILING BEGIN[Forward]
I1226 07:42:22.576992 89800 MultiSolver.cpp:109] [6] PROFILING END[Backward]
I1226 07:42:22.577085 89800 solver.cpp:303] [6] Iteration 19, loss = 3.30952
I1226 07:42:22.577162 89800 solver.cpp:329]     Train net output #0: accuracy = 0.34375
I1226 07:42:22.577265 89800 solver.cpp:329]     Train net output #1: loss = 3.30952 (* 1 = 3.30952 loss)
I1226 07:42:22.577514 89800 MultiSolver.cpp:76] [6] PROFILING BEGIN[Forward]
I1226 07:42:22.588693 86513 MultiSolver.cpp:109] [7] PROFILING END[Backward]
I1226 07:42:22.588790 86513 solver.cpp:303] [7] Iteration 19, loss = 3.21867
I1226 07:42:22.588867 86513 solver.cpp:329]     Train net output #0: accuracy = 0.296875
I1226 07:42:22.588944 86513 solver.cpp:329]     Train net output #1: loss = 3.21867 (* 1 = 3.21867 loss)
I1226 07:42:22.589022 86513 MultiSolver.cpp:76] [7] PROFILING BEGIN[Forward]
I1226 07:42:24.035630 90196 MultiSolver.cpp:93] [4] PROFILING END[Forward]
I1226 07:42:24.035754 90196 MultiSolver.cpp:95] [4] PROFILING BEGIN[Backward]
I1226 07:42:24.130710 90196 MultiSolver.cpp:109] [4] PROFILING END[Backward]
I1226 07:42:24.130794 90196 solver.cpp:303] [4] Iteration 20, loss = 3.42287
I1226 07:42:24.130859 90196 solver.cpp:329]     Train net output #0: accuracy = 0.21875
I1226 07:42:24.130928 90196 solver.cpp:329]     Train net output #1: loss = 3.42287 (* 1 = 3.42287 loss)
I1226 07:42:24.131049 90196 MultiSolver.cpp:76] [4] PROFILING BEGIN[Forward]
I1226 07:42:21.195875 89666 MultiSolver.cpp:93] [5] PROFILING END[Forward]
I1226 07:42:21.195989 89666 MultiSolver.cpp:95] [5] PROFILING BEGIN[Backward]
I1226 07:42:24.465965 89800 MultiSolver.cpp:93] [6] PROFILING END[Forward]
I1226 07:42:24.466092 89800 MultiSolver.cpp:95] [6] PROFILING BEGIN[Backward]
I1226 07:42:24.469655 86513 MultiSolver.cpp:93] [7] PROFILING END[Forward]
I1226 07:42:24.469772 86513 MultiSolver.cpp:95] [7] PROFILING BEGIN[Backward]
I1226 07:42:21.285012 89666 MultiSolver.cpp:109] [5] PROFILING END[Backward]
I1226 07:42:21.285104 89666 solver.cpp:303] [5] Iteration 20, loss = 3.626
I1226 07:42:21.285181 89666 solver.cpp:329]     Train net output #0: accuracy = 0.328125
I1226 07:42:21.285259 89666 solver.cpp:329]     Train net output #1: loss = 3.626 (* 1 = 3.626 loss)
I1226 07:42:21.285336 89666 MultiSolver.cpp:76] [5] PROFILING BEGIN[Forward]
I1226 07:42:24.610416 89800 MultiSolver.cpp:109] [6] PROFILING END[Backward]
I1226 07:42:24.610512 89800 solver.cpp:303] [6] Iteration 20, loss = 3.26895
I1226 07:42:24.610590 89800 solver.cpp:329]     Train net output #0: accuracy = 0.265625
I1226 07:42:24.610666 89800 solver.cpp:329]     Train net output #1: loss = 3.26895 (* 1 = 3.26895 loss)
I1226 07:42:24.610745 89800 MultiSolver.cpp:76] [6] PROFILING BEGIN[Forward]
I1226 07:42:24.616443 86513 MultiSolver.cpp:109] [7] PROFILING END[Backward]
I1226 07:42:24.616533 86513 solver.cpp:303] [7] Iteration 20, loss = 3.02918
I1226 07:42:24.616611 86513 solver.cpp:329]     Train net output #0: accuracy = 0.25
I1226 07:42:24.616686 86513 solver.cpp:329]     Train net output #1: loss = 3.02918 (* 1 = 3.02918 loss)
I1226 07:42:24.616796 86513 MultiSolver.cpp:76] [7] PROFILING BEGIN[Forward]
I1226 07:42:25.518934 89771 MultiSolver.cpp:93] [0] PROFILING END[Forward]
I1226 07:42:25.519039 89771 MultiSolver.cpp:95] [0] PROFILING BEGIN[Backward]
I1226 07:42:25.614367 89771 MultiSolver.cpp:109] [0] PROFILING END[Backward]
I1226 07:42:25.614459 89771 solver.cpp:303] [0] Iteration 8, loss = 3.24509
I1226 07:42:25.614544 89771 solver.cpp:329]     Train net output #0: accuracy = 0.296875
I1226 07:42:25.614616 89771 solver.cpp:329]     Train net output #1: loss = 3.24509 (* 1 = 3.24509 loss)
I1226 07:42:25.614699 89771 MultiSolver.cpp:76] [0] PROFILING BEGIN[Forward]
I1226 07:42:26.173077 90196 MultiSolver.cpp:93] [4] PROFILING END[Forward]
I1226 07:42:26.173183 90196 MultiSolver.cpp:95] [4] PROFILING BEGIN[Backward]
I1226 07:42:26.264654 90196 MultiSolver.cpp:109] [4] PROFILING END[Backward]
I1226 07:42:26.264744 90196 solver.cpp:303] [4] Iteration 21, loss = 3.6785
I1226 07:42:26.264809 90196 solver.cpp:329]     Train net output #0: accuracy = 0.203125
I1226 07:42:26.264875 90196 solver.cpp:329]     Train net output #1: loss = 3.6785 (* 1 = 3.6785 loss)
I1226 07:42:26.264933 90196 MultiSolver.cpp:76] [4] PROFILING BEGIN[Forward]
I1226 07:42:26.615242 89800 MultiSolver.cpp:93] [6] PROFILING END[Forward]
I1226 07:42:26.615352 89800 MultiSolver.cpp:95] [6] PROFILING BEGIN[Backward]
I1226 07:42:23.346189 89666 MultiSolver.cpp:93] [5] PROFILING END[Forward]
I1226 07:42:23.346315 89666 MultiSolver.cpp:95] [5] PROFILING BEGIN[Backward]
I1226 07:42:26.617672 86513 MultiSolver.cpp:93] [7] PROFILING END[Forward]
I1226 07:42:26.617786 86513 MultiSolver.cpp:95] [7] PROFILING BEGIN[Backward]
I1226 07:42:26.684306 88308 MultiSolver.cpp:93] [2] PROFILING END[Forward]
I1226 07:42:26.684418 88308 MultiSolver.cpp:95] [2] PROFILING BEGIN[Backward]
I1226 07:42:23.435374 89666 MultiSolver.cpp:109] [5] PROFILING END[Backward]
I1226 07:42:23.435472 89666 solver.cpp:303] [5] Iteration 21, loss = 3.35711
I1226 07:42:23.435572 89666 solver.cpp:329]     Train net output #0: accuracy = 0.34375
I1226 07:42:23.435690 89666 solver.cpp:329]     Train net output #1: loss = 3.35711 (* 1 = 3.35711 loss)
I1226 07:42:23.435770 89666 MultiSolver.cpp:76] [5] PROFILING BEGIN[Forward]
I1226 07:42:26.699700 89665 MultiSolver.cpp:93] [3] PROFILING END[Forward]
I1226 07:42:26.699856 89665 MultiSolver.cpp:95] [3] PROFILING BEGIN[Backward]
I1226 07:42:26.743301 93793 MultiSolver.cpp:93] [1] PROFILING END[Forward]
I1226 07:42:26.743674 93793 MultiSolver.cpp:95] [1] PROFILING BEGIN[Backward]
I1226 07:42:26.762296 89800 MultiSolver.cpp:109] [6] PROFILING END[Backward]
I1226 07:42:26.762428 89800 solver.cpp:303] [6] Iteration 21, loss = 2.99005
I1226 07:42:26.762506 89800 solver.cpp:329]     Train net output #0: accuracy = 0.34375
I1226 07:42:26.762583 89800 solver.cpp:329]     Train net output #1: loss = 2.99005 (* 1 = 2.99005 loss)
I1226 07:42:26.762673 89800 MultiSolver.cpp:76] [6] PROFILING BEGIN[Forward]
I1226 07:42:26.765651 86513 MultiSolver.cpp:109] [7] PROFILING END[Backward]
I1226 07:42:26.765745 86513 solver.cpp:303] [7] Iteration 21, loss = 3.17983
I1226 07:42:26.765821 86513 solver.cpp:329]     Train net output #0: accuracy = 0.359375
I1226 07:42:26.765897 86513 solver.cpp:329]     Train net output #1: loss = 3.17983 (* 1 = 3.17983 loss)
I1226 07:42:26.765985 86513 MultiSolver.cpp:76] [7] PROFILING BEGIN[Forward]
I1226 07:42:26.826059 88308 MultiSolver.cpp:109] [2] PROFILING END[Backward]
I1226 07:42:26.826151 88308 solver.cpp:303] [2] Iteration 8, loss = 3.81036
I1226 07:42:26.826227 88308 solver.cpp:329]     Train net output #0: accuracy = 0.234375
I1226 07:42:26.826304 88308 solver.cpp:329]     Train net output #1: loss = 3.81036 (* 1 = 3.81036 loss)
I1226 07:42:26.826390 88308 MultiSolver.cpp:76] [2] PROFILING BEGIN[Forward]
I1226 07:42:26.834477 89665 MultiSolver.cpp:109] [3] PROFILING END[Backward]
I1226 07:42:26.834563 89665 solver.cpp:303] [3] Iteration 8, loss = 3.18559
I1226 07:42:26.834631 89665 solver.cpp:329]     Train net output #0: accuracy = 0.3125
I1226 07:42:26.834699 89665 solver.cpp:329]     Train net output #1: loss = 3.18559 (* 1 = 3.18559 loss)
I1226 07:42:26.834775 89665 MultiSolver.cpp:76] [3] PROFILING BEGIN[Forward]
I1226 07:42:27.640292 93793 MultiSolver.cpp:109] [1] PROFILING END[Backward]
I1226 07:42:27.640388 93793 solver.cpp:303] [1] Iteration 8, loss = 3.20147
I1226 07:42:27.640451 93793 solver.cpp:329]     Train net output #0: accuracy = 0.34375
I1226 07:42:27.640516 93793 solver.cpp:329]     Train net output #1: loss = 3.20147 (* 1 = 3.20147 loss)
I1226 07:42:27.640579 93793 MultiSolver.cpp:76] [1] PROFILING BEGIN[Forward]
I1226 07:42:28.219538 90196 MultiSolver.cpp:93] [4] PROFILING END[Forward]
I1226 07:42:28.219660 90196 MultiSolver.cpp:95] [4] PROFILING BEGIN[Backward]
I1226 07:42:28.313134 90196 MultiSolver.cpp:109] [4] PROFILING END[Backward]
I1226 07:42:28.313216 90196 solver.cpp:303] [4] Iteration 22, loss = 2.86307
I1226 07:42:28.313283 90196 solver.cpp:329]     Train net output #0: accuracy = 0.421875
I1226 07:42:28.313349 90196 solver.cpp:329]     Train net output #1: loss = 2.86307 (* 1 = 2.86307 loss)
I1226 07:42:28.313474 90196 MultiSolver.cpp:76] [4] PROFILING BEGIN[Forward]
I1226 07:42:28.632112 89800 MultiSolver.cpp:93] [6] PROFILING END[Forward]
I1226 07:42:28.632227 89800 MultiSolver.cpp:95] [6] PROFILING BEGIN[Backward]
I1226 07:42:25.363204 89666 MultiSolver.cpp:93] [5] PROFILING END[Forward]
I1226 07:42:25.363317 89666 MultiSolver.cpp:95] [5] PROFILING BEGIN[Backward]
I1226 07:42:28.635054 86513 MultiSolver.cpp:93] [7] PROFILING END[Forward]
I1226 07:42:28.635180 86513 MultiSolver.cpp:95] [7] PROFILING BEGIN[Backward]
I1226 07:42:25.453021 89666 MultiSolver.cpp:109] [5] PROFILING END[Backward]
I1226 07:42:25.453120 89666 solver.cpp:303] [5] Iteration 22, loss = 3.01847
I1226 07:42:25.453199 89666 solver.cpp:329]     Train net output #0: accuracy = 0.296875
I1226 07:42:25.453276 89666 solver.cpp:329]     Train net output #1: loss = 3.01847 (* 1 = 3.01847 loss)
I1226 07:42:25.453356 89666 MultiSolver.cpp:76] [5] PROFILING BEGIN[Forward]
I1226 07:42:28.775436 89800 MultiSolver.cpp:109] [6] PROFILING END[Backward]
I1226 07:42:28.775533 89800 solver.cpp:303] [6] Iteration 22, loss = 3.7279
I1226 07:42:28.775610 89800 solver.cpp:329]     Train net output #0: accuracy = 0.234375
I1226 07:42:28.775686 89800 solver.cpp:329]     Train net output #1: loss = 3.7279 (* 1 = 3.7279 loss)
I1226 07:42:28.775776 89800 MultiSolver.cpp:76] [6] PROFILING BEGIN[Forward]
I1226 07:42:28.780325 86513 MultiSolver.cpp:109] [7] PROFILING END[Backward]
I1226 07:42:28.780419 86513 solver.cpp:303] [7] Iteration 22, loss = 3.23665
I1226 07:42:28.780498 86513 solver.cpp:329]     Train net output #0: accuracy = 0.28125
I1226 07:42:28.780575 86513 solver.cpp:329]     Train net output #1: loss = 3.23665 (* 1 = 3.23665 loss)
I1226 07:42:28.780691 86513 MultiSolver.cpp:76] [7] PROFILING BEGIN[Forward]
I1226 07:42:30.236687 90196 MultiSolver.cpp:93] [4] PROFILING END[Forward]
I1226 07:42:30.236798 90196 MultiSolver.cpp:95] [4] PROFILING BEGIN[Backward]
I1226 07:42:30.328059 90196 MultiSolver.cpp:109] [4] PROFILING END[Backward]
I1226 07:42:30.328150 90196 solver.cpp:303] [4] Iteration 23, loss = 3.41463
I1226 07:42:30.328215 90196 solver.cpp:329]     Train net output #0: accuracy = 0.28125
I1226 07:42:30.328281 90196 solver.cpp:329]     Train net output #1: loss = 3.41463 (* 1 = 3.41463 loss)
I1226 07:42:30.328382 90196 MultiSolver.cpp:76] [4] PROFILING BEGIN[Forward]
I1226 07:42:27.388852 89666 MultiSolver.cpp:93] [5] PROFILING END[Forward]
I1226 07:42:30.658116 89800 MultiSolver.cpp:93] [6] PROFILING END[Forward]
I1226 07:42:27.388972 89666 MultiSolver.cpp:95] [5] PROFILING BEGIN[Backward]
I1226 07:42:30.658231 89800 MultiSolver.cpp:95] [6] PROFILING BEGIN[Backward]
I1226 07:42:30.658383 86513 MultiSolver.cpp:93] [7] PROFILING END[Forward]
I1226 07:42:30.658493 86513 MultiSolver.cpp:95] [7] PROFILING BEGIN[Backward]
I1226 07:42:27.481726 89666 MultiSolver.cpp:109] [5] PROFILING END[Backward]
I1226 07:42:27.481818 89666 solver.cpp:303] [5] Iteration 23, loss = 3.62058
I1226 07:42:27.481896 89666 solver.cpp:329]     Train net output #0: accuracy = 0.3125
I1226 07:42:27.481974 89666 solver.cpp:329]     Train net output #1: loss = 3.62058 (* 1 = 3.62058 loss)
I1226 07:42:27.482239 89666 MultiSolver.cpp:76] [5] PROFILING BEGIN[Forward]
I1226 07:42:30.802686 86513 MultiSolver.cpp:109] [7] PROFILING END[Backward]
I1226 07:42:30.802777 86513 solver.cpp:303] [7] Iteration 23, loss = 3.22311
I1226 07:42:30.802853 86513 solver.cpp:329]     Train net output #0: accuracy = 0.375
I1226 07:42:30.802964 86513 solver.cpp:329]     Train net output #1: loss = 3.22311 (* 1 = 3.22311 loss)
I1226 07:42:30.803280 86513 MultiSolver.cpp:76] [7] PROFILING BEGIN[Forward]
I1226 07:42:30.807344 89800 MultiSolver.cpp:109] [6] PROFILING END[Backward]
I1226 07:42:30.807488 89800 solver.cpp:303] [6] Iteration 23, loss = 2.84823
I1226 07:42:30.807565 89800 solver.cpp:329]     Train net output #0: accuracy = 0.390625
I1226 07:42:30.807644 89800 solver.cpp:329]     Train net output #1: loss = 2.84823 (* 1 = 2.84823 loss)
I1226 07:42:30.807740 89800 MultiSolver.cpp:76] [6] PROFILING BEGIN[Forward]
I1226 07:42:31.063938 89771 MultiSolver.cpp:93] [0] PROFILING END[Forward]
I1226 07:42:31.064054 89771 MultiSolver.cpp:95] [0] PROFILING BEGIN[Backward]
I1226 07:42:31.155073 89771 MultiSolver.cpp:109] [0] PROFILING END[Backward]
I1226 07:42:31.155160 89771 solver.cpp:303] [0] Iteration 9, loss = 4.08011
I1226 07:42:31.155222 89771 solver.cpp:329]     Train net output #0: accuracy = 0.1875
I1226 07:42:31.155298 89771 solver.cpp:329]     Train net output #1: loss = 4.08011 (* 1 = 4.08011 loss)
I1226 07:42:31.155372 89771 MultiSolver.cpp:76] [0] PROFILING BEGIN[Forward]
I1226 07:42:32.289294 90196 MultiSolver.cpp:93] [4] PROFILING END[Forward]
I1226 07:42:32.289414 90196 MultiSolver.cpp:95] [4] PROFILING BEGIN[Backward]
I1226 07:42:32.384124 90196 MultiSolver.cpp:109] [4] PROFILING END[Backward]
I1226 07:42:32.384237 90196 solver.cpp:303] [4] Iteration 24, loss = 3.14795
I1226 07:42:32.384302 90196 solver.cpp:329]     Train net output #0: accuracy = 0.359375
I1226 07:42:32.384371 90196 solver.cpp:329]     Train net output #1: loss = 3.14795 (* 1 = 3.14795 loss)
I1226 07:42:32.384521 90196 MultiSolver.cpp:76] [4] PROFILING BEGIN[Forward]
I1226 07:42:32.607825 88308 MultiSolver.cpp:93] [2] PROFILING END[Forward]
I1226 07:42:32.607930 88308 MultiSolver.cpp:95] [2] PROFILING BEGIN[Backward]
I1226 07:42:32.614935 89665 MultiSolver.cpp:93] [3] PROFILING END[Forward]
I1226 07:42:32.615063 89665 MultiSolver.cpp:95] [3] PROFILING BEGIN[Backward]
I1226 07:42:32.668501 93793 MultiSolver.cpp:93] [1] PROFILING END[Forward]
I1226 07:42:32.668619 93793 MultiSolver.cpp:95] [1] PROFILING BEGIN[Backward]
I1226 07:42:32.724027 89800 MultiSolver.cpp:93] [6] PROFILING END[Forward]
I1226 07:42:32.724155 89800 MultiSolver.cpp:95] [6] PROFILING BEGIN[Backward]
I1226 07:42:29.455727 89666 MultiSolver.cpp:93] [5] PROFILING END[Forward]
I1226 07:42:29.455843 89666 MultiSolver.cpp:95] [5] PROFILING BEGIN[Backward]
I1226 07:42:32.726147 86513 MultiSolver.cpp:93] [7] PROFILING END[Forward]
I1226 07:42:32.726263 86513 MultiSolver.cpp:95] [7] PROFILING BEGIN[Backward]
I1226 07:42:32.747752 88308 MultiSolver.cpp:109] [2] PROFILING END[Backward]
I1226 07:42:32.747843 88308 solver.cpp:303] [2] Iteration 9, loss = 3.20964
I1226 07:42:32.747918 88308 solver.cpp:329]     Train net output #0: accuracy = 0.375
I1226 07:42:32.747994 88308 solver.cpp:329]     Train net output #1: loss = 3.20964 (* 1 = 3.20964 loss)
I1226 07:42:32.748067 88308 MultiSolver.cpp:76] [2] PROFILING BEGIN[Forward]
I1226 07:42:32.769596 89665 MultiSolver.cpp:109] [3] PROFILING END[Backward]
I1226 07:42:32.769686 89665 solver.cpp:303] [3] Iteration 9, loss = 3.4821
I1226 07:42:32.769765 89665 solver.cpp:329]     Train net output #0: accuracy = 0.328125
I1226 07:42:32.769886 89665 solver.cpp:329]     Train net output #1: loss = 3.4821 (* 1 = 3.4821 loss)
I1226 07:42:32.769992 89665 MultiSolver.cpp:76] [3] PROFILING BEGIN[Forward]
I1226 07:42:29.544301 89666 MultiSolver.cpp:109] [5] PROFILING END[Backward]
I1226 07:42:29.544399 89666 solver.cpp:303] [5] Iteration 24, loss = 3.15125
I1226 07:42:29.544476 89666 solver.cpp:329]     Train net output #0: accuracy = 0.3125
I1226 07:42:29.544551 89666 solver.cpp:329]     Train net output #1: loss = 3.15125 (* 1 = 3.15125 loss)
I1226 07:42:29.544663 89666 MultiSolver.cpp:76] [5] PROFILING BEGIN[Forward]
I1226 07:42:32.867900 89800 MultiSolver.cpp:109] [6] PROFILING END[Backward]
I1226 07:42:32.867996 89800 solver.cpp:303] [6] Iteration 24, loss = 3.22707
I1226 07:42:32.868074 89800 solver.cpp:329]     Train net output #0: accuracy = 0.328125
I1226 07:42:32.868150 89800 solver.cpp:329]     Train net output #1: loss = 3.22707 (* 1 = 3.22707 loss)
I1226 07:42:32.868242 89800 MultiSolver.cpp:76] [6] PROFILING BEGIN[Forward]
I1226 07:42:32.874095 86513 MultiSolver.cpp:109] [7] PROFILING END[Backward]
I1226 07:42:32.874191 86513 solver.cpp:303] [7] Iteration 24, loss = 3.13771
I1226 07:42:32.874269 86513 solver.cpp:329]     Train net output #0: accuracy = 0.359375
I1226 07:42:32.874387 86513 solver.cpp:329]     Train net output #1: loss = 3.13771 (* 1 = 3.13771 loss)
I1226 07:42:32.874500 86513 MultiSolver.cpp:76] [7] PROFILING BEGIN[Forward]
I1226 07:42:33.584087 93793 MultiSolver.cpp:109] [1] PROFILING END[Backward]
I1226 07:42:33.584172 93793 solver.cpp:303] [1] Iteration 9, loss = 3.56023
I1226 07:42:33.584241 93793 solver.cpp:329]     Train net output #0: accuracy = 0.28125
I1226 07:42:33.584308 93793 solver.cpp:329]     Train net output #1: loss = 3.56023 (* 1 = 3.56023 loss)
I1226 07:42:33.584374 93793 MultiSolver.cpp:76] [1] PROFILING BEGIN[Forward]
I1226 07:42:34.338917 90196 MultiSolver.cpp:93] [4] PROFILING END[Forward]
I1226 07:42:34.339046 90196 MultiSolver.cpp:95] [4] PROFILING BEGIN[Backward]
I1226 07:42:34.431088 90196 MultiSolver.cpp:109] [4] PROFILING END[Backward]
I1226 07:42:34.431171 90196 solver.cpp:303] [4] Iteration 25, loss = 3.15121
I1226 07:42:34.431237 90196 solver.cpp:329]     Train net output #0: accuracy = 0.3125
I1226 07:42:34.431304 90196 solver.cpp:329]     Train net output #1: loss = 3.15121 (* 1 = 3.15121 loss)
I1226 07:42:34.431478 90196 MultiSolver.cpp:76] [4] PROFILING BEGIN[Forward]
I1226 07:42:31.494740 89666 MultiSolver.cpp:93] [5] PROFILING END[Forward]
I1226 07:42:34.764070 89800 MultiSolver.cpp:93] [6] PROFILING END[Forward]
I1226 07:42:31.494865 89666 MultiSolver.cpp:95] [5] PROFILING BEGIN[Backward]
I1226 07:42:34.764181 89800 MultiSolver.cpp:95] [6] PROFILING BEGIN[Backward]
I1226 07:42:34.765593 86513 MultiSolver.cpp:93] [7] PROFILING END[Forward]
I1226 07:42:34.765715 86513 MultiSolver.cpp:95] [7] PROFILING BEGIN[Backward]
I1226 07:42:31.588176 89666 MultiSolver.cpp:109] [5] PROFILING END[Backward]
I1226 07:42:31.588264 89666 solver.cpp:303] [5] Iteration 25, loss = 3.05193
I1226 07:42:31.588345 89666 solver.cpp:329]     Train net output #0: accuracy = 0.359375
I1226 07:42:31.588421 89666 solver.cpp:329]     Train net output #1: loss = 3.05193 (* 1 = 3.05193 loss)
I1226 07:42:31.588666 89666 MultiSolver.cpp:76] [5] PROFILING BEGIN[Forward]
I1226 07:42:34.905053 89800 MultiSolver.cpp:109] [6] PROFILING END[Backward]
I1226 07:42:34.905145 89800 solver.cpp:303] [6] Iteration 25, loss = 3.14
I1226 07:42:34.905249 89800 solver.cpp:329]     Train net output #0: accuracy = 0.328125
I1226 07:42:34.905526 89800 solver.cpp:329]     Train net output #1: loss = 3.14 (* 1 = 3.14 loss)
I1226 07:42:34.905614 89800 MultiSolver.cpp:76] [6] PROFILING BEGIN[Forward]
I1226 07:42:34.910673 86513 MultiSolver.cpp:109] [7] PROFILING END[Backward]
I1226 07:42:34.910763 86513 solver.cpp:303] [7] Iteration 25, loss = 3.77285
I1226 07:42:34.910840 86513 solver.cpp:329]     Train net output #0: accuracy = 0.25
I1226 07:42:34.910915 86513 solver.cpp:329]     Train net output #1: loss = 3.77285 (* 1 = 3.77285 loss)
I1226 07:42:34.911176 86513 MultiSolver.cpp:76] [7] PROFILING BEGIN[Forward]
I1226 07:42:36.369719 90196 MultiSolver.cpp:93] [4] PROFILING END[Forward]
I1226 07:42:36.369832 90196 MultiSolver.cpp:95] [4] PROFILING BEGIN[Backward]
I1226 07:42:36.463439 90196 MultiSolver.cpp:109] [4] PROFILING END[Backward]
I1226 07:42:36.463557 90196 solver.cpp:303] [4] Iteration 26, loss = 3.70201
I1226 07:42:36.463625 90196 solver.cpp:329]     Train net output #0: accuracy = 0.28125
I1226 07:42:36.463690 90196 solver.cpp:329]     Train net output #1: loss = 3.70201 (* 1 = 3.70201 loss)
I1226 07:42:36.463759 90196 MultiSolver.cpp:76] [4] PROFILING BEGIN[Forward]
I1226 07:42:33.522107 89666 MultiSolver.cpp:93] [5] PROFILING END[Forward]
I1226 07:42:33.522239 89666 MultiSolver.cpp:95] [5] PROFILING BEGIN[Backward]
I1226 07:42:36.792901 89800 MultiSolver.cpp:93] [6] PROFILING END[Forward]
I1226 07:42:36.793045 89800 MultiSolver.cpp:95] [6] PROFILING BEGIN[Backward]
I1226 07:42:36.796264 86513 MultiSolver.cpp:93] [7] PROFILING END[Forward]
I1226 07:42:36.796407 86513 MultiSolver.cpp:95] [7] PROFILING BEGIN[Backward]
I1226 07:42:33.611759 89666 MultiSolver.cpp:109] [5] PROFILING END[Backward]
I1226 07:42:33.611852 89666 solver.cpp:303] [5] Iteration 26, loss = 3.12654
I1226 07:42:33.611930 89666 solver.cpp:329]     Train net output #0: accuracy = 0.296875
I1226 07:42:33.612009 89666 solver.cpp:329]     Train net output #1: loss = 3.12654 (* 1 = 3.12654 loss)
I1226 07:42:33.612102 89666 MultiSolver.cpp:76] [5] PROFILING BEGIN[Forward]
I1226 07:42:36.936202 89800 MultiSolver.cpp:109] [6] PROFILING END[Backward]
I1226 07:42:36.936295 89800 solver.cpp:303] [6] Iteration 26, loss = 2.94406
I1226 07:42:36.936372 89800 solver.cpp:329]     Train net output #0: accuracy = 0.375
I1226 07:42:36.936480 89800 solver.cpp:329]     Train net output #1: loss = 2.94406 (* 1 = 2.94406 loss)
I1226 07:42:36.936555 89800 MultiSolver.cpp:76] [6] PROFILING BEGIN[Forward]
I1226 07:42:36.972419 86513 MultiSolver.cpp:109] [7] PROFILING END[Backward]
I1226 07:42:36.972513 86513 solver.cpp:303] [7] Iteration 26, loss = 3.37631
I1226 07:42:36.972589 86513 solver.cpp:329]     Train net output #0: accuracy = 0.296875
I1226 07:42:36.972666 86513 solver.cpp:329]     Train net output #1: loss = 3.37631 (* 1 = 3.37631 loss)
I1226 07:42:36.972791 86513 MultiSolver.cpp:76] [7] PROFILING BEGIN[Forward]
I1226 07:42:37.129582 89771 MultiSolver.cpp:93] [0] PROFILING END[Forward]
I1226 07:42:37.129698 89771 MultiSolver.cpp:95] [0] PROFILING BEGIN[Backward]
I1226 07:42:37.226446 89771 MultiSolver.cpp:109] [0] PROFILING END[Backward]
I1226 07:42:37.226570 89771 solver.cpp:303] [0] Iteration 10, loss = 3.03976
I1226 07:42:37.226631 89771 solver.cpp:329]     Train net output #0: accuracy = 0.4375
I1226 07:42:37.226711 89771 solver.cpp:329]     Train net output #1: loss = 3.03976 (* 1 = 3.03976 loss)
I1226 07:42:37.226807 89771 MultiSolver.cpp:76] [0] PROFILING BEGIN[Forward]
I1226 07:42:38.481889 88308 MultiSolver.cpp:93] [2] PROFILING END[Forward]
I1226 07:42:38.482007 88308 MultiSolver.cpp:95] [2] PROFILING BEGIN[Backward]
I1226 07:42:38.500355 89665 MultiSolver.cpp:93] [3] PROFILING END[Forward]
I1226 07:42:38.500506 89665 MultiSolver.cpp:95] [3] PROFILING BEGIN[Backward]
I1226 07:42:38.506994 93793 MultiSolver.cpp:93] [1] PROFILING END[Forward]
I1226 07:42:38.507114 93793 MultiSolver.cpp:95] [1] PROFILING BEGIN[Backward]
I1226 07:42:38.573020 90196 MultiSolver.cpp:93] [4] PROFILING END[Forward]
I1226 07:42:38.573138 90196 MultiSolver.cpp:95] [4] PROFILING BEGIN[Backward]
I1226 07:42:38.620367 88308 MultiSolver.cpp:109] [2] PROFILING END[Backward]
I1226 07:42:38.620455 88308 solver.cpp:303] [2] Iteration 10, loss = 3.15352
I1226 07:42:38.620532 88308 solver.cpp:329]     Train net output #0: accuracy = 0.390625
I1226 07:42:38.620641 88308 solver.cpp:329]     Train net output #1: loss = 3.15352 (* 1 = 3.15352 loss)
I1226 07:42:38.620913 88308 MultiSolver.cpp:76] [2] PROFILING BEGIN[Forward]
I1226 07:42:38.661764 89665 MultiSolver.cpp:109] [3] PROFILING END[Backward]
I1226 07:42:38.661885 89665 solver.cpp:303] [3] Iteration 10, loss = 3.0079
I1226 07:42:38.661960 89665 solver.cpp:329]     Train net output #0: accuracy = 0.34375
I1226 07:42:38.662039 89665 solver.cpp:329]     Train net output #1: loss = 3.0079 (* 1 = 3.0079 loss)
I1226 07:42:38.662132 89665 MultiSolver.cpp:76] [3] PROFILING BEGIN[Forward]
I1226 07:42:38.667212 90196 MultiSolver.cpp:109] [4] PROFILING END[Backward]
I1226 07:42:38.667299 90196 solver.cpp:303] [4] Iteration 27, loss = 2.87974
I1226 07:42:38.667362 90196 solver.cpp:329]     Train net output #0: accuracy = 0.390625
I1226 07:42:38.667434 90196 solver.cpp:329]     Train net output #1: loss = 2.87974 (* 1 = 2.87974 loss)
I1226 07:42:38.667568 90196 MultiSolver.cpp:76] [4] PROFILING BEGIN[Forward]
I1226 07:42:35.716249 89666 MultiSolver.cpp:93] [5] PROFILING END[Forward]
I1226 07:42:38.985607 89800 MultiSolver.cpp:93] [6] PROFILING END[Forward]
I1226 07:42:35.716367 89666 MultiSolver.cpp:95] [5] PROFILING BEGIN[Backward]
I1226 07:42:38.985718 89800 MultiSolver.cpp:95] [6] PROFILING BEGIN[Backward]
I1226 07:42:38.987845 86513 MultiSolver.cpp:93] [7] PROFILING END[Forward]
I1226 07:42:38.987959 86513 MultiSolver.cpp:95] [7] PROFILING BEGIN[Backward]
I1226 07:42:35.807950 89666 MultiSolver.cpp:109] [5] PROFILING END[Backward]
I1226 07:42:35.808037 89666 solver.cpp:303] [5] Iteration 27, loss = 3.5169
I1226 07:42:35.808115 89666 solver.cpp:329]     Train net output #0: accuracy = 0.34375
I1226 07:42:35.808195 89666 solver.cpp:329]     Train net output #1: loss = 3.5169 (* 1 = 3.5169 loss)
I1226 07:42:35.808275 89666 MultiSolver.cpp:76] [5] PROFILING BEGIN[Forward]
I1226 07:42:39.127993 89800 MultiSolver.cpp:109] [6] PROFILING END[Backward]
I1226 07:42:39.128083 89800 solver.cpp:303] [6] Iteration 27, loss = 3.09329
I1226 07:42:39.128186 89800 solver.cpp:329]     Train net output #0: accuracy = 0.359375
I1226 07:42:39.128262 89800 solver.cpp:329]     Train net output #1: loss = 3.09329 (* 1 = 3.09329 loss)
I1226 07:42:39.128556 89800 MultiSolver.cpp:76] [6] PROFILING BEGIN[Forward]
I1226 07:42:39.129926 86513 MultiSolver.cpp:109] [7] PROFILING END[Backward]
I1226 07:42:39.130020 86513 solver.cpp:303] [7] Iteration 27, loss = 2.82509
I1226 07:42:39.130096 86513 solver.cpp:329]     Train net output #0: accuracy = 0.421875
I1226 07:42:39.130174 86513 solver.cpp:329]     Train net output #1: loss = 2.82509 (* 1 = 2.82509 loss)
I1226 07:42:39.130331 86513 MultiSolver.cpp:76] [7] PROFILING BEGIN[Forward]
I1226 07:42:39.403022 93793 MultiSolver.cpp:109] [1] PROFILING END[Backward]
I1226 07:42:39.403105 93793 solver.cpp:303] [1] Iteration 10, loss = 3.82152
I1226 07:42:39.403172 93793 solver.cpp:329]     Train net output #0: accuracy = 0.25
I1226 07:42:39.403239 93793 solver.cpp:329]     Train net output #1: loss = 3.82152 (* 1 = 3.82152 loss)
I1226 07:42:39.403304 93793 MultiSolver.cpp:76] [1] PROFILING BEGIN[Forward]
I1226 07:42:40.586295 90196 MultiSolver.cpp:93] [4] PROFILING END[Forward]
I1226 07:42:40.586406 90196 MultiSolver.cpp:95] [4] PROFILING BEGIN[Backward]
I1226 07:42:40.680289 90196 MultiSolver.cpp:109] [4] PROFILING END[Backward]
I1226 07:42:40.680371 90196 solver.cpp:303] [4] Iteration 28, loss = 3.1899
I1226 07:42:40.680440 90196 solver.cpp:329]     Train net output #0: accuracy = 0.375
I1226 07:42:40.680541 90196 solver.cpp:329]     Train net output #1: loss = 3.1899 (* 1 = 3.1899 loss)
I1226 07:42:40.680811 90196 MultiSolver.cpp:76] [4] PROFILING BEGIN[Forward]
I1226 07:42:37.742241 89666 MultiSolver.cpp:93] [5] PROFILING END[Forward]
I1226 07:42:37.742357 89666 MultiSolver.cpp:95] [5] PROFILING BEGIN[Backward]
I1226 07:42:41.012123 89800 MultiSolver.cpp:93] [6] PROFILING END[Forward]
I1226 07:42:41.012293 89800 MultiSolver.cpp:95] [6] PROFILING BEGIN[Backward]
I1226 07:42:41.013273 86513 MultiSolver.cpp:93] [7] PROFILING END[Forward]
I1226 07:42:41.013420 86513 MultiSolver.cpp:95] [7] PROFILING BEGIN[Backward]
I1226 07:42:37.833854 89666 MultiSolver.cpp:109] [5] PROFILING END[Backward]
I1226 07:42:37.833945 89666 solver.cpp:303] [5] Iteration 28, loss = 3.25172
I1226 07:42:37.834022 89666 solver.cpp:329]     Train net output #0: accuracy = 0.34375
I1226 07:42:37.834100 89666 solver.cpp:329]     Train net output #1: loss = 3.25172 (* 1 = 3.25172 loss)
I1226 07:42:37.834177 89666 MultiSolver.cpp:76] [5] PROFILING BEGIN[Forward]
I1226 07:42:41.160195 89800 MultiSolver.cpp:109] [6] PROFILING END[Backward]
I1226 07:42:41.160289 89800 solver.cpp:303] [6] Iteration 28, loss = 3.18631
I1226 07:42:41.160365 89800 solver.cpp:329]     Train net output #0: accuracy = 0.3125
I1226 07:42:41.160478 89800 solver.cpp:329]     Train net output #1: loss = 3.18631 (* 1 = 3.18631 loss)
I1226 07:42:41.160590 89800 MultiSolver.cpp:76] [6] PROFILING BEGIN[Forward]
I1226 07:42:41.163759 86513 MultiSolver.cpp:109] [7] PROFILING END[Backward]
I1226 07:42:41.163852 86513 solver.cpp:303] [7] Iteration 28, loss = 3.54864
I1226 07:42:41.163928 86513 solver.cpp:329]     Train net output #0: accuracy = 0.265625
I1226 07:42:41.164005 86513 solver.cpp:329]     Train net output #1: loss = 3.54864 (* 1 = 3.54864 loss)
I1226 07:42:41.164078 86513 MultiSolver.cpp:76] [7] PROFILING BEGIN[Forward]
I1226 07:42:42.608675 90196 MultiSolver.cpp:93] [4] PROFILING END[Forward]
I1226 07:42:42.608789 90196 MultiSolver.cpp:95] [4] PROFILING BEGIN[Backward]
I1226 07:42:42.700243 90196 MultiSolver.cpp:109] [4] PROFILING END[Backward]
I1226 07:42:42.700407 90196 solver.cpp:303] [4] Iteration 29, loss = 3.64293
I1226 07:42:42.700503 90196 solver.cpp:329]     Train net output #0: accuracy = 0.3125
I1226 07:42:42.700574 90196 solver.cpp:329]     Train net output #1: loss = 3.64293 (* 1 = 3.64293 loss)
I1226 07:42:42.700666 90196 MultiSolver.cpp:76] [4] PROFILING BEGIN[Forward]
I1226 07:42:39.755785 89666 MultiSolver.cpp:93] [5] PROFILING END[Forward]
I1226 07:42:43.025089 89800 MultiSolver.cpp:93] [6] PROFILING END[Forward]
I1226 07:42:39.755913 89666 MultiSolver.cpp:95] [5] PROFILING BEGIN[Backward]
I1226 07:42:43.025212 89800 MultiSolver.cpp:95] [6] PROFILING BEGIN[Backward]
I1226 07:42:43.027266 86513 MultiSolver.cpp:93] [7] PROFILING END[Forward]
I1226 07:42:43.027415 86513 MultiSolver.cpp:95] [7] PROFILING BEGIN[Backward]
I1226 07:42:39.848203 89666 MultiSolver.cpp:109] [5] PROFILING END[Backward]
I1226 07:42:39.848382 89666 solver.cpp:303] [5] Iteration 29, loss = 3.22512
I1226 07:42:39.848461 89666 solver.cpp:329]     Train net output #0: accuracy = 0.328125
I1226 07:42:39.848538 89666 solver.cpp:329]     Train net output #1: loss = 3.22512 (* 1 = 3.22512 loss)
I1226 07:42:39.848654 89666 MultiSolver.cpp:76] [5] PROFILING BEGIN[Forward]
I1226 07:42:43.171489 89800 MultiSolver.cpp:109] [6] PROFILING END[Backward]
I1226 07:42:43.171676 89800 solver.cpp:303] [6] Iteration 29, loss = 2.81267
I1226 07:42:43.171753 89800 solver.cpp:329]     Train net output #0: accuracy = 0.328125
I1226 07:42:43.171829 89800 solver.cpp:329]     Train net output #1: loss = 2.81267 (* 1 = 2.81267 loss)
I1226 07:42:43.171941 89800 MultiSolver.cpp:76] [6] PROFILING BEGIN[Forward]
I1226 07:42:43.175282 86513 MultiSolver.cpp:109] [7] PROFILING END[Backward]
I1226 07:42:43.175529 86513 solver.cpp:303] [7] Iteration 29, loss = 2.81587
I1226 07:42:43.175607 86513 solver.cpp:329]     Train net output #0: accuracy = 0.390625
I1226 07:42:43.175684 86513 solver.cpp:329]     Train net output #1: loss = 2.81587 (* 1 = 2.81587 loss)
I1226 07:42:43.175765 86513 MultiSolver.cpp:76] [7] PROFILING BEGIN[Forward]
I1226 07:42:43.368999 89771 MultiSolver.cpp:93] [0] PROFILING END[Forward]
I1226 07:42:43.369110 89771 MultiSolver.cpp:95] [0] PROFILING BEGIN[Backward]
I1226 07:42:43.528380 89771 MultiSolver.cpp:109] [0] PROFILING END[Backward]
I1226 07:42:43.528461 89771 solver.cpp:303] [0] Iteration 11, loss = 3.78444
I1226 07:42:43.528553 89771 solver.cpp:329]     Train net output #0: accuracy = 0.25
I1226 07:42:43.528630 89771 solver.cpp:329]     Train net output #1: loss = 3.78444 (* 1 = 3.78444 loss)
I1226 07:42:43.528726 89771 MultiSolver.cpp:76] [0] PROFILING BEGIN[Forward]
User defined signal 2
